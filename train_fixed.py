#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆè®­ç»ƒè„šæœ¬
è§£å†³äº†æ•°å€¼ç¨³å®šæ€§å’Œæ”¶æ•›é—®é¢˜çš„è®­ç»ƒè„šæœ¬
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from datetime import datetime
import json
import os
from pathlib import Path

class FixedAgent(nn.Module):
    """ä¿®å¤ç‰ˆæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(FixedAgent, self).__init__()
        
        # æ”¹è¿›çš„ç½‘ç»œæ¶æ„
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),  # æ·»åŠ å±‚å½’ä¸€åŒ–
            nn.ReLU(),
            nn.Dropout(0.1),  # æ·»åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, action_dim),
            nn.Tanh()
        )
        
        self.critic = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        """å‰å‘ä¼ æ’­"""
        return self.actor(state)
    
    def get_q_value(self, state, action):
        """è·å–Qå€¼"""
        return self.critic(torch.cat([state, action], dim=1))

class FixedTrainer:
    """ä¿®å¤ç‰ˆè®­ç»ƒå™¨"""
    
    def __init__(self, state_dim=20, action_dim=5, lr=0.0003):
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # åˆ›å»ºç½‘ç»œ
        self.actor = FixedAgent(state_dim, action_dim)
        self.critic = FixedAgent(state_dim, action_dim)
        self.target_actor = FixedAgent(state_dim, action_dim)
        self.target_critic = FixedAgent(state_dim, action_dim)
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic.load_state_dict(self.critic.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.actor_scheduler = optim.lr_scheduler.StepLR(self.actor_optimizer, step_size=1000, gamma=0.95)
        self.critic_scheduler = optim.lr_scheduler.StepLR(self.critic_optimizer, step_size=1000, gamma=0.95)
        
        # è®­ç»ƒå‚æ•°
        self.batch_size = 128
        self.memory_size = 50000
        self.memory = []
        self.tau = 0.005  # è½¯æ›´æ–°å‚æ•°
        self.gamma = 0.99  # æŠ˜æ‰£å› å­
        self.noise_std = 0.1  # å™ªå£°æ ‡å‡†å·®
        
        # æ¢¯åº¦è£å‰ª
        self.max_grad_norm = 1.0
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'episodes': 0,
            'actor_losses': [],
            'critic_losses': [],
            'rewards': [],
            'q_values': []
        }
    
    def add_noise(self, action):
        """æ·»åŠ æ¢ç´¢å™ªå£°"""
        noise = torch.normal(0, self.noise_std, size=action.shape)
        return torch.clamp(action + noise, -1, 1)
    
    def store_experience(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        experience = (
            state.cpu().numpy(),
            action.cpu().numpy(),
            reward,
            next_state.cpu().numpy(),
            done
        )
        
        if len(self.memory) >= self.memory_size:
            self.memory.pop(0)
        
        self.memory.append(experience)
    
    def sample_batch(self):
        """é‡‡æ ·æ‰¹æ¬¡"""
        if len(self.memory) < self.batch_size:
            return None
        
        indices = np.random.choice(len(self.memory), self.batch_size, replace=False)
        batch = [self.memory[i] for i in indices]
        
        states = torch.FloatTensor([exp[0] for exp in batch])
        actions = torch.FloatTensor([exp[1] for exp in batch])
        rewards = torch.FloatTensor([exp[2] for exp in batch]).unsqueeze(1)
        next_states = torch.FloatTensor([exp[3] for exp in batch])
        dones = torch.BoolTensor([exp[4] for exp in batch]).unsqueeze(1)
        
        return states, actions, rewards, next_states, dones
    
    def update_target_networks(self):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
        
        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
    
    def train_step(self):
        """è®­ç»ƒæ­¥éª¤"""
        batch = self.sample_batch()
        if batch is None:
            return 0, 0
        
        states, actions, rewards, next_states, dones = batch
        
        # è®­ç»ƒCritic
        with torch.no_grad():
            next_actions = self.target_actor(next_states)
            next_actions = self.add_noise(next_actions)  # ç›®æ ‡ç­–ç•¥å¹³æ»‘
            target_q = self.target_critic.get_q_value(next_states, next_actions)
            target_q = rewards + self.gamma * target_q * (~dones)
        
        current_q = self.critic.get_q_value(states, actions)
        critic_loss = nn.MSELoss()(current_q, target_q)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
        self.critic_optimizer.step()
        
        # è®­ç»ƒActor (å»¶è¿Ÿæ›´æ–°)
        actor_loss = 0
        if len(self.training_stats['critic_losses']) % 2 == 0:  # æ¯2æ­¥æ›´æ–°ä¸€æ¬¡Actor
            predicted_actions = self.actor(states)
            actor_loss = -self.critic.get_q_value(states, predicted_actions).mean()
            
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
            self.actor_optimizer.step()
            
            # æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self.update_target_networks()
            
            actor_loss = actor_loss.item()
        
        return actor_loss, critic_loss.item()
    
    def generate_environment_data(self):
        """ç”Ÿæˆç¯å¢ƒæ•°æ®"""
        # æ¨¡æ‹Ÿè½¦è”ç½‘ç¯å¢ƒçŠ¶æ€
        vehicle_states = np.random.uniform(0, 1, 5)  # è½¦è¾†çŠ¶æ€
        rsu_states = np.random.uniform(0, 1, 5)      # RSUçŠ¶æ€
        uav_states = np.random.uniform(0, 1, 5)      # UAVçŠ¶æ€
        network_states = np.random.uniform(0, 1, 5)  # ç½‘ç»œçŠ¶æ€
        
        state = np.concatenate([vehicle_states, rsu_states, uav_states, network_states])
        return torch.FloatTensor(state).unsqueeze(0)
    
    def calculate_reward(self, action, state):
        """è®¡ç®—å¥–åŠ±"""
        # åŸºäºåŠ¨ä½œå’ŒçŠ¶æ€è®¡ç®—å¥–åŠ±
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„å¥–åŠ±å‡½æ•°
        
        # æ—¶å»¶å¥–åŠ± (åŠ¨ä½œè¶Šå°æ—¶å»¶è¶Šä½)
        delay_reward = -torch.sum(torch.abs(action)) * 0.1
        
        # èƒ½è€—å¥–åŠ± (åŠ¨ä½œå¹³è¡¡æ€§)
        energy_reward = -torch.var(action) * 0.05
        
        # ç¼“å­˜å‘½ä¸­å¥–åŠ± (åŸºäºåŠ¨ä½œçš„ç¬¬ä¸€ä¸ªç»´åº¦)
        cache_reward = torch.sigmoid(action[0, 0]) * 2.0
        
        total_reward = delay_reward + energy_reward + cache_reward
        return total_reward.item()
    
    def train_episode(self):
        """è®­ç»ƒä¸€ä¸ªå›åˆ"""
        state = self.generate_environment_data()
        episode_reward = 0
        episode_q_values = []
        
        for step in range(200):  # æ¯å›åˆ200æ­¥
            # é€‰æ‹©åŠ¨ä½œ
            with torch.no_grad():
                action = self.actor(state)
                if len(self.memory) < self.batch_size * 10:  # åˆæœŸå¢åŠ æ¢ç´¢
                    action = self.add_noise(action)
            
            # è®¡ç®—å¥–åŠ±
            reward = self.calculate_reward(action, state)
            
            # ç”Ÿæˆä¸‹ä¸€çŠ¶æ€
            next_state = self.generate_environment_data()
            done = step == 199
            
            # å­˜å‚¨ç»éªŒ
            self.store_experience(state, action, reward, next_state, done)
            
            # è®­ç»ƒ
            if len(self.memory) >= self.batch_size:
                actor_loss, critic_loss = self.train_step()
                
                if actor_loss > 0:  # åªæœ‰å½“Actoræ›´æ–°æ—¶æ‰è®°å½•
                    self.training_stats['actor_losses'].append(actor_loss)
                self.training_stats['critic_losses'].append(critic_loss)
                
                # è®°å½•Qå€¼
                with torch.no_grad():
                    q_value = self.critic.get_q_value(state, action).item()
                    episode_q_values.append(q_value)
            
            episode_reward += reward
            state = next_state
        
        # æ›´æ–°å­¦ä¹ ç‡
        self.actor_scheduler.step()
        self.critic_scheduler.step()
        
        # æ›´æ–°ç»Ÿè®¡
        self.training_stats['episodes'] += 1
        self.training_stats['rewards'].append(episode_reward)
        if episode_q_values:
            self.training_stats['q_values'].append(np.mean(episode_q_values))
        
        return episode_reward
    
    def train(self, num_episodes=2000):
        """è®­ç»ƒä¸»å¾ªç¯"""
        print("ğŸš€ å¼€å§‹ä¿®å¤ç‰ˆè®­ç»ƒ...")
        print(f"è®­ç»ƒå‚æ•°: episodes={num_episodes}, batch_size={self.batch_size}")
        
        for episode in range(num_episodes):
            episode_reward = self.train_episode()
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(self.training_stats['rewards'][-100:])
                avg_actor_loss = np.mean(self.training_stats['actor_losses'][-50:]) if self.training_stats['actor_losses'] else 0
                avg_critic_loss = np.mean(self.training_stats['critic_losses'][-100:]) if self.training_stats['critic_losses'] else 0
                avg_q_value = np.mean(self.training_stats['q_values'][-100:]) if self.training_stats['q_values'] else 0
                
                print(f"Episode {episode+1}/{num_episodes}")
                print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.3f}")
                print(f"  ActoræŸå¤±: {avg_actor_loss:.6f}")
                print(f"  CriticæŸå¤±: {avg_critic_loss:.6f}")
                print(f"  å¹³å‡Qå€¼: {avg_q_value:.3f}")
                print(f"  å­¦ä¹ ç‡: {self.actor_optimizer.param_groups[0]['lr']:.6f}")
        
        print("âœ… ä¿®å¤ç‰ˆè®­ç»ƒå®Œæˆï¼")
        return self.training_stats
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'target_actor_state_dict': self.target_actor.state_dict(),
            'target_critic_state_dict': self.target_critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'training_stats': self.training_stats
        }, filepath)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {filepath}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ä¿®å¤ç‰ˆè®­ç»ƒç³»ç»Ÿ")
    print("=" * 50)
    
    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    Path("results/fixed_training").mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = FixedTrainer(state_dim=20, action_dim=5, lr=0.0003)
    
    # å¼€å§‹è®­ç»ƒ
    results = trainer.train(num_episodes=1000)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜æ¨¡å‹
    model_path = f"results/fixed_training/fixed_model_{timestamp}.pth"
    trainer.save_model(model_path)
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    results_path = f"results/fixed_training/fixed_results_{timestamp}.json"
    with open(results_path, 'w') as f:
        json_results = {
            'episodes': results['episodes'],
            'actor_losses': results['actor_losses'],
            'critic_losses': results['critic_losses'],
            'rewards': results['rewards'],
            'q_values': results['q_values'],
            'timestamp': timestamp,
            'final_performance': {
                'avg_reward': np.mean(results['rewards'][-100:]) if results['rewards'] else 0,
                'final_actor_loss': results['actor_losses'][-1] if results['actor_losses'] else 0,
                'final_critic_loss': results['critic_losses'][-1] if results['critic_losses'] else 0,
                'convergence_episode': len(results['rewards'])
            }
        }
        json.dump(json_results, f, indent=2)
    
    print(f"ğŸ“Š è®­ç»ƒç»“æœå·²ä¿å­˜: {results_path}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    print("\nğŸ“ˆ æœ€ç»ˆè®­ç»ƒç»Ÿè®¡:")
    print(f"  æ€»å›åˆæ•°: {results['episodes']}")
    if results['rewards']:
        print(f"  æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(results['rewards'][-100:]):.3f}")
    if results['actor_losses']:
        print(f"  æœ€ç»ˆActoræŸå¤±: {results['actor_losses'][-1]:.6f}")
    if results['critic_losses']:
        print(f"  æœ€ç»ˆCriticæŸå¤±: {results['critic_losses'][-1]:.6f}")

if __name__ == "__main__":
    main()