#!/usr/bin/env python3
"""
ç¼“å­˜æ„ŸçŸ¥è®­ç»ƒè„šæœ¬
ä¸“é—¨é’ˆå¯¹ç¼“å­˜ä¼˜åŒ–çš„è®­ç»ƒæ–¹æ³•
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from datetime import datetime
import json
import os

class CacheAwareAgent(nn.Module):
    """ç¼“å­˜æ„ŸçŸ¥æ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(CacheAwareAgent, self).__init__()
        
        # ç¼“å­˜çŠ¶æ€ç¼–ç å™¨
        self.cache_encoder = nn.Sequential(
            nn.Linear(state_dim // 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2)
        )
        
        # ç½‘ç»œçŠ¶æ€ç¼–ç å™¨
        self.network_encoder = nn.Sequential(
            nn.Linear(state_dim // 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2)
        )
        
        # èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()
        )
    
    def forward(self, state):
        """å‰å‘ä¼ æ’­"""
        # åˆ†ç¦»ç¼“å­˜çŠ¶æ€å’Œç½‘ç»œçŠ¶æ€
        cache_state = state[:, :state.size(1)//2]
        network_state = state[:, state.size(1)//2:]
        
        # ç¼–ç 
        cache_features = self.cache_encoder(cache_state)
        network_features = self.network_encoder(network_state)
        
        # èåˆ
        combined_features = torch.cat([cache_features, network_features], dim=1)
        action = self.fusion_layer(combined_features)
        
        return action

class CacheAwareTrainer:
    """ç¼“å­˜æ„ŸçŸ¥è®­ç»ƒå™¨"""
    
    def __init__(self, state_dim=20, action_dim=5, lr=0.001):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.lr = lr
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agent = CacheAwareAgent(state_dim, action_dim)
        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)
        
        # è®­ç»ƒå‚æ•°
        self.batch_size = 64
        self.memory_size = 10000
        self.memory = []
        
        # ç¼“å­˜ç›¸å…³å‚æ•°
        self.cache_hit_reward = 10.0
        self.cache_miss_penalty = -2.0
        self.cache_update_cost = -0.5
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'episodes': 0,
            'total_reward': 0,
            'cache_hit_rate': 0,
            'avg_delay': 0
        }
    
    def simulate_cache_environment(self, action):
        """æ¨¡æ‹Ÿç¼“å­˜ç¯å¢ƒ"""
        # æ¨¡æ‹Ÿç¼“å­˜å‘½ä¸­/æœªå‘½ä¸­
        cache_hit_prob = torch.sigmoid(action[0]).item()
        cache_hit = np.random.random() < cache_hit_prob
        
        # è®¡ç®—å¥–åŠ±
        if cache_hit:
            reward = self.cache_hit_reward
            delay = np.random.exponential(0.01)  # ç¼“å­˜å‘½ä¸­ï¼Œä½å»¶è¿Ÿ
        else:
            reward = self.cache_miss_penalty
            delay = np.random.exponential(0.1)   # ç¼“å­˜æœªå‘½ä¸­ï¼Œé«˜å»¶è¿Ÿ
        
        # ç¼“å­˜æ›´æ–°æˆæœ¬
        if action[1] > 0.5:  # å†³å®šæ›´æ–°ç¼“å­˜
            reward += self.cache_update_cost
        
        return reward, delay, cache_hit
    
    def generate_state(self):
        """ç”ŸæˆçŠ¶æ€"""
        # ç¼“å­˜çŠ¶æ€ (å‰ä¸€åŠ)
        cache_state = np.random.random(self.state_dim // 2)
        
        # ç½‘ç»œçŠ¶æ€ (åä¸€åŠ)
        network_state = np.random.random(self.state_dim // 2)
        
        state = np.concatenate([cache_state, network_state])
        return torch.FloatTensor(state).unsqueeze(0)
    
    def store_experience(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        experience = (state, action, reward, next_state, done)
        
        if len(self.memory) >= self.memory_size:
            self.memory.pop(0)
        
        self.memory.append(experience)
    
    def sample_batch(self):
        """é‡‡æ ·æ‰¹æ¬¡"""
        if len(self.memory) < self.batch_size:
            return None
        
        batch = np.random.choice(len(self.memory), self.batch_size, replace=False)
        experiences = [self.memory[i] for i in batch]
        
        states = torch.cat([exp[0] for exp in experiences])
        actions = torch.cat([exp[1] for exp in experiences])
        rewards = torch.FloatTensor([exp[2] for exp in experiences])
        next_states = torch.cat([exp[3] for exp in experiences])
        dones = torch.BoolTensor([exp[4] for exp in experiences])
        
        return states, actions, rewards, next_states, dones
    
    def train_step(self):
        """è®­ç»ƒæ­¥éª¤"""
        batch = self.sample_batch()
        if batch is None:
            return 0
        
        states, actions, rewards, next_states, dones = batch
        
        # è®¡ç®—å½“å‰Qå€¼
        current_actions = self.agent(states)
        
        # è®¡ç®—ç›®æ ‡Qå€¼ (ç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ç”¨Criticç½‘ç»œ)
        with torch.no_grad():
            next_actions = self.agent(next_states)
            target_q = rewards + 0.99 * torch.sum(next_actions, dim=1) * (~dones)
        
        # è®¡ç®—æŸå¤±
        current_q = torch.sum(current_actions, dim=1)
        loss = nn.MSELoss()(current_q, target_q)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def train_episode(self):
        """è®­ç»ƒä¸€ä¸ªå›åˆ"""
        state = self.generate_state()
        episode_reward = 0
        episode_cache_hits = 0
        episode_delays = []
        steps = 0
        
        for step in range(100):  # æ¯å›åˆ100æ­¥
            # é€‰æ‹©åŠ¨ä½œ
            with torch.no_grad():
                action = self.agent(state)
            
            # ç¯å¢ƒäº¤äº’
            reward, delay, cache_hit = self.simulate_cache_environment(action[0])
            
            # ç”Ÿæˆä¸‹ä¸€çŠ¶æ€
            next_state = self.generate_state()
            done = step == 99
            
            # å­˜å‚¨ç»éªŒ
            self.store_experience(state, action, reward, next_state, done)
            
            # æ›´æ–°ç»Ÿè®¡
            episode_reward += reward
            if cache_hit:
                episode_cache_hits += 1
            episode_delays.append(delay)
            
            # è®­ç»ƒ
            if len(self.memory) >= self.batch_size:
                self.train_step()
            
            state = next_state
            steps += 1
        
        # æ›´æ–°ç»Ÿè®¡
        self.training_stats['episodes'] += 1
        self.training_stats['total_reward'] += episode_reward
        self.training_stats['cache_hit_rate'] = episode_cache_hits / steps
        self.training_stats['avg_delay'] = np.mean(episode_delays)
        
        return episode_reward, episode_cache_hits / steps, np.mean(episode_delays)
    
    def train(self, num_episodes=1000):
        """è®­ç»ƒä¸»å¾ªç¯"""
        print("ğŸš€ å¼€å§‹ç¼“å­˜æ„ŸçŸ¥è®­ç»ƒ...")
        
        episode_rewards = []
        cache_hit_rates = []
        avg_delays = []
        
        for episode in range(num_episodes):
            reward, hit_rate, delay = self.train_episode()
            
            episode_rewards.append(reward)
            cache_hit_rates.append(hit_rate)
            avg_delays.append(delay)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                avg_hit_rate = np.mean(cache_hit_rates[-100:])
                avg_delay = np.mean(avg_delays[-100:])
                
                print(f"Episode {episode+1}/{num_episodes}")
                print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
                print(f"  ç¼“å­˜å‘½ä¸­ç‡: {avg_hit_rate:.2%}")
                print(f"  å¹³å‡æ—¶å»¶: {avg_delay:.4f}s")
        
        print("âœ… ç¼“å­˜æ„ŸçŸ¥è®­ç»ƒå®Œæˆï¼")
        
        return {
            'episode_rewards': episode_rewards,
            'cache_hit_rates': cache_hit_rates,
            'avg_delays': avg_delays,
            'final_stats': self.training_stats
        }
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.agent.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'training_stats': self.training_stats
        }, filepath)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {filepath}")
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath)
        self.agent.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.training_stats = checkpoint['training_stats']
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {filepath}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ç¼“å­˜æ„ŸçŸ¥è®­ç»ƒç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CacheAwareTrainer(state_dim=20, action_dim=5, lr=0.001)
    
    # å¼€å§‹è®­ç»ƒ
    results = trainer.train(num_episodes=500)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜æ¨¡å‹
    model_path = f"results/cache_aware_model_{timestamp}.pth"
    trainer.save_model(model_path)
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    results_path = f"results/cache_aware_results_{timestamp}.json"
    with open(results_path, 'w') as f:
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        json_results = {
            'episode_rewards': [float(x) for x in results['episode_rewards']],
            'cache_hit_rates': [float(x) for x in results['cache_hit_rates']],
            'avg_delays': [float(x) for x in results['avg_delays']],
            'final_stats': results['final_stats'],
            'timestamp': timestamp
        }
        json.dump(json_results, f, indent=2)
    
    print(f"ğŸ“Š è®­ç»ƒç»“æœå·²ä¿å­˜: {results_path}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    print("\nğŸ“ˆ æœ€ç»ˆè®­ç»ƒç»Ÿè®¡:")
    print(f"  æ€»å›åˆæ•°: {results['final_stats']['episodes']}")
    print(f"  æœ€ç»ˆç¼“å­˜å‘½ä¸­ç‡: {results['cache_hit_rates'][-1]:.2%}")
    print(f"  æœ€ç»ˆå¹³å‡æ—¶å»¶: {results['avg_delays'][-1]:.4f}s")

if __name__ == "__main__":
    main()