"""
ğŸ› OPTIMIZED_TD3 è°ƒè¯•ç‰ˆè®­ç»ƒè„šæœ¬
=================================

ç”¨é€”: è¯Šæ–­OPTIMIZED_TD3ç®—æ³•ä¸å­¦ä¹ çš„æ ¹æœ¬åŸå› 

è°ƒè¯•å†…å®¹:
1. âœ… åŠ¨ä½œä¼ æ’­è¿½è¸ª - éªŒè¯agentè¾“å‡ºçš„actionæ˜¯å¦è¢«simulatoræ­£ç¡®ä½¿ç”¨
2. âœ… å¥–åŠ±åˆ†é‡åˆ†æ - è¯¦ç»†è®°å½•delay/energy/cacheå„ç»„ä»¶çš„è´¡çŒ®
3. âœ… çŠ¶æ€å‘é‡è´¨é‡ - æ£€æŸ¥state normalizationå’Œä¿¡æ¯å®Œæ•´æ€§
4. âœ… ç½‘ç»œæ¢¯åº¦ç›‘æ§ - è¿½è¸ªactor/criticçš„æ¢¯åº¦æ›´æ–°
5. âœ… ç»éªŒå›æ”¾é‡‡æ · - éªŒè¯Queue-aware replayæ˜¯å¦ç”Ÿæ•ˆ

ä½¿ç”¨æ–¹æ³•:
python train_single_agent_debug.py --algorithm OPTIMIZED_TD3 --episodes 50 --num-vehicles 12 --seed 42

è¾“å‡º:
- debug_log_<timestamp>.txt: è¯¦ç»†è°ƒè¯•æ—¥å¿—
- debug_metrics_<timestamp>.json: ç»“æ„åŒ–è°ƒè¯•æ•°æ®
"""

import sys
sys.path.insert(0, 'd:\\VEC_mig_caching')

from train_single_agent import SingleAgentTrainingEnvironment, config, generate_timestamp
import numpy as np
import json
import os
from datetime import datetime
from typing import Dict, Any, Optional, List
import time


class DebugSingleAgentTraining(SingleAgentTrainingEnvironment):
    """è°ƒè¯•ç‰ˆè®­ç»ƒç¯å¢ƒ - æ·»åŠ è¯¦ç»†çš„æ—¥å¿—è¾“å‡º"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # åˆ›å»ºè°ƒè¯•æ—¥å¿—æ–‡ä»¶
        timestamp = generate_timestamp()
        self.debug_log_file = f"debug_log_{timestamp}.txt"
        self.debug_metrics_file = f"debug_metrics_{timestamp}.json"
        
        # è°ƒè¯•æ•°æ®æ”¶é›†å™¨
        self.debug_data = {
            'action_traces': [],
            'reward_components': [],
            'state_samples': [],
            'gradient_norms': [],
            'replay_priorities': [],
            'system_states': []
        }
        
        # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
        self.log_file_handle = open(self.debug_log_file, 'w', encoding='utf-8')
        self.debug_log(f"{'='*80}")
        self.debug_log(f"ğŸ› è°ƒè¯•ä¼šè¯å¼€å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.debug_log(f"ç®—æ³•: {self.algorithm}")
        self.debug_log(f"é…ç½®: è½¦è¾†={self.num_vehicles}, RSU={self.num_rsus}, UAV={self.num_uavs}")
        self.debug_log(f"{'='*80}\n")
        
        # é‡‡æ ·é¢‘ç‡æ§åˆ¶ï¼ˆé¿å…æ—¥å¿—è¿‡å¤§ï¼‰
        self.log_every_n_steps = 10  # æ¯10æ­¥è¯¦ç»†è®°å½•ä¸€æ¬¡
        self.step_counter = 0
    
    def debug_log(self, message: str, level: str = "INFO"):
        """å†™å…¥è°ƒè¯•æ—¥å¿—"""
        timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]
        formatted_msg = f"[{timestamp}] [{level}] {message}"
        print(formatted_msg)  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        self.log_file_handle.write(formatted_msg + '\n')
        self.log_file_handle.flush()
    
    def run_episode(self, episode: int, max_steps: Optional[int] = None, visualizer: Optional[Any] = None) -> Dict:
        """å¢å¼ºç‰ˆepisodeè¿è¡Œ - æ·»åŠ è°ƒè¯•è¾“å‡º"""
        if max_steps is None:
            max_steps = config.experiment.max_steps_per_episode
        
        self.debug_log(f"\n{'â”€'*80}")
        self.debug_log(f"â–¶ Episode {episode} å¼€å§‹ (æœ€å¤§æ­¥æ•°: {max_steps})")
        self.debug_log(f"{'â”€'*80}")
        
        # é‡ç½®ç¯å¢ƒ
        self._episode_counters_initialized = False
        state = self.reset_environment()
        
        self.visualizer = visualizer
        self._current_episode = episode
        self._current_episode_step = 0
        
        # ğŸ“Š è®°å½•åˆå§‹çŠ¶æ€æ ·æœ¬
        self._log_state_vector(state, 0, "INITIAL")
        
        episode_reward = 0.0
        episode_info = {}
        step = 0
        info = {}
        
        # PPOç‰¹æ®Šå¤„ç†
        if self.algorithm == "PPO":
            return self._run_ppo_episode(episode, max_steps, visualizer)
        
        for step in range(max_steps):
            self.step_counter += 1
            should_log_detail = (step % self.log_every_n_steps == 0) or (step < 5)
            
            if should_log_detail:
                self.debug_log(f"\nâ”Œâ”€â”€ Step {step + 1}/{max_steps} â”€â”€â”")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # é˜¶æ®µ 1: é€‰æ‹©åŠ¨ä½œ
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.algorithm == "DQN":
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                action_idx = self._encode_discrete_action(actions_dict)
                action = action_idx
            else:
                # è¿ç»­åŠ¨ä½œç®—æ³• (TD3, OPTIMIZED_TD3ç­‰)
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                action = self._encode_continuous_action(actions_dict)
            
            # ğŸ“Š è®°å½•åŠ¨ä½œè¯¦æƒ…
            if should_log_detail:
                self._log_action_details(action, actions_dict, step)
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # é˜¶æ®µ 2: æ‰§è¡ŒåŠ¨ä½œ
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self._current_episode_step += 1
            
            # å°†å‘é‡åŠ¨ä½œæ¢å¤ä¸ºå­—å…¸ä¾›æ¨¡æ‹Ÿå™¨æ¶ˆè´¹
            sim_actions_dict = actions_dict if isinstance(actions_dict, dict) else self._build_actions_from_vector(action)
            
            # æ‰§è¡ŒåŠ¨ä½œå¹¶è·å–å¥–åŠ±
            next_state, reward, done, info = self.step(action, state, sim_actions_dict)
            
            # ğŸ“Š è®°å½•å¥–åŠ±åˆ†é‡
            if should_log_detail:
                self._log_reward_breakdown(reward, info, step)
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # é˜¶æ®µ 3: è®­ç»ƒæ™ºèƒ½ä½“
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # æ›´æ–°é˜Ÿåˆ—æŒ‡æ ‡
            if hasattr(self.agent_env, 'update_queue_metrics'):
                step_stats = info.get('step_stats', {})
                try:
                    self.agent_env.update_queue_metrics(step_stats)
                except Exception as e:
                    if self._current_episode % 100 == 0:
                        self.debug_log(f"âš ï¸ é˜Ÿåˆ—æŒ‡æ ‡æ›´æ–°å¤±è´¥: {e}", "WARNING")
            
            training_info = {}
            
            if self.algorithm == "DQN":
                safe_action = self._safe_int_conversion(action)
                training_info = self.agent_env.train_step(state, safe_action, reward, next_state, done)
            elif self.algorithm in ["DDPG", "TD3", "TD3_LATENCY_ENERGY", "SAC", "OPTIMIZED_TD3"]:
                safe_action = action if isinstance(action, np.ndarray) else np.array([action], dtype=np.float32)
                training_info = self.agent_env.train_step(state, safe_action, reward, next_state, done)
            elif self.algorithm == "PPO":
                training_info = self.agent_env.train_step(state, action, reward, next_state, done)
            else:
                training_info = {'message': f'Unknown algorithm: {self.algorithm}'}
            
            # ğŸ“Š è®°å½•è®­ç»ƒä¿¡æ¯
            if should_log_detail and training_info:
                self._log_training_info(training_info, step)
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # é˜¶æ®µ 4: æ›´æ–°çŠ¶æ€
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            episode_reward += reward
            episode_info = training_info
            
            # ğŸ“Š æ¯Næ­¥è®°å½•çŠ¶æ€æ ·æœ¬
            if should_log_detail:
                self._log_state_vector(next_state, step + 1, "NEXT")
                self.debug_log(f"â””â”€â”€ Step {step + 1} å®Œæˆ (ç´¯ç§¯å¥–åŠ±: {episode_reward:.4f}) â”€â”€â”˜")
            
            state = next_state
            if done:
                self.debug_log(f"â¹ Episode æå‰ç»“æŸäº step {step + 1}", "WARNING")
                break
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Episode ç»“æŸç»Ÿè®¡
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        steps_taken = step + 1
        system_metrics = info.get('system_metrics', {})
        self._record_episode_metrics(system_metrics, episode_steps=steps_taken)
        
        avg_step_reward = episode_reward / steps_taken if steps_taken > 0 else 0
        
        self.debug_log(f"\n{'â”€'*80}")
        self.debug_log(f"â¸ Episode {episode} å®Œæˆ")
        self.debug_log(f"  â€¢ æ€»æ­¥æ•°: {steps_taken}")
        self.debug_log(f"  â€¢ Episodeæ€»å¥–åŠ±: {episode_reward:.4f}")
        self.debug_log(f"  â€¢ å¹³å‡æ¯æ­¥å¥–åŠ±: {avg_step_reward:.4f}")
        
        # è®°å½•ç³»ç»ŸæŒ‡æ ‡
        if system_metrics:
            self.debug_log(f"  â€¢ ç³»ç»Ÿå¹³å‡å»¶è¿Ÿ: {system_metrics.get('avg_task_delay', 0):.4f} s")
            self.debug_log(f"  â€¢ ç³»ç»Ÿæ€»èƒ½è€—: {system_metrics.get('total_energy_consumption', 0):.4f} J")
            self.debug_log(f"  â€¢ ä»»åŠ¡å®Œæˆç‡: {system_metrics.get('task_completion_rate', 0):.4%}")
            self.debug_log(f"  â€¢ ç¼“å­˜å‘½ä¸­ç‡: {system_metrics.get('cache_hit_rate', 0):.4%}")
        
        self.debug_log(f"{'â”€'*80}\n")
        
        return {
            'episode_reward': episode_reward,
            'avg_reward': avg_step_reward,
            'episode_info': episode_info,
            'system_metrics': system_metrics,
            'steps': steps_taken
        }
    
    def _log_action_details(self, action: np.ndarray, actions_dict: Dict, step: int):
        """è®°å½•åŠ¨ä½œè¯¦ç»†ä¿¡æ¯"""
        self.debug_log(f"â”‚ ğŸ¯ åŠ¨ä½œç”Ÿæˆ:")
        
        if isinstance(action, np.ndarray):
            self.debug_log(f"â”‚   â€¢ åŠ¨ä½œç»´åº¦: {action.shape}")
            self.debug_log(f"â”‚   â€¢ åŠ¨ä½œèŒƒå›´: [{action.min():.4f}, {action.max():.4f}]")
            self.debug_log(f"â”‚   â€¢ åŠ¨ä½œå‡å€¼: {action.mean():.4f}, æ ‡å‡†å·®: {action.std():.4f}")
            self.debug_log(f"â”‚   â€¢ å‰5ç»´: {action[:5] if len(action) >= 5 else action}")
        
        # è§£æå¸è½½å†³ç­–
        if isinstance(actions_dict, dict) and 'vehicle_agent' in actions_dict:
            vehicle_action = np.array(actions_dict['vehicle_agent']).reshape(-1)
            if len(vehicle_action) >= 3:
                raw = vehicle_action[:3]
                raw_scaled = np.clip(raw, -1.0, 1.0) * 5.0
                exp = np.exp(raw_scaled - np.max(raw_scaled))
                probs = exp / np.sum(exp)
                
                self.debug_log(f"â”‚   â€¢ å¸è½½æ¦‚ç‡: Local={probs[0]:.4f}, RSU={probs[1]:.4f}, UAV={probs[2]:.4f}")
        
        # ä¿å­˜åŠ¨ä½œæ ·æœ¬åˆ°è°ƒè¯•æ•°æ®
        self.debug_data['action_traces'].append({
            'episode': self._current_episode,
            'step': step,
            'action_vector': action.tolist() if isinstance(action, np.ndarray) else action,
            'offload_probs': {
                'local': float(probs[0]) if 'probs' in locals() else 0,
                'rsu': float(probs[1]) if 'probs' in locals() and len(probs) > 1 else 0,
                'uav': float(probs[2]) if 'probs' in locals() and len(probs) > 2 else 0
            }
        })
    
    def _log_reward_breakdown(self, reward: float, info: Dict, step: int):
        """è®°å½•å¥–åŠ±åˆ†é‡"""
        self.debug_log(f"â”‚ ğŸ’° å¥–åŠ±åˆ†æ:")
        self.debug_log(f"â”‚   â€¢ æ€»å¥–åŠ±: {reward:.6f}")
        
        # å°è¯•ä»infoä¸­æå–å¥–åŠ±åˆ†é‡
        step_stats = info.get('step_stats', {})
        reward_components = step_stats.get('reward_components', {})
        
        if reward_components:
            delay_component = reward_components.get('delay', 0)
            energy_component = reward_components.get('energy', 0)
            cache_component = reward_components.get('cache', 0)
            penalty_component = reward_components.get('penalty', 0)
            
            self.debug_log(f"â”‚   â€¢ å»¶è¿Ÿåˆ†é‡: {delay_component:.6f}")
            self.debug_log(f"â”‚   â€¢ èƒ½è€—åˆ†é‡: {energy_component:.6f}")
            self.debug_log(f"â”‚   â€¢ ç¼“å­˜åˆ†é‡: {cache_component:.6f}")
            self.debug_log(f"â”‚   â€¢ æƒ©ç½šåˆ†é‡: {penalty_component:.6f}")
            
            # ä¿å­˜åˆ°è°ƒè¯•æ•°æ®
            self.debug_data['reward_components'].append({
                'episode': self._current_episode,
                'step': step,
                'total_reward': float(reward),
                'delay': float(delay_component),
                'energy': float(energy_component),
                'cache': float(cache_component),
                'penalty': float(penalty_component)
            })
        else:
            self.debug_log(f"â”‚   âš ï¸ æ— æ³•è·å–å¥–åŠ±åˆ†é‡è¯¦æƒ…", "WARNING")
        
        # è®°å½•å½“å‰æ­¥çš„ç³»ç»ŸçŠ¶æ€
        if 'avg_delay' in step_stats:
            self.debug_log(f"â”‚   â€¢ å½“å‰å»¶è¿Ÿ: {step_stats.get('avg_delay', 0):.4f} s")
            self.debug_log(f"â”‚   â€¢ å½“å‰èƒ½è€—: {step_stats.get('avg_energy', 0):.4f} J")
    
    def _log_state_vector(self, state: np.ndarray, step: int, label: str):
        """è®°å½•çŠ¶æ€å‘é‡ä¿¡æ¯"""
        if not isinstance(state, np.ndarray):
            state = np.array(state)
        
        if step % 20 == 0:  # å‡å°‘çŠ¶æ€æ—¥å¿—é¢‘ç‡
            self.debug_log(f"â”‚ ğŸ“Š çŠ¶æ€å‘é‡ ({label}):")
            self.debug_log(f"â”‚   â€¢ ç»´åº¦: {state.shape}")
            self.debug_log(f"â”‚   â€¢ èŒƒå›´: [{state.min():.4f}, {state.max():.4f}]")
            self.debug_log(f"â”‚   â€¢ å‡å€¼: {state.mean():.4f}, æ ‡å‡†å·®: {state.std():.4f}")
            self.debug_log(f"â”‚   â€¢ æ˜¯å¦æœ‰NaN: {np.isnan(state).any()}")
            self.debug_log(f"â”‚   â€¢ æ˜¯å¦æœ‰Inf: {np.isinf(state).any()}")
            
            # é‡‡æ ·ä¿å­˜éƒ¨åˆ†çŠ¶æ€
            if step % 50 == 0:
                self.debug_data['state_samples'].append({
                    'episode': self._current_episode,
                    'step': step,
                    'label': label,
                    'state_sample': state[:20].tolist() if len(state) >= 20 else state.tolist(),
                    'state_stats': {
                        'min': float(state.min()),
                        'max': float(state.max()),
                        'mean': float(state.mean()),
                        'std': float(state.std())
                    }
                })
    
    def _log_training_info(self, training_info: Dict, step: int):
        """è®°å½•è®­ç»ƒä¿¡æ¯"""
        if not training_info:
            return
        
        self.debug_log(f"â”‚ ğŸ”§ è®­ç»ƒæ›´æ–°:")
        
        # è®°å½•æŸå¤±å€¼
        if 'critic_loss' in training_info:
            self.debug_log(f"â”‚   â€¢ Critic Loss: {training_info['critic_loss']:.6f}")
        
        if 'actor_loss' in training_info:
            self.debug_log(f"â”‚   â€¢ Actor Loss: {training_info['actor_loss']:.6f}")
        
        # è®°å½•Qå€¼
        if 'q_value' in training_info:
            self.debug_log(f"â”‚   â€¢ Qå€¼: {training_info['q_value']:.6f}")
        
        # è®°å½•æ¢¯åº¦èŒƒæ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if 'actor_grad_norm' in training_info:
            self.debug_log(f"â”‚   â€¢ Actoræ¢¯åº¦èŒƒæ•°: {training_info['actor_grad_norm']:.6f}")
        
        if 'critic_grad_norm' in training_info:
            self.debug_log(f"â”‚   â€¢ Criticæ¢¯åº¦èŒƒæ•°: {training_info['critic_grad_norm']:.6f}")
        
        # è®°å½•ç»éªŒæ± å¤§å°
        if 'buffer_size' in training_info:
            self.debug_log(f"â”‚   â€¢ ç»éªŒæ± å¤§å°: {training_info['buffer_size']}")
        
        # ä¿å­˜æ¢¯åº¦ä¿¡æ¯
        if 'actor_grad_norm' in training_info or 'critic_grad_norm' in training_info:
            self.debug_data['gradient_norms'].append({
                'episode': self._current_episode,
                'step': step,
                'actor_grad': training_info.get('actor_grad_norm', 0),
                'critic_grad': training_info.get('critic_grad_norm', 0),
                'actor_loss': training_info.get('actor_loss', 0),
                'critic_loss': training_info.get('critic_loss', 0)
            })
    
    def save_debug_data(self):
        """ä¿å­˜è°ƒè¯•æ•°æ®åˆ°JSONæ–‡ä»¶"""
        self.debug_log(f"\n{'='*80}")
        self.debug_log(f"ğŸ’¾ ä¿å­˜è°ƒè¯•æ•°æ®åˆ° {self.debug_metrics_file}")
        
        try:
            with open(self.debug_metrics_file, 'w', encoding='utf-8') as f:
                json.dump(self.debug_data, f, indent=2, ensure_ascii=False)
            
            self.debug_log(f"âœ… è°ƒè¯•æ•°æ®ä¿å­˜æˆåŠŸ")
            self.debug_log(f"  â€¢ åŠ¨ä½œæ ·æœ¬æ•°: {len(self.debug_data['action_traces'])}")
            self.debug_log(f"  â€¢ å¥–åŠ±æ ·æœ¬æ•°: {len(self.debug_data['reward_components'])}")
            self.debug_log(f"  â€¢ çŠ¶æ€æ ·æœ¬æ•°: {len(self.debug_data['state_samples'])}")
            self.debug_log(f"  â€¢ æ¢¯åº¦æ ·æœ¬æ•°: {len(self.debug_data['gradient_norms'])}")
        except Exception as e:
            self.debug_log(f"âŒ ä¿å­˜è°ƒè¯•æ•°æ®å¤±è´¥: {e}", "ERROR")
        
        self.debug_log(f"{'='*80}\n")
    
    def __del__(self):
        """ææ„å‡½æ•° - å…³é—­æ—¥å¿—æ–‡ä»¶"""
        if hasattr(self, 'log_file_handle'):
            self.log_file_handle.close()


def main():
    """è°ƒè¯•è®­ç»ƒä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='OPTIMIZED_TD3 è°ƒè¯•è®­ç»ƒ')
    parser.add_argument('--algorithm', type=str, default='OPTIMIZED_TD3', help='ç®—æ³•åç§°')
    parser.add_argument('--episodes', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--num-vehicles', type=int, default=12, help='è½¦è¾†æ•°é‡')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    print(f"\n{'='*80}")
    print(f"ğŸ› OPTIMIZED_TD3 è°ƒè¯•è®­ç»ƒå¯åŠ¨")
    print(f"{'='*80}")
    print(f"é…ç½®:")
    print(f"  â€¢ ç®—æ³•: {args.algorithm}")
    print(f"  â€¢ è½®æ•°: {args.episodes}")
    print(f"  â€¢ è½¦è¾†: {args.num_vehicles}")
    print(f"  â€¢ ç§å­: {args.seed}")
    print(f"{'='*80}\n")
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    import random
    random.seed(args.seed)
    try:
        import torch
        torch.manual_seed(args.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(args.seed)
    except ImportError:
        pass
    
    # åˆ›å»ºè°ƒè¯•ç¯å¢ƒ
    override_scenario = {'num_vehicles': args.num_vehicles}
    
    debug_env = DebugSingleAgentTraining(
        algorithm=args.algorithm,
        override_scenario=override_scenario,
        use_enhanced_cache=True,
        disable_migration=False
    )
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nå¼€å§‹è°ƒè¯•è®­ç»ƒ...\n")
    
    for episode in range(1, args.episodes + 1):
        result = debug_env.run_episode(episode, max_steps=200)
        
        # æ¯10ä¸ªepisodeè¾“å‡ºæ‘˜è¦
        if episode % 10 == 0:
            print(f"\n{'â”€'*80}")
            print(f"ğŸ“ˆ Episode {episode} æ‘˜è¦:")
            print(f"  â€¢ Episodeå¥–åŠ±: {result['episode_reward']:.4f}")
            print(f"  â€¢ å¹³å‡æ­¥å¥–åŠ±: {result['avg_reward']:.4f}")
            if result.get('system_metrics'):
                sm = result['system_metrics']
                print(f"  â€¢ å¹³å‡å»¶è¿Ÿ: {sm.get('avg_task_delay', 0):.4f} s")
                print(f"  â€¢ æ€»èƒ½è€—: {sm.get('total_energy_consumption', 0):.4f} J")
                print(f"  â€¢ å®Œæˆç‡: {sm.get('task_completion_rate', 0):.4%}")
            print(f"{'â”€'*80}\n")
    
    # ä¿å­˜è°ƒè¯•æ•°æ®
    debug_env.save_debug_data()
    
    print(f"\n{'='*80}")
    print(f"âœ… è°ƒè¯•è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*80}")
    print(f"è°ƒè¯•è¾“å‡ºæ–‡ä»¶:")
    print(f"  ğŸ“„ æ—¥å¿—æ–‡ä»¶: {debug_env.debug_log_file}")
    print(f"  ğŸ“Š æ•°æ®æ–‡ä»¶: {debug_env.debug_metrics_file}")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
