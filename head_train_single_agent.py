"""
é¦ƒå¹† CAMTD3ç’î… ç²Œé‘´æ°­æ¹°é”›åœ•ache-Aware Migration with Twin Delayed DDPGé”›?

éŠ†æ„®éƒ´ç¼ç†¸ç¦é‹å‹©â‚¬?
CAMTD3 = é©è½°ç°¬æ¶“î…ãç’§å‹¬ç°®é’å—›å¤é¨å‹­ç´¦ç€›æ¨»åŠ…é­ãƒ¤æ¢é”Â¤ç¸¼ç»‰è¤éƒ´ç¼?
éˆ¹æº¾æ”¢éˆ¹â‚¬ Phase 1: æ¶“î…ãé…é¸¿å…˜æµ£æ’¹ç¥«å©§æ„¬åé–°å¶…å–…ç»›æ µç´™éç¨¿ç¸¾é’æ¶™æŸŠé”›?
éˆ¹?  éˆ¹æº¾æ”¢éˆ¹â‚¬ é˜èˆµâ‚¬ä½ºâ”–é—‚? 80ç¼è¾¾ç´™æï¹ç· +RSU+UAVéã„¥çœ¬é˜èˆµâ‚¬ä¾Šç´š
éˆ¹?  éˆ¹æº¾æ”¢éˆ¹â‚¬ é”ã„¤ç¶”ç»Œæ´ªæ£¿: 30ç¼è¾¾ç´™ç”¯ï¹€î†”+ç’ï¼„ç•»ç’§å‹¬ç°®é’å—›å¤éšæˆ¦å™ºé”›?
éˆ¹?  éˆ¹æ–ºæ”¢éˆ¹â‚¬ ç» æ¥ç¡¶: TD3/SAC/DDPG/PPO
éˆ¹æº¾æ”¢éˆ¹â‚¬ Phase 2: éˆî„€æ¹´æµ è¯²å§ŸéµÑ†î”‘
éˆ¹?  éˆ¹æº¾æ”¢éˆ¹â‚¬ ç¼‚æ’³ç“¨éå´‡ç“¥é”›åœ•ache-Awareé”›?
éˆ¹?  éˆ¹æº¾æ”¢éˆ¹â‚¬ æµ è¯²å§Ÿæ©ä½ºĞ©é”›åœ¡igrationé”›?
éˆ¹?  éˆ¹æ–ºæ”¢éˆ¹â‚¬ æµ è¯²å§Ÿç’‹å†¨å®³

python train_single_agent.py --algorithm OPTIMIZED_TD3 --episodes 1000 --num-vehicles 12 --seed 42

Queue-aware Replay
é‰?ç’î… ç²Œéå ¢å·¼é»æ„¬å´Œ5éŠ?
é‰?è¹‡î‚¦â‚¬ç†·î„Ÿæ¶”çŠ»ç®ç’ç†»æµ‡é¦çƒ˜æ«™
é‰?é–½å î‡®VECé—ƒç†·åªç» ï¼„æ‚Šé¥æ¶šå£
GNN Attention
é‰?ç¼‚æ’³ç“¨é›æˆ’è…‘éœå›¨å½é—?20éŠ?
é‰?é…é¸¿å…˜ç€›ï¸¿ç¯„é‘ºå‚œå£é—å¿ç¶”éå´‡éƒ´
é‰?é–«å‚šç°²é”ã„¦â‚¬ä½¹å«‡éµæˆå½‰é–?
éŠ†æ„ªå¨‡é¢ã„¦æŸŸå¨‰æ›˜â‚¬?
# CAMTD3éå›§å™¯ç’î… ç²Œé”›å ¥ç²¯ç’ã‚†Äå¯®å¿¥ç´š
python train_single_agent.py --algorithm TD3 --episodes 200
python train_single_agent.py --algorithm SAC --episodes 200

é‰? é™î„æƒé¢ã„¥å§©é¬ä½¸ç”«ç€¹è—‰åé–°?
python train_single_agent.py --algorithm TD3 --episodes 200 --dynamic-bandwidth


# æ¿¡å‚æ¸¶ç»‚ä½ºæ•¤æ¶“î…ãç’§å‹¬ç°®é’å—›å¤é”›å œç¬‰éºã„¨å´˜é”›å±¼ç²é¢ã„¤ç°¬å¨‘å £ç€ºç€¹ç‚ºç™é”›?
python train_single_agent.py --algorithm TD3 --episodes 200 --no-central-resource

é¦ƒæ‚•é¦ƒæ¼é””å¿¦ç…‹?

é—æ›Ÿæ«¤é‘³æˆ’ç¶‹ç» æ¥ç¡¶ç’î… ç²Œé‘´æ°­æ¹°
é€îˆ›å¯”DDPGéŠ†ä¹€D3éŠ†ä¹€D3-LEéŠ†ä¸QNéŠ†ä¸³POéŠ†ä¸¼ACç»›å¤Œç•»å¨‰æ› æ®‘ç’î… ç²Œéœå±¾ç˜®æˆ?
python train_single_agent.py --compare --episodes 200  # å§£æ—‡ç·éµâ‚¬éˆå¤Œç•»å¨‰?
é¦ƒæ®Œ æ¾§ç‚²å·±ç¼‚æ’³ç“¨å¦¯â€³ç´¡ (æ¦›æ¨¿î…»éšîˆœæ•¤ - é’å——çœ°L1/L2 + é‘·îˆâ‚¬å‚šç°²é‘î…å®³ç»›æ «æš + RSUé—å¿ç¶”):
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 8
python train_single_agent.py --algorithm TD3 --episodes 1000 --num-vehicles 12
python train_single_agent.py --algorithm TD3 --episodes 800 --num-vehicles 12 --silent-mode  # é—ˆæ¬“ç²¯æ·‡æ¿†ç“¨ç¼æ’´ç‰
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 16
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 20
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 24
python train_single_agent.py --algorithm TD3-LE --episodes 1600 --num-vehicles 12
python train_single_agent.py --algorithm SAC --episodes 800
python train_single_agent.py --algorithm PPO --episodes 800

é¦ƒå¯ª ç€¹ç‚´æ¤‚é™îˆî‹é–?
python train_single_agent.py --algorithm DDPG --episodes 100 --realtime-vis --vis-port 8080

é¦ƒæ‚• é¢ç†¸åšç€›ï¸½æ¹³é¥æã€ƒ:
python generate_academic_charts.py results/single_agent/td3/training_results_20251007_220900.json

é’æ‹Œæªéœå›§î‡®å§£æ—“ç´°python experiments/arrival_rate_analysis/run_td3_arrival_rate_sweep_silent.py --rates 1.0 1.5 2.0 2.5 3.0 3.5 --episodes 800


""" 
import os
import sys
import random

# é¦ƒæ•¡ æ·‡î†¼î˜²Windowsç¼‚æ «çˆœé—‚î‡€î•½
if sys.platform == 'win32':
    try:
        if hasattr(sys.stdout, 'reconfigure'):
            sys.stdout.reconfigure(encoding='utf-8', errors='replace')  # type: ignore[attr-defined]
        elif hasattr(sys.stdout, 'buffer'):
            import io
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=True)
    except Exception:
        pass
    try:
        if hasattr(sys.stderr, 'reconfigure'):
            sys.stderr.reconfigure(encoding='utf-8', errors='replace')  # type: ignore[attr-defined]
        elif hasattr(sys.stderr, 'buffer'):
            import io
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace', line_buffering=True)
    except Exception:
        pass

import argparse
import json
from tools.fixed_topology_optimizer import FixedTopologyOptimizer
import numpy as np
import matplotlib.pyplot as plt
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any

# ç€µç…å†éç¨¿ç¸¾å¦¯â€³æ½¡
from config import config
from evaluation.system_simulator import CompleteSystemSimulator
try:
    from evaluation.enhanced_system_simulator import EnhancedSystemSimulator
    ENHANCED_CACHE_AVAILABLE = True
except ImportError:
    ENHANCED_CACHE_AVAILABLE = False
    print("[Warning] Enhanced cache system not available, using standard simulator")
from utils import MovingAverage
from utils.normalization_utils import (
    normalize_distribution,
    normalize_feature_vector,
    normalize_ratio,
    normalize_scalar,
)
# é¦ƒî˜» ç€µç…å†é‘·îˆâ‚¬å‚šç°²éºÑƒåŸ—ç¼å‹ªæ¬¢
from utils.adaptive_control import AdaptiveCacheController, AdaptiveMigrationController, map_agent_actions_to_params
from decision.strategy_coordinator import StrategyCoordinator
from utils.unified_reward_calculator import update_reward_targets

# ç€µç…å†éšå‹­î’é—æ›Ÿæ«¤é‘³æˆ’ç¶‹ç» æ¥ç¡¶
from single_agent.ddpg import DDPGEnvironment
from single_agent.td3 import TD3Environment
from single_agent.td3_hybrid_fusion import CAMTD3Environment
from single_agent.td3_latency_energy import TD3LatencyEnergyEnvironment
from single_agent.dqn import DQNEnvironment
from single_agent.ppo import PPOEnvironment
from single_agent.sac import SACEnvironment
# ç€µç…å†ç»®å‰§ç•æµ¼æ¨ºå¯²TD3 (æµ åŒ­ueue-aware + GNN)
from single_agent.optimized_td3_wrapper import OptimizedTD3Environment

# ç€µç…å†HTMLé¶ãƒ¥æ†¡é¢ç†¸åšé£?
from utils.html_report_generator import HTMLReportGenerator

# ç€µç…å†ç’î… ç²Œç¼æ’´ç‰æ·‡æ¿†ç“¨éœå²€ç²¯é¥æƒ§ä¼é?
from utils.training_results import save_single_training_results, plot_single_training_curves

# é¦ƒå¯ª ç€µç…å†ç€¹ç‚´æ¤‚é™îˆî‹é–æ ¨Äé§?
try:
    from scripts.visualize.realtime_visualization import create_visualizer
    REALTIME_AVAILABLE = True
except ImportError:
    REALTIME_AVAILABLE = False
    print("éˆ¿ç‹…ç¬  ç€¹ç‚´æ¤‚é™îˆî‹é–æ §å§›é‘³æˆ’ç¬‰é™îˆœæ•¤é”›å²ƒî‡¬æ©æ„¯î”‘: pip install flask flask-socketio")

# é¦ƒå¸¹ ç€µç…å†æ¥‚æ¨¼î¬ç’î… ç²Œé™îˆî‹é–æ §æ«’
try:
    from utils.advanced_training_visualizer import create_visualizer as create_advanced_visualizer
    ADVANCED_VIS_AVAILABLE = True
except ImportError:
    ADVANCED_VIS_AVAILABLE = False
    print("éˆ¿ç‹…ç¬  æ¥‚æ¨¼î¬é™îˆî‹é–æ §å§›é‘³æˆ’ç¬‰é™îˆœæ•¤")

# çæ¿Šç˜¯ç€µç…å†PyTorchæµ ãƒ¨î†•ç¼ƒî‡€æ®¢éˆè™¹î’ç€›æ„¶ç´±æ¿¡å‚›ç‰æ¶“å¶…å½²é¢ã„¥å¯ç’ºå® ç¹ƒ
try:
    import torch
except ImportError:  # pragma: no cover -ç€¹å½’æ•Šæ¾¶å‹­æ‚Š
    torch = None


def _apply_global_seed_from_env():
    """éè§„åµéœîˆšî•¨é™æ©€å™ºRANDOM_SEEDç’å‰§ç–†é—…å¿”æº€ç»‰å¶…ç“™é”›å²€â€˜æ·‡æ¿†å½²é–²å¶…î˜²é¬?""
    seed_env = os.environ.get('RANDOM_SEED')
    if not seed_env:
        return
    try:
        seed = int(seed_env)
    except ValueError:
        print(f"éˆ¿ç‹…ç¬  RANDOM_SEED éœîˆšî•¨é™æ©€å™ºéƒçŠ³æ™¥: {seed_env}")
        return

    random.seed(seed)
    np.random.seed(seed)
    if torch is not None:
        torch.manual_seed(seed)
        if torch.cuda.is_available():  # pragma: no cover - GPUé™îˆâ‚¬?
            torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True  # type: ignore[attr-defined]
        torch.backends.cudnn.benchmark = False  # type: ignore[attr-defined]
    config.random_seed = seed
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"é¦ƒæ”¼ éã„¥çœ¬é—…å¿”æº€ç»‰å¶…ç“™å®¸èŒ¶î†•ç¼ƒî†»è´Ÿ {seed}")


def _maybe_apply_reward_smoothing_from_env() -> None:
    """Optionally enable reward smoothing via environment variables.

    RL_SMOOTH_DELAY, RL_SMOOTH_ENERGY, RL_SMOOTH_ALPHA can be provided.
    """
    try:
        d = os.environ.get('RL_SMOOTH_DELAY')
        e = os.environ.get('RL_SMOOTH_ENERGY')
        a = os.environ.get('RL_SMOOTH_ALPHA')
        if d is not None:
            setattr(config.rl, 'reward_smooth_delay_weight', float(d))
        if e is not None:
            setattr(config.rl, 'reward_smooth_energy_weight', float(e))
        if a is not None:
            setattr(config.rl, 'reward_smooth_alpha', float(a))
    except Exception:
        pass


def _apply_reward_overrides_from_env() -> None:
    """Allow quick reward/target retuning via environment variables for hard scenarios."""
    latency_target = os.environ.get("RL_LATENCY_TARGET")
    energy_target = os.environ.get("RL_ENERGY_TARGET")
    disable_dynamic_targets = os.environ.get("RL_DISABLE_DYNAMIC_TARGETS", "0") != "0"
    # é‚æ¿î–ƒé”›æ°­æ•®é¸ä½¸æ©é–«ç†¸å½æ¥‚æ¨¹æ¶ªé–?ç€¹å±¾åšéœå›¨å„µç¼ƒæ°¾ç´æ¦›æ¨¿î…»é‡æ’®å™¸ç»¾ï¸½æ½«æ¥‚æ¨¿ç¤‹ææˆ’ç¬‰é™îˆæ½¬ç›å±¼è´Ÿ
    default_loss_weight = 1.4
    default_completion_gap = 0.7
    default_drop_penalty = 0.18
    if not getattr(config.rl, "reward_weight_loss_ratio", 0.0):
        setattr(config.rl, "reward_weight_loss_ratio", default_loss_weight)
    if not getattr(config.rl, "reward_weight_completion_gap", 0.0):
        setattr(config.rl, "reward_weight_completion_gap", default_completion_gap)
    if getattr(config.rl, "reward_penalty_dropped", 0.0) < default_drop_penalty:
        setattr(config.rl, "reward_penalty_dropped", default_drop_penalty)

    weight_overrides = {
        "RL_WEIGHT_DELAY": "reward_weight_delay",
        "RL_WEIGHT_ENERGY": "reward_weight_energy",
        "RL_WEIGHT_CACHE": "reward_weight_cache",
        "RL_WEIGHT_CACHE_PRESSURE": "reward_weight_cache_pressure",
        "RL_WEIGHT_MIGRATION": "reward_weight_migration",
        "RL_WEIGHT_QUEUE_OVERLOAD": "reward_weight_queue_overload",
        "RL_WEIGHT_REMOTE_REJECT": "reward_weight_remote_reject",
        "RL_WEIGHT_LOSS_RATIO": "reward_weight_loss_ratio",
        "RL_WEIGHT_COMPLETION_GAP": "reward_weight_completion_gap",
        "RL_PENALTY_DROPPED": "reward_penalty_dropped",
    }

    def _try_set(env_key: str, attr: str) -> None:
        val = os.environ.get(env_key)
        if not val:
            return
        try:
            setattr(config.rl, attr, float(val))
            print(f"[RewardOverride] {attr} <- {val}")
        except Exception:
            pass

    for env_key, attr in weight_overrides.items():
        _try_set(env_key, attr)

    # Targets need to sync with the reward calculator singletons
    target_changed = False
    try:
        if latency_target is not None:
            config.rl.latency_target = float(latency_target)
            target_changed = True
            print(f"[RewardOverride] latency_target <- {latency_target}")
    except Exception:
        pass
    try:
        if energy_target is not None:
            config.rl.energy_target = float(energy_target)
            target_changed = True
            print(f"[RewardOverride] energy_target <- {energy_target}")
    except Exception:
        pass
    if disable_dynamic_targets:
        # ç’å‰§ç–†éã„¥çœ¬ç»‚ä½ºæ•¤é”ã„¦â‚¬ä½¹æ–ç€¹?
        os.environ['DYNAMIC_TARGET_DISABLE'] = '1'
        print("[RewardOverride] é”ã„¦â‚¬ä½ºæ´°éå›¨æ–ç€¹è—‰å‡¡ç»‚ä½ºæ•¤ (RL_DISABLE_DYNAMIC_TARGETS=1)")
    if target_changed:
        try:
            update_reward_targets(
                latency_target=config.rl.latency_target,
                energy_target=config.rl.energy_target,
            )
        except Exception:
            # keep training even if sync fails
            pass

def _build_scenario_config() -> Dict[str, Any]:
    """é‹å‹«ç¼“å¦¯â„ƒå«™éœîˆšî•¨é–°å¶‡ç–†é”›å±½å‘ç’æâ‚¬æ°³ç¹ƒéœîˆšî•¨é™æ©€å™ºç‘•å—™æ´Šæ¦›æ¨¿î…»éŠ?""
    # é¦ƒæ•¡ é€îˆ›å¯”æµ åº£å¹†æ¾§å†¨å½‰é–²å¿šî›«é©æ ¦æ¢é”â€³åŸŒæˆå‰§å·¼é”›å ¢æ•¤æµœåº¡å¼¬éç‰ˆæ™±é°ç†¸â‚¬Ñƒåé‹æ„¶ç´š
    task_arrival_rate = getattr(getattr(config, "task", None), "arrival_rate", 1.8)
    if os.environ.get('TASK_ARRIVAL_RATE'):
        try:
            arrival_rate_str = os.environ.get('TASK_ARRIVAL_RATE')
            if arrival_rate_str is not None:
                task_arrival_rate = float(arrival_rate_str)
                print(f"é¦ƒæ•¡ æµ åº£å¹†æ¾§å†¨å½‰é–²å¿šî›«é©æ ¦æ¢é”â€³åŸŒæˆå‰§å·¼: {task_arrival_rate} tasks/s")
        except ValueError:
            print(f"éˆ¿ç‹…ç¬  éœîˆšî•¨é™æ©€å™ºTASK_ARRIVAL_RATEéƒçŠ³æ™¥é”›å±¼å¨‡é¢ã„©ç²¯ç’ã‚…â‚¬?)

    def _get_or_default(obj: Optional[Any], attr: str, default: Any) -> Any:
        return getattr(obj, attr, default) if obj is not None else default

    network_cfg = getattr(config, "network", None)
    vehicle_cfg = getattr(network_cfg, "vehicle_config", {}) if network_cfg else {}
    rsu_cfg = getattr(network_cfg, "rsu_config", {}) if network_cfg else {}
    uav_cfg = getattr(network_cfg, "uav_config", {}) if network_cfg else {}
    comm_cfg = getattr(network_cfg, "communication_config", {}) if network_cfg else {}
    compute_cfg = getattr(config, "compute", None)
    service_cfg = getattr(config, "service", None)
    communication_cfg = getattr(config, "communication", None)

    def _normalize_bandwidth(value: Optional[float], fallback: float) -> float:
        if value is None:
            return fallback
        bw = float(value)
        if bw < 1e3:  # assume MHz éˆ«?Hz
            bw *= 1e6
        return bw

    scenario = {
        "num_vehicles": getattr(config, "num_vehicles", vehicle_cfg.get('num_vehicles', 12)),
        "num_rsus": getattr(config, "num_rsus", rsu_cfg.get('num_rsus', 4)),
        "num_uavs": getattr(config, "num_uavs", uav_cfg.get('num_uavs', 2)),
        "task_arrival_rate": task_arrival_rate,
        "time_slot": getattr(config, "time_slot", _get_or_default(network_cfg, 'time_slot_duration', 0.1)),
        "simulation_time": getattr(config, "simulation_time", 1000),
        "computation_capacity": float(vehicle_cfg.get('computation_capacity', 1000)),
        "bandwidth": _normalize_bandwidth(
            comm_cfg.get('bandwidth'),
            _get_or_default(communication_cfg, 'total_bandwidth', 50e6),
        ),
        "coverage_radius": float(rsu_cfg.get('coverage_radius', 300)),
        "cache_capacity": float(rsu_cfg.get('cache_capacity', 120)),
        "transmission_power": float(vehicle_cfg.get('transmission_power', 0.15)),
        "computation_power": float(_get_or_default(compute_cfg, 'vehicle_static_power', 1.2)),
        "thermal_noise_density": float(comm_cfg.get('thermal_noise_density', -174.0)),
        "noise_figure": float(_get_or_default(communication_cfg, 'noise_figure', 9.0)),
        "high_load_mode": getattr(getattr(config, "task", None), "high_load_mode", False),
        "task_complexity_multiplier": float(
            getattr(getattr(config, "task", None), "complexity_multiplier", 1.1)
        ),
        "rsu_load_divisor": float(_get_or_default(service_cfg, 'rsu_queue_boost_divisor', 4.0)),
        "uav_load_divisor": float(_get_or_default(service_cfg, 'uav_queue_boost_divisor', 2.0)),
        "enhanced_task_generation": True,
    }

    override_env = os.environ.get('TRAINING_SCENARIO_OVERRIDES')
    if override_env:
        try:
            overrides = json.loads(override_env)
            if isinstance(overrides, dict):
                scenario.update(overrides)
            else:
                print("éˆ¿ç‹…ç¬  TRAINING_SCENARIO_OVERRIDES é—‡â‚¬æ¶“ç¯”SONç€µç¡…è–„é”›å±½å‡¡è¹‡ç•ŒæšéŠ†?)
        except json.JSONDecodeError as exc:
            print(f"éˆ¿ç‹…ç¬  TRAINING_SCENARIO_OVERRIDES ç‘™ï½†ç€½æ¾¶è¾«è§¦: {exc}")

    return scenario


_apply_global_seed_from_env()
_maybe_apply_reward_smoothing_from_env()


def generate_timestamp() -> str:
    """é¢ç†¸åšéƒå •æ£¿é´?""
    if config.experiment.use_timestamp:
        return datetime.now().strftime(config.experiment.timestamp_format)
    else:
        return ""

def get_timestamped_filename(base_name: str, extension: str = ".json") -> str:
    """é‘¾å³°å½‡ç”¯ï¸½æ¤‚é—‚å­˜åŸ‘é¨å‹¬æƒæµ è·ºæ‚•"""
    timestamp = generate_timestamp()
    if timestamp:
        name_parts = base_name.split('.')
        if len(name_parts) > 1:
            base = '.'.join(name_parts[:-1])
            return f"{base}_{timestamp}{extension}"
        else:
            return f"{base_name}_{timestamp}{extension}"
    else:
        return f"{base_name}{extension}"


class SingleAgentTrainingEnvironment:
    """é—æ›Ÿæ«¤é‘³æˆ’ç¶‹ç’î… ç²Œéœîˆšî•¨é©è™¹è¢«"""
    
    def _apply_optimized_td3_defaults(self) -> None:
        """æ¶“ç¯›PTIMIZED_TD3ç’å‰§ç–†é‡æ‘å·±é¨å‹«å½²é—ˆçŠ³â‚¬?éºãˆ¢å‚¨æ¦›æ¨¿î…»éŠç¡·ç´™é™îˆî¦éœîˆšî•¨é™æ©€å™ºç‘•å—™æ´Šé”›å¤ˆâ‚¬?""
        if not hasattr(self, 'algorithm') and hasattr(self, 'input_algorithm'):
            alg = str(self.input_algorithm).upper()
        else:
            alg = getattr(self, 'algorithm', '').upper()
        if alg != "OPTIMIZED_TD3":
            return
        rl = getattr(config, "rl", None)
        if rl is None:
            return

        def _set_if_absent(env_key: str, attr: str, value: float, use_max: bool = False) -> None:
            if os.environ.get(env_key) is not None:
                return
            current = float(getattr(rl, attr, 0.0) or 0.0)
            val_to_set = max(current, value) if use_max else (current if current else value)
            setattr(rl, attr, val_to_set)

        def _force_override(env_key: str, attr: str, value: float) -> None:
            """ç€µç­„PTIMIZED_TD3æµ£è·¨æ•¤é‡å­˜ä¿¯éœå²€æ®‘é‰å†®å™¸/é©î†½çˆ£é”›å²„æª·æµ£åº¡îš›é”è¾¨æŸŸå®¸î†ºâ‚¬?""
            if os.environ.get(env_key) is not None:
                return
            setattr(rl, attr, float(value))

        # é™îˆæ½¬é¬Ñ„æ½ˆé–²å¶ç´™é€èˆµæšƒé™å¬ªã‚½é—å Ÿæ¹°é”›?        _set_if_absent("RL_WEIGHT_LOSS_RATIO", "reward_weight_loss_ratio", 1.2)
        _set_if_absent("RL_WEIGHT_COMPLETION_GAP", "reward_weight_completion_gap", 0.7)
        _set_if_absent("RL_PENALTY_DROPPED", "reward_penalty_dropped", 0.15, use_max=True)
        _set_if_absent("RL_WEIGHT_QUEUE_OVERLOAD", "reward_weight_queue_overload", 0.8, use_max=True)
        _set_if_absent("RL_WEIGHT_REMOTE_REJECT", "reward_weight_remote_reject", 0.25, use_max=True)

        # éç¨¿ç¸¾é‰å†®å™¸é”›æ°±æ´¿éºãƒ¨î›«é©æ §åçâ‚¬æˆå†©ç¸ºæ©æ¶šæ®‘æ¦›æ¨¿î…»éŠ?        _force_override("RL_WEIGHT_CACHE", "reward_weight_cache", 0.2)
        _force_override("RL_WEIGHT_CACHE_BONUS", "reward_weight_cache_bonus", 0.3)
        _force_override("RL_WEIGHT_DELAY", "reward_weight_delay", 1.8)
        _force_override("RL_WEIGHT_ENERGY", "reward_weight_energy", 1.2)
        _force_override("RL_WEIGHT_OFFLOAD_BONUS", "reward_weight_offload_bonus", 3.0)
        _force_override("RL_WEIGHT_LOCAL_PENALTY", "reward_weight_local_penalty", 1.0)

        # ğŸ”§ P0ä¿®å¤ï¼šç§»é™¤å¼ºåˆ¶è¦†ç›–ç›®æ ‡å€¼ï¼Œå°Šé‡config.rlé»˜è®¤å€¼(0.4s/3500J)
        # æ—§ç‰ˆæœ¬ï¼šå¼ºåˆ¶è¦†ç›–ä¸º2.3s/9600Jï¼Œä¸config.rlé»˜è®¤å€¼å†²çª
        # æ–°ç­–ç•¥ï¼šä½¿ç”¨config.rlé»˜è®¤å€¼ï¼Œå¦‚éœ€è°ƒæ•´åº”é€šè¿‡ç¯å¢ƒå˜é‡ RL_LATENCY_TARGET/RL_ENERGY_TARGET
        # æˆ–é€šè¿‡ override_scenario ä¸­çš„ num_vehicles è§¦å‘åŠ¨æ€è°ƒæ•´
        # _force_override("RL_LATENCY_TARGET", "latency_target", 2.3)  # âŒ ç§»é™¤
        # _force_override("RL_LATENCY_UPPER_TOL", "latency_upper_tolerance", 3.5)  # âŒ ç§»é™¤
        # _force_override("RL_ENERGY_TARGET", "energy_target", 9600.0)  # âŒ ç§»é™¤
        # _force_override("RL_ENERGY_UPPER_TOL", "energy_upper_tolerance", 14000.0)  # âŒ ç§»é™¤
        try:
            update_reward_targets(
                latency_target=float(getattr(rl, "latency_target", 0.4)),
                energy_target=float(getattr(rl, "energy_target", 3500.0)),
            )
            print(f"  âœ… å¥–åŠ±è®¡ç®—å™¨å·²åŒæ­¥ç›®æ ‡å€¼")
        except Exception as e:
            print(f"  âš ï¸  å¥–åŠ±ç›®æ ‡åŒæ­¥å¤±è´¥: {e}")

    def __init__(
        self,
        algorithm: str,
        override_scenario: Optional[Dict[str, Any]] = None,
        use_enhanced_cache: bool = False,
        disable_migration: bool = False,
        enforce_offload_mode: Optional[str] = None,
        fixed_offload_policy: Optional[str] = None,
        joint_controller: bool = False,
    ):
        self.input_algorithm = algorithm
        normalized_algorithm = algorithm.upper().replace('-', '_')
        alias_map = {
            "TD3LE": "TD3_LATENCY_ENERGY",
            "TD3_LE": "TD3_LATENCY_ENERGY",
            "TD3LATENCY": "TD3_LATENCY_ENERGY",
            "TD3_LATENCY": "TD3_LATENCY_ENERGY",
            "TD3_LATENCY_ENERGY": "TD3_LATENCY_ENERGY",
            "CAMTD3": "CAM_TD3",
            "CAM_TD3": "CAM_TD3",
            "HYBRID_EDGE-TD3": "CAM_TD3",
        }
        alias_key = normalized_algorithm.replace('_', '')
        self.algorithm = alias_map.get(normalized_algorithm, alias_map.get(alias_key, normalized_algorithm))
        self._apply_optimized_td3_defaults()
        scenario_config = _build_scenario_config()
        # æ´æ—‚æ•¤æ¾¶æ ­å„´ç‘•å—™æ´Š
        central_env_value = os.environ.get('CENTRAL_RESOURCE', '')
        self.central_resource_enabled = central_env_value.strip() in {'1', 'true', 'True'}
        self.joint_controller = bool(joint_controller)
        if self.joint_controller and not self.central_resource_enabled:
            os.environ['CENTRAL_RESOURCE'] = '1'
            self.central_resource_enabled = True

        if override_scenario:
            scenario_config.update(override_scenario)
            scenario_config['override_topology'] = True
            
            # é¦ƒæ•¡ éæŠ½æ•­æ·‡î†¼î˜²é”›æ°¬å§©é¬ä½·æ…¨é€ç‘°åçâ‚¬configæµ ãƒ¦æ•®é¸ä½¸å¼¬éæ‹Œî›«é©?
            # é˜ç†·æ´œé”›æ­‚odeç»«è®³å¨‡é¢ã„¥åçâ‚¬configé‘°å²„æ½ªscenario_config
            network_cfg = getattr(config, "network", None)

            def _sync_topology(attr_name: str, component_attr: str, dict_key: str, value: int) -> None:
                setattr(config, attr_name, value)
                if network_cfg is not None:
                    setattr(network_cfg, attr_name, value)
                    component_cfg = getattr(network_cfg, component_attr, None)
                    if isinstance(component_cfg, dict):
                        component_cfg[dict_key] = value
            
            # é·æ’´å¢¤éä¼´å™ºé™å‚›æšŸ
            if 'num_vehicles' in override_scenario:
                num_vehicles_override = int(override_scenario['num_vehicles'])
                _sync_topology('num_vehicles', 'vehicle_config', 'num_vehicles', num_vehicles_override)
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†¿æº…æˆå—˜æšŸé–²? {num_vehicles_override}")
                
                # é¦ƒæ•¡ é‚æ¿î–ƒé”›æ°­ç‰´é¹î†¿æº…æˆå—˜æšŸé”ã„¦â‚¬ä½½çšŸéå¯¸æ´°éå›§â‚¬?
                # æµ¼æ‰®ç•»é”›æ°­ç˜¡æï¹ç· ç»¾?0.3s éƒè·ºæ¬¢, 1000J é‘³å€Ÿâ‚¬?
                if os.environ.get('RL_LATENCY_TARGET') is None:  # æµ å‘­ç¶‹éˆî…å¢œé”ã„¦å¯šç€¹æ°­æ¤‚
                    auto_latency_target = 0.5 + num_vehicles_override * 0.15  # 6æï¸¹å¢—1.4s, 12æï¸¹å¢—2.3s, 20æï¸¹å¢—3.5s
                    config.rl.latency_target = auto_latency_target
                    config.rl.latency_upper_tolerance = auto_latency_target * 2.5
                    print(f"  éˆ«?é‘·î„å§©ç’‹å†©æš£ latency_target: {auto_latency_target:.2f}s")
                
                if os.environ.get('RL_ENERGY_TARGET') is None:  # æµ å‘­ç¶‹éˆî…å¢œé”ã„¦å¯šç€¹æ°­æ¤‚
                    auto_energy_target = num_vehicles_override * 800.0  # 6æï¸¹å¢—4800J, 12æï¸¹å¢—9600J, 20æï¸¹å¢¾16000J
                    config.rl.energy_target = auto_energy_target
                    config.rl.energy_upper_tolerance = auto_energy_target * 2.0
                    print(f"  éˆ«?é‘·î„å§©ç’‹å†©æš£ energy_target: {auto_energy_target:.0f}J")
                
                # éšå±¾î„é’æ¿åçâ‚¬æ¿‚æ §å§³ç’ï¼„ç•»é£?
                try:
                    from utils.unified_reward_calculator import update_reward_targets
                    update_reward_targets(
                        latency_target=float(config.rl.latency_target),
                        energy_target=float(config.rl.energy_target)
                    )
                except Exception as e:
                    print(f"  éˆ¿ç‹…ç¬  æ¿‚æ §å§³é©î†½çˆ£éšå±¾î„æ¾¶è¾«è§¦: {e}")
            if 'num_rsus' in override_scenario:
                num_rsus_override = int(override_scenario['num_rsus'])
                _sync_topology('num_rsus', 'rsu_config', 'num_rsus', num_rsus_override)
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒç”ŠSUéä¼´å™º: {num_rsus_override}")
            if 'num_uavs' in override_scenario:
                num_uav_override = int(override_scenario['num_uavs'])
                _sync_topology('num_uavs', 'uav_config', 'num_uavs', num_uav_override)
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒç”AVéä¼´å™º: {num_uav_override}")

            # ç”¯ï¹€î†”é™å‚›æšŸ
            if 'bandwidth' in override_scenario or 'total_bandwidth' in override_scenario:
                bw_value = override_scenario.get('total_bandwidth') or override_scenario.get('bandwidth')
                if bw_value:
                    config.communication.total_bandwidth = float(bw_value)
                    network_comm_cfg = getattr(network_cfg, "communication_config", None)
                    if isinstance(network_comm_cfg, dict):
                        network_comm_cfg['bandwidth'] = float(bw_value)
                    # é¦ƒæ•¡ éæŠ½æ•­æ·‡î†¼î˜²é”›æ°¬æ‚“å§ãƒ¥åŸŒscenario_configé”›å²€â€˜æ·‡æ¿…è±¢éªç†·æ«’æµ£è·¨æ•¤å§ï½‡â€˜é¨å‹«ç”«ç€¹?
                    scenario_config['total_bandwidth'] = float(bw_value)
                    scenario_config['bandwidth'] = float(bw_value)  # éç…î†æ¶“ã‚‡î’é›è—‰æ‚•
                    print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†¼ç”«ç€¹? {float(bw_value)/1e6:.1f} MHz")
            
            # é¦ƒå¹† é¬æ˜ç¥«å©§æ„­çœé™å‚›æšŸé”›å œç´­éå ¢éª‡æ¥‚æ¨¹ç°¬é—æ›¡å¦­éå½’î•¶éœå›·ç´š
        if override_scenario is not None and 'total_vehicle_compute' in override_scenario:
            total_compute = float(override_scenario['total_vehicle_compute'])
            config.compute.total_vehicle_compute = total_compute
            # é‘·î„å§©ç’ï¼„ç•»å§£å¿šæº…éªå†²æ½æ£°æˆ å·¼
            avg_freq = total_compute / config.num_vehicles
            config.compute.vehicle_initial_freq = avg_freq
            config.compute.vehicle_default_freq = avg_freq
            config.compute.vehicle_cpu_freq = avg_freq
            config.compute.vehicle_cpu_freq_range = (avg_freq, avg_freq)
            # éšå±¾î„ scenario_configé”›å±¼è±¢éªç†·æ«’ override_topology=True éƒå‰æ´¿éºãƒ¨î‡°é™æ ¬ç¹–æµœæ¶˜â‚¬?
            scenario_config['total_vehicle_compute'] = total_compute
            scenario_config['vehicle_cpu_freq'] = avg_freq
            scenario_config['vehicle_default_freq'] = avg_freq
            scenario_config['vehicle_initial_freq'] = avg_freq
            print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†½â‚¬ç»˜æ¹°é¦æ‹Œî…¸ç» ? {total_compute/1e9:.1f} GHz (å§£å¿šæº…{avg_freq/1e9:.3f} GHz)")

        if override_scenario is not None and 'total_rsu_compute' in override_scenario:
            total_compute = float(override_scenario['total_rsu_compute'])
            config.compute.total_rsu_compute = total_compute
            avg_freq = total_compute / config.num_rsus
            config.compute.rsu_initial_freq = avg_freq
            config.compute.rsu_default_freq = avg_freq
            config.compute.rsu_cpu_freq = avg_freq
            config.compute.rsu_cpu_freq_range = (avg_freq, avg_freq)
            scenario_config['total_rsu_compute'] = total_compute
            scenario_config['rsu_cpu_freq'] = avg_freq
            scenario_config['rsu_default_freq'] = avg_freq
            scenario_config['rsu_initial_freq'] = avg_freq
            print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†½â‚¬ç±–SUç’ï¼„ç•»: {total_compute/1e9:.1f} GHz (å§£å»ŸSU{avg_freq/1e9:.1f} GHz)")

        if override_scenario is not None and 'total_uav_compute' in override_scenario:
            total_compute = float(override_scenario['total_uav_compute'])
            config.compute.total_uav_compute = total_compute
            avg_freq = total_compute / config.num_uavs
            config.compute.uav_initial_freq = avg_freq
            config.compute.uav_default_freq = avg_freq
            config.compute.uav_cpu_freq = avg_freq
            config.compute.uav_cpu_freq_range = (avg_freq, avg_freq)
            scenario_config['total_uav_compute'] = total_compute
            scenario_config['uav_cpu_freq'] = avg_freq
            scenario_config['uav_default_freq'] = avg_freq
            scenario_config['uav_initial_freq'] = avg_freq
            print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†½â‚¬ç±™AVç’ï¼„ç•»: {total_compute/1e9:.1f} GHz (å§£å»¢AV{avg_freq/1e9:.1f} GHz)")

        # CPUæ£°æˆ å·¼é™å‚›æšŸé”›å å´Ÿé‘ºå‚œå£æ£°æˆ å·¼é”›å±½å‹ç€¹è§„æ£«æµ ï½‡çˆœé”›?
        if override_scenario is not None and 'vehicle_cpu_freq' in override_scenario and 'total_vehicle_compute' not in override_scenario:
            freq_value = override_scenario['vehicle_cpu_freq']
            # é‡å­˜æŸŠé‘¼å†¨æ´¿éœå²„ç²¯ç’ã‚…â‚¬?
            config.compute.vehicle_cpu_freq_range = (freq_value, freq_value)
            config.compute.vehicle_cpu_freq = freq_value
            scenario_config['vehicle_cpu_freq'] = freq_value
            scenario_config.setdefault('vehicle_default_freq', freq_value)
            scenario_config.setdefault('vehicle_initial_freq', freq_value)
            print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†¿æº…æˆå’°PUæ£°æˆ å·¼: {float(freq_value)/1e9:.2f} GHz")

        if override_scenario is not None and 'rsu_cpu_freq' in override_scenario and 'total_rsu_compute' not in override_scenario:
            freq_value = override_scenario['rsu_cpu_freq']
            config.compute.rsu_cpu_freq_range = (freq_value, freq_value)
            config.compute.rsu_cpu_freq = freq_value
            scenario_config['rsu_cpu_freq'] = freq_value
            scenario_config.setdefault('rsu_default_freq', freq_value)
            scenario_config.setdefault('rsu_initial_freq', freq_value)
            print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒç”ŠSU CPUæ£°æˆ å·¼: {float(freq_value)/1e9:.2f} GHz")

        if override_scenario is not None and 'uav_cpu_freq' in override_scenario and 'total_uav_compute' not in override_scenario:
            freq_value = override_scenario['uav_cpu_freq']
            config.compute.uav_cpu_freq_range = (freq_value, freq_value)
            config.compute.uav_cpu_freq = freq_value
            scenario_config['uav_cpu_freq'] = freq_value
            scenario_config.setdefault('uav_default_freq', freq_value)
            scenario_config.setdefault('uav_initial_freq', freq_value)
            print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒç”AV CPUæ£°æˆ å·¼: {float(freq_value)/1e9:.2f} GHz")
            
            # æµ è¯²å§Ÿéç‰ˆåµæ¾¶Ñƒçš¬é™å‚›æšŸ
            if override_scenario is not None and ('task_data_size_min_kb' in override_scenario or 'task_data_size_max_kb' in override_scenario):
                min_kb = override_scenario.get('task_data_size_min_kb')
                max_kb = override_scenario.get('task_data_size_max_kb')
                if min_kb is not None and max_kb is not None:
                    # æî„å´²æ¶“å“„ç“§é‘º?
                    min_bytes = float(min_kb) * 1024
                    max_bytes = float(max_kb) * 1024
                    config.task.data_size_range = (min_bytes, max_bytes)
                    config.task.task_data_size_range = (min_bytes, max_bytes)
                    print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†»æ¢é”â„ƒæšŸé¹î†¼ã‡ç? {min_kb}-{max_kb} KB")
            
            # æµ è¯²å§Ÿæ¾¶å¶†æ½…æ´ï¹€å¼¬é?
            if override_scenario is not None and 'task_complexity_multiplier' in override_scenario:
                multiplier = override_scenario['task_complexity_multiplier']
                # é–«æ°³ç¹ƒéœîˆšî•¨é™æ©€å™ºæµ¼çŠ»â‚¬æ”ç²°TaskConfig
                os.environ['TASK_COMPLEXITY_MULTIPLIER'] = str(multiplier)
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†»æ¢é”â€³î˜²é‰å‚šå®³éŠå¶†æšŸ: {multiplier}x")
            
            if override_scenario is not None and 'task_compute_density' in override_scenario:
                density = override_scenario['task_compute_density']
                config.task.task_compute_density = int(float(density))  # type: ignore
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†»æ¢é”Â¤î…¸ç» æ¥€ç˜‘æ´? {density} cycles/bit")
            
            # ç¼‚æ’³ç“¨ç€¹å½’å™ºé™å‚›æšŸ
            if override_scenario is not None and 'cache_capacity' in override_scenario:
                capacity_mb = override_scenario['cache_capacity']
                # é–«æ°³ç¹ƒéœîˆšî•¨é™æ©€å™ºæµ¼çŠ»â‚¬æç´™è¤°åæ·éµâ‚¬éˆå¤å¦­éç™¸ç´š
                os.environ['CACHE_CAPACITY_MB'] = str(capacity_mb)
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†¾ç´¦ç€›æ¨ºî†é–²? {capacity_mb} MB")

            # éˆå¶…å§Ÿé‘³è—‰å§é™å‚›æšŸ
            if override_scenario is not None and 'rsu_base_service' in override_scenario:
                value = int(override_scenario['rsu_base_service'])
                config.service.rsu_base_service = value
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒç”ŠSUé©è™¹î”…éˆå¶…å§Ÿé‘³è—‰å§: {value}")
            if override_scenario is not None and 'rsu_max_service' in override_scenario:
                value = int(override_scenario['rsu_max_service'])
                config.service.rsu_max_service = value
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒç”ŠSUéˆâ‚¬æ¾¶Ñ„æ¹‡é”Â¤å…˜é”? {value}")
            if override_scenario is not None and 'rsu_work_capacity' in override_scenario:
                value = float(override_scenario['rsu_work_capacity'])
                config.service.rsu_work_capacity = value
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒç”ŠSUå®¸ãƒ¤ç¶”ç€¹å½’å™º: {value}")
            if override_scenario is not None and 'uav_base_service' in override_scenario:
                value = int(override_scenario['uav_base_service'])
                config.service.uav_base_service = value
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒç”AVé©è™¹î”…éˆå¶…å§Ÿé‘³è—‰å§: {value}")
            if override_scenario is not None and 'uav_max_service' in override_scenario:
                value = int(override_scenario['uav_max_service'])
                config.service.uav_max_service = value
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒç”AVéˆâ‚¬æ¾¶Ñ„æ¹‡é”Â¤å…˜é”? {value}")
            if override_scenario is not None and 'uav_work_capacity' in override_scenario:
                value = float(override_scenario['uav_work_capacity'])
                config.service.uav_work_capacity = value
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒç”AVå®¸ãƒ¤ç¶”ç€¹å½’å™º: {value}")
            
            # æµ è¯²å§Ÿé’æ‹Œæªéœå›§å¼¬é?
            if override_scenario is not None and 'task_arrival_rate' in override_scenario:
                arrival_rate = override_scenario['task_arrival_rate']
                config.task.arrival_rate = float(arrival_rate)
                # éšå±¾æ¤‚ç’å‰§ç–†éœîˆšî•¨é™æ©€å™ºæµ ãƒ¥å‹ç€¹è§„æ£«æµ ï½‡çˆœ
                os.environ['TASK_ARRIVAL_RATE'] = str(arrival_rate)
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†»æ¢é”â€³åŸŒæˆå‰§å·¼: {arrival_rate} tasks/s")
            
            # é—æ›šç«´æµ è¯²å§Ÿéç‰ˆåµæ¾¶Ñƒçš¬é™å‚›æšŸé”›å ¢æ•¤æµœåº¢è´©éšå £ç¤‹æè—‰ç–„æ¥ å²‹ç´š
            if override_scenario is not None and 'task_data_size_kb' in override_scenario:
                size_kb = override_scenario['task_data_size_kb']
                size_bytes = float(size_kb) * 1024
                config.task.data_size_range = (size_bytes, size_bytes)
                config.task.task_data_size_range = (size_bytes, size_bytes)
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†»æ¢é”â„ƒæšŸé¹î†¼ã‡ç? {size_kb} KB")
            
            # é–«æ°«ä¿Šé™å‚›æšŸé”›å æ«”æ¾¹æ¿å§›éœå›¥â‚¬ä½½çŸ¾å¯°å‹¬å´¯é‘°æ¥‹ç´š
            if override_scenario is not None and 'noise_power_dbm' in override_scenario:
                noise_power = override_scenario['noise_power_dbm']
                setattr(config.communication, 'noise_power_dbm', float(noise_power))  # type: ignore
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†¼æ«”æ¾¹æ¿å§›éœ? {noise_power} dBm")
            
            if override_scenario is not None and 'path_loss_exponent' in override_scenario:
                exponent = override_scenario['path_loss_exponent']
                setattr(config.communication, 'path_loss_exponent', float(exponent))  # type: ignore
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†¿çŸ¾å¯°å‹¬å´¯é‘°æ¥å¯šé? {exponent}")
            
            # ç’§å‹¬ç°®å¯®å‚›ç€¯é¬Ñƒå¼¬é?
            if override_scenario is not None and 'heterogeneity_level' in override_scenario:
                hetero_level = override_scenario['heterogeneity_level']
                os.environ['HETEROGENEITY_LEVEL'] = str(hetero_level)
                print(f"é¦ƒæ•¡ [Override] é”ã„¦â‚¬ä½½î†•ç¼ƒî†¿ç¥«å©§æ„¬ç´“é‹å‹¬â‚¬Ñ…éª‡é’? {hetero_level}")
        
        mode_aliases = {
            'local': 'local_only',
            'local_only': 'local_only',
            'remote': 'remote_only',
            'remote_only': 'remote_only',
            '': ''
        }
        forced_mode_input = (
            enforce_offload_mode
            or scenario_config.get('forced_offload_mode')
            or os.environ.get('FORCE_OFFLOAD_MODE', '')
        )
        requested_mode = mode_aliases.get(str(forced_mode_input).strip().lower(), '')
        if requested_mode not in {'', 'local_only', 'remote_only'}:
            print(f"éˆ¿ç‹…ç¬ éˆî‡ç˜‘é’î‚¤æ®‘å¯®å“„åŸ—é—æ­Œæµ‡å¦¯â€³ç´¡: {forced_mode_input}, çå——æ‹·é£ãƒ£â‚¬?)
            requested_mode = ''
        self.enforce_offload_mode = requested_mode
        if self.enforce_offload_mode:
            scenario_config['forced_offload_mode'] = self.enforce_offload_mode
            if self.enforce_offload_mode == 'remote_only':
                scenario_config.setdefault('allow_local_processing', False)
            elif self.enforce_offload_mode == 'local_only':
                scenario_config.setdefault('allow_local_processing', True)

        if self.enforce_offload_mode == 'local_only':
            print("é¦ƒĞ¥ å¯®å“„åŸ—é—æ­Œæµ‡å¦¯â€³ç´¡: éã„©å„´éˆî„€æ¹´æ¾¶å‹­æ‚Šé”›åœ ocal-Onlyé”›?)
        elif self.enforce_offload_mode == 'remote_only':
            print("é¦ƒĞ¥ å¯®å“„åŸ—é—æ­Œæµ‡å¦¯â€³ç´¡: éã„©å„´æ©æ»…î¬éµÑ†î”‘é”›åœ§emote-Onlyé”›?)
        
        # é¦ƒå¹† é¥å“„ç•¾é—æ­Œæµ‡ç»›æ «æšé’æ¿†îé–?
        self.fixed_offload_policy = None
        self.fixed_policy_name = None
        if fixed_offload_policy:
            try:
                import sys
                import importlib.util
                from pathlib import Path
                
                # é”ã„¦â‚¬ä½¹åŠé”?experiments é©î†¼ç¶é’?Python ç’ºîˆšç·
                exp_path = Path(__file__).parent / 'experiments'
                if str(exp_path) not in sys.path:
                    sys.path.insert(0, str(exp_path))
                
                # æµ£è·¨æ•¤ importlib é”ã„¦â‚¬ä½¸î‡±éãƒ¦Äé§æ¥‹ç´™é–¬å®å¤é—ˆæ¬â‚¬ä½¸åé‹æ„¯î„Ÿé›å©ç´š
                module_path = exp_path / 'fallback_baselines.py'
                if module_path.exists():
                    spec = importlib.util.spec_from_file_location("fallback_baselines", module_path)
                    if spec and spec.loader:
                        fallback_module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(fallback_module)
                        create_baseline_algorithm = fallback_module.create_baseline_algorithm
                    else:
                        raise ImportError(f"éƒçŠ³ç¡¶é”çŠºæµ‡å¦¯â€³æ½¡ {module_path}")
                else:
                    raise ImportError(f"å¦¯â€³æ½¡é‚å›¦æ¬¢æ¶“å¶…ç“¨é¦? {module_path}")
                
                self.fixed_offload_policy = create_baseline_algorithm(fixed_offload_policy)
                self.fixed_policy_name = fixed_offload_policy
                print(f"é¦ƒå¹‰ é¥å“„ç•¾é—æ­Œæµ‡ç»›æ «æš: {fixed_offload_policy} (é—æ­Œæµ‡éå´‡ç“¥æ¶“å¶‡æ•±é…é¸¿å…˜æµ£æ’³î„Ÿæ¶”?")
                print(f"   éæœµç²¬éå´‡ç“¥é”›å ¢ç´¦ç€›æ¨¸â‚¬ä½½ç¸¼ç»‰æ±‡â‚¬ä½½ç¥«å©§æ„¬åé–°å¶ç´šæµ å¶‡æ•±é…é¸¿å…˜æµ£æ’³î„Ÿæ¶”?)
            except Exception as e:
                print(f"éˆ¿ç‹…ç¬  éƒçŠ³ç¡¶é’æ¶˜ç¼“é¥å“„ç•¾ç»›æ «æš '{fixed_offload_policy}': {e}")
                print(f"   çå—•å¨‡é¢ã„¦æ«¤é‘³æˆ’ç¶‹ç€›ï¸¿ç¯„é—æ­Œæµ‡éå´‡ç“¥")
                self.fixed_offload_policy = None
        
        # é–«å¤‹å«¨æµ è·¨æ¹¡é£ã„§è¢«é¨?
        self.use_enhanced_cache = use_enhanced_cache and ENHANCED_CACHE_AVAILABLE
        env_disable_migration = os.environ.get("DISABLE_MIGRATION", "").strip() == "1"
        self.disable_migration = disable_migration or env_disable_migration
        
        # é¦ƒæ•¡ é‚æ¿î–ƒé”›æ°¬î›§é‹æ»„æ¹­é–«æ°³ç¹ƒoverrideç’å‰§ç–†é©î†½çˆ£éŠç¡·ç´éè§„åµè¤°æ’³å¢ æï¹ç· éæ‹Œåšœé”ã„¨çšŸé?
        if 'num_vehicles' not in (override_scenario or {}):
            current_num_vehicles = scenario_config.get('num_vehicles', config.num_vehicles)
            if os.environ.get('RL_LATENCY_TARGET') is None:
                auto_latency_target = 0.5 + current_num_vehicles * 0.15
                config.rl.latency_target = auto_latency_target
                config.rl.latency_upper_tolerance = auto_latency_target * 2.5
                print(f"é¦ƒå¹† é‘·î„å§©ç’‹å†©æš£ latency_target: {auto_latency_target:.2f}s (é©è½°ç°¬{current_num_vehicles}æˆå—šæº…)")
            
            if os.environ.get('RL_ENERGY_TARGET') is None:
                auto_energy_target = current_num_vehicles * 800.0
                config.rl.energy_target = auto_energy_target
                config.rl.energy_upper_tolerance = auto_energy_target * 2.0
                print(f"é¦ƒå¹† é‘·î„å§©ç’‹å†©æš£ energy_target: {auto_energy_target:.0f}J (é©è½°ç°¬{current_num_vehicles}æˆå—šæº…)")
            
            # éšå±¾î„é’æ¿åçâ‚¬æ¿‚æ §å§³ç’ï¼„ç•»é£?
            try:
                from utils.unified_reward_calculator import update_reward_targets
                update_reward_targets(
                    latency_target=float(config.rl.latency_target),
                    energy_target=float(config.rl.energy_target)
                )
            except Exception:
                pass
        
        simulator: CompleteSystemSimulator
        if self.use_enhanced_cache:
            print("é¦ƒæ®Œ [Training] Using Enhanced Cache System (Default) with:")
            print("   - Hierarchical L1/L2 caching (3GB + 7GB)")
            print("   - Adaptive HeatBasedCacheStrategy")
            print("   - Inter-RSU collaboration")
            simulator = EnhancedSystemSimulator(scenario_config)  # type: ignore[assignment]
        else:
            simulator = CompleteSystemSimulator(scenario_config)
        self.simulator: CompleteSystemSimulator = simulator
        
        # é¦ƒî˜» é’æ¿†îé–æ ¬åšœé–«å‚šç°²éºÑƒåŸ—ç¼å‹ªæ¬¢
        self.adaptive_cache_controller = AdaptiveCacheController()
        self.adaptive_migration_controller = AdaptiveMigrationController()
        if self.disable_migration:
            print("é¦ƒî˜» é‘·îˆâ‚¬å‚šç°²ç¼‚æ’³ç“¨å®¸æ’æƒé¢îŸ’ç´±æ©ä½ºĞ©éºÑƒåŸ—å®¸èŒ¬î›¦é¢îŸ’ç´™DISABLE_MIGRATION å¦¯â€³ç´¡é”›?)
        else:
            print(f"é¦ƒî˜» å®¸æ’æƒé¢ã„¨åšœé–«å‚šç°²ç¼‚æ’³ç“¨éœå²ƒç¸¼ç»‰ç»˜å¸¶é’è·ºå§›é‘³?)

        self.strategy_coordinator = StrategyCoordinator(
            self.adaptive_cache_controller,
            None if self.disable_migration else self.adaptive_migration_controller
        )
        self.strategy_coordinator.register_simulator(self.simulator)
        setattr(self.simulator, 'strategy_coordinator', self.strategy_coordinator)
        
        # æµ åºè±¢éªç†·æ«’é‘¾å³°å½‡ç€¹ç‚ºæª¯ç¼ƒæˆ ç²¶é·æ’´å¢¤é™å‚›æšŸ
        num_vehicles = len(self.simulator.vehicles)
        num_rsus = len(self.simulator.rsus)
        num_uavs = len(self.simulator.uavs)
        self.num_vehicles = num_vehicles
        self.num_rsus = num_rsus
        self.num_uavs = num_uavs
        
        # é¦ƒå¹† é‡å­˜æŸŠé¥å“„ç•¾ç»›æ «æšé¨å‹­å¹†æ¾§å†§ä¿Šé­?
        if self.fixed_offload_policy is not None:
            try:
                # é’æ¶˜ç¼“æ¶“â‚¬æ¶“î†ç•é–æ «æ®‘éœîˆšî•¨ç€µç¡…è–„æ¸šæ¶˜æµç€¹æ°±ç“¥é£ãƒ¤å¨‡é¢?
                class SimpleEnv:
                    def __init__(self, simulator):
                        self.simulator = simulator
                        self.agent_env = type('obj', (object,), {
                            'action_dim': 18,  # æ¦›æ¨¿î…»actionç¼æ‘å®³
                        })()
                
                simple_env = SimpleEnv(self.simulator)
                self.fixed_offload_policy.update_environment(simple_env)
                print(f"   é¥å“„ç•¾ç»›æ «æšå®¸å‰æ´¿é‚æ‰®å¹†æ¾§å†§ä¿Šé­? {num_vehicles}æï¹ç· , {num_rsus}RSU, {num_uavs}UAV")
            except Exception as e:
                print(f"éˆ¿ç‹…ç¬  é¥å“„ç•¾ç»›æ «æšé‡å­˜æŸŠéœîˆšî•¨æ¾¶è¾«è§¦: {e}")
        
        # æ´æ—‚æ•¤é¥å“„ç•¾é·æ’´å¢¤é¨å‹«å¼¬éé¢ç´­é–æ µç´™æ·‡æ¿‡å¯”4 RSU + 2 UAVé”›?
        if self.algorithm in {"TD3", "TD3_LATENCY_ENERGY"}:
            topology_optimizer = FixedTopologyOptimizer()
            opt_params = topology_optimizer.get_optimized_params(num_vehicles)
            
            # æ´æ—‚æ•¤æµ¼æ¨ºå¯²é¨å‹®ç§´é™å‚›æšŸé’ç™ŸD3é–°å¶‡ç–†
            os.environ['TD3_HIDDEN_DIM'] = str(opt_params.get('hidden_dim', 400))
            os.environ['TD3_ACTOR_LR'] = str(opt_params.get('actor_lr', 1e-4))
            os.environ['TD3_CRITIC_LR'] = str(opt_params.get('critic_lr', 8e-5))
            os.environ['TD3_BATCH_SIZE'] = str(opt_params.get('batch_size', 256))
            
            print(f"[FIXED-TOPOLOGY] æï¹ç· é?{num_vehicles} éˆ«?Hidden:{opt_params['hidden_dim']}, LR:{opt_params['actor_lr']:.1e}, Batch:{opt_params['batch_size']}")
            print(f"[FIXED-TOPOLOGY] æ·‡æ¿‡å¯”é¥å“„ç•¾: RSU=4, UAV=2é”›å ¥ç™ç’‡ä½ºç•»å¨‰æ› ç“¥é£ãƒ¦æ¹éå Ÿâ‚¬Ñç´š")
        
        # é¦ƒæ•¡ æµ¼æ¨ºå¯²é”›æ°­å¢éˆå¤Œç•»å¨‰æ› ç²ºæ¶“â‚¬æµ¼çŠ²å†é·æ’´å¢¤é™å‚›æšŸé”›å±½ç–„éœæ¿å§©é¬ä¾€â‚¬å‚å¤
        if self.algorithm == "DDPG":
            self.agent_env = DDPGEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "TD3":
            # TD3æ¦›æ¨¿î…»éšîˆœæ•¤æ¶“î…ãç’§å‹¬ç°®å¦¯â€³ç´¡é”›å å½²é–«æ°³ç¹ƒéœîˆšî•¨é™æ©€å™ºCENTRAL_RESOURCE=0ç»‚ä½ºæ•¤é”›?
            if not self.central_resource_enabled:
                central_env_override = os.environ.get('CENTRAL_RESOURCE', '1')  # æ¦›æ¨¿î…»éšîˆœæ•¤
                self.central_resource_enabled = central_env_override.strip() in {'1', 'true', 'True'}
            self.agent_env = TD3Environment(
                num_vehicles,
                num_rsus,
                num_uavs,
                use_central_resource=self.central_resource_enabled,
            )
        elif self.algorithm == "TD3_LATENCY_ENERGY":
            self.agent_env = TD3LatencyEnergyEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "CAM_TD3":
            # CAM_TD3æ¦›æ¨¿î…»éšîˆœæ•¤æ¶“î…ãç’§å‹¬ç°®å¦¯â€³ç´¡é”›å å½²é–«æ°³ç¹ƒéœîˆšî•¨é™æ©€å™ºCENTRAL_RESOURCE=0ç»‚ä½ºæ•¤é”›?
            if not self.central_resource_enabled:
                central_env_override = os.environ.get('CENTRAL_RESOURCE', '1')  # æ¦›æ¨¿î…»éšîˆœæ•¤
                self.central_resource_enabled = central_env_override.strip() in {'1', 'true', 'True'}
            self.agent_env = CAMTD3Environment(
                num_vehicles, num_rsus, num_uavs,
                use_central_resource=self.central_resource_enabled
            )
        elif self.algorithm == "DQN":
            self.agent_env = DQNEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "PPO":
            self.agent_env = PPOEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "SAC":
            self.agent_env = SACEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "OPTIMIZED_TD3":
            # ç»®å‰§ç•æµ¼æ¨ºå¯²TD3 (Queue-aware Replay + GNN Attention)
            self.agent_env = OptimizedTD3Environment(
                num_vehicles,
                num_rsus,
                num_uavs,
                use_central_resource=self.central_resource_enabled
            )
            print(f"[OptimizedTD3] æµ£è·¨æ•¤ç»®å‰§ç•æµ¼æ¨ºå¯²é–°å¶‡ç–† (Queue+GNN)")
        else:
            raise ValueError(f"æ¶“å¶†æ•®é¸ä½ºæ®‘ç» æ¥ç¡¶: {algorithm}")

        # é¦ƒå¹† æ¶“î…ãç’§å‹¬ç°®é’å—›å¤å¦¯â€³ç´¡éƒãƒ¥ç¹”
        import sys
        print(f"\n[ç’§å‹¬ç°®é’å—›å¤å¦¯â€³ç´¡å¦«â‚¬éŒî™£", file=sys.stderr)
        print(f"  CENTRAL_RESOURCE éœîˆšî•¨é™æ©€å™º: '{central_env_value}'", file=sys.stderr)
        print(f"  use_central_resource: {self.central_resource_enabled}", file=sys.stderr)
        
        self.central_resource_action_dim = getattr(self.agent_env, 'central_resource_action_dim', 0)
        self.central_resource_state_dim = getattr(self.agent_env, 'central_state_dim', 0)
        self.base_action_dim = getattr(self.agent_env, 'base_action_dim', getattr(self.agent_env, 'action_dim', 0) - self.central_resource_action_dim)
        
        if self.central_resource_enabled and self.central_resource_action_dim > 0:
            print(f"é‰?éšîˆœæ•¤æ¶“î…ãç’§å‹¬ç°®é’å—›å¤é‹èˆµç€¯é”›æ­…hase 1(éå´‡ç“¥) + Phase 2(éµÑ†î”‘)", file=sys.stderr)
            print(f"   éœîˆšî•¨ç»«è¯²ç€·: {type(self.agent_env).__name__}", file=sys.stderr)
            print(f"   é©è™¹î”…é”ã„¤ç¶”ç¼æ‘å®³: {self.base_action_dim}", file=sys.stderr)
            print(f"   æ¶“î…ãç’§å‹¬ç°®é”ã„¤ç¶”ç¼æ‘å®³: {self.central_resource_action_dim}", file=sys.stderr)
            if self.central_resource_state_dim:
                print(f"   é˜èˆµâ‚¬ä½¹å¢¿çæ› æ·®æ´? +{self.central_resource_state_dim}", file=sys.stderr)
        else:
            print(f"  æµ£è·¨æ•¤éå›§å™¯å¦¯â€³ç´¡é”›å æ½é–â‚¬ç’§å‹¬ç°®é’å—›å¤é”›?, file=sys.stderr)
        
        # é¦ƒî¥ é‘»ãƒ¦å¯šç€¹æ°«ç°¡é—ƒèˆµî†Œæ¶“â‚¬ç» æ¥ç¡¶é”›å ¥â‚¬æ°³ç¹ƒéœîˆšî•¨é™æ©€å™ºé”›å¤›ç´é¢â€•ualStageçä½½î—Šé£ã„§ç²éšå œè¢±æ¶“îˆæ¨å¨ˆ?
        stage1_alg = os.environ.get('STAGE1_ALG', '').strip().lower()
        if stage1_alg:
            try:
                from single_agent.dual_stage_controller import DualStageControllerEnv
                self.agent_env = DualStageControllerEnv(self.agent_env, self.simulator, stage1_strategy=stage1_alg)
                print(f"é¦ƒî¥ éšîˆœæ•¤æ¶“ã‚‰æ¨å¨ˆå«å¸¶é’è®¹ç´°Stage1={stage1_alg} + Stage2={self.algorithm}")
                # Two-stage planner inside simulator becomes redundant
                os.environ['TWO_STAGE_MODE'] = '0'
            except Exception as e:
                print(f"éˆ¿ç‹…ç¬ æ¶“ã‚‰æ¨å¨ˆå«å¸¶é’è·ºçšç‘å‘­ã‘ç’ãƒ¯ç´é¥ç‚ºâ‚¬â‚¬é’æ¿å´Ÿç» æ¥ç¡¶: {e}")
        
        # ç’î… ç²Œç¼ç†»î…¸
        self.episode_rewards = []
        self.episode_losses = {}
        self.episode_metrics = {
            'avg_delay': [],
            'total_energy': [],
            'data_loss_bytes': [],
            'data_loss_ratio_bytes': [],
            'task_completion_rate': [],
            'cache_hit_rate': [],
            'cache_utilization': [],
            'cache_evictions': [],
            'cache_eviction_rate': [],
            'cache_requests': [],
            'cache_collaborative_writes': [],
            'local_cache_hits': [],
            'migration_avg_cost': [],
            'migration_avg_delay_saved': [],
            'migration_success_rate': [],
            'queue_rho_sum': [],
            'queue_rho_max': [],
            'queue_overload_flag': [],  # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°±â€˜æ·‡æ¿Šî†‡è¤°æ›šç°©éŠè‰°ç¹ƒæèŠ¥çˆ£è¹‡?
            'queue_overload_events': [],  # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°³î†‡è¤°æ› ç–®ç’Â¤ç¹ƒææˆ’ç°¨æµ èˆµæšŸ
            'episode_steps': [],  # é¦ƒæ•¡ é‚æ¿î–ƒé”›æ°³î†‡è¤°æ›Ÿç˜¡æ¶“çŒ pisodeé¨å‹«ç–„é—„å‘®î„é?
            'avg_step_reward': [],  # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°³î†‡è¤°æ›é’©é§å›¨ç˜¡å§ãƒ¥îš›é”?
            'task_type_queue_share_1': [],
            'task_type_queue_share_2': [],
            'task_type_queue_share_3': [],
            'task_type_queue_share_4': [],
            'task_type_deadline_norm_1': [],
            'task_type_deadline_norm_2': [],
            'task_type_deadline_norm_3': [],
            'task_type_deadline_norm_4': [],
            'task_type_drop_rate_1': [],
            'task_type_drop_rate_2': [],
            'task_type_drop_rate_3': [],
            'task_type_drop_rate_4': [],
            'task_type_queue_share_ep_1': [],
            'task_type_queue_share_ep_2': [],
            'task_type_queue_share_ep_3': [],
            'task_type_queue_share_ep_4': [],
            'rsu_hotspot_mean': [],  # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°³î†‡è¤°æ›Ÿç˜¡æ¶“çŒ pisodeé¨å‡´SUé‘î… å£éªå†²æ½å¯®å“„å®³
            'rsu_hotspot_peak': [],  # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°³î†‡è¤°æ›Ÿç˜¡æ¶“çŒ pisodeé¨å‡´SUé‘î… å£å®„æ¿â‚¬ç…å·±æ´?
            'rsu_hotspot_mean_series': [],
            'rsu_hotspot_peak_series': [],
            'mm1_queue_error': [],
            'mm1_delay_error': [],
            'normalized_delay': [],
            'normalized_energy': [],
            'normalized_reward': [],
            # é¦ƒå¹† é‚æ¿î–ƒé”›æ­ŠSUç’§å‹¬ç°®é’â•ƒæ•¤éœå›§æ‹°é—æ­Œæµ‡éœå›©ç²ºç’â˜…ç´™æ·‡î†¼î˜²bugé”›?
            'rsu_utilization': [],
            'offload_ratio': [],  # remote_execution_ratio (rsu+uav)
            'rsu_offload_ratio': [],
            'uav_offload_ratio': [],
            'local_offload_ratio': [],
            # é¦ƒæ®Œ é‚æ¿î–ƒé”›æ°³ç¸¼ç»‰æ˜å…˜é‘°æ¥å¯šé?
            'rsu_migration_energy': [],
            'uav_migration_energy': [],
        }
        
        # é¬Ñ†å…˜æ©å€Ÿé‡œé£?
        self.performance_tracker = {
            'recent_rewards': MovingAverage(100),
            'recent_step_rewards': MovingAverage(100),
            'recent_delays': MovingAverage(100),
            'recent_energy': MovingAverage(100),
            'recent_completion': MovingAverage(100)
        }
        self._reward_baseline: Dict[str, float] = {}
        self._energy_target_per_vehicle = float(os.environ.get('ENERGY_TARGET_PER_VEHICLE', 180.0))
        self._dynamic_energy_target = float(getattr(config.rl, 'energy_target', 2200.0))
        heuristic_energy_target = max(
            self._dynamic_energy_target,
            self.num_vehicles * self._energy_target_per_vehicle
        )
        if heuristic_energy_target > self._dynamic_energy_target * 1.02:
            self._dynamic_energy_target = heuristic_energy_target
            update_reward_targets(energy_target=heuristic_energy_target)
            print(
                f"éˆ¿æ µç¬ é”ã„¦â‚¬ä½½çšŸéç£‹å…˜é‘°æ¥ƒæ´°é? {heuristic_energy_target:.1f}J "
                f"(æï¹ç· é?{self.num_vehicles}, å§£å¿šæº…æ£°å‹­ç•»={self._energy_target_per_vehicle:.1f}J)"
            )
        self._energy_target_ema = self._dynamic_energy_target
        self._energy_target_warmup = max(40, int(config.experiment.num_episodes * 0.1))
        self._last_energy_target_update = 0
        # é‘·îˆâ‚¬å‚šç°²å¯¤æƒ°ç¹œé©î†½çˆ£é”›å ¥ç®ç’ç†»æµ‡é¦çƒ˜æ«™é‘·î„å§©é€æƒ§î†”é”›å²„ä¼©éå¶…îš›é”éã‚±éœå²‹ç´š
        self._dynamic_latency_target = float(getattr(config.rl, 'latency_target', 0.4))
        self._delay_target_ema = self._dynamic_latency_target
        self._last_delay_target_update = 0
        self._reward_smoothing_alpha = float(getattr(config.rl, 'reward_smooth_alpha', 0.35))
        self._reward_ema_delay: Optional[float] = None
        self._reward_ema_energy: Optional[float] = None
        self._episode_counters_initialized = False
        
        print(f"é‰?{self.algorithm}ç’î… ç²Œéœîˆšî•¨é’æ¿†îé–æ §ç•¬é´?)
        print(f"é‰?ç» æ¥ç¡¶ç»«è¯²ç€·: é—æ›Ÿæ«¤é‘³æˆ’ç¶‹")
    
    def _calculate_correct_cache_utilization(self, cache: Dict, cache_capacity_mb: float) -> float:
        """
        é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°­î„œçº­î†¿î…¸ç» æ¥ƒç´¦ç€›æ¨ºåŸ„é¢ã„§å·¼
        
        Args:
            cache: ç¼‚æ’³ç“¨ç€›æ¥€å€
            cache_capacity_mb: ç¼‚æ’³ç“¨ç€¹å½’å™º(MB)
        Returns:
            ç¼‚æ’³ç“¨é’â•ƒæ•¤éœ?[0.0, 1.0]
        """
        if not cache or cache_capacity_mb <= 0:
            return 0.0
        
        total_used_mb = 0.0
        for item in cache.values():
            if isinstance(item, dict) and 'size' in item:
                total_used_mb += float(item.get('size', 0.0))
            else:
                # éç…î†éƒÑ„ç‰¸å¯®å¿¥ç´æµ£è·¨æ•¤realisticæ¾¶Ñƒçš¬
                total_used_mb += 1.0  # æ¦›æ¨¿î…»1MB
        
        utilization = total_used_mb / cache_capacity_mb
        return min(1.0, max(0.0, utilization))
    
    def _initialize_episode_counters(self, stats: Optional[Dict[str, Any]] = None) -> None:
        """Reset per-episode baseline counters to avoid carrying over cumulative stats."""
        stats_dict: Dict[str, Any]
        if stats is None:
            stats_dict = {}
        else:
            try:
                stats_dict = dict(stats)
            except Exception:
                stats_dict = {}

        self._episode_energy_base = float(stats_dict.get('total_energy', 0.0) or 0.0)
        self._episode_processed_base = int(stats_dict.get('processed_tasks', 0) or 0)
        self._episode_dropped_base = int(stats_dict.get('dropped_tasks', 0) or 0)
        self._episode_generated_bytes_base = float(stats_dict.get('generated_data_bytes', 0.0) or 0.0)
        self._episode_dropped_bytes_base = float(stats_dict.get('dropped_data_bytes', 0.0) or 0.0)
        remote_stats = stats_dict.get('remote_rejections', {})
        if isinstance(remote_stats, dict):
            self._episode_remote_reject_base = int(remote_stats.get('total', 0) or 0)
        else:
            self._episode_remote_reject_base = 0

        # Cache controllers keep their own cumulative counters; snapshot them as the new baseline
        if hasattr(self, 'adaptive_cache_controller'):
            cache_metrics = self.adaptive_cache_controller.get_cache_metrics()
            self._episode_cache_requests_base = int(cache_metrics.get('total_requests', 0) or 0)
            self._episode_cache_evictions_base = int(cache_metrics.get('evicted_items', 0) or 0)
            self._episode_cache_collab_base = int(cache_metrics.get('collaborative_writes', 0) or 0)
        else:
            self._episode_cache_requests_base = 0
            self._episode_cache_evictions_base = 0
            self._episode_cache_collab_base = 0

        self._episode_queue_overload_events_base = int(stats_dict.get('queue_overload_events', 0) or 0)
        delay_buckets = ('delay_processing', 'delay_uplink', 'delay_downlink', 'delay_cache', 'delay_waiting')
        energy_buckets = ('energy_compute', 'energy_transmit_uplink', 'energy_transmit_downlink', 'energy_cache')
        self._episode_delay_component_base = {
            bucket: float(stats_dict.get(bucket, 0.0) or 0.0) for bucket in delay_buckets
        }
        self._episode_energy_component_base = {
            bucket: float(stats_dict.get(bucket, 0.0) or 0.0) for bucket in energy_buckets
        }
        self._episode_queue_overflow_base = int(stats_dict.get('queue_overflow_drops', 0) or 0)
        
        # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°­åŠé”çŠ²æ¬¢æ©ç†¸â‚¬å©šå™ºé©è™¹åšé”›å²€æ•¤æµœåº¤î…¸ç» æ¢•pisodeéªå†²æ½å¯¤æƒ°ç¹œ
        self._episode_delay_base = float(stats_dict.get('total_delay', 0.0) or 0.0)
        
        # Initialize task count accumulators
        self._episode_local_tasks = 0
        self._episode_rsu_tasks = 0
        self._episode_uav_tasks = 0
        
        self._episode_counters_initialized = True

    def _reset_reward_baseline(self, stats: Optional[Dict[str, Any]] = None) -> None:
        """é’æ¿†îé–?é–²å¶‡ç–†æ¿‚æ §å§³æ¾§ç‚ºå™ºé©è™¹åšéŠ†?""
        def _safe_scalar(value: Any, default: float = 0.0) -> float:
            try:
                val = float(value)
            except (TypeError, ValueError):
                return default
            return val if np.isfinite(val) else default

        def _safe_int(value: Any, default: int = 0) -> int:
            try:
                val = float(value)
            except (TypeError, ValueError):
                return default
            return int(val) if np.isfinite(val) else default

        base = stats or {}
        self._reward_baseline = {
            'processed': _safe_int(base.get('processed_tasks', 0)),
            'dropped': _safe_int(base.get('dropped_tasks', 0)),
            'delay': _safe_scalar(base.get('total_delay', 0.0)),
            'energy': _safe_scalar(base.get('total_energy', 0.0)),
            'generated_bytes': _safe_scalar(base.get('generated_data_bytes', 0.0)),
            'dropped_bytes': _safe_scalar(base.get('dropped_data_bytes', 0.0)),
        }
        self._reward_ema_delay = None
        self._reward_ema_energy = None

    def _build_reward_snapshot(self, stats: Dict[str, Any]) -> Dict[str, float]:
        """é©è½°ç°¬ç»±îˆî…¸ç¼ç†»î…¸ç’ï¼„ç•»é—æ›Ÿî„æ¿‚æ §å§³éµâ‚¬é—‡â‚¬é¨å‹«î–ƒé–²å¿”å¯šéå›¥â‚¬?""
        safe_scalar = lambda v, d=0.0: float(v) if isinstance(v, (int, float, np.floating, np.integer)) and np.isfinite(float(v)) else d  # type: ignore[arg-type]
        def safe_int(v: Any, default: int = 0) -> int:
            try:
                val = float(v)
            except (TypeError, ValueError):
                return default
            return int(val) if np.isfinite(val) else default

        raw_baseline = getattr(self, '_reward_baseline', None) or {}
        baseline = {
            'processed': safe_int(raw_baseline.get('processed', 0)),
            'dropped': safe_int(raw_baseline.get('dropped', 0)),
            'delay': safe_scalar(raw_baseline.get('delay', 0.0)),
            'energy': safe_scalar(raw_baseline.get('energy', 0.0)),
            'generated_bytes': safe_scalar(raw_baseline.get('generated_bytes', 0.0)),
            'dropped_bytes': safe_scalar(raw_baseline.get('dropped_bytes', 0.0)),
        }

        total_processed = safe_int(stats.get('processed_tasks', 0))
        total_dropped = safe_int(stats.get('dropped_tasks', 0))
        total_delay = safe_scalar(stats.get('total_delay', 0.0))
        total_energy = safe_scalar(stats.get('total_energy', 0.0))
        total_generated = safe_scalar(stats.get('generated_data_bytes', 0.0))
        total_dropped_bytes = safe_scalar(stats.get('dropped_data_bytes', 0.0))

        delta_processed = max(0, total_processed - baseline['processed'])
        delta_dropped = max(0, total_dropped - baseline['dropped'])
        delta_delay = max(0.0, total_delay - baseline['delay'])
        delta_energy = max(0.0, total_energy - baseline['energy'])
        delta_generated = max(0.0, total_generated - baseline['generated_bytes'])
        delta_loss_bytes = max(0.0, total_dropped_bytes - baseline['dropped_bytes'])

        tasks_for_delay = delta_processed if delta_processed > 0 else max(1, total_processed)
        avg_delay_increment = delta_delay / max(1, tasks_for_delay)

        completion_total = delta_processed + delta_dropped
        completion_rate = normalize_ratio(delta_processed, completion_total, default=1.0)
        loss_ratio = normalize_ratio(delta_loss_bytes, delta_generated)

        avg_delay_increment = safe_scalar(avg_delay_increment)
        avg_delay_for_reward = avg_delay_increment if avg_delay_increment > 0 else safe_scalar(stats.get('avg_task_delay', 0.0))
        energy_per_task = delta_energy / max(1, delta_processed) if delta_processed > 0 else 0.0
        energy_per_task = safe_scalar(energy_per_task)
        smoothed_delay, smoothed_energy_per_task = self._apply_reward_smoothing(
            avg_delay_for_reward,
            energy_per_task
        )
        smoothed_energy_total = smoothed_energy_per_task * max(1, delta_processed)

        reward_snapshot = {
            'avg_task_delay': smoothed_delay,
            'total_energy_consumption': smoothed_energy_total if smoothed_energy_total > 0 else safe_scalar(stats.get('total_energy_consumption', 0.0)),
            'dropped_tasks': delta_dropped,
            'task_completion_rate': completion_rate,
            'data_loss_bytes': delta_loss_bytes,
            'data_loss_ratio_bytes': loss_ratio,
        }

        self._reward_baseline = {
            'processed': total_processed,
            'dropped': total_dropped,
            'delay': total_delay,
            'energy': total_energy,
            'generated_bytes': total_generated,
            'dropped_bytes': total_dropped_bytes,
        }

        return reward_snapshot

    def _apply_reward_smoothing(self, delay_value: float, energy_per_task: float) -> Tuple[float, float]:
        """ç€µç‘°îš›é”åå§é–¿î†½å¯šéå›ªç¹˜ç›å±¾å¯šéæ¿é’©å©Šæˆ¯ç´é‘å¿“çš¬TD3ç’î… ç²Œé£î„ï¼éŠ†?""
        delay_value = float(delay_value) if np.isfinite(delay_value) else 0.0
        energy_per_task = float(energy_per_task) if np.isfinite(energy_per_task) else 0.0
        if self._reward_smoothing_alpha <= 0.0:
            return delay_value, energy_per_task
        alpha = self._reward_smoothing_alpha
        if self._reward_ema_delay is None or not np.isfinite(self._reward_ema_delay):
            self._reward_ema_delay = delay_value
        else:
            self._reward_ema_delay = (1.0 - alpha) * self._reward_ema_delay + alpha * delay_value
            if not np.isfinite(self._reward_ema_delay):
                self._reward_ema_delay = delay_value
        if self._reward_ema_energy is None or not np.isfinite(self._reward_ema_energy):
            self._reward_ema_energy = energy_per_task
        else:
            self._reward_ema_energy = (1.0 - alpha) * self._reward_ema_energy + alpha * energy_per_task
            if not np.isfinite(self._reward_ema_energy):
                self._reward_ema_energy = energy_per_task
        return self._reward_ema_delay, self._reward_ema_energy

    def _maybe_update_dynamic_energy_target(self, episode: int, episode_energy: float) -> None:
        """éè§„åµç€¹ç‚ºæª¯é‘³å€Ÿâ‚¬æ¥„åšœé”ã„¦æ–ç€¹ç•Œæ´°éå›·ç´é–¬å®å¤æ¶“å¶…å½²æˆå‰§å®³é‰ç†·î‡±é‘·å­˜å°Ÿé‘½Â°â‚¬?""
        if os.environ.get('DYNAMIC_TARGET_DISABLE', '0') != '0':
            return
        if episode_energy <= 0:
            return
        decay = 0.9
        self._energy_target_ema = decay * self._energy_target_ema + (1.0 - decay) * episode_energy
        if episode < self._energy_target_warmup:
            return
        if episode - self._last_energy_target_update < 5:
            return
        target = self._dynamic_energy_target
        ema = self._energy_target_ema
        if ema > target * 1.2:
            new_target = min(ema * 0.95, target * 3.0)
            self._dynamic_energy_target = new_target
            self._last_energy_target_update = episode
            update_reward_targets(energy_target=new_target)
            print(
                f"[DynamicTarget] Energy EMA {ema:.1f}J > target {target:.1f}J -> {new_target:.1f}J (Episode {episode})"
            )

    def _maybe_update_dynamic_latency_target(self, episode: int, episode_delay: float) -> None:
        """éè§„åµç€¹ç‚ºæª¯éƒè·ºæ¬¢é‘·î„å§©é€æƒ§î†”é©î†½çˆ£é”›å²„ä¼©éå¶‰ç®ç’ç†»æµ‡é¦çƒ˜æ«™æ¿‚æ §å§³æ¥—åæ‹°éŠ†?""
        if os.environ.get('DYNAMIC_TARGET_DISABLE', '0') != '0':
            return
        if episode_delay <= 0:
            return
        decay = 0.9
        self._delay_target_ema = decay * self._delay_target_ema + (1.0 - decay) * episode_delay
        warmup = max(20, int(config.experiment.num_episodes * 0.05))
        if episode < warmup:
            return
        if episode - self._last_delay_target_update < 5:
            return
        target = self._dynamic_latency_target
        ema = self._delay_target_ema
        if ema > target * 1.2:
            new_target = min(ema * 0.95, target * 3.0)
            self._dynamic_latency_target = new_target
            self._last_delay_target_update = episode
            update_reward_targets(latency_target=new_target)
            print(
                f"[DynamicTarget] Delay EMA {ema:.3f}s > target {target:.3f}s -> {new_target:.3f}s (Episode {episode})"
            )

    def reset_environment(self) -> np.ndarray:
        """é–²å¶‡ç–†éœîˆšî•¨éªæƒ°ç¹‘é¥ç‚²åµæ¿®å¬¬å§¸é¬?""
        # é–²å¶‡ç–†æµ è·¨æ¹¡é£ã„§å§¸é¬?
        self._episode_counters_initialized = False
        self.simulator._setup_scenario()
        
        # é€å •æ³¦ç»¯è¤ç²ºé˜èˆµâ‚¬?
        node_states = {}
        
        # æï¹ç· é˜èˆµâ‚¬ä¾Šç´™æ¶“å·—tepæ·‡æ¿‡å¯”æ¶“â‚¬é‘·å¯¸æ®‘è¤°æç«´é–æ ¨æŸŸå¯®å¿¥ç´š
        for i, vehicle in enumerate(self.simulator.vehicles):
            vehicle_state = np.array([
                normalize_scalar(vehicle['position'][0], 'vehicle_position_range', 2060.0),
                normalize_scalar(vehicle['position'][1], 'vehicle_position_range', 2060.0),
                normalize_scalar(vehicle.get('velocity', 0.0), 'vehicle_speed_range', 50.0),
                normalize_scalar(len(vehicle.get('tasks', [])), 'vehicle_queue_capacity', 20.0),
                normalize_scalar(vehicle.get('energy_consumed', 0.0), 'vehicle_energy_reference', 1000.0),
            ])
            node_states[f'vehicle_{i}'] = vehicle_state

        # RSUé˜èˆµâ‚¬ä¾Šç´™ç¼ç†¶ç«´è¤°æç«´é–?ç‘ä½¸å£€é”›?
        for i, rsu in enumerate(self.simulator.rsus):
            # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°­åŠé”ç‡™PUæ£°æˆ å·¼é—ç‘°ç·›é”›å²ƒî†€é…é¸¿å…˜æµ£æ’¶ç…¡é–¬æœSUé¨å‹®î…¸ç» æ¥€î†é–²å¿ç´­é”?
            cpu_freq_norm = normalize_scalar(rsu.get('cpu_freq', 12.5e9), 'cpu_frequency_range', 20e9)  # è¤°æç«´é–æ §åŸŒ[0,1]
            rsu_state = np.array([
                normalize_scalar(rsu['position'][0], 'rsu_position_range', 2060.0),
                normalize_scalar(rsu['position'][1], 'rsu_position_range', 2060.0),
                self._calculate_correct_cache_utilization(rsu.get('cache', {}), rsu.get('cache_capacity', 1000.0)),
                normalize_scalar(len(rsu.get('computation_queue', [])), 'rsu_queue_capacity', 20.0),
                normalize_scalar(rsu.get('energy_consumed', 0.0), 'rsu_energy_reference', 1000.0),
                cpu_freq_norm,  # é¦ƒæ•¡ é‚æ¿î–ƒé”›æ°±îƒ‡6ç¼?- CPUæ£°æˆ å·¼ (RSUç»¾?2.5GHz/20GHz=0.625)
            ])
            node_states[f'rsu_{i}'] = rsu_state

        # UAVé˜èˆµâ‚¬ä¾Šç´™ç¼ç†¶ç«´è¤°æç«´é–?ç‘ä½¸å£€é”›?
        for i, uav in enumerate(self.simulator.uavs):
            # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°­åŠé”ç‡™PUæ£°æˆ å·¼é—ç‘°ç·›é”›å²ƒî†€é…é¸¿å…˜æµ£æ’¶ç…¡é–¬æ¢AVé¨å‹®î…¸ç» æ¥€î†é–²å¿•æµ‰ç€µç¡…ç·å¯®?
            cpu_freq_norm = normalize_scalar(uav.get('cpu_freq', 5.0e9), 'cpu_frequency_range', 20e9)  # è¤°æç«´é–æ §åŸŒ[0,1]
            uav_state = np.array([
                normalize_scalar(uav['position'][0], 'uav_position_range', 2060.0),
                normalize_scalar(uav['position'][1], 'uav_position_range', 2060.0),
                normalize_scalar(uav['position'][2], 'uav_altitude_range', 200.0),
                self._calculate_correct_cache_utilization(uav.get('cache', {}), uav.get('cache_capacity', 200.0)),
                normalize_scalar(uav.get('energy_consumed', 0.0), 'uav_energy_reference', 1000.0),
                cpu_freq_norm,  # é¦ƒæ•¡ é‚æ¿î–ƒé”›æ°±îƒ‡6ç¼?- CPUæ£°æˆ å·¼ (UAVç»¾?.0GHz/20GHz=0.25)
            ])
            node_states[f'uav_{i}'] = uav_state
        
        # é’æ¿†îç»¯è¤ç²ºé¸å›¨çˆ£
        system_metrics = {
            'avg_task_delay': 0.0,
            'total_energy_consumption': 0.0,
            'data_loss_bytes': 0.0,
            'data_loss_ratio_bytes': 0.0,
            'cache_hit_rate': 0.0,
            'migration_success_rate': 0.0
        }
        
        # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°¶å™¸ç¼ƒî†¿å…˜é‘°æ¥„æ‹·éŸªî„æ«’é”›å²„ä¼©éå¶ˆæ³•episodeç»±îˆœĞ
        if hasattr(self, '_last_total_energy'):
            delattr(self, '_last_total_energy')

        # é‘¾å³°å½‡é’æ¿†îé˜èˆµâ‚¬ä½¸æ‚œé–²?
        if isinstance(self.agent_env, (TD3Environment, TD3LatencyEnergyEnvironment, CAMTD3Environment)):
            state = self.agent_env.get_state_vector(node_states, system_metrics, {'vehicles': [], 'rsus': [], 'uavs': []})  # type: ignore[call-arg]
        else:
            state = self.agent_env.get_state_vector(node_states, system_metrics)  # type: ignore[call-arg]
        
        return state
    
    def step(self, action, state, actions_dict: Optional[Dict] = None) -> Tuple[np.ndarray, float, bool, Dict]:
        """éµÑ†î”‘æ¶“â‚¬å§ãƒ¤è±¢éªç‡‚ç´æ´æ—‚æ•¤é…é¸¿å…˜æµ£æ’³å§©æµ£æ»ƒåŸŒæµ è·¨æ¹¡é£?""
        # é¦ƒå¹† æµ£è·¨æ•¤é¥å“„ç•¾é—æ­Œæµ‡ç»›æ «æšé”›å î›§é‹æ»†î†•ç¼ƒî‡†ç´š
        if self.fixed_offload_policy is not None and actions_dict is not None:
            try:
                # æµ£è·¨æ•¤é¥å“„ç•¾ç»›æ «æšé¢ç†¸åšé—æ­Œæµ‡éå´‡ç“¥
                fixed_action = self.fixed_offload_policy.select_action(state)
                
                # çå——æµç€¹æ°±ç“¥é£ãƒ§æ®‘actionæî„å´²æ¶“ç°…ffload preference
                # é¥å“„ç•¾ç»›æ «æšæ©æ–¿æ´–é¨åˆŸctionéç…ç´¡: [local_score, rsu_score, uav_score, ...]
                if isinstance(fixed_action, np.ndarray) and len(fixed_action) >= 3:
                    local_pref = float(fixed_action[0])
                    rsu_pref = float(fixed_action[1])
                    uav_pref = float(fixed_action[2])
                    
                    # è¤°æç«´é–æ ¦è´Ÿå§’å‚œå·¼é’å——ç«·
                    total = abs(local_pref) + abs(rsu_pref) + abs(uav_pref)
                    if total > 1e-6:
                        local_pref = abs(local_pref) / total
                        rsu_pref = abs(rsu_pref) / total
                        uav_pref = abs(uav_pref) / total
                    else:
                        local_pref, rsu_pref, uav_pref = 0.33, 0.33, 0.34
                    
                    # ç‘•å—™æ´Šé…é¸¿å…˜æµ£æ’¶æ®‘é—æ­Œæµ‡éå´‡ç“¥é”›å±¼ç¹šé£æ¬å¾æµ æ §å–…ç»›æ µç´™ç¼‚æ’³ç“¨éŠ†ä½½ç¸¼ç»‰è¤ç“‘é”›?
                    if 'offload_preference' in actions_dict:
                        actions_dict['offload_preference'] = {
                            'local': local_pref,
                            'rsu': rsu_pref,
                            'uav': uav_pref
                        }
            except Exception as e:
                # æ¿¡å‚›ç‰é¥å“„ç•¾ç»›æ «æšæ¾¶è¾«è§¦é”›å±½æ´–é–«â‚¬é’ç‰ˆæ«¤é‘³æˆ’ç¶‹éå´‡ç“¥
                pass
        
        # é‹å‹¯â‚¬çŠ±ç´¶é–«æ”ç²°æµ è·¨æ¹¡é£ã„§æ®‘é”ã„¤ç¶”é”›å çš¢æ©ç‚µç”»é”ã„¤ç¶”é„çŠ²çš æ¶“çƒ˜æ¹°é¦?RSU/UAVé‹å¿“ã‚½é”›?
        sim_actions = self._build_simulator_actions(actions_dict)
        
        # éµÑ†î”‘æµ è·¨æ¹¡å§ãƒ©î€ƒé”›å œç´¶éãƒ¥å§©æµ£æ»ç´š
        step_stats = self.simulator.run_simulation_step(0, sim_actions)
        resource_state = self._collect_resource_state()
        
        # é€å •æ³¦æ¶“å¬©ç«´å§ãƒ§å§¸é¬?
        node_states = {}
        
        # æï¹ç· é˜èˆµâ‚¬?(5ç¼?- ç¼ç†¶ç«´è¤°æç«´é–?
        for i, vehicle in enumerate(self.simulator.vehicles):
            vehicle_state = np.array([
                normalize_scalar(vehicle['position'][0], 'vehicle_position_range', 2060.0),  # æµ£å¶‡ç–†x
                normalize_scalar(vehicle['position'][1], 'vehicle_position_range', 2060.0),  # æµ£å¶‡ç–†y
                normalize_scalar(vehicle.get('velocity', 0.0), 'vehicle_speed_range', 50.0),  # é–«ç†·å®³
                normalize_scalar(len(vehicle.get('tasks', [])), 'vehicle_queue_capacity', 20.0),  # é—ƒç†·åª
                normalize_scalar(vehicle.get('energy_consumed', 0.0), 'vehicle_energy_reference', 1000.0),  # é‘³å€Ÿâ‚¬?
            ])
            node_states[f'vehicle_{i}'] = vehicle_state

        # RSUé˜èˆµâ‚¬?(6ç¼?- å¨£è¯²å§CPUæ£°æˆ å·¼)
        for i, rsu in enumerate(self.simulator.rsus):
            # éå›§å™¯é–æ §ç¶Šæ¶“â‚¬é–æ µç´°çº­î†»ç¹šéµâ‚¬éˆå¤Šâ‚¬ç…æ¹ª[0,1]é‘¼å†¨æ´¿
            # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°­åŠé”ç‡™PUæ£°æˆ å·¼é—ç‘°ç·›
            cpu_freq_norm = normalize_scalar(rsu.get('cpu_freq', 12.5e9), 'cpu_frequency_range', 20e9)
            rsu_state = np.array([
                normalize_scalar(rsu['position'][0], 'rsu_position_range', 2060.0),  # æµ£å¶‡ç–†x
                normalize_scalar(rsu['position'][1], 'rsu_position_range', 2060.0),  # æµ£å¶‡ç–†y
                self._calculate_correct_cache_utilization(rsu.get('cache', {}), rsu.get('cache_capacity', 1000.0)),  # ç¼‚æ’³ç“¨é’â•ƒæ•¤éœ?
                normalize_scalar(len(rsu.get('computation_queue', [])), 'rsu_queue_capacity', 20.0),  # é—ƒç†·åªé’â•ƒæ•¤éœ?
                normalize_scalar(rsu.get('energy_consumed', 0.0), 'rsu_energy_reference', 1000.0),  # é‘³å€Ÿâ‚¬?
                cpu_freq_norm,  # é¦ƒæ•¡ é‚æ¿î–ƒé”›æ°±îƒ‡6ç¼?- CPUæ£°æˆ å·¼
            ])
            node_states[f'rsu_{i}'] = rsu_state

        # UAVé˜èˆµâ‚¬?(6ç¼?- å¨£è¯²å§CPUæ£°æˆ å·¼)
        for i, uav in enumerate(self.simulator.uavs):
            # éå›§å™¯é–æ §ç¶Šæ¶“â‚¬é–æ µç´°çº­î†»ç¹šéµâ‚¬éˆå¤Šâ‚¬ç…æ¹ª[0,1]é‘¼å†¨æ´¿
            # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°­åŠé”ç‡™PUæ£°æˆ å·¼é—ç‘°ç·›
            cpu_freq_norm = normalize_scalar(uav.get('cpu_freq', 5.0e9), 'cpu_frequency_range', 20e9)
            uav_state = np.array([
                normalize_scalar(uav['position'][0], 'uav_position_range', 2060.0),  # æµ£å¶‡ç–†x
                normalize_scalar(uav['position'][1], 'uav_position_range', 2060.0),  # æµ£å¶‡ç–†y
                normalize_scalar(uav['position'][2], 'uav_altitude_range', 200.0),   # æµ£å¶‡ç–†zé”›å ¥ç®æ´ï¸¼ç´š
                self._calculate_correct_cache_utilization(uav.get('cache', {}), uav.get('cache_capacity', 200.0)),  # ç¼‚æ’³ç“¨é’â•ƒæ•¤éœ?
                normalize_scalar(uav.get('energy_consumed', 0.0), 'uav_energy_reference', 1000.0),  # é‘³å€Ÿâ‚¬?
                cpu_freq_norm,  # é¦ƒæ•¡ é‚æ¿î–ƒé”›æ°±îƒ‡6ç¼?- CPUæ£°æˆ å·¼
            ])
            node_states[f'uav_{i}'] = uav_state
        
        # ç’ï¼„ç•»ç»¯è¤ç²ºé¸å›¨çˆ£
        system_metrics = self._calculate_system_metrics(step_stats)
        
        # é‘¾å³°å½‡æ¶“å¬©ç«´é˜èˆµâ‚¬?
        if isinstance(self.agent_env, (TD3Environment, TD3LatencyEnergyEnvironment, CAMTD3Environment)):
            next_state = self.agent_env.get_state_vector(node_states, system_metrics, resource_state)  # type: ignore[call-arg]
        else:
            next_state = self.agent_env.get_state_vector(node_states, system_metrics)  # type: ignore[call-arg]
        
        # é¦ƒæ•¡ æ¾§ç‚²å·±é”›æ°³î…¸ç» æ¥€å¯˜éšî‚¢ç“™ç»¯è¤ç²ºé¸å›¨çˆ£é¨å‹«îš›é”?
        cache_metrics = self.adaptive_cache_controller.get_cache_metrics()
        migration_metrics = self.adaptive_migration_controller.get_migration_metrics()
        if hasattr(self, 'strategy_coordinator') and self.strategy_coordinator is not None:
            try:
                self.strategy_coordinator.observe_step(
                    system_metrics,
                    cache_metrics,
                    migration_metrics,
                    step_stats,
                )
            except Exception as exc:
                print(f"éˆ¿ç‹…ç¬ é‘±æ–¿æ‚ç»›æ «æšé—å¿šçšŸé£ã„¨î‡å¨´å¬ªç´“ç”¯? {exc}")

        # é™å¶‰î›­éæŠ½æ•­ç»¯è¤ç²ºé¸å›¨çˆ£ç¼æ©³D3ç»›æ «æšé¸å›§î‡±å¦¯â€³æ½¡é”›å²„â”é”ã„¨å…˜é‘°?å¯¤æƒ°ç¹œå¨“â•å®³é‘·îˆâ‚¬å‚šç°²
        agent_core = getattr(self.agent_env, 'agent', None)
        if agent_core is not None and hasattr(agent_core, 'update_guidance_feedback'):
            try:
                agent_core.update_guidance_feedback(system_metrics, cache_metrics, migration_metrics)
            except Exception as exc:
                if getattr(self, '_current_episode', 0) % 200 == 0:
                    print(f"éˆ¿ç‹…ç¬ é¸å›§î‡±é™å¶‰î›­é‡å­˜æŸŠæ¾¶è¾«è§¦: {exc}")

        reward_source = system_metrics.get('reward_snapshot', system_metrics)
        reward = self.agent_env.calculate_reward(reward_source, cache_metrics, migration_metrics)
        if not np.isfinite(reward):
            reward = 0.0
        try:
            system_metrics['normalized_reward'] = self._normalize_reward_value(reward)
        except Exception:
            system_metrics['normalized_reward'] = 0.0
        
        task_type_queue = system_metrics.get('task_type_queue_distribution', [])
        task_type_deadline = system_metrics.get('task_type_deadline_remaining', [])
        task_type_drop = system_metrics.get('task_type_drop_rate', [])
        for idx in range(4):
            queue_val = float(task_type_queue[idx]) if idx < len(task_type_queue) else 0.0
            deadline_val = float(task_type_deadline[idx]) if idx < len(task_type_deadline) else 0.0
            drop_val = float(task_type_drop[idx]) if idx < len(task_type_drop) else 0.0
            self.episode_metrics[f'task_type_queue_share_{idx+1}'].append(queue_val)
            self.episode_metrics[f'task_type_deadline_norm_{idx+1}'].append(deadline_val)
            self.episode_metrics[f'task_type_drop_rate_{idx+1}'].append(drop_val)

        hotspot_mean = float(system_metrics.get('rsu_hotspot_mean', 0.0))
        hotspot_peak = float(system_metrics.get('rsu_hotspot_peak', 0.0))
        self.episode_metrics['rsu_hotspot_mean_series'].append(hotspot_mean)
        self.episode_metrics['rsu_hotspot_peak_series'].append(hotspot_peak)
        self.episode_metrics['mm1_queue_error'].append(float(system_metrics.get('mm1_queue_error', 0.0)))
        self.episode_metrics['mm1_delay_error'].append(float(system_metrics.get('mm1_delay_error', 0.0)))
        
        # é¦ƒæ®Œ é‚æ¿î–ƒé”›æ°³î†‡è¤°æ›¡ç¸¼ç»‰æ˜å…˜é‘°?
        self.episode_metrics['rsu_migration_energy'].append(float(system_metrics.get('rsu_migration_energy', 0.0)))
        self.episode_metrics['uav_migration_energy'].append(float(system_metrics.get('uav_migration_energy', 0.0)))
        
        # é’ã‚†æŸ‡é„îˆšæƒç¼æ’´æ½«
        done = False  # é—æ›Ÿæ«¤é‘³æˆ’ç¶‹éœîˆšî•¨é–«æ°¬çˆ¶æ¶“å¶„ç´°é»æ„¬å¢ ç¼æ’´æ½«
        
        # é—„å‹«å§æ·‡â„ƒä¼…
        info = {
            'step_stats': step_stats,
            'system_metrics': system_metrics
        }
        
        return next_state, reward, done, info
    
    def _calculate_system_metrics(self, step_stats: Dict) -> Dict:
        """ç’ï¼„ç•»ç»¯è¤ç²ºé¬Ñ†å…˜é¸å›¨çˆ£ - éˆâ‚¬ç¼å œæ…¨æ¾¶å¶‡å¢—é”›å²€â€˜æ·‡æ¿‡æšŸéŠç…æ¹ªéšå ¢æ‚Šé‘¼å†¨æ´¿"""
        import numpy as np
        
        # ç€¹å¤Šåé‘¾å³°å½‡éæ¿â‚¬?
        def safe_get(key: str, default: float = 0.0) -> float:
            value = step_stats.get(key, default)
            if np.isnan(value) or np.isinf(value):
                return default
            return max(0.0, value)  # çº­î†»ç¹šé—ˆç‚¶ç¤‹
        
        # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°«å¨‡é¢â•¡pisodeç»¾ÑƒåŸ†ç¼ç†»î…¸é‘°å²„æ½ªç»±îˆœĞç¼ç†»î…¸é”›å²„ä¼©éå¶…îš›é”è¾©ç–®ç»‰îˆ›ä¼“é–?
        # ç’ï¼„ç•»éˆç«pisodeé¨å‹«î–ƒé–²å¿•ç²ºç’?
        total_processed = int(safe_get('processed_tasks', 0))  # ç»±îˆî…¸ç€¹å±¾åš
        total_dropped = int(safe_get('dropped_tasks', 0))  # ç»±îˆî…¸æ¶“ãˆ ç´”é”›å ŸæšŸé–²å¿¥ç´š
        
        # ç’ï¼„ç•»éˆç«pisodeæ¾§ç‚ºå™º
        episode_processed = total_processed - getattr(self, '_episode_processed_base', 0)
        episode_dropped = total_dropped - getattr(self, '_episode_dropped_base', 0)
        
        # éç‰ˆåµæ¶“ãˆ ã‘é–²å¿¥ç´°æµ£è·¨æ•¤éˆç«pisodeæ¾§ç‚ºå™º
        current_generated_bytes = float(step_stats.get('generated_data_bytes', 0.0))
        current_dropped_bytes = float(step_stats.get('dropped_data_bytes', 0.0))
        episode_generated_bytes = current_generated_bytes - getattr(self, '_episode_generated_bytes_base', 0.0)
        episode_dropped_bytes = current_dropped_bytes - getattr(self, '_episode_dropped_bytes_base', 0.0)
        
        # ç’ï¼„ç•»éˆç«pisodeæµ è¯²å§Ÿé¬ç»˜æšŸéœå±½ç•¬é´æ„®å·¼é”›å ¥ä¼©éå¶‡ç–®ç»‰îˆ›æ™¥æ´æ—“ç´š
        episode_total = episode_processed + episode_dropped
        completion_rate = normalize_ratio(episode_processed, episode_total, default=0.5)
        
        cache_hits = int(safe_get('cache_hits', 0))
        cache_misses = int(safe_get('cache_misses', 0))
        cache_requests_total = cache_hits + cache_misses
        reported_requests = int(step_stats.get('cache_requests', cache_requests_total) or cache_requests_total)
        reported_hit_rate = step_stats.get('cache_hit_rate')
        if reported_requests > 0:
            cache_requests_total = reported_requests
        cache_hit_rate = normalize_ratio(cache_hits, cache_requests_total)
        if isinstance(reported_hit_rate, (int, float)):
            cache_hit_rate = float(np.clip(reported_hit_rate, 0.0, 1.0))
        # é¦ƒæ•¡ é¥ç‚ºâ‚¬â‚¬é’æ‰®ç´¦ç€›æ¨»å¸¶é’è·ºæ«’é¨å‹­ç²ºç’â˜…ç´é–¬å®å¤éƒãƒ¥ç¹”ç¼‚å“„ã‘ç€µè‰°åš§0é›æˆ’è…‘
        cache_metrics = getattr(self, "adaptive_cache_controller", None)
        if cache_metrics is not None:
            cache_metrics = cache_metrics.get_cache_metrics()
            cm_requests = int(cache_metrics.get('total_requests', 0) or 0)
            cm_hit_rate = float(cache_metrics.get('hit_rate', 0.0) or 0.0)
            if cm_requests > 0 and cache_hit_rate <= 0.0:
                cache_requests_total = cm_requests
                cache_hit_rate = float(np.clip(cm_hit_rate, 0.0, 1.0))
        local_cache_hits = int(safe_get('local_cache_hits', 0))
        
        # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°­î„œçº­î†¿î…¸ç» æ¥€é’©é§å›§æ¬¢æ©?- æµ£è·¨æ•¤episodeç»¾ÑƒåŸ†æ¾§ç‚ºå™ºé”›å²ƒâ‚¬å²„æ½ªç»±îˆœĞéŠ?
        total_delay_ç»±îˆœĞ = safe_get('total_delay', 0.0)
        # ç’ï¼„ç•»éˆç«pisodeé¨å‹«æ¬¢æ©ç†·î–ƒé–²?
        delay_base_value = getattr(self, '_episode_delay_base', 0.0)
        episode_delay = max(0.0, total_delay_ç»±îˆœĞ - delay_base_value)
        # æµ£è·¨æ•¤éˆç«pisodeé¨å‹ªæ¢é”â„ƒæšŸ
        processed_for_delay = max(1, episode_processed) if episode_processed > 0 else max(1, total_processed)
        # ç’ï¼„ç•»éˆç«pisodeé¨å‹«é’©é§å›§æ¬¢æ©?
        avg_delay = episode_delay / processed_for_delay if processed_for_delay > 0 else 0.0
        
        # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°±Ğ©é—„ã‚‰æ•Šç’‡îˆœæ®‘clipé”›å±½æ¬¢æ©ç†·ç°²ç’‡ãƒ¦ç‰´é¹î†¼ç–„é—„å‘®å„éä½ƒåšœé’è·ºçéœ?
        # é™î„æ¹ªé„åº¢æ¨‰å¯®å‚šçˆ¶éƒèˆµå¢ æ©æ¶œî”‘ç‘ä½¸å£€é”›å œç·¥æ¿¡å‚ç§´æ©?0ç»‰æç´ç’‡å­˜æ§‘ç’ï¼„ç•»éˆå¤î‡¤é”›?
        if avg_delay > 60.0 or not np.isfinite(avg_delay):
            print(f"éˆ¿ç‹…ç¬ å¯®å‚šçˆ¶å¯¤æƒ°ç¹œå¦«â‚¬å¨´? {avg_delay:.2f}sé”›å²„å™¸ç¼ƒî†»è´Ÿ0.0s")
            avg_delay = 0.0
        avg_delay = max(0.0, avg_delay)  # çº­î†»ç¹šé—ˆç‚¶ç¤‹

        delay_base = getattr(self, '_episode_delay_component_base', {})
        delay_processing_total = safe_get('delay_processing', 0.0)
        delay_uplink_total = safe_get('delay_uplink', 0.0)
        delay_downlink_total = safe_get('delay_downlink', 0.0)
        delay_cache_total = safe_get('delay_cache', 0.0)
        delay_wait_total = safe_get('delay_waiting', 0.0)
        def _episode_delay(bucket_total: float, bucket_key: str) -> float:
            return max(0.0, bucket_total - delay_base.get(bucket_key, 0.0))
        episode_delay_processing = _episode_delay(delay_processing_total, 'delay_processing')
        episode_delay_uplink = _episode_delay(delay_uplink_total, 'delay_uplink')
        episode_delay_downlink = _episode_delay(delay_downlink_total, 'delay_downlink')
        episode_delay_cache = _episode_delay(delay_cache_total, 'delay_cache')
        episode_delay_wait = _episode_delay(delay_wait_total, 'delay_waiting')
        delay_denominator = max(1, episode_processed) if episode_processed > 0 else max(1, processed_for_delay)
        avg_processing_delay_component = episode_delay_processing / delay_denominator
        avg_uplink_delay_component = episode_delay_uplink / delay_denominator
        avg_downlink_delay_component = episode_delay_downlink / delay_denominator
        avg_cache_delay_component = episode_delay_cache / delay_denominator
        avg_wait_delay_component = episode_delay_wait / delay_denominator
        
        # é¦ƒæ•¡ æ·‡î†¼î˜²é‘³å€Ÿâ‚¬æ¥„î…¸ç» æ¥‹ç´°æµ£è·¨æ•¤éªç†·ç–„episodeæ¾§ç‚ºå™ºé‘³å€Ÿâ‚¬?
        current_total_energy = safe_get('total_energy', 0.0)

        if not getattr(self, '_episode_counters_initialized', False):
            self._initialize_episode_counters(step_stats)

        # é‘·îˆâ‚¬å‚šç°²éºÑƒåŸ—é£ã„§ç²ºç’â˜…ç´™é¢ã„¤ç°¬æ¿‚æ §å§³æ¶“åº¢å¯šéå›§ç¶Šæ¶“â‚¬é–æ µç´š
        cache_metrics = self.adaptive_cache_controller.get_cache_metrics()
        migration_metrics = self.adaptive_migration_controller.get_migration_metrics()
        cache_total_requests = int(cache_metrics.get('total_requests', 0) or 0)
        cache_total_evictions = int(cache_metrics.get('evicted_items', 0) or 0)
        cache_total_collab = int(cache_metrics.get('collaborative_writes', 0) or 0)

        queue_rho_sum = float(step_stats.get('queue_rho_sum', 0.0) or 0.0)
        queue_rho_max = float(step_stats.get('queue_rho_max', 0.0) or 0.0)
        queue_overload_flag = 1.0 if bool(step_stats.get('queue_overload_flag', False)) else 0.0
        queue_rho_by_node = step_stats.get('queue_rho_by_node', {}) or {}
        queue_overloaded_nodes = step_stats.get('queue_overloaded_nodes', {}) or {}
        queue_warning_nodes = step_stats.get('queue_warning_nodes', {}) or {}
        queue_overload_events_total = int(step_stats.get('queue_overload_events', 0) or 0)
        queue_overload_events = max(0, queue_overload_events_total - getattr(self, '_episode_queue_overload_events_base', 0))
        queue_overflow_total = int(step_stats.get('queue_overflow_drops', 0) or 0)
        queue_overflow_drops = max(0, queue_overflow_total - getattr(self, '_episode_queue_overflow_base', 0))
        remote_stats = step_stats.get('remote_rejections', {}) or {}
        remote_total = int(remote_stats.get('total', 0) or 0)
        episode_remote_rejects = max(0, remote_total - getattr(self, '_episode_remote_reject_base', 0))
        remote_rejection_rate = normalize_ratio(episode_remote_rejects, episode_total, default=0.0)

        mm1_predictions_raw = step_stats.get('mm1_predictions', {}) or {}
        mm1_predictions: Dict[str, Dict[str, float]] = {}
        mm1_queue_errors: List[float] = []
        mm1_delay_errors: List[float] = []
        if isinstance(mm1_predictions_raw, dict):
            for node_key, pred in mm1_predictions_raw.items():
                if not isinstance(pred, dict):
                    continue
                arrival_rate = float(pred.get('arrival_rate', 0.0) or 0.0)
                service_rate = float(pred.get('service_rate', 0.0) or 0.0)
                rho_val = pred.get('rho')
                if rho_val is None:
                    rho = np.inf if service_rate <= 0.0 and arrival_rate > 0.0 else 0.0
                else:
                    try:
                        rho = float(rho_val)
                    except (TypeError, ValueError):
                        rho = np.inf
                stable = bool(pred.get('stable', False))
                rho_storable = float(rho) if np.isfinite(rho) else float('inf')
                theoretical_queue = pred.get('theoretical_queue')
                actual_queue = float(pred.get('actual_queue', 0.0) or 0.0)
                theoretical_delay = pred.get('theoretical_delay')
                actual_delay_obs = float(pred.get('actual_delay', 0.0) or 0.0)

                if theoretical_queue is not None:
                    try:
                        theo_queue_val = float(theoretical_queue)
                    except (TypeError, ValueError):
                        theo_queue_val = None
                else:
                    theo_queue_val = None

                if theoretical_delay is not None:
                    try:
                        theo_delay_val = float(theoretical_delay)
                    except (TypeError, ValueError):
                        theo_delay_val = None
                else:
                    theo_delay_val = None

                mm1_predictions[node_key] = {  # type: ignore[assignment]
                    'arrival_rate': float(arrival_rate),
                    'service_rate': float(service_rate),
                    'rho': float(rho_storable) if rho_storable is not None else 0.0,
                    'stable': bool(stable),
                    'theoretical_queue': float(theo_queue_val) if theo_queue_val is not None else 0.0,
                    'actual_queue': float(actual_queue),
                    'theoretical_delay': float(theo_delay_val) if theo_delay_val is not None else 0.0,
                    'actual_delay': float(actual_delay_obs),
                }

                if theo_queue_val is not None:
                    mm1_queue_errors.append(abs(actual_queue - theo_queue_val))
                if theo_delay_val is not None:
                    mm1_delay_errors.append(abs(actual_delay_obs - theo_delay_val))

        mm1_queue_error = float(np.mean(mm1_queue_errors)) if mm1_queue_errors else 0.0
        mm1_delay_error = float(np.mean(mm1_delay_errors)) if mm1_delay_errors else 0.0

        
        # é¦ƒæ•¡ P0æ·‡î†¼î˜²é”›æ°±Ğ©é—„ã‚ˆå…˜é‘°æ¤¾åŠç» æ¥…ç“Ÿå¨‰æ›ŸæšŸç€›æ¥‹ç´æ¿¡å‚›ç‰æ¶“?é’æ¬æ¨‰ç»€é¸¿î„Ÿé›å©çµ¾æ¶“å¶„å¨‡é¢ã„¨æ«„é‹å›§â‚¬?
        if current_total_energy <= 0.0:
            # æµ£è·¨æ•¤æ¶“å©ç«´episodeé¨å‹®å…˜é‘°æ¤¾ç¶”æ¶“å“„ç†€ç»¾åŒ¡ç´™é‡æ‘æ‚éå—­ç´š
            episode_incremental_energy = 0.0
            total_energy = 0.0
            print(f"éˆ¿ç‹…ç¬ æµ è·¨æ¹¡é£ã„¨å…˜é‘°æ¤¾è´Ÿ0é”›å²ƒî‡¬å¦«â‚¬éŒãƒ¤è±¢éªç†·æ«’é‘³å€Ÿâ‚¬æ¥Äé¨å¬¶ç´’")
        else:
            episode_incremental_energy = max(0.0, current_total_energy - getattr(self, '_episode_energy_base', 0.0))
            total_energy = episode_incremental_energy

        energy_base = getattr(self, '_episode_energy_component_base', {})
        def _episode_energy(bucket_key: str) -> float:
            return max(0.0, safe_get(bucket_key, 0.0) - energy_base.get(bucket_key, 0.0))
        energy_compute_component = _episode_energy('energy_compute')
        energy_tx_uplink_component = _episode_energy('energy_transmit_uplink')
        energy_tx_downlink_component = _episode_energy('energy_transmit_downlink')
        energy_cache_component = _episode_energy('energy_cache')
        energy_denominator = max(1, episode_processed) if episode_processed > 0 else 1
        avg_energy_compute_component = energy_compute_component / energy_denominator
        avg_energy_uplink_component = energy_tx_uplink_component / energy_denominator
        avg_energy_downlink_component = energy_tx_downlink_component / energy_denominator
        avg_energy_cache_component = energy_cache_component / energy_denominator
        
        # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°«å¨‡é¢â•¡pisodeç»¾ÑƒåŸ†éç‰ˆåµæ¶“ãˆ ã‘é–²å¿¥ç´é–¬å®å¤ç»±îˆœĞéå ç°²
        data_loss_bytes = max(0.0, episode_dropped_bytes)
        data_generated_bytes = max(0.0, episode_generated_bytes)
        data_loss_ratio_bytes = normalize_ratio(data_loss_bytes, data_generated_bytes)
        
        # é¦ƒæ•Ÿ é‚æ¿î–ƒé”›æ°³î…¸ç» æ¥€åµæèŠ¥ç˜®æ¸šå¬¶ç´™local/rsu/uavé”›?
        # Accumulate task counts from per-step stats
        self._episode_local_tasks += int(step_stats.get('local_tasks', 0))
        self._episode_rsu_tasks += int(step_stats.get('rsu_tasks', 0))
        self._episode_uav_tasks += int(step_stats.get('uav_tasks', 0))

        local_tasks_count = self._episode_local_tasks
        rsu_tasks_count = self._episode_rsu_tasks
        uav_tasks_count = self._episode_uav_tasks
        total_offload_tasks = local_tasks_count + rsu_tasks_count + uav_tasks_count
        
        if total_offload_tasks > 0:
            local_offload_ratio = float(local_tasks_count) / float(total_offload_tasks)
            rsu_offload_ratio = float(rsu_tasks_count) / float(total_offload_tasks)
            uav_offload_ratio = float(uav_tasks_count) / float(total_offload_tasks)
            # é¦ƒå¹† æ·‡î†¼î˜²é”›æ°³î…¸ç» æ¥â‚¬æ˜ç¹™ç»‹å¬ªåµæèŠ¥ç˜®æ¸šå¬¶ç´™RSU+UAVé”›?
            remote_execution_ratio = rsu_offload_ratio + uav_offload_ratio
        else:
            # æ¦›æ¨¿î…»éŠç¡·ç´°éã„©å„´éˆî„€æ¹´æ¾¶å‹­æ‚Š
            local_offload_ratio = 1.0
            rsu_offload_ratio = 0.0
            uav_offload_ratio = 0.0
            remote_execution_ratio = 0.0
        
        # é¦ƒå¹† æ·‡î†¼î˜²é”›æ°³î…¸ç» æ¡¼SUç’§å‹¬ç°®é’â•ƒæ•¤éœå›·ç´™ç’ï¼„ç•»é—ƒç†·åªé—çŠµæ•¤éœå›·ç´š
        rsu_total_utilization = 0.0
        rsu_count = len(self.simulator.rsus) if hasattr(self.simulator, 'rsus') else 0
        if rsu_count > 0:
            for rsu in self.simulator.rsus:
                queue_len = len(rsu.get('computation_queue', []))
                queue_capacity = rsu.get('queue_capacity', 20)  # æ¦›æ¨¿î…»ç€¹å½’å™º20
                rsu_total_utilization += float(queue_len) / max(1.0, float(queue_capacity))
            rsu_utilization = rsu_total_utilization / float(rsu_count)
        else:
            rsu_utilization = 0.0
        
        # æ©ä½ºĞ©é´æ„¬å§›éœå›·ç´™é‰ãƒ¨åšœæµ è·¨æ¹¡é£ã„§ç²ºç’â˜…ç´š
        migrations_executed = int(safe_get('migrations_executed', 0))
        migrations_successful = int(safe_get('migrations_successful', 0))
        migration_success_rate = normalize_ratio(migrations_successful, migrations_executed)
        
        # é¦ƒæ•¡ ç’‹å†­ç˜¯æ©ä½ºĞ©ç¼ç†»î…¸
        if migrations_executed > 0:
            print(f"é¦ƒæ”³ æ©ä½ºĞ©ç¼ç†»î…¸: éµÑ†î”‘{migrations_executed}å¨†? é´æ„¬å§›{migrations_successful}å¨†? é´æ„¬å§›éœå™migration_success_rate:.1%}")

        episode_cache_requests = max(
            0,
            cache_total_requests - getattr(self, '_episode_cache_requests_base', 0)
        )
        episode_cache_evictions = max(
            0,
            cache_total_evictions - getattr(self, '_episode_cache_evictions_base', 0)
        )
        episode_cache_collab = max(
            0,
            cache_total_collab - getattr(self, '_episode_cache_collab_base', 0)
        )
        cache_eviction_rate = normalize_ratio(episode_cache_evictions, episode_cache_requests)

        def _normalize_vector(key: str, length: int = 4, clip: bool = True) -> List[float]:
            raw = step_stats.get(key)
            if isinstance(raw, (np.ndarray, list, tuple)):
                values = [float(v) for v in raw]  # é„åº£â€˜æî„å´²æ¶“ç¯ºloaté’æ¥„ã€ƒ
            else:
                values = []
            return normalize_feature_vector(values, length, clip=clip)  # type: ignore[arg-type]

        queue_distribution = _normalize_vector('task_type_queue_distribution')
        active_distribution = _normalize_vector('task_type_active_distribution')
        deadline_remaining = _normalize_vector('task_type_deadline_remaining')
        queue_counts = _normalize_vector('task_type_queue_counts', clip=False)
        active_counts = _normalize_vector('task_type_active_counts', clip=False)
        hotspot_list = _normalize_vector(
            'rsu_hotspot_intensity',
            length=getattr(self.simulator, 'num_rsus', 0) or 4
        )
        rsu_hotspot_mean = float(np.mean(hotspot_list)) if hotspot_list else 0.0
        rsu_hotspot_peak = float(np.max(hotspot_list)) if hotspot_list else 0.0

        task_generation_stats = step_stats.get('task_generation')
        gen_by_type = task_generation_stats.get('by_type', {}) if isinstance(task_generation_stats, dict) else {}
        drop_stats = step_stats.get('drop_stats')
        drop_by_type = drop_stats.get('by_type', {}) if isinstance(drop_stats, dict) else {}

        generated_counts: List[float] = []
        drop_rate: List[float] = []
        for task_type in range(1, 5):
            generated = float(gen_by_type.get(task_type, 0.0))
            dropped = float(drop_by_type.get(task_type, 0.0))
            generated_counts.append(generated)
            drop_rate.append(normalize_ratio(dropped, generated))
        generated_share = normalize_distribution(generated_counts) if generated_counts else []

        # é¦ƒæ”³ ç’‹å†­ç˜¯éƒãƒ¥ç¹”é”›æ°³å…˜é‘°æ¤¾ç¬Œæ©ä½ºĞ©éå¿”åŠ…é–æ´ªæ£¿
        current_episode = getattr(self, '_current_episode', 0)
        if current_episode > 0 and (current_episode % 50 == 0 or avg_delay > 0.2 or migration_success_rate < 0.9):
            # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°³î…¸ç» æ¤¾æ¢é”â„ƒæšŸé–²å¿”å´¯æ¾¶è¾©å·¼é”›å±¼ç¬Œç€¹å±¾åšéœå›§î‡®æ´?
            task_drop_rate = normalize_ratio(episode_dropped, episode_total)
            print(
                f"[ç’‹å†­ç˜¯] Episode {current_episode:04d}: å¯¤æƒ°ç¹œ {avg_delay:.3f}s, é‘³å€Ÿâ‚¬?{total_energy:.2f}J, "
                f"ç€¹å±¾åšéœ?{completion_rate:.1%}, æ©ä½ºĞ©é´æ„¬å§›éœ?{migration_success_rate:.1%}, "
                f"ç¼‚æ’³ç“¨é›æˆ’è…‘ {cache_hit_rate:.1%}, éç‰ˆåµé¹ç†·ã‘ {data_loss_ratio_bytes:.1%}, "
                f"ç¼‚æ’³ç“¨å¨£æ¨»å‘éœ?{cache_eviction_rate:.1%}"
            )
            # é¦ƒæ•Ÿ é‚æ¿î–ƒé”›æ°­æ¨‰ç»€å“„åµæè—‰åç”¯å†ªç²ºç’â€³æ‹°é¹ç†·ã‘éœå›§î‡®å§£?
            print(
                f"  æµ è¯²å§Ÿé’å——ç«·: éˆî„€æ¹´ {local_tasks_count}æ¶“?{local_offload_ratio:.1%}), "
                f"RSU {rsu_tasks_count}æ¶“?{rsu_offload_ratio:.1%}), "
                f"UAV {uav_tasks_count}æ¶“?{uav_offload_ratio:.1%}), "
                f"æ¶“ãˆ ç´” {episode_dropped}æ¶“?
            )
            # é¦ƒå• å¨£è¯²å§é”›æ°«æ¢é”â„ƒæšŸé–²å¼™séç‰ˆåµé–²å¿•æ®‘ç€µè§„ç˜®ç’‡å­˜æ§‘
            if abs(task_drop_rate - data_loss_ratio_bytes) > 0.1:  # å®¸î†¼ç´“>10%éƒèˆµå½ç»€?
                print(
                    f"  éˆ¿ç‹…ç¬ å¨‰ã„¦å‰°: æµ è¯²å§Ÿéä¼´å™ºæ¶“ãˆ ã‘éœå™task_drop_rate:.1%} vs éç‰ˆåµé–²å¿æ¶ªæ¾¶è¾©å·¼{data_loss_ratio_bytes:.1%} "
                    f"(å®¸î†¼ç´“{abs(task_drop_rate - data_loss_ratio_bytes)*100:.1f}%é”›å²ƒî‡©é„åºæ¶ªå¯®å†§æ¢é”ï¼„æ®‘éç‰ˆåµé–²å¿šç·æ¾¶?"
                )

        # é¦ƒî˜» é‡å­˜æŸŠç¼‚æ’³ç“¨éºÑƒåŸ—é£ã„§ç²ºç’â˜…ç´™æ¿¡å‚›ç‰éˆå¤Šç–„é—„å‘®æšŸé¹î‡†ç´š
        if cache_hit_rate > 0:
            # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°­î„œçº­î†¿î…¸ç» æ¥ƒç´¦ç€›æ¨¼ç²ºç’?
            total_utilization = 0.0
            for rsu in self.simulator.rsus:
                utilization = self._calculate_correct_cache_utilization(
                    rsu.get('cache', {}), 
                    rsu.get('cache_capacity', 1000.0)
                )
                total_utilization += utilization
            
            self.adaptive_cache_controller.cache_stats['current_utilization'] = (
                total_utilization / max(1, len(self.simulator.rsus))
            )
        
        latency_target = max(1e-6, getattr(config.rl, 'latency_target', 0.4))
        energy_target = max(1e-6, getattr(config.rl, 'energy_target', 1200.0))

        reward_snapshot = self._build_reward_snapshot(step_stats)

        return {
            'avg_task_delay': avg_delay,
            'total_energy_consumption': total_energy,
            'data_loss_bytes': data_loss_bytes,
            'data_loss_ratio_bytes': data_loss_ratio_bytes,
            'task_completion_rate': completion_rate,
            'cache_hit_rate': cache_hit_rate,
            'local_cache_hits': local_cache_hits,
            'migration_success_rate': migration_success_rate,
            'dropped_tasks': episode_dropped,
            'avg_processing_delay': avg_processing_delay_component,
            'avg_uplink_delay': avg_uplink_delay_component,
            'avg_downlink_delay': avg_downlink_delay_component,
            'avg_cache_delay': avg_cache_delay_component,
            'avg_waiting_delay': avg_wait_delay_component,
            'energy_compute': energy_compute_component,
            'energy_transmit_uplink': energy_tx_uplink_component,
            'energy_transmit_downlink': energy_tx_downlink_component,
            'energy_cache': energy_cache_component,
            'avg_energy_compute': avg_energy_compute_component,
            'avg_energy_uplink': avg_energy_uplink_component,
            'avg_energy_downlink': avg_energy_downlink_component,
            'avg_energy_cache': avg_energy_cache_component,
            # é¦ƒî˜» é‚æ¿î–ƒé‘·îˆâ‚¬å‚šç°²éºÑƒåŸ—é¸å›¨çˆ£
            'adaptive_cache_effectiveness': cache_metrics.get('effectiveness', 0.0),
            'adaptive_migration_effectiveness': migration_metrics.get('effectiveness', 0.0),
            'migration_avg_cost': migration_metrics.get('avg_cost', 0.0),
            'migration_avg_delay_saved': migration_metrics.get('avg_delay_saved', 0.0),
            'cache_utilization': cache_metrics.get('utilization', 0.0),
            'cache_evictions': episode_cache_evictions,
            'cache_eviction_rate': cache_eviction_rate,
            'cache_requests': episode_cache_requests,
            'cache_collaborative_writes': episode_cache_collab,
            'adaptive_cache_params': cache_metrics.get('agent_params', {}),
            'adaptive_migration_params': migration_metrics.get('agent_params', {}),
            'task_type_queue_distribution': queue_distribution,
            'task_type_active_distribution': active_distribution,
            'task_type_deadline_remaining': deadline_remaining,
            'task_type_queue_counts': queue_counts,
            'task_type_active_counts': active_counts,
            'task_type_drop_rate': drop_rate,
            'task_type_generated_share': generated_share,
            'queue_rho_sum': queue_rho_sum,
            'queue_rho_max': queue_rho_max,
            'queue_overload_flag': queue_overload_flag,
            'queue_overload_events': queue_overload_events,
            'queue_rho_by_node': queue_rho_by_node,
            'queue_overloaded_nodes': queue_overloaded_nodes,
            'queue_warning_nodes': queue_warning_nodes,
            'queue_overflow_drops': queue_overflow_drops,
            'mm1_queue_error': mm1_queue_error,
            'mm1_delay_error': mm1_delay_error,
            'mm1_predictions': mm1_predictions,
            'rsu_hotspot_intensity_list': hotspot_list,
            'rsu_hotspot_mean': rsu_hotspot_mean,
            'rsu_hotspot_peak': rsu_hotspot_peak,
            'remote_rejection_count': episode_remote_rejects,
            'remote_rejection_rate': remote_rejection_rate,
            'normalized_delay': avg_delay / latency_target,
            'normalized_energy': total_energy / energy_target,
            'reward_snapshot': reward_snapshot,
            # é¦ƒæ•Ÿ é‚æ¿î–ƒé”›æ°¬åµæèŠ¥ç˜®æ¸šå¬¬ç²ºç’?
            'local_offload_ratio': local_offload_ratio,
            'rsu_offload_ratio': rsu_offload_ratio,
            'uav_offload_ratio': uav_offload_ratio,
            'local_tasks_count': local_tasks_count,
            'rsu_tasks_count': rsu_tasks_count,
            'uav_tasks_count': uav_tasks_count,
            # é¦ƒå¹† æ·‡î†¼î˜´bugé”›æ°­åŠé”çŠ²å§é–¿î†½å¯šé?
            'rsu_utilization': rsu_utilization,  # RSUç’§å‹¬ç°®é’â•ƒæ•¤éœ?
            'offload_ratio': remote_execution_ratio,  # é¬æ˜ç¹™ç»‹å¬ªåµæèŠ¥ç˜®æ¸šå¬¶ç´™RSU+UAVé”›?
            'remote_execution_ratio': remote_execution_ratio,  # é’î‚¢æ‚•é”›å±½å‹ç€¹è§„æ£«æµ ï½‡çˆœ
            # é¦ƒæ®Œ é‚æ¿î–ƒé”›æ°³ç¸¼ç»‰æ˜å…˜é‘°æ¥å¯šé?
            'rsu_migration_energy': _episode_energy('rsu_migration_energy'),
            'uav_migration_energy': _episode_energy('uav_migration_energy'),
        }

    def _normalize_reward_value(self, reward: float) -> float:
        """çå——îš›é”åâ‚¬è‰°æµ†é¹î­è´ŸéƒçŠ»å™ºç»¾å‰ç˜®æ¸šå¬¶ç´æ¸šå¤¸ç°¬æ¶“åº¡å¾æµ æ ¨å¯šéå›§î‡®å§£æ–»â‚¬?""
        import numpy as np
        rl_config = getattr(config, 'rl', None)
        reward_scale = float(
            getattr(
                rl_config,
                'reward_normalizer',
                getattr(rl_config, 'reward_weight_delay', 1.0)
                + getattr(rl_config, 'reward_weight_energy', 1.0)
            )
        )
        reward_scale = max(reward_scale, 1e-6)
        normalized = -reward / reward_scale
        return float(np.clip(normalized, -5.0, 5.0))
    
    def _record_episode_metrics(self, system_metrics: Dict, episode_steps: Optional[int] = None) -> None:
        """çå—™éƒ´ç¼ç†¸å¯šéå›§å•“éî™«pisode_metricsé”›å±¾æŸŸæ¸šå®æ‚—ç¼î…Ÿå§¤é›?é™îˆî‹é–æ ¦å¨‡é¢ã„£â‚¬?""
        import numpy as np

        metric_mapping = {
            'avg_task_delay': 'avg_delay',
            'total_energy_consumption': 'total_energy',
            'data_loss_bytes': 'data_loss_bytes',
            'data_loss_ratio_bytes': 'data_loss_ratio_bytes',
            'task_completion_rate': 'task_completion_rate',
            'cache_hit_rate': 'cache_hit_rate',
            'cache_utilization': 'cache_utilization',
            'cache_evictions': 'cache_evictions',
            'cache_eviction_rate': 'cache_eviction_rate',
            'cache_requests': 'cache_requests',
            'cache_collaborative_writes': 'cache_collaborative_writes',
            'local_cache_hits': 'local_cache_hits',
            'migration_success_rate': 'migration_success_rate',
            'queue_rho_sum': 'queue_rho_sum',
            'queue_rho_max': 'queue_rho_max',
            'queue_overload_flag': 'queue_overload_flag',
            'queue_overload_events': 'queue_overload_events',
            'avg_step_reward': 'avg_step_reward',  # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°­åŠé”çŠ²é’©é§å›¨ç˜¡å§ãƒ¥îš›é”è¾¨æ§§ç?
            'migration_avg_cost': 'migration_avg_cost',
            'migration_avg_delay_saved': 'migration_avg_delay_saved',
            'rsu_hotspot_mean': 'rsu_hotspot_mean',  # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°±â€˜æ·‡æ¿Šî†‡è¤°æ˜¬pisodeç»¾ÑƒåŸ†é‘î… å£éªå†²æ½
            'rsu_hotspot_peak': 'rsu_hotspot_peak',  # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°±â€˜æ·‡æ¿Šî†‡è¤°æ˜¬pisodeç»¾ÑƒåŸ†é‘î… å£å®„æ¿â‚¬?
            'normalized_delay': 'normalized_delay',
            'normalized_energy': 'normalized_energy',
            'normalized_reward': 'normalized_reward',
            'avg_processing_delay': 'avg_processing_delay',
            'avg_uplink_delay': 'avg_uplink_delay',
            'avg_downlink_delay': 'avg_downlink_delay',
            'avg_cache_delay': 'avg_cache_delay',
            'avg_waiting_delay': 'avg_waiting_delay',
            'energy_compute': 'energy_compute',
            'energy_transmit_uplink': 'energy_transmit_uplink',
            'energy_transmit_downlink': 'energy_transmit_downlink',
            'energy_cache': 'energy_cache',
            'avg_energy_compute': 'avg_energy_compute',
            'avg_energy_uplink': 'avg_energy_uplink',
            'avg_energy_downlink': 'avg_energy_downlink',
            'avg_energy_cache': 'avg_energy_cache',
            'queue_overflow_drops': 'queue_overflow_drops',
            # é¦ƒå¹† æ·‡î†¼î˜´bugé”›æ°­åŠé”çŠ²å§é–¿î†½å¯šéå›¨æ§§ç?
            'rsu_utilization': 'rsu_utilization',
            'offload_ratio': 'offload_ratio',
            'rsu_offload_ratio': 'rsu_offload_ratio',
            'uav_offload_ratio': 'uav_offload_ratio',
            'local_offload_ratio': 'local_offload_ratio',
        }

        def _coerce_scalar(value: Any) -> Optional[float]:
            if isinstance(value, (list, tuple, dict)):
                return None
            try:
                return float(value)
            except (TypeError, ValueError):
                if isinstance(value, np.ndarray) and value.size == 1:
                    return float(value.item())
                return None

        for system_key, episode_key in metric_mapping.items():
            if episode_key not in self.episode_metrics:
                continue
            scalar_value = _coerce_scalar(system_metrics.get(system_key))
            if scalar_value is None:
                continue
            self.episode_metrics[episode_key].append(scalar_value)

        queue_distribution_ep = system_metrics.get('task_type_queue_distribution')
        if isinstance(queue_distribution_ep, (list, tuple, np.ndarray)):
            for idx, value in enumerate(queue_distribution_ep):
                key = f'task_type_queue_share_ep_{idx+1}'
                if key in self.episode_metrics:
                    coerced = _coerce_scalar(value)
                    if coerced is not None:
                        self.episode_metrics[key].append(coerced)

        if episode_steps is not None and 'episode_steps' in self.episode_metrics:
            self.episode_metrics['episode_steps'].append(int(episode_steps))
        
        # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°³î…¸ç» æ¥€è‹Ÿç’æ¿ç¶éªå†²æ½å§£å¿”î„æ¿‚æ §å§³
        if episode_steps and episode_steps > 0:
            # æµ åº¢æ¸¶éšåºç«´æ¶“çŒ pisode_rewardç’ï¼„ç•»avg_step_reward
            if hasattr(self, 'episode_rewards') and self.episode_rewards:
                last_episode_reward = self.episode_rewards[-1]
                avg_step_reward = last_episode_reward / episode_steps
                system_metrics['avg_step_reward'] = avg_step_reward
    
    def run_episode(self, episode: int, max_steps: Optional[int] = None) -> Dict:
        """æ©æ„¯î”‘æ¶“â‚¬æ¶“î„ç•¬éå¯¸æ®‘ç’î… ç²Œæî†½î‚¼"""
        # æµ£è·¨æ•¤é–°å¶‡ç–†æ¶“î… æ®‘éˆâ‚¬æ¾¶Ñ„î„é?
        if max_steps is None:
            max_steps = config.experiment.max_steps_per_episode
        
        # é–²å¶‡ç–†éœîˆšî•¨
        self._episode_counters_initialized = False
        state = self.reset_environment()
        
        # é¦ƒæ•¡ P1æ·‡î†¼î˜²é”›æ°¬å·±é’è·ºåµæ¿®å¬ªå¯²episodeç’â„ƒæšŸé£îŸ’ç´çº­î†»ç¹šç»—îƒ¿ç«´æ¶“çŒ pisodeç¼ç†»î…¸å§ï½‡â€˜
        if hasattr(self, 'simulator') and hasattr(self.simulator, 'stats'):
            self._initialize_episode_counters(self.simulator.stats)
        
        # é¦ƒæ•¡ æ·‡æ¿†ç“¨è¤°æ’³å¢ episodeç¼‚æ §å½¿
        self._current_episode = episode
        
        # é¦ƒæ•¡ é–²å¶‡ç–†episodeå§ãƒ¦æšŸç’ºç†»é‡œé”›å±¼æ…¨æ¾¶å¶ˆå…˜é‘°æ¥„î…¸ç» ?
        self._current_episode_step = 0
        
        # é¦ƒå¹† é’æ¿†îé–æ ¨æ¹°episodeé¨å‰†tepç¼ç†»î…¸é’æ¥„ã€ƒ
        episode_step_stats = []
        
        episode_reward = 0.0
        episode_info = {}
        step = 0
        info = {}  # é’æ¿†îé–æ‹nfoé™æ©€å™º
        
        # PPOé—‡â‚¬ç‘•ä½ºå£’å¨ˆå©‚î˜©é?
        if self.algorithm == "PPO":
            return self._run_ppo_episode(episode, max_steps)
        
        for step in range(max_steps):
            # é–«å¤‹å«¨é”ã„¤ç¶”
            if self.algorithm == "DQN":
                # DQNæ©æ–¿æ´–ç»‚ç»˜æšé”ã„¤ç¶”
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    # æ¾¶å‹­æ‚Šé™îˆå…˜é¨å‹«å“ç¼å‹®ç¹‘é¥?
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                        
                # é—‡â‚¬ç‘•ä½¸çš¢é”ã„¤ç¶”é„çŠ²çš é¥ç‚²åçâ‚¬é”ã„¤ç¶”ç»±ãˆ ç´©
                action_idx = self._encode_discrete_action(actions_dict)
                action = action_idx
            else:
                # æ©ç‚µç”»é”ã„¤ç¶”ç» æ¥ç¡¶
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    # æ¾¶å‹­æ‚Šé™îˆå…˜é¨å‹«å“ç¼å‹®ç¹‘é¥?
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                action = self._encode_continuous_action(actions_dict)
            
            # é¦ƒæ•¡ é‡å­˜æŸŠepisodeå§ãƒ¦æšŸç’â„ƒæšŸé£?
            self._current_episode_step += 1
            
            # éµÑ†î”‘é”ã„¤ç¶”é”›å çš¢é”ã„¤ç¶”ç€›æ¥€å€æµ¼çŠ²å†æµ ãƒ¥å¥–éå¶„è±¢éªç†·æ«’é—æ­Œæµ‡é‹å¿“ã‚½é”›?
            next_state, reward, done, info = self.step(action, state, actions_dict)
            
            # é¦ƒå¹† æ·‡æ¿†ç“¨éˆî„î„é¨å‰†tep_statsæ¸šæ¶—æ¢é”â€³åç”¯å†ªç²ºç’â€²å¨‡é¢?
            step_stats = info.get('step_stats', {})
            episode_step_stats.append(step_stats)

            # çå”–tepç»¾ÑƒåŸ†é¨å‹¯æ§¦é’æ¥å¯šéå›§æ‚“å§ãƒ§ç²°é€îˆ›å¯”é¨å‹¬æ«¤é‘³æˆ’ç¶‹é”›åœ¦ueue-aware Replayé”›?
            if hasattr(self.agent_env, 'update_queue_metrics'):
                try:
                    self.agent_env.update_queue_metrics(step_stats)  # type: ignore[attr-defined]
                except Exception:
                    pass

            # çå—›æ§¦é’?ç¼‚æ’³ç“¨é˜å¬ªå§æµ¼çŠ»â‚¬æ”ç²°é€îˆ›å¯”é¨å‹¬æ«¤é‘³æˆ’ç¶‹é¢ã„¤ç°¬PERæµ¼æ¨ºå›æ´ï¸½æ–æ¾¶?
            if hasattr(self.agent_env, 'update_priority_signal'):
                try:
                    queue_pressure = float(max(
                        step_stats.get('queue_overload_flag', 0.0),
                        step_stats.get('queue_rho_max', 0.0),
                        step_stats.get('cache_eviction_rate', 0.0),
                    ))
                    self.agent_env.update_priority_signal(queue_pressure)  # type: ignore[attr-defined]
                except Exception:
                    pass
            
            # é’æ¿†îé–æ¢raining_info
            training_info = {}
            
            # ç’î… ç²Œé…é¸¿å…˜æµ£?- éµâ‚¬éˆå¤Œç•»å¨‰æ› å¹‡é¦ã„©å…˜é€îˆ›å¯”Unionç»«è¯²ç€·ç¼ç†¶ç«´éºãƒ¥å½›
            # çº­î†»ç¹šactionç»«è¯²ç€·ç€¹å¤Šåæî„å´²
            if self.algorithm == "DQN":
                # DQNæ££æ ­â‚¬å¤‹æš£éæ¿å§©æµ£æ»ç´æµ£å—˜å¸´é™æ¡¿nionç»«è¯²ç€·
                safe_action = self._safe_int_conversion(action)
                training_info = self.agent_env.train_step(state, safe_action, reward, next_state, done)
            elif self.algorithm in ["DDPG", "TD3", "TD3_LATENCY_ENERGY", "SAC", "OPTIMIZED_TD3"]:
                # æ©ç‚µç”»é”ã„¤ç¶”ç» æ¥ç¡¶æ££æ ­â‚¬å¡¶umpyéæ‰®ç²é”›å±¼çµ¾éºãƒ¥å½ˆUnionç»«è¯²ç€·
                if isinstance(action, np.ndarray):
                    safe_action = action
                elif isinstance(action, (int, float)):
                    safe_action = np.array([float(action)], dtype=np.float32)
                else:
                    safe_action = np.array(action, dtype=np.float32)
                training_info = self.agent_env.train_step(state, safe_action, reward, next_state, done)  # type: ignore[arg-type]
            elif self.algorithm == "PPO":
                # PPOæµ£è·¨æ•¤é—è§„ç•©é¨åˆ¥pisodeç»¾ÑƒåŸ†ç’î… ç²Œé”›å®¼rain_stepæ¶“å“„å´°æµ£å¶‡îƒ
                # æ·‡æ¿‡å¯”é˜ç„Œctionç»«è¯²ç€·é—å†²å½²é”›å±½æ´œæ¶“ç¯œPOé¨å‰‡rain_stepæ¶“å¶…ä»›ç€¹ç‚ºæª¯æ¾¶å‹­æ‚Š
                training_info = self.agent_env.train_step(state, action, reward, next_state, done)  # type: ignore[arg-type]
            else:
                # éæœµç²¬ç» æ¥ç¡¶é¨å‹¯ç²¯ç’ã‚…î˜©é?
                training_info = {'message': f'Unknown algorithm: {self.algorithm}'}
            
            episode_info = training_info
            
            # é‡å­˜æŸŠé˜èˆµâ‚¬?
            state = next_state
            episode_reward += reward
            
            # å¦«â‚¬éŒãƒ¦æ§¸éšï¸¾ç²¨é‰?
            if done:
                break
        
        episode_reward = float(episode_reward)
        if not np.isfinite(episode_reward):
            episode_reward = 0.0

        # ç’æ¿ç¶æî†½î‚¼ç¼ç†»î…¸
        system_metrics = info.get('system_metrics', {})
        self._record_episode_metrics(system_metrics, episode_steps=step + 1)
        self._maybe_update_dynamic_energy_target(
            episode,
            float(system_metrics.get('total_energy_consumption', 0.0) or 0.0)
        )
        self._maybe_update_dynamic_latency_target(
            episode,
            float(system_metrics.get('avg_task_delay', 0.0) or 0.0)
        )
        
        # ç’‹å†ªæ•¤CAM-TD3 episodeç¼æ’´æ½«é¥ç‚¶çšŸé”›å±¾æ´¿é‚æ‹Œç€ºéšå ¢ç“¥é£?
        if isinstance(self.agent_env, CAMTD3Environment) and hasattr(self.agent_env, 'on_episode_end'):
            self.agent_env.on_episode_end(episode_reward)
        
        return {
            'episode_reward': episode_reward,
            'avg_reward': episode_reward,
            'episode_info': episode_info,
            'system_metrics': system_metrics,
            'steps': step + 1,
            'step_stats_list': episode_step_stats  # é¦ƒå¹† æ©æ–¿æ´–å§£å¿é‡œstepé¨å‹­ç²ºç’â„ƒæšŸé¹?
        }
    
    def _run_ppo_episode(self, episode: int, max_steps: int = 100) -> Dict:
        """æ©æ„¯î”‘PPOæ¶“æ’¶æ•¤episode"""
        state = self.reset_environment()
        episode_reward = 0.0
        
        # é’æ¿†îé–æ §å½‰é–²?
        done = False
        step = 0
        info = {}
        
        for step in range(max_steps):
            # é‘¾å³°å½‡é”ã„¤ç¶”éŠ†ä½¸î‡®éç‰ˆî›§éœå›§æ‹°æµ å³°â‚¬?
            if hasattr(self.agent_env, 'get_actions'):
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, tuple) and len(actions_result) == 3:
                    actions_dict, log_prob, value = actions_result
                else:
                    # æ¿¡å‚›ç‰æ¶“å¶†æ§¸éå†ªç²é”›å±½æ°¨æµ£è·¨æ•¤æ¦›æ¨¿î…»éŠ?
                    actions_dict = actions_result if isinstance(actions_result, dict) else {}
                    log_prob = 0.0
                    value = 0.0
            else:
                actions_dict = {}
                log_prob = 0.0
                value = 0.0
                
            action = self._encode_continuous_action(actions_dict)
            
            # éµÑ†î”‘é”ã„¤ç¶”
            next_state, reward, done, info = self.step(action, state, actions_dict)
            
            # ç€›æ¨ºåç¼å¿›ç™ - éµâ‚¬éˆå¤Œç•»å¨‰æ›¢å…˜é€îˆ›å¯”ç¼ç†¶ç«´éºãƒ¥å½›
            # çº­î†»ç¹šé™å‚›æšŸç»«è¯²ç€·å§ï½‡â€˜
            log_prob_float = float(log_prob) if not isinstance(log_prob, float) else log_prob
            value_float = float(value) if not isinstance(value, float) else value
            # æµ£è·¨æ•¤é›è—‰æ‚•é™å‚›æšŸé–¬å®å¤æµ£å¶‡ç–†é™å‚›æšŸæ¤¤å“„ç°­é—‚î‡€î•½
            self.agent_env.store_experience(
                state=state, 
                action=action, 
                reward=reward, 
                next_state=next_state, 
                done=done, 
                log_prob=log_prob_float, 
                value=value_float
            )
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        # é¦ƒæ•¡ PPOé‡å­˜æŸŠç»›æ «æšæ·‡î†¼î˜²é”›æ°±ç–®ç»‰îˆšî˜¿æ¶“çŒ pisodeéšåº¡å•€é‡å­˜æŸŠ
        last_value = 0.0
        if not done:
            if hasattr(self.agent_env, 'get_actions'):
                actions_result = self.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, tuple) and len(actions_result) >= 3:
                    _, _, last_value = actions_result
                else:
                    last_value = 0.0
        
        # çº­î†»ç¹š last_value æ¶“?float ç»«è¯²ç€·
        last_value_float = float(last_value) if not isinstance(last_value, float) else last_value
        
        # å¦«â‚¬éŒãƒ¦æ§¸éšï¹€ç°²ç’‡ãƒ¦æ´¿é‚å¸®ç´™å§£å»šæ¶“çŒ pisodeé´æœ¾ufferè¹‡î‚£å¼§éƒè®¹ç´š
        ppo_config = self.agent_env.config
        update_freq = getattr(ppo_config, 'update_frequency', 1)
        buffer_size = getattr(ppo_config, 'buffer_size', 1000)
        agent_obj = getattr(self.agent_env, 'agent', None)
        if agent_obj is not None:
            agent_buffer = getattr(agent_obj, 'buffer', None)  # type: ignore[union-attr]
            buffer_current_size = agent_buffer.size if agent_buffer is not None else 0
        else:
            buffer_current_size = 0
        should_update = (
            episode % max(1, update_freq) == 0 or  # å§£å»šæ¶“çŒ pisode
            buffer_current_size >= buffer_size * 0.9  # bufferéºãƒ¨ç¹å©Š?
        )
        
        # æ©æ¶œî”‘é‡å­˜æŸŠ
        # PPOEnvironment.updateé™î…å¸´é™æ¢ast_valueé™å‚›æšŸ
        training_info: Dict = {}
        if agent_obj is not None:
            training_info = agent_obj.update(last_value_float)  # type: ignore[call-arg]
        
        system_metrics = info.get('system_metrics', {})
        
        return {
            'episode_reward': episode_reward,
            'avg_reward': episode_reward,
            'episode_info': training_info,
            'system_metrics': system_metrics,
            'steps': step + 1
        }

    def _build_simulator_actions(self, actions_dict: Optional[Dict]) -> Optional[Dict]:
        """çå—™ç•»å¨‰æ›å§©æµ£æ»ƒç“§éæ­Œæµ†é¹î­è´Ÿæµ è·¨æ¹¡é£ã„¥å½²å¨‘å £å‚é¨å‹­ç•é—æ›Ÿå¸¶é’æœµä¿Šé™æ«â‚¬?
        é¦ƒî˜» éµâ•çé€îˆ›å¯”é‘±æ–¿æ‚é”ã„¤ç¶”ç»Œæ´ªæ£¿é”›?
        - vehicle_agent é“?ç¼?éˆ«?é˜ç†¸æ¹æµ è¯²å§Ÿé’å—›å¤é‹å¿“ã‚½
        - æ¶“î…¢æ£¿ num_rsus/num_uavs ç¼?éˆ«?é‘ºå‚œå£é–«å¤‹å«¨é‰å†®å™¸
        - éˆî‚¢ç†¬10ç¼?éˆ«?ç¼‚æ’³ç“¨éŠ†ä½½ç¸¼ç»‰è¯²å¼·é‘±æ–¿å§©éºÑƒåŸ—é™å‚›æšŸ
        """
        if not isinstance(actions_dict, dict):
            return None
        vehicle_action = actions_dict.get('vehicle_agent')
        if vehicle_action is None:
            return None
        try:
            import numpy as np
            
            vehicle_action_array = np.array(vehicle_action, dtype=np.float32).reshape(-1)
            expected_dim = getattr(self.agent_env, 'action_dim', vehicle_action_array.size)
            if vehicle_action_array.size < expected_dim:
                padded = np.zeros(expected_dim, dtype=np.float32)
                padded[:vehicle_action_array.size] = vehicle_action_array
                vehicle_action_array = padded
            else:
                vehicle_action_array = vehicle_action_array[:expected_dim]
            
            # =============== é˜ç†¸æ¹æµ è¯²å§Ÿé’å—›å¤é–«æ˜ç·« (æ·‡æ¿‡å¯”éç…î†) ===============
            raw = vehicle_action_array[:3]
            raw = np.clip(raw, -5.0, 5.0)
            
            # é‰?ç»‰å©šæ«é‹å¿•ç–†é”›å²ƒî†€é…é¸¿å…˜æµ£æ’»â‚¬æ°³ç¹ƒæ¿‚æ §å§³æ·‡â€³å½¿éªç†¸î„œç€›ï¸¿ç¯„
            # æ¿‚æ §å§³é‘èŠ¥æšŸå®¸èŒ¬ç²¡å¯®å“„å¯²é”›æ­ŠSU=8.0, UAV=1.0, Local penalty=4.0
            # æ©æ¬ç´°é»æ„ªç·µå¨“å‘®æ«šé¨å‹«î„Ÿæ¶”çŠ±ä¿Šé™å‡¤ç´å¯®æ›î‡±é…é¸¿å…˜æµ£æ’³æ‚œRSUé—æ­Œæµ‡
            
            exp = np.exp(raw - np.max(raw))
            probs = exp / np.sum(exp)
            
            sim_actions = {
                'vehicle_offload_pref': {
                    'local': float(probs[0]),
                    'rsu': float(probs[1] if probs.size > 1 else 0.33),
                    'uav': float(probs[2] if probs.size > 2 else 0.34)
                },
                # ç’æ¿ç¶é˜ç†·îsoftmaxé¢ã„¤ç°¬ç’‡å©ƒæŸ‡
                'offload_probs_raw': probs.tolist()
            }
            # RSUé–«å¤‹å«¨å§’å‚œå·¼
            num_rsus = self.num_rsus
            rsu_action = actions_dict.get('rsu_agent')
            if isinstance(rsu_action, (list, tuple, np.ndarray)) and num_rsus > 0:
                rsu_raw = np.array(rsu_action[:num_rsus], dtype=np.float32)
            else:
                rsu_raw = vehicle_action_array[3:3 + num_rsus]
            if num_rsus > 0:
                rsu_raw = np.clip(rsu_raw, -5.0, 5.0)
                rsu_exp = np.exp(rsu_raw - np.max(rsu_raw))
                rsu_probs = rsu_exp / np.sum(rsu_exp)
                sim_actions['rsu_selection_probs'] = rsu_probs.tolist()  # type: ignore[assignment]
            
            # UAVé–«å¤‹å«¨å§’å‚œå·¼
            num_uavs = self.num_uavs
            uav_action = actions_dict.get('uav_agent')
            if isinstance(uav_action, (list, tuple, np.ndarray)) and num_uavs > 0:
                uav_raw = np.array(uav_action[:num_uavs], dtype=np.float32)
            else:
                uav_raw = vehicle_action_array[3 + num_rsus:3 + num_rsus + num_uavs]
            if num_uavs > 0:
                uav_raw = np.clip(uav_raw, -5.0, 5.0)
                uav_exp = np.exp(uav_raw - np.max(uav_raw))
                uav_probs = uav_exp / np.sum(uav_exp)
                sim_actions['uav_selection_probs'] = uav_probs.tolist()  # type: ignore[assignment]
            
            # é¦ƒî˜» =============== é‚æ¿î–ƒé‘±æ–¿æ‚ç¼‚æ’³ç“¨-æ©ä½ºĞ©éºÑƒåŸ—é™å‚›æšŸ ===============
            control_start = 3 + num_rsus + num_uavs
            control_end = control_start + 10
            cache_migration_actions = vehicle_action_array[control_start:control_end]
            if cache_migration_actions.size < 10:
                padded = np.zeros(10, dtype=np.float32)
                padded[:cache_migration_actions.size] = cache_migration_actions
                cache_migration_actions = padded
            cache_migration_actions = np.clip(cache_migration_actions, -1.0, 1.0)

            cache_params, migration_params, joint_params = map_agent_actions_to_params(cache_migration_actions)

            self.adaptive_cache_controller.update_agent_params(cache_params)
            if not self.disable_migration:
                self.adaptive_migration_controller.update_agent_params(migration_params)
            if getattr(self, 'strategy_coordinator', None) is not None:
                self.strategy_coordinator.update_joint_params(joint_params)

            payload = {
                'adaptive_cache_params': cache_params,
                'cache_controller': self.adaptive_cache_controller,
                'joint_strategy_params': joint_params,
            }
            if not self.disable_migration:
                payload.update({
                    'adaptive_migration_params': migration_params,
                    'migration_controller': self.adaptive_migration_controller
                })
            sim_actions.update(payload)

            # é¦ƒæ”£ ç’â•ƒéƒ´ç¼ç†¸Äé·ç†·æ«’éºãƒ¦æ•¹Actorç€µç…åš­é¨å‹¬å¯šç€µé—´ä¿Šé™å‡¤ç´™ç¼ç†¶ç«´é–¿î†¼æ‚•æ¶“ç°‰l_guidanceé”›?
            guidance_payload = actions_dict.get('guidance') if isinstance(actions_dict, dict) else None
            if isinstance(guidance_payload, dict) and guidance_payload:
                sim_actions['rl_guidance'] = guidance_payload

            # é¦ƒå¹† =============== æ¶“î…ãç’§å‹¬ç°®é’å—›å¤é”ã„¤ç¶” (Phase 1) ===============
            if self.central_resource_enabled and self.central_resource_action_dim > 0:
                central_start = self.base_action_dim
                central_end = central_start + self.central_resource_action_dim
                central_vector = vehicle_action_array[central_start:central_end]
                allocations = self._decode_central_resource_actions(central_vector)
                if allocations:
                    try:
                        self.simulator.apply_resource_allocation(allocations)
                        sim_actions['central_resource_allocation'] = allocations  # type: ignore[assignment]
                    except Exception as exc:
                        print(f"éˆ¿ç‹…ç¬ æ¶“î…ãç’§å‹¬ç°®é’å—›å¤æ´æ—‚æ•¤æ¾¶è¾«è§¦: {exc}")
            
            forced_mode = getattr(self, 'enforce_offload_mode', '')
            if forced_mode == 'local_only':
                sim_actions['vehicle_offload_pref'] = {'local': 1.0, 'rsu': 0.0, 'uav': 0.0}
            elif forced_mode == 'remote_only':
                if num_rsus == 0 and num_uavs == 0:
                    sim_actions['vehicle_offload_pref'] = {'local': 1.0, 'rsu': 0.0, 'uav': 0.0}
                elif num_rsus == 0:
                    sim_actions['vehicle_offload_pref'] = {'local': 0.0, 'rsu': 0.0, 'uav': 1.0}
                elif num_uavs == 0:
                    sim_actions['vehicle_offload_pref'] = {'local': 0.0, 'rsu': 1.0, 'uav': 0.0}
                else:
                    sim_actions['vehicle_offload_pref'] = {'local': 0.0, 'rsu': 0.5, 'uav': 0.5}

            # Attach distance-cache tradeoff gate for heuristic guidance (if actor exposes it)
            try:
                import numpy as _np  # safe local import
                actor_obj = getattr(self.agent_env, 'agent', None)
                if actor_obj is not None:
                    actor_obj = getattr(actor_obj, 'actor', None)
                gate = None
                if actor_obj is not None:
                    gate = getattr(actor_obj, 'last_tradeoff_gate', None)
                    if gate is None:
                        enc = getattr(actor_obj, 'encoder', None)
                        if enc is not None:
                            gate = getattr(enc, 'last_gate', None)
                if gate is not None:
                    try:
                        sim_actions['dc_tradeoff_gate'] = float(_np.clip(gate, 0.0, 1.0))  # type: ignore[assignment]
                    except Exception:
                        pass
            except Exception:
                pass

            return sim_actions
        except Exception as e:
            print(f"éˆ¿ç‹…ç¬ é”ã„¤ç¶”é‹å‹¯â‚¬çŠ²ç´“ç”¯? {e}")
            return None
    
    def _collect_resource_state(self) -> Optional[Dict[str, Any]]:
        if not self.central_resource_enabled:
            return None
        resource_pool = getattr(self.simulator, 'resource_pool', None)
        if resource_pool is None:
            return None
        try:
            return resource_pool.get_resource_state()
        except Exception:
            return None
    
    @staticmethod
    def _normalize_allocation(vector: np.ndarray, size: int) -> np.ndarray:
        if size <= 0:
            return np.zeros(0, dtype=np.float32)
        vec = np.array(vector, dtype=np.float32).reshape(-1)
        if vec.size < size:
            vec = np.pad(vec, (0, size - vec.size), constant_values=0.0)
        elif vec.size > size:
            vec = vec[:size]
        vec = np.clip(vec, 0.0, 1.0)
        total = float(np.sum(vec))
        if total <= 1e-6:
            return np.full(size, 1.0 / size, dtype=np.float32)
        return (vec / total).astype(np.float32)
    
    def _decode_central_resource_actions(
        self, central_vector: np.ndarray
    ) -> Optional[Dict[str, np.ndarray]]:
        if not self.central_resource_enabled or self.central_resource_action_dim <= 0:
            return None
        vector = np.array(central_vector, dtype=np.float32).reshape(-1)
        expected = self.central_resource_action_dim
        if vector.size < expected:
            padded = np.zeros(expected, dtype=np.float32)
            padded[:vector.size] = vector
            vector = padded
        elif vector.size > expected:
            vector = vector[:expected]
        vector = np.clip(vector, 0.0, 1.0)
        
        idx = 0
        bandwidth = self._normalize_allocation(
            vector[idx:idx + self.num_vehicles], self.num_vehicles
        )
        idx += self.num_vehicles
        vehicle_compute = self._normalize_allocation(
            vector[idx:idx + self.num_vehicles], self.num_vehicles
        )
        idx += self.num_vehicles
        rsu_compute = self._normalize_allocation(
            vector[idx:idx + self.num_rsus], self.num_rsus
        )
        idx += self.num_rsus
        uav_compute = self._normalize_allocation(
            vector[idx:idx + self.num_uavs], self.num_uavs
        )
        
        return {
            'bandwidth': bandwidth,
            'vehicle_compute': vehicle_compute,
            'rsu_compute': rsu_compute,
            'uav_compute': uav_compute,
        }
    
    def _encode_continuous_action(self, actions_dict) -> np.ndarray:
        """
        é¦ƒî˜» çå——å§©æµ£æ»ƒç“§éå“¥ç´ªé®ä½·è´Ÿæ©ç‚µç”»é”ã„¤ç¶”éšæˆ¦å™º - é”ã„¦â‚¬ä¾€â‚¬å‚å¤é”ã„¤ç¶”ç¼æ‘å®³
        """
        # æ¾¶å‹­æ‚Šé™îˆå…˜é¨å‹ªç¬‰éšå²ƒç·­éãƒ§è¢«é¨?
        action_dim = getattr(self.agent_env, 'action_dim', 18)
        if not isinstance(actions_dict, dict):
            # æ¿¡å‚›ç‰æ¶“å¶†æ§¸ç€›æ¥€å€é”›å²ƒç¹‘é¥ç‚ºç²¯ç’ã‚…å§©æµ£æ»…æ·®æ´?
            return np.zeros(action_dim, dtype=np.float32)

        # é¦ƒî˜» é™îƒå¨‡é¢â•²ehicle_agenté¨å‹«ç•¬éæ‘å§©æµ£æ»ƒæ‚œé–²?
        vehicle_action = actions_dict.get('vehicle_agent')
        if isinstance(vehicle_action, (list, tuple, np.ndarray)):
            vehicle_action = np.array(vehicle_action, dtype=np.float32)
            if vehicle_action.size >= action_dim:
                return vehicle_action[:action_dim]
            action = np.zeros(action_dim, dtype=np.float32)
            action[:vehicle_action.size] = vehicle_action
            return action

        # æ¦›æ¨¿î…»æ©æ–¿æ´–éã„©æµ‚é”ã„¤ç¶”
        return np.zeros(action_dim, dtype=np.float32)
    
    def _build_actions_from_vector(self, action_vector: np.ndarray) -> Dict[str, np.ndarray]:
        """çå—šç¹›ç¼î…å§©æµ£æ»ƒæ‚œé–²å¿”ä»®æ¾¶å¶„è´Ÿæµ è·¨æ¹¡é£ã„©æ¸¶ç‘•ä½ºæ®‘é”ã„¤ç¶”ç€›æ¥€å€é”›å å§©é¬ä½ºæ·®æ´ï¸¼ç´š"""
        import numpy as np

        if not isinstance(action_vector, np.ndarray):
            action_vector = np.array(action_vector, dtype=np.float32)

        action_dim = getattr(self.agent_env, 'action_dim', action_vector.size)
        if action_vector.size < action_dim:
            padded = np.zeros(action_dim, dtype=np.float32)
            padded[:action_vector.size] = action_vector
            action_vector = padded
        else:
            action_vector = action_vector.astype(np.float32)[:action_dim]

        num_rsus = len(getattr(self.simulator, 'rsus', []))
        num_uavs = len(getattr(self.simulator, 'uavs', []))
        rsu_start = 3
        rsu_end = rsu_start + num_rsus
        uav_end = rsu_end + num_uavs

        return {
            'vehicle_agent': action_vector,
            'rsu_agent': action_vector[rsu_start:rsu_end],
            'uav_agent': action_vector[rsu_end:uav_end]
        }

    def _encode_discrete_action(self, actions_dict) -> int:
        """çå——å§©æµ£æ»ƒç“§éå“¥ç´ªé®ä½·è´Ÿç»‚ç»˜æšé”ã„¤ç¶”ç»±ãˆ ç´©"""
        # æ¾¶å‹­æ‚Šé™îˆå…˜é¨å‹ªç¬‰éšå²ƒç·­éãƒ§è¢«é¨?
        if not isinstance(actions_dict, dict):
            return 0  # æ¦›æ¨¿î…»é”ã„¤ç¶”ç»±ãˆ ç´©
        
        # ç» â‚¬é–æ §ç–„éœå¸®ç´°çå—˜ç˜¡æ¶“î…æ«¤é‘³æˆ’ç¶‹é¨å‹«å§©æµ£æ»…ç²éšå Ÿåšæ¶“â‚¬æ¶“î†å‚¨å¯®?
        vehicle_action = actions_dict.get('vehicle_agent', 0)
        rsu_action = actions_dict.get('rsu_agent', 0)
        uav_action = actions_dict.get('uav_agent', 0)
        
        # ç€¹å¤Šåé¦æ¿çš¢é”ã„¤ç¶”æî„å´²æ¶“çƒ˜æš£é?
        def safe_int_conversion(value):
            if isinstance(value, (int, np.integer)):
                return int(value)
            elif isinstance(value, np.ndarray):
                if value.size == 1:
                    return int(value.item())
                else:
                    return int(value[0])  # é™æ «îƒ‡æ¶“â‚¬æ¶“î„å“ç»±?
            elif isinstance(value, (float, np.floating)):
                return int(value)
            else:
                return 0
        
        vehicle_action = safe_int_conversion(vehicle_action)
        rsu_action = safe_int_conversion(rsu_action)
        uav_action = safe_int_conversion(uav_action)
        
        # 5^3 = 125 ç»‰å¶‡ç²éš?
        return vehicle_action * 25 + rsu_action * 5 + uav_action
    
    def _safe_int_conversion(self, value) -> int:
        """ç€¹å¤Šåé¦æ¿çš¢æ¶“å¶…æ‚“ç»«è¯²ç€·æî„å´²æ¶“çƒ˜æš£é?""
        if isinstance(value, (int, np.integer)):
            return int(value)
        elif isinstance(value, np.ndarray):
            if value.size == 1:
                return int(value.item())
            else:
                return int(value[0])  # é™æ «îƒ‡æ¶“â‚¬æ¶“î„å“ç»±?
        elif isinstance(value, (float, np.floating)):
            return int(round(value))
        else:
            return 0  # ç€¹å¤Šåé¥ç‚ºâ‚¬â‚¬éŠ?


def train_single_algorithm(algorithm: str, num_episodes: Optional[int] = None, eval_interval: Optional[int] = None,
                          save_interval: Optional[int] = None, enable_realtime_vis: bool = False,
                          vis_port: int = 5000, silent_mode: bool = False, override_scenario: Optional[Dict[str, Any]] = None,
                          use_enhanced_cache: bool = False, disable_migration: bool = False,
                          enforce_offload_mode: Optional[str] = None, fixed_offload_policy: Optional[str] = None,
                          resume_from: Optional[str] = None, resume_lr_scale: Optional[float] = None,
                          joint_controller: bool = False, enable_advanced_vis: bool = False) -> Dict:
    """ç’î… ç²Œé—æ›šé‡œç» æ¥ç¡¶
    
    Args:
        algorithm: ç» æ¥ç¡¶éšå¶‡Ğ
        num_episodes: ç’î… ç²Œæî†½î‚¼
        eval_interval: ç’‡å‹ªåŠé—‚æ’®æ®§
        save_interval: æ·‡æ¿†ç“¨é—‚æ’®æ®§
        enable_realtime_vis: é„îˆšæƒéšîˆœæ•¤ç€¹ç‚´æ¤‚é™îˆî‹é–?
        vis_port: é™îˆî‹é–æ ¨æ¹‡é”â€³æ«’ç»”îˆšå½›
        silent_mode: é—ˆæ¬“ç²¯å¦¯â€³ç´¡é”›å²ƒçƒ¦æ©å›©æ•¤é´èœ‚æ°¦æµœæç´™é¢ã„¤ç°¬éµå½’å™ºç€¹ç‚ºç™é”›?
        resume_from: å®¸èŒ¶î†„ç¼å†©Äé¨å¬­çŸ¾å¯°å‹¶ç´™.pth é´æ «æ´°è¤°æ›å¢ ç¼‚â‚¬é”›å¤›ç´é¢ã„¤ç°¬warm-startç¼Ñ…ç”»ç’î… ç²Œ
        resume_lr_scale: Warm-startéšåº¡î‡®ç€›ï¸¿ç¯„éœå›©æ®‘ç¼‚â•‚æ–ç»¯ç»˜æšŸé”›å ¥ç²¯ç’?.5é”›å­¨oneç›ã„§ãšæ·‡æ¿‡å¯”é˜ç†·â‚¬ç¡·ç´š
        enable_advanced_vis: é„îˆšæƒéšîˆœæ•¤æ¥‚æ¨¼î¬ç’î… ç²Œé™îˆî‹é–?
    """
    # ç€µç…å†æµ è¯²å§Ÿé’å——ç«·ç¼ç†»î…¸å¦¯â€³æ½¡
    from utils.training_analytics_integration import TaskAnalyticsTracker
    
    # æµ£è·¨æ•¤é–°å¶‡ç–†æ¶“î… æ®‘æ¦›æ¨¿î…»éŠ?
    if num_episodes is None:
        num_episodes = config.experiment.num_episodes

    # éä½½î†é¢ã„§å¹†æ¾§å†¨å½‰é–²å¿“æ©é–«ç†¼å™¸ç’æƒ§îš›é”è¾¨æ½ˆé–²?é©î†½çˆ£é”›å±¼ç©¶æµœåº¨ç®ç’ç†»æµ‡é¦çƒ˜æ«™é€èˆµæšƒ
    _apply_reward_overrides_from_env()
    
    # é¦ƒæ•¡ é‘·î„å§©ç’‹å†©æš£ç’‡å‹ªåŠé—‚æ’®æ®§éœå±¼ç¹šç€›æ©€æ£¿é—…?
    def auto_adjust_intervals(total_episodes: int):
        """éè§„åµé¬æ˜ç–†éæ‹Œåšœé”ã„¨çšŸéæ’®æ£¿é—…?""
        # ç’‡å‹ªåŠé—‚æ’®æ®§é”›æ°­â‚¬æ˜ç–†éæ‰®æ®‘5-8%é”›å²ƒå¯–é¥ç¢µ10, 100]
        auto_eval = max(10, min(100, int(total_episodes * 0.06)))
        
        # æ·‡æ¿†ç“¨é—‚æ’®æ®§é”›æ°­â‚¬æ˜ç–†éæ‰®æ®‘15-20%é”›å²ƒå¯–é¥ç¢µ50, 500]  
        auto_save = max(50, min(500, int(total_episodes * 0.18)))
        
        return auto_eval, auto_save
    
    # æ´æ—‚æ•¤é‘·î„å§©ç’‹å†©æš£é”›å œç²è¤°æ’¶æ•¤é´é”‹æ¹­é¸å›§ç•¾éƒè®¹ç´š
    if eval_interval is None or save_interval is None:
        auto_eval, auto_save = auto_adjust_intervals(num_episodes)
        if eval_interval is None:
            eval_interval = auto_eval
        if save_interval is None:
            save_interval = auto_save
    
    # éˆâ‚¬ç¼å æ´–é–«â‚¬é’ä¼´å¤ç¼ƒî‡€ç²¯ç’ã‚…â‚¬?
    if eval_interval is None:
        eval_interval = config.experiment.eval_interval
    if save_interval is None:
        save_interval = config.experiment.save_interval
    
    print(f"\n>> å¯®â‚¬æ¿®åª¨algorithm}é—æ›Ÿæ«¤é‘³æˆ’ç¶‹ç» æ¥ç¡¶ç’î… ç²Œ")
    print("=" * 60)
    
    # é’æ¶˜ç¼“ç’î… ç²Œéœîˆšî•¨é”›å ç°²é¢ã„©î–‚æ¾¶æ §æº€é…îˆî›«é©æ µç´š
    training_env = SingleAgentTrainingEnvironment(
        algorithm,
        override_scenario=override_scenario,
        use_enhanced_cache=use_enhanced_cache,
        disable_migration=disable_migration,
        enforce_offload_mode=enforce_offload_mode,
        fixed_offload_policy=fixed_offload_policy,
        joint_controller=joint_controller,
    )
    canonical_algorithm = training_env.algorithm
    if canonical_algorithm != algorithm:
        print(f"éˆ¿æ¬™ç¬  ç‘™å‹®å¯–é–æ «ç•»å¨‰æ›Ÿçˆ£ç’‡? {canonical_algorithm}")
    algorithm = canonical_algorithm

    resume_loaded = False
    resume_target_path = None
    if resume_from:
        loader = getattr(training_env.agent_env, 'load_models', None)
        if callable(loader):
            try:
                resume_target_path = loader(resume_from) or resume_from
                resume_loaded = True
                print(f"éˆ¾ä¼™ç¬  æµ åº¡å‡¡éˆå¤‹Äé¨å¬ªå§æèŠ¥åšé”? {resume_target_path}")
            except Exception as exc:  # pragma: no cover - ç€¹å½’æ•Šç’ºîˆšç·
                print(f"éˆ¿ç‹…ç¬  é”çŠºæµ‡å®¸å‰æ¹å¦¯â€³ç€·æ¾¶è¾«è§¦ ({resume_from}): {exc}")
        else:
            print("éˆ¿ç‹…ç¬  è¤°æ’³å¢ ç» æ¥ç¡¶éœîˆšî•¨æ¶“å¶†æ•®é¸ä½¸å§æè—‰å‡¡éˆå¤‹Äé¨å¬¶ç´è¹‡ç•Œæš --resume-from")

        if resume_loaded:
            agent_obj = getattr(training_env.agent_env, 'agent', None)
            warmup_adjusted = False
            original_warmup = 0
            new_warmup = 0
            if agent_obj and hasattr(agent_obj, 'config') and hasattr(agent_obj.config, 'warmup_steps'):
                original_warmup = int(getattr(agent_obj.config, 'warmup_steps', 0) or 0)
                new_warmup = max(500, original_warmup // 4) if original_warmup else 500
                if original_warmup and new_warmup < original_warmup:
                    agent_obj.config.warmup_steps = new_warmup
                    warmup_adjusted = True
            if warmup_adjusted:
                print(f"   éˆ¥?Warm-up å§ãƒ¦æšŸé¢?{original_warmup} ç¼‚â•å™ºé‘·?{new_warmup}é”›å±½å§é–«ç†ºç²¡æ¥ å²€ç´¦éæŸ¥å™¸é‚æ¿ï½é?)

            lr_scale_value = resume_lr_scale if resume_lr_scale is not None else 0.5
            lr_info = None
            lr_callback = getattr(training_env.agent_env, 'apply_late_stage_lr', None)
            if callable(lr_callback) and lr_scale_value:
                try:
                    lr_info = lr_callback(factor=lr_scale_value, min_lr=5e-5)
                except Exception:
                    lr_info = None
            elif agent_obj and hasattr(agent_obj, 'apply_lr_schedule') and lr_scale_value:
                try:
                    lr_info = agent_obj.apply_lr_schedule(factor=lr_scale_value, min_lr=5e-5)
                except Exception:
                    lr_info = None
            if lr_info and isinstance(lr_info, dict):
                print(f"   éˆ¥?ç€›ï¸¿ç¯„éœå›©ç¼‰é€? actor_lr={lr_info.get('actor_lr', 0):.2e}, critic_lr={lr_info.get('critic_lr', 0):.2e}")
            elif resume_lr_scale:
                print("   éˆ¥?ç€›ï¸¿ç¯„éœå›©ç¼‰é€æî‡¬å§¹å‚›æ¹­éµÑ†î”‘é”›å ç¶‹é“å¶‡ç•»å¨‰æ› å¹†æ¾§å†©æ¹­ç€¹ç‚µå¹‡ apply_lr_scheduleé”›?)

    lr_decay_episode: Optional[int] = None
    late_stage_lr_factor = 0.5
    lr_decay_applied = resume_loaded  # warm-start å®¸èŒ¬ç²¡ç¼‚â•‚æ–æ©å›¦ç«´å¨†â€³î„Ÿæ¶”çŠµå·¼
    if algorithm.upper() == 'TD3' and num_episodes >= 1200:
        lr_decay_episode = 1200

    # é¦ƒå¯ª é’æ¶˜ç¼“ç€¹ç‚´æ¤‚é™îˆî‹é–æ §æ«’é”›å î›§é‹æ»ƒæƒé¢îŸ’ç´š
    visualizer = None
    advanced_visualizer = None
    
    # é¦ƒå¸¹ æµ¼æ¨ºå›æµ£è·¨æ•¤æ¥‚æ¨¼î¬é™îˆî‹é–æ µç´™é‡æ‘ã‚½é¨å‹¬æ¨‰ç»€çƒ˜æ™¥é‹æ»ç´š
    if enable_advanced_vis and ADVANCED_VIS_AVAILABLE:
        print("é¦ƒå¸¹ éšîˆšå§©æ¥‚æ¨¼î¬ç’î… ç²Œé™îˆî‹é–?Dashboard")
        advanced_visualizer = create_advanced_visualizer(max_history=min(500, num_episodes))  # type: ignore[name-defined]
        advanced_visualizer.start(interval=1000)  # å§£å¿•î—é’é”‹æŸŠæ¶“â‚¬å¨†?
        print("é‰?æ¥‚æ¨¼î¬é™îˆî‹é–æ §å‡¡éšîˆœæ•¤")
        print("   - é¸?'p' é†å‚šä» /ç¼Ñ…ç”»")
        print("   - é¸?'s' æ·‡æ¿†ç“¨é´î„æµ˜")
        print("   - é¸?'q' é–«â‚¬é‘?)
    elif enable_advanced_vis and not ADVANCED_VIS_AVAILABLE:
        print("éˆ¿ç‹…ç¬  æ¥‚æ¨¼î¬é™îˆî‹é–æ ¨æ¹­éšîˆœæ•¤é”›å ¢å·±çæˆœç··ç’§æ §å¯˜é”›?)
    
    # é¦ƒå¯ª Fallbacké’ç™¢ebé™îˆî‹é–?
    if enable_realtime_vis and REALTIME_AVAILABLE and not advanced_visualizer:
        print(f"é¦ƒå¯ª éšîˆšå§©ç€¹ç‚´æ¤‚é™îˆî‹é–æ ¨æ¹‡é”â€³æ«’ (ç»”îˆšå½›: {vis_port})")
        # éä½½î†é–«æ°³ç¹ƒéœîˆšî•¨é™æ©€å™ºç‘•å—™æ´Šé™îˆî‹é–æ §çç»€å“„æ‚•é”›å ¢æ•¤æµœåºè¢±é—ƒèˆµî†Œéå›©î„·é”›?
        display_name = os.environ.get('ALGO_DISPLAY_NAME', algorithm)
        visualizer = create_visualizer(  # type: ignore[name-defined]
            algorithm=display_name,
            total_episodes=num_episodes,
            port=vis_port,
            auto_open=True
        )
        print(f"é‰?ç€¹ç‚´æ¤‚é™îˆî‹é–æ §å‡¡éšîˆœæ•¤é”›å²ƒî†–é—‚?http://localhost:{vis_port}")
    elif enable_realtime_vis and not REALTIME_AVAILABLE:
        print("éˆ¿ç‹…ç¬  ç€¹ç‚´æ¤‚é™îˆî‹é–æ ¨æ¹­éšîˆœæ•¤é”›å ¢å·±çæˆœç··ç’§æ §å¯˜é”›?)
    
    print(f"ç’î… ç²Œé–°å¶‡ç–†:")
    print(f"  ç» æ¥ç¡¶: {algorithm}")
    print(f"  é¬æ˜ç–†å¨†? {num_episodes}")
    print(f"  ç’‡å‹ªåŠé—‚æ’®æ®§: {eval_interval} (é‘·î„å§©ç’‹å†©æš£)" if eval_interval != config.experiment.eval_interval else f"  ç’‡å‹ªåŠé—‚æ’®æ®§: {eval_interval}")
    print(f"  æ·‡æ¿†ç“¨é—‚æ’®æ®§: {save_interval} (é‘·î„å§©ç’‹å†©æš£)" if save_interval != config.experiment.save_interval else f"  æ·‡æ¿†ç“¨é—‚æ’®æ®§: {save_interval}")
    print(f"  æ¥‚æ¨¼î¬é™îˆî‹é–? {'éšîˆœæ•¤ é‰? if advanced_visualizer else 'ç»‚ä½ºæ•¤'}")
    print(f"  ç€¹ç‚´æ¤‚é™îˆî‹é–? {'éšîˆœæ•¤ é‰? if visualizer else 'ç»‚ä½ºæ•¤'}")
    if hasattr(config, 'rl'):
        print(
            f"  æ¿‚æ §å§³é‰å†®å™¸: å¯¤æƒ°ç¹œ={getattr(config.rl, 'reward_weight_delay', 0.0):.2f}, "
            f"é‘³å€Ÿâ‚¬?{getattr(config.rl, 'reward_weight_energy', 0.0):.2f}, "
            f"æ¶“ãˆ ç´”={getattr(config.rl, 'reward_penalty_dropped', 0.0):.2f}"
        )
        print(
            f"  é©î†½çˆ£ç»¾ï¸½æ½«: éƒè·ºæ¬¢éˆ®î˜¡getattr(config.rl, 'latency_target', 0.0):.2f}s, "
            f"é‘³å€Ÿâ‚¬æ¤»å¢¹{getattr(config.rl, 'energy_target', 0.0):.0f}J"
        )
    print("-" * 60)
    
    # é’æ¶˜ç¼“ç¼æ’´ç‰é©î†¼ç¶
    os.makedirs(f"results/single_agent/{algorithm.lower()}", exist_ok=True)
    os.makedirs(f"results/models/single_agent/{algorithm.lower()}", exist_ok=True)
    
    # é¦ƒå¹† é’æ¿†îé–æ ¦æ¢é”â€³î˜©éå—˜æŸŸå¯®å¿“åç”¯å†ªç²ºç’Â¤çª¡éŸªî„æ«’
    # éè§„åµepisodeéæ‹Œåšœé”ã„¨çšŸéå­˜æ£©è¹‡æ¥„ç·­é‘æ´ªæ£¿é—…?
    log_interval = max(1, num_episodes // 20) if num_episodes > 0 else 10
    analytics_tracker = TaskAnalyticsTracker(
        enable_logging=True,
        log_interval=log_interval
    )
    print(f"\né¦ƒæ³ å®¸æ’æƒé¢ã„¤æ¢é”â€³î˜©éå—˜æŸŸå¯®å¿“åç”¯å†ªç²ºç’â˜…ç´™å§£å¼¡log_interval}æ¶“çŒ pisodeæˆæ’³åš­æ¶“â‚¬å¨†â˜…ç´š")
    
    # ç’î… ç²Œå¯°î†å¹†
    # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ­±er-stepæ¿‚æ §å§³é‘¼å†¨æ´¿ç»¾ï¸¿è´Ÿ-2.0é’?0.5é”›å±½åµæ¿®å¬ªâ‚¬ç…ç°²é©ç¨¿ç°²ç’‹å†©æš£
    best_avg_reward = -10.0  # per-stepæ¿‚æ §å§³é’æ¿†îé—ƒå â‚¬ç¡·ç´™ç’ç†·â‚¬è‰°ç§ºæ¾¶Ñ†ç§ºæ¿‚æ–¤ç´š
    training_start_time = time.time()
    
    for episode in range(1, num_episodes + 1):
        episode_start_time = time.time()
        
        # é¦ƒå¹† å¯®â‚¬æ¿®å¬­î†‡è¤°æ›¡î‡šepisodeé¨å‹ªæ¢é”â€³åç”¯å†ªç²ºç’?
        analytics_tracker.start_episode(episode)
        
        # æ©æ„¯î”‘ç’î… ç²Œæî†½î‚¼
        episode_result = training_env.run_episode(episode)
        avg_reward_safe = float(episode_result.get('avg_reward', 0.0) or 0.0)
        if not np.isfinite(avg_reward_safe):
            avg_reward_safe = 0.0
        episode_result['avg_reward'] = avg_reward_safe
        episode_result['episode_reward'] = float(episode_result.get('episode_reward', avg_reward_safe) or avg_reward_safe)
        
        # é¦ƒå¹† ç’æ¿ç¶éˆç«pisodeéå‘®å¢éˆå¤Œæ®‘stepç¼ç†»î…¸
        step_stats_list = episode_result.get('step_stats_list', [])
        for step_idx, step_stats in enumerate(step_stats_list):
            analytics_tracker.record_step(step_idx, step_stats)
        
        # é¦ƒå¹† ç¼æ’´æ½«ç’‡î™«pisodeé¨å‹ªæ¢é”â€³åç”¯å†ªç²ºç’?
        episode_stats = analytics_tracker.end_episode()
        
        # ç’æ¿ç¶ç’î… ç²Œéç‰ˆåµ
        training_env.episode_rewards.append(episode_result['avg_reward'])
        
        episode_steps = episode_result.get('steps', config.experiment.max_steps_per_episode)
        
        # é¦ƒæ”§ é–²å¶ˆî›¦é”›æ°¬î‡®æµœå¶°PTIMIZED_TD3é”›å±¾æ´¿é‚ç™®genté¨åˆ¥pisodeç’â„ƒæšŸé”›å åºœé”â•…ä¼©éå¶…çœ¬é–®ã„¦æ¸¶æµ¼æ©ˆç´š
        if algorithm.upper() == 'OPTIMIZED_TD3' and hasattr(training_env.agent_env, 'agent'):
            agent = training_env.agent_env.agent
            if hasattr(agent, 'set_episode_count'):
                agent.set_episode_count(episode, episode_result['avg_reward'])
            
            # é¦ƒæ•Ÿ é‚æ¿î–ƒé”›æ°­î—…éŒãƒ¦æ§¸éšï¹€ç°²ç’‡ãƒ¦å½é“å¶‡ç²“å§ãˆ£î†„ç¼?(600æî†¼æ‚—)
            if hasattr(agent, 'check_early_stopping'):
                if agent.check_early_stopping():
                    print(f"\né‰?ç’î… ç²Œé¦â€¥pisode {episode}é»æ„¬å¢ ç¼å Ÿî„›é”›å±½å‡¡é€èˆµæšƒ")
                    # é‡å­˜æŸŠnum_episodesæµ ãƒ¦å½é“å¶‰â‚¬â‚¬é‘?
                    num_episodes = episode
                    break
        
        # é¦ƒæ”§ é–½å î‡®OPTIMIZED_TD3é—ç‘°å¾æ¾¶å‹­æ‚Šé”›æ°­æ´¿é‚ç‰ˆå¸°ç»±ãˆ¤å™¸éšîˆå¤ç¼ƒ?
        if algorithm.upper() == 'OPTIMIZED_TD3' and hasattr(training_env.agent_env, 'agent'):
            agent = training_env.agent_env.agent
            if hasattr(agent, 'exploration_reset_interval'):
                # éºãˆ¢å‚¨é–²å¶…æƒé—‚æ’®æ®§é™îˆ™äº’éè§„åµé—‡â‚¬ç‘•ä½½çšŸéè¾¾ç´™é©î†¼å¢ 100episodeé”›?
                pass
        per_step_reward = episode_result['avg_reward'] / max(1, episode_steps)
        training_env.performance_tracker['recent_step_rewards'].update(per_step_reward)
        
        system_metrics = episode_result['system_metrics']
        training_env.performance_tracker['recent_delays'].update(system_metrics.get('avg_task_delay', 0))
        training_env.performance_tracker['recent_energy'].update(system_metrics.get('total_energy_consumption', 0))
        training_env.performance_tracker['recent_completion'].update(system_metrics.get('task_completion_rate', 0))
        
        # é¦ƒå¸¹ é‡å­˜æŸŠæ¥‚æ¨¼î¬é™îˆî‹é–?
        if advanced_visualizer:
            # é€å •æ³¦ç’‡ï¸¾ç²é¸å›¨çˆ£
            vis_metrics = {
                'reward': episode_result['avg_reward'],
                'loss': episode_result.get('loss', 0),  # æ¿¡å‚›ç‰éˆå¤‹å´¯æ¾¶åâ‚¬?
                'hit_rate': system_metrics.get('cache_hit_rate', 0),
                'delay': system_metrics.get('avg_task_delay', 0) * 1000,  # æî„å´²æ¶“ç°ƒs
                'energy': system_metrics.get('total_energy_consumption', 0),
                'success_rate': system_metrics.get('task_completion_rate', 0),
                'action': episode_result.get('last_action'),  # éˆâ‚¬éšåºç«´æ¶“î„å§©æµ£?
                'gradient_norm': episode_result.get('gradient_norm')  # æ¿¡å‚›ç‰éˆå¤‹îªæ´ï¹å¯–é?
            }
            advanced_visualizer.update(episode, vis_metrics)
            
            # ç€¹æ°­æ¹¡æ·‡æ¿†ç“¨é™îˆî‹é–æ ¨åŸ…é¥?
            if episode % save_interval == 0:
                advanced_visualizer.save(f"results/single_agent/{algorithm.lower()}/viz_checkpoint_{episode}.png")
        
        # é¦ƒå¯ª é‡å­˜æŸŠç€¹ç‚´æ¤‚é™îˆî‹é–?
        if visualizer:
            vis_metrics = {
                'avg_delay': system_metrics.get('avg_task_delay', 0),
                'total_energy': system_metrics.get('total_energy_consumption', 0),
                'task_completion_rate': system_metrics.get('task_completion_rate', 0),
                'cache_hit_rate': system_metrics.get('cache_hit_rate', 0),
                'data_loss_ratio_bytes': system_metrics.get('data_loss_ratio_bytes', 0),
                'migration_success_rate': system_metrics.get('migration_success_rate', 0),
                'local_tasks_count': system_metrics.get('local_tasks_count', 0),
                'rsu_tasks_count': system_metrics.get('rsu_tasks_count', 0),
                'uav_tasks_count': system_metrics.get('uav_tasks_count', 0)
            }
            visualizer.update(episode, episode_result['avg_reward'], vis_metrics)
        
        episode_time = time.time() - episode_start_time
        
        # ç€¹æ°­æ¹¡æˆæ’³åš­æ©æ¶˜å®³
        if episode % 10 == 0:
            avg_reward_step = training_env.performance_tracker['recent_step_rewards'].get_average()
            avg_delay = training_env.performance_tracker['recent_delays'].get_average()
            avg_completion = training_env.performance_tracker['recent_completion'].get_average()
            
            print(f"æî†½î‚¼ {episode:4d}/{num_episodes}:")
            print(f"  éªå†²æ½å§£å¿”î„æ¿‚æ §å§³: {avg_reward_step:8.3f}")
            print(f"  éªå†²æ½éƒè·ºæ¬¢: {avg_delay:8.3f}s")
            print(f"  ç€¹å±¾åšéœ?   {avg_completion:8.1%}")
            print(f"  æî†½î‚¼é¢ã„¦æ¤‚: {episode_time:6.3f}s")
        
        # ç’‡å‹ªåŠå¦¯â€³ç€·
        if episode % eval_interval == 0:
            eval_result = evaluate_single_model(algorithm, training_env, episode)
            print(f"\né¦ƒæ³ æî†½î‚¼ {episode} ç’‡å‹ªåŠç¼æ’´ç‰:")
            print(f"  Per-Stepæ¿‚æ §å§³: {eval_result['avg_reward']:.3f}")
            print(f"  ç’‡å‹ªåŠéƒè·ºæ¬¢: {eval_result['avg_delay']:.3f}s")
            print(f"  ç’‡å‹ªåŠç€¹å±¾åšéœ? {eval_result['completion_rate']:.1%}")
            
            # æ·‡æ¿†ç“¨éˆâ‚¬æµ£è™«Äé¨?
            if eval_result['avg_reward'] > best_avg_reward:
                best_avg_reward = eval_result['avg_reward']
                best_model_base = f"results/models/single_agent/{algorithm.lower()}/best_model"
                saved_target = training_env.agent_env.save_models(best_model_base)
                saved_display = saved_target or best_model_base
                print(f"  é¦ƒæ‘ æ·‡æ¿†ç“¨éˆâ‚¬æµ£è™«Äé¨?-> {saved_display} (Per-Stepæ¿‚æ §å§³: {best_avg_reward:.3f})")
        
        # æˆæƒ§åŸŒéšåº¢æ¹¡é—ƒèˆµî†Œéƒå‰ç¼‰é€ç¶¯D3ç€›ï¸¿ç¯„éœå›·ç´™æ¶“â‚¬å¨†â„ƒâ‚¬Ñç´š
        if (lr_decay_episode is not None and not lr_decay_applied and episode >= lr_decay_episode):
            lr_info = None
            lr_callback = getattr(training_env.agent_env, 'apply_late_stage_lr', None)
            if callable(lr_callback):
                lr_info = lr_callback(factor=late_stage_lr_factor, min_lr=5e-5)
                lr_decay_applied = True
            elif hasattr(training_env.agent_env, 'agent'):
                agent_obj = getattr(training_env.agent_env, 'agent')
                if hasattr(agent_obj, 'apply_lr_schedule'):
                    lr_info = agent_obj.apply_lr_schedule(factor=late_stage_lr_factor, min_lr=5e-5)
                    lr_decay_applied = True
            if lr_info and isinstance(lr_info, dict):
                print(
                    f"é¦ƒæ•¡ ç»—ç‘Šepisode}æî†¿Ğ•é™æ…£D3ç€›ï¸¿ç¯„éœå›©ç¼‰é€?-> "
                    f"actor_lr={lr_info.get('actor_lr', 0):.2e}, critic_lr={lr_info.get('critic_lr', 0):.2e}"
                )

        # ç€¹æ°­æ¹¡æ·‡æ¿†ç“¨å¦¯â€³ç€·
        if episode % save_interval == 0:
            checkpoint_base = f"results/models/single_agent/{algorithm.lower()}/checkpoint_{episode}"
            checkpoint_path = training_env.agent_env.save_models(checkpoint_base)
            checkpoint_display = checkpoint_path or checkpoint_base
            print(f"é¦ƒæ‘ æ·‡æ¿†ç“¨å¦«â‚¬éŒãƒ§å£: {checkpoint_display}")
    
    # ç’î… ç²Œç€¹å±¾åš
    total_training_time = time.time() - training_start_time
    
    # é¦ƒå¸¹ æ·‡æ¿†ç“¨æ¥‚æ¨¼î¬é™îˆî‹é–æ ¨æ¸¶ç¼å ¢ç²¨é‹?
    if advanced_visualizer:
        final_viz_path = f"results/single_agent/{algorithm.lower()}/final_training_viz.png"
        advanced_visualizer.save(final_viz_path)
        print(f"é¦ƒæ‘ æ¥‚æ¨¼î¬é™îˆî‹é–æ §å‡¡æ·‡æ¿†ç“¨: {final_viz_path}")
    
    # é¦ƒå¯ª éå›ªî†‡ç€¹ç‚´æ¤‚é™îˆî‹é–æ §ç•¬é´?
    if visualizer:
        visualizer.complete()
        print(f"é‰?ç€¹ç‚´æ¤‚é™îˆî‹é–æ §å‡¡éå›ªî†‡ç€¹å±¾åš")
    
    print("\n" + "=" * 60)
    print(f"é¦ƒå¸€ {algorithm}ç’î… ç²Œç€¹å±¾åš!")
    print(f"éˆ´æ†‹ç¬  é¬æ˜î†„ç¼å†©æ¤‚é—‚? {total_training_time/3600:.2f} çå¿”æ¤‚")
    print(f"é¦ƒå¼³ éˆâ‚¬æµ£ç …er-Stepæ¿‚æ §å§³: {best_avg_reward:.3f}")
    
    # é¦ƒæ³ æˆæ’³åš­æµ è¯²å§Ÿæ¾¶å‹­æ‚Šé‚ç‘°ç´¡é’å——ç«·ç¼ç†»î…¸
    print("\n" + "=" * 60)
    print("é¦ƒæ³ æµ è¯²å§Ÿæ¾¶å‹­æ‚Šé‚ç‘°ç´¡é’å——ç«·ç¼ç†»î…¸")
    print("=" * 60)
    
    # éµæ’³åµƒç’î… ç²Œå§¹å›¨â‚¬è¤ç²ºç’?
    analytics_tracker.print_training_summary()
    
    # éµæ’³åµƒéˆâ‚¬æ©æ…›æ¶“çŒ pisodeé¨å‹®î‡›ç¼å—™ç²ºç’?
    analytics_tracker.print_summary(top_n=min(20, num_episodes))
    
    # ç€µç…åš­CSVéç‰ˆåµé¢ã„¤ç°¬éšåº£ç”»é’å—˜ç€½
    csv_export_path = f"results/single_agent/{algorithm.lower()}/task_distribution_analysis.csv"
    analytics_tracker.export_csv(csv_export_path)
    
    # é‘¾å³°å½‡å©•æ–¿å¯²ç“’å¬ªå¨
    evolution_trends = analytics_tracker.get_evolution_trend()
    if evolution_trends and evolution_trends.get('episodes'):
        print(f"\né¦ƒæ± æµ è¯²å§Ÿæ¾¶å‹­æ‚Šé‚ç‘°ç´¡å©•æ–¿å¯²ç“’å¬ªå¨é’å—˜ç€½:")
        print(f"   - éˆî„€æ¹´æ¾¶å‹­æ‚Šé—çŠ³ç˜®: {evolution_trends['local_ratio'][-1]:.1%} (é’æ¿†î: {evolution_trends['local_ratio'][0]:.1%})")
        print(f"   - RSUæ¾¶å‹­æ‚Šé—çŠ³ç˜®: {evolution_trends['rsu_ratio'][-1]:.1%} (é’æ¿†î: {evolution_trends['rsu_ratio'][0]:.1%})")
        print(f"   - UAVæ¾¶å‹­æ‚Šé—çŠ³ç˜®: {evolution_trends['uav_ratio'][-1]:.1%} (é’æ¿†î: {evolution_trends['uav_ratio'][0]:.1%})")
        print(f"   - æµ è¯²å§Ÿé´æ„¬å§›éœ? {evolution_trends['success_ratio'][-1]:.1%} (é’æ¿†î: {evolution_trends['success_ratio'][0]:.1%})")
    
    # é€å •æ³¦ç»¯è¤ç²ºç¼ç†»î…¸æ·‡â„ƒä¼…é¢ã„¤ç°¬é¶ãƒ¥æ†¡
    simulator_stats = {}
    
    # é¦ƒå½š é„å‰§ãšæ¶“î…ãRSUç’‹å†¨å®³é£ã„¦å§¤é›?
    try:
        central_report = training_env.simulator.get_central_scheduling_report()
        if central_report.get('status') != 'not_available' and central_report.get('status') != 'error':
            print(f"\né¦ƒå½š æ¶“î…ãRSUæ¥ ã„¥å…±ç’‹å†¨å®³é£ã„¦â‚¬è¤ç²¨:")
            print(f"   é¦ƒæ³ ç’‹å†¨å®³ç’‹å†ªæ•¤å¨†â„ƒæšŸ: {central_report.get('scheduling_calls', 0)}")
            
            scheduler_status = central_report.get('central_scheduler_status', {})
            if 'global_metrics' in scheduler_status:
                metrics = scheduler_status['global_metrics']
                print(f"   éˆ¿æ µç¬ ç’ç†»æµ‡é§å›ªã€€é¸å›¨æšŸ: {metrics.get('load_balance_index', 0.0):.3f}")
                print(f"   é¦ƒæŒŒ ç»¯è¤ç²ºé‹ãƒ¥æ‚é˜èˆµâ‚¬? {scheduler_status.get('system_health', 'N/A')}")
                
                # é€å •æ³¦ç’‹å†¨å®³é£ã„§ç²ºç’â€²ä¿Šé­?
                simulator_stats['scheduling_calls'] = central_report.get('scheduling_calls', 0)
                simulator_stats['load_balance_index'] = metrics.get('load_balance_index', 0.0)
                simulator_stats['system_health'] = scheduler_status.get('system_health', 'N/A')
            
            # é„å‰§ãšéšå‡´SUç’ç†»æµ‡é’å——ç«·
            rsu_details = central_report.get('rsu_details', {})
            if rsu_details:
                print(f"   é¦ƒæ‘— éšå‡´SUç’ç†»æµ‡é˜èˆµâ‚¬?")
                for rsu_id, details in rsu_details.items():
                    print(f"      {rsu_id}: CPUç’ç†»æµ‡={details['cpu_usage']:.1%}, æµ è¯²å§Ÿé—ƒç†·åª={details['queue_length']}")
        else:
            print(f"é¦ƒæµ æ¶“î…ãç’‹å†¨å®³é£ã„§å§¸é¬? {central_report.get('message', 'éˆî„æƒé¢?)}")
        
        # é¦ƒæ”² é„å‰§ãšéˆå¤Œåšé¥ç‚°ç´¶ç¼ƒæˆ ç²¶ç¼ç†»î…¸
        rsu_migration_delay = training_env.simulator.stats.get('rsu_migration_delay', 0.0)
        rsu_migration_energy = training_env.simulator.stats.get('rsu_migration_energy', 0.0)
        rsu_migration_data = training_env.simulator.stats.get('rsu_migration_data', 0.0)
        backhaul_collection_delay = training_env.simulator.stats.get('backhaul_collection_delay', 0.0)
        backhaul_command_delay = training_env.simulator.stats.get('backhaul_command_delay', 0.0)
        backhaul_total_energy = training_env.simulator.stats.get('backhaul_total_energy', 0.0)
        
        # é¦ƒæ®« é„å‰§ãšéšå‹­î’æ©ä½ºĞ©ç¼ç†»î…¸
        handover_migrations = training_env.simulator.stats.get('handover_migrations', 0)
        uav_migration_count = training_env.simulator.stats.get('uav_migration_count', 0)
        uav_migration_distance = training_env.simulator.stats.get('uav_migration_distance', 0.0)
        
        # é€å •æ³¦æ©ä½ºĞ©ç¼ç†»î…¸æ·‡â„ƒä¼…
        simulator_stats['rsu_migration_delay'] = rsu_migration_delay
        simulator_stats['rsu_migration_energy'] = rsu_migration_energy
        simulator_stats['rsu_migration_data'] = rsu_migration_data
        simulator_stats['backhaul_total_energy'] = backhaul_total_energy
        simulator_stats['handover_migrations'] = handover_migrations
        simulator_stats['uav_migration_count'] = uav_migration_count
        
        if rsu_migration_data > 0 or backhaul_total_energy > 0 or handover_migrations > 0 or uav_migration_count > 0:
            print(f"\né¦ƒæ”² éˆå¤Œåšé¥ç‚°ç´¶ç¼ƒæˆ ç²¶æ¶“åº¤ç¸¼ç»‰è¤ç²ºç’?")
            print(f"   é¦ƒæ‘— RSUæ©ä½ºĞ©éç‰ˆåµ: {rsu_migration_data:.1f}MB")
            print(f"   éˆ´æ†‹ç¬ RSUæ©ä½ºĞ©å¯¤æƒ°ç¹œ: {rsu_migration_delay*1000:.1f}ms")
            print(f"   éˆ¿?RSUæ©ä½ºĞ©é‘³å€Ÿâ‚¬? {rsu_migration_energy:.2f}J")
            print(f"   é¦ƒæ³ æ·‡â„ƒä¼…é€å •æ³¦å¯¤æƒ°ç¹œ: {backhaul_collection_delay*1000:.1f}ms")
            print(f"   é¦ƒæ‘› é¸å›¦æŠ¤é’å——å½‚å¯¤æƒ°ç¹œ: {backhaul_command_delay*1000:.1f}ms")
            print(f"   é¦ƒæ”± é¥ç‚°ç´¶ç¼ƒæˆ ç²¶é¬æ˜å…˜é‘°? {backhaul_total_energy:.2f}J")
            if handover_migrations > 0:
                print(f"   é¦ƒæ®« æï¹ç· ç’ºç†¼æ®¢æ©ä½ºĞ©: {handover_migrations} å¨†?)
            if uav_migration_count > 0:
                avg_distance = uav_migration_distance / uav_migration_count if uav_migration_count > 0 else 0
                print(f"   é¦ƒæ® UAVæ©ä½ºĞ©: {uav_migration_count} å¨†? éªå†²æ½ç’ºæ¿ˆî‡{avg_distance:.1f}m")
    except Exception as e:
        print(f"éˆ¿ç‹…ç¬ æ¶“î…ãç’‹å†¨å®³é¶ãƒ¥æ†¡é‘¾å³°å½‡æ¾¶è¾«è§¦: {e}")
    
    # æ·‡æ¿†ç“¨ç’î… ç²Œç¼æ’´ç‰
    results = save_single_training_results(algorithm, training_env, total_training_time, override_scenario=override_scenario)
    
    # ç¼æ¨ºåŸ—ç’î… ç²Œé‡èŒ¬åš
    plot_single_training_curves(algorithm, training_env)
    
    # é¢ç†¸åšHTMLç’î… ç²Œé¶ãƒ¥æ†¡
    print("\n" + "=" * 60)
    print("é¦ƒæ‘‘ é¢ç†¸åšç’î… ç²Œé¶ãƒ¥æ†¡...")
    
    try:
        report_generator = HTMLReportGenerator()
        html_content = report_generator.generate_full_report(
            algorithm=algorithm,
            training_env=training_env,
            training_time=total_training_time,
            results=results,
            simulator_stats=simulator_stats
        )
        
        # é¢ç†¸åšé¶ãƒ¥æ†¡é‚å›¦æ¬¢éš?
        timestamp = generate_timestamp()
        report_filename = f"training_report_{timestamp}.html" if timestamp else "training_report.html"
        report_path = f"results/single_agent/{algorithm.lower()}/{report_filename}"
        
        print(f"é‰?ç’î… ç²Œé¶ãƒ¥æ†¡å®¸èŒ¬æ•“é´?)
        print(f"é¦ƒæ« é¶ãƒ¥æ†¡é–å‘­æƒˆ:")
        print(f"   - éµÑ†î”‘é½æ¨¿î›¦æ¶“åº¡å§é–¿î†½å¯šé?)
        print(f"   - ç’î… ç²Œé–°å¶‡ç–†ç’‡ï¸½å„")
        print(f"   - é¬Ñ†å…˜é¸å›¨çˆ£é™îˆî‹é–æ §æµ˜ç›?)
        print(f"   - ç’‡ï¸¾ç²é¨å‹­éƒ´ç¼ç†ºç²ºç’â€²ä¿Šé­?)
        print(f"   - é‘·îˆâ‚¬å‚šç°²éºÑƒåŸ—é£ã„¥åé‹?)
        print(f"   - æµ¼æ¨ºå¯²å¯¤é¸¿î†…æ¶“åº£ç²¨ç’?)
        
        # ç’‡ãˆ¤æ£¶é¢ã„¦åŸ›é„îˆšæƒæ·‡æ¿†ç“¨é¶ãƒ¥æ†¡é”›å ¥æ½¤æ¦›æ¨»Äå¯®å¿ç¬…é‘·î„å§©æ·‡æ¿†ç“¨é”›?
        if silent_mode:
            # é—ˆæ¬“ç²¯å¦¯â€³ç´¡é”›æ°³åšœé”ã„¤ç¹šç€›æ©ˆç´æ¶“å¶†å¢¦å¯®â‚¬å¨´å¿šîé£?
            if report_generator.save_report(html_content, report_path):
                print(f"é‰?é¶ãƒ¥æ†¡å®¸èŒ¶åšœé”ã„¤ç¹šç€›æ¨ºåŸŒ: {report_path}")
            else:
                print("é‰‚?é¶ãƒ¥æ†¡æ·‡æ¿†ç“¨æ¾¶è¾«è§¦")
        else:
            # æµœã‚„ç°°å¦¯â€³ç´¡é”›æ°³î‡—é—‚î†¾æ•¤é´?
            print("\n" + "-" * 60)
            save_choice = input("é¦ƒæ‘ é„îˆšæƒæ·‡æ¿†ç“¨HTMLç’î… ç²Œé¶ãƒ¥æ†¡? (y/n, æ¦›æ¨¿î…»y): ").strip().lower()
            
            if save_choice in ['', 'y', 'yes', 'é„?]:
                if report_generator.save_report(html_content, report_path):
                    print(f"é‰?é¶ãƒ¥æ†¡å®¸è¹­ç¹šç€›æ¨ºåŸŒ: {report_path}")
                    print(f"é¦ƒæŒ• é»æ„®ãš: æµ£è·¨æ•¤å¨´å¿šîé£ã„¦å¢¦å¯®â‚¬ç’‡ãƒ¦æƒæµ è·ºåµ†é™îˆ›ç…¡éªå¬ªç•¬éå­˜å§¤é›?)
                    
                    # çæ¿Šç˜¯é‘·î„å§©éµæ’³ç´‘é¶ãƒ¥æ†¡é”›å å½²é–«å¤›ç´š
                    auto_open = input("é¦ƒå¯ª é„îˆšæƒé¦ã„¦ç¥»ç‘™å æ«’æ¶“î…Ÿå¢¦å¯®â‚¬é¶ãƒ¥æ†¡? (y/n, æ¦›æ¨¿î…»n): ").strip().lower()
                    if auto_open in ['y', 'yes', 'é„?]:
                        import webbrowser
                        abs_path = os.path.abspath(report_path)
                        webbrowser.open(f'file://{abs_path}')
                        print("é‰?é¶ãƒ¥æ†¡å®¸æ’æ¹ªå¨´å¿šîé£ã„¤è…‘éµæ’³ç´‘")
                else:
                    print("é‰‚?é¶ãƒ¥æ†¡æ·‡æ¿†ç“¨æ¾¶è¾«è§¦")
            else:
                print("éˆ©ç™¸ç¬ é¶ãƒ¥æ†¡éˆîƒç¹šç€›?)
                print(f"é¦ƒæŒ• æ¿¡å‚æ¸¶éŒãƒ§æ¹…é”›å²ƒî‡¬éµå¬ªå§©æ©æ„¯î”‘é¶ãƒ¥æ†¡é¢ç†¸åšé”ç†»å…˜")
    
    except Exception as e:
        print(f"éˆ¿ç‹…ç¬ é¢ç†¸åšç’î… ç²Œé¶ãƒ¥æ†¡éƒè·ºåš­é–¿? {e}")
        print("ç’î… ç²Œéç‰ˆåµå®¸å‰î„œç”¯é•ç¹šç€›æ©ˆç´é™îˆœâ—¢éšåº¢å¢œé”ã„§æ•“é´æ„­å§¤é›?)
    
    return results


def evaluate_single_model(algorithm: str, training_env: SingleAgentTrainingEnvironment, 
                         episode: int, num_eval_episodes: int = 5) -> Dict:
    """ç’‡å‹ªåŠé—æ›Ÿæ«¤é‘³æˆ’ç¶‹å¦¯â€³ç€·é¬Ñ†å…˜ - æ·‡î†¼î˜²é—å ¬ç´é—ƒå‰î„›inféœå®¯an"""
    import numpy as np
    
    eval_rewards = []
    eval_delays = []
    eval_completions = []
    
    def safe_value(value: float, default: float = 0.0, max_val: float = 1e6) -> float:
        """ç€¹å¤Šåæ¾¶å‹­æ‚Šéæ¿â‚¬ç¡·ç´é—ƒå‰î„›inféœå®¯an"""
        if np.isnan(value) or np.isinf(value):
            return default
        return np.clip(value, -max_val, max_val)
    
    eval_max_steps = getattr(config.experiment, 'max_steps_per_episode', 200)
    eval_max_steps = max(50, int(eval_max_steps))
    
    for _ in range(num_eval_episodes):
        state = training_env.reset_environment()
        episode_reward = 0.0
        episode_delay = 0.0
        episode_completion = 0.0
        steps = 0
        
        for step in range(eval_max_steps):
            if algorithm == "DQN":
                actions_result = training_env.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                action = training_env._encode_discrete_action(actions_dict)
            else:
                actions_result = training_env.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, tuple):  # PPOæ©æ–¿æ´–éå†ªç²
                    actions_dict = actions_result[0]
                elif isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    actions_dict = {}
                action = training_env._encode_continuous_action(actions_dict)
            
            # ç’‡å‹ªåŠéƒæœµç¯ƒæµ¼çŠ²å†é”ã„¤ç¶”ç€›æ¥€å€é”›å²€â€˜æ·‡æ¿†äº¸æ¿‚ç•Œæ•“é?
            next_state, reward, done, info = training_env.step(action, state, actions_dict)
            
            # ç€¹å¤Šåæ¾¶å‹­æ‚Šæ¿‚æ §å§³éœå±¾å¯šé?
            safe_reward = safe_value(reward, -10.0, 120.0)
            episode_reward += safe_reward
            
            system_metrics = info['system_metrics']
            safe_delay = safe_value(system_metrics.get('avg_task_delay', 0), 0.0, 10.0)
            safe_completion = safe_value(system_metrics.get('task_completion_rate', 0), 0.0, 1.0)
            
            episode_delay += safe_delay
            episode_completion += safe_completion
            steps += 1
            
            state = next_state
            
            if done:
                break
        
        # ç€¹å¤Šåç’ï¼„ç•»éªå†²æ½éŠ?
        steps = max(1, steps)  # é—ƒå‰î„›é—„ã‚‰æµ‚
        eval_rewards.append(safe_value(episode_reward / steps, -20.0, 80.0))
        eval_delays.append(safe_value(episode_delay / steps, 0.0, 10.0))
        eval_completions.append(safe_value(episode_completion / steps, 0.0, 1.0))
    
    # ç€¹å¤Šåç’ï¼„ç•»éˆâ‚¬ç¼å ¢ç²¨é‹?
    if len(eval_rewards) == 0:
        return {'avg_reward': -1.0, 'avg_delay': 1.0, 'completion_rate': 0.0}
    
    avg_reward = safe_value(float(np.mean(eval_rewards)), -20.0, 80.0)
    avg_delay = safe_value(float(np.mean(eval_delays)), 0.0, 10.0)
    avg_completion = safe_value(float(np.mean(eval_completions)), 0.0, 1.0)
    
    return {
        'avg_reward': avg_reward,
        'avg_delay': avg_delay,
        'completion_rate': avg_completion
    }

 
def _finite_mean(values: List[float], default: float = 0.0) -> float:
    """ç’ï¼„ç•»éˆå¤æªºéŠè‚©æ®‘é§å›§â‚¬ç¡·ç´æ©å›¨æŠ¤éºå¡aN/InféŠ†?""
    finite_values: List[float] = []
    for v in values:
        try:
            val = float(v)
        except (TypeError, ValueError):
            continue
        if np.isfinite(val):
            finite_values.append(val)
    return float(np.mean(finite_values)) if finite_values else default


def _calculate_stable_delay_average(training_env: SingleAgentTrainingEnvironment) -> float:
    """
    ç’ï¼„ç•»ç»‹å†²ç•¾é¨å‹¬æ¤‚å¯¤è·ºé’©é§å›§â‚¬ç¡·ç´é–¬å®å¤MovingAverage(100)é¨å‹®î†„ç¼å†©å°é”ã„¥å¥–é?
    
    ç»›æ «æšé”›?
    1. æµ¼æ¨ºå›æµ£è·¨æ•¤episode_metricsæ¶“î… æ®‘ç€¹å±¾æš£éç‰ˆåµé”›å î›§é‹æ»ƒå½²é¢îŸ’ç´š
    2. æµ£è·¨æ•¤éš?0%é¨å‹¬æšŸé¹î‡†ç´™éºæ—æ«é“å¶†æ¹¡ç€›ï¸¿ç¯„é—ƒèˆµî†Œé”›?
    3. æ¿¡å‚›ç‰éç‰ˆåµæ¶“å¶ˆå†»é”›å±½æ´–é–«â‚¬é’ç™•ovingAverage(100)
    
    Returns:
        float: ç»‹å†²ç•¾é¨å‹«é’©é§å›¨æ¤‚å¯¤?
    """
    # çæ¿Šç˜¯æµ å·ˆpisode_metricsé‘¾å³°å½‡ç€¹å±¾æš£éƒè·ºæ¬¢éç‰ˆåµ
    if hasattr(training_env, 'episode_metrics') and 'avg_delay' in training_env.episode_metrics:
        delay_history = training_env.episode_metrics['avg_delay']
        
        if len(delay_history) >= 100:
            # æµ£è·¨æ•¤éš?0%é¨å‹¬æšŸé¹î‡†ç´™é‡å­˜åšé”ç†ºæ®‘ç»›æ «æšé”›?
            half_point = len(delay_history) // 2
            converged_delays = delay_history[half_point:]
            return _finite_mean(converged_delays, training_env.performance_tracker['recent_delays'].get_average())
        elif len(delay_history) >= 50:
            # æ¿¡å‚›ç‰æ¶“å¶ˆå†»100æî‡†ç´æµ£è·¨æ•¤éš?0æ?
            return _finite_mean(delay_history[-30:], training_env.performance_tracker['recent_delays'].get_average())
        elif len(delay_history) > 0:
            # éç‰ˆåµå¯°å çš¯é”›å±¼å¨‡é¢ã„¥åé–®?
            return _finite_mean(delay_history, training_env.performance_tracker['recent_delays'].get_average())
    
    # é¥ç‚ºâ‚¬â‚¬é”›æ°«å¨‡é¢âˆ•ovingAverage
    recent_delay = training_env.performance_tracker['recent_delays'].get_average()
    return _finite_mean([recent_delay], 0.0)


def _calculate_stable_completion_average(training_env: SingleAgentTrainingEnvironment) -> float:
    """
    ç’ï¼„ç•»ç»‹å†²ç•¾é¨å‹«ç•¬é´æ„®å·¼éªå†²æ½éŠ?
    
    Returns:
        float: ç»‹å†²ç•¾é¨å‹«é’©é§å›§ç•¬é´æ„®å·¼
    """
    # çæ¿Šç˜¯æµ å·ˆpisode_metricsé‘¾å³°å½‡ç€¹å±¾æš£ç€¹å±¾åšéœå›¨æšŸé¹?
    if hasattr(training_env, 'episode_metrics') and 'task_completion_rate' in training_env.episode_metrics:
        completion_history = training_env.episode_metrics['task_completion_rate']
        
        if len(completion_history) >= 100:
            # æµ£è·¨æ•¤éš?0%é¨å‹¬æšŸé¹?
            half_point = len(completion_history) // 2
            converged_completions = completion_history[half_point:]
            return _finite_mean(converged_completions, training_env.performance_tracker['recent_completion'].get_average())
        elif len(completion_history) >= 50:
            # æ¿¡å‚›ç‰æ¶“å¶ˆå†»100æî‡†ç´æµ£è·¨æ•¤éš?0æ?
            return _finite_mean(completion_history[-30:], training_env.performance_tracker['recent_completion'].get_average())
        elif len(completion_history) > 0:
            # éç‰ˆåµå¯°å çš¯é”›å±¼å¨‡é¢ã„¥åé–®?
            return _finite_mean(completion_history, training_env.performance_tracker['recent_completion'].get_average())
    
    # é¥ç‚ºâ‚¬â‚¬é”›æ°«å¨‡é¢âˆ•ovingAverage
    recent_completion = training_env.performance_tracker['recent_completion'].get_average()
    return _finite_mean([recent_completion], 0.0)


def _calculate_raw_cost_for_training(training_env: SingleAgentTrainingEnvironment) -> float:
    """
    æµ åº¤î†„ç¼å†¨îš›é”è¾«î…¸ç» æ¢¤aw_costé”›å îš›é”è¾¨æ¹°éŸ¬î‚¢æ°¨é„îˆç¤‹é´æ„­æ¹°é”›?
    
    ç’î… ç²Œéƒè®¹ç´°reward = -costé”›å Ÿåšéˆî„ƒç§ºæµ£åº¯ç´æ¿‚æ §å§³ç“’å©‡ç®é”›?
    é¥çŠ³î„é”›æ­³aw_cost = -reward
    
    Returns:
        float: raw_costé”›å Ÿî„œéŠç¡·ç´ç“’å©‚çš¬ç“’å©‚ã‚½é”›?
    """
    # é‘¾å³°å½‡é€èˆµæšƒéšåº£æ®‘éªå†²æ½æ¿‚æ §å§³
    if hasattr(training_env, 'episode_rewards') and len(training_env.episode_rewards) > 0:
        rewards = []
        for r in training_env.episode_rewards:
            try:
                val = float(r)
            except (TypeError, ValueError):
                continue
            if np.isfinite(val):
                rewards.append(val)
        if not rewards:
            rewards = [training_env.performance_tracker['recent_rewards'].get_average()]
        if len(rewards) >= 100:
            # æµ£è·¨æ•¤éš?0%éç‰ˆåµé”›å Ÿæ•¹éæ¶˜æ‚—é”›?
            half_point = len(rewards) // 2
            converged_rewards = rewards[half_point:]
            avg_reward = _finite_mean(converged_rewards, 0.0)
        elif len(rewards) >= 50:
            avg_reward = _finite_mean(rewards[-30:], 0.0)
        else:
            avg_reward = _finite_mean(rewards, 0.0)
    else:
        recent_avg = training_env.performance_tracker['recent_rewards'].get_average()
        avg_reward = _finite_mean([recent_avg], 0.0)
    
    # rewardé„îˆç¤‹é´æ„­æ¹°é”›å±¾å¢æµ î™¸aw_cost = -reward
    raw_cost = -avg_reward
    return float(raw_cost)


def save_single_training_results(algorithm: str, training_env: SingleAgentTrainingEnvironment, 
                                training_time: float,
                                override_scenario: Optional[Dict[str, Any]] = None) -> Dict:
    """æ·‡æ¿†ç“¨ç’î… ç²Œç¼æ’´ç‰"""
    # é¢ç†¸åšéƒå •æ£¿é´?
    timestamp = generate_timestamp()
    
    # é¦ƒæ•¡ éšå±¾æ¤‚é»æ„ªç·µEpisodeé¬è¯²îš›é”åæ‹°Per-Stepéªå†²æ½æ¿‚æ §å§³
    reward_samples = list(training_env.episode_rewards[-100:]) if hasattr(training_env, 'episode_rewards') else []
    reward_samples.append(training_env.performance_tracker['recent_rewards'].get_average())
    recent_episode_reward = _finite_mean(reward_samples, 0.0)
    
    # é¦ƒæ•¡ æµ¼æ¨ºå¯²é”›æ°«å¨‡é¢ã„¥ç–„é—„å‘­é’©é§å›¨î„éæ‹Œî…¸ç» ?avg_step_reward
    if 'episode_steps' in training_env.episode_metrics and training_env.episode_metrics['episode_steps']:
        # æµ£è·¨æ•¤éˆâ‚¬æ©?00æ¶“çŒ pisodeé¨å‹«é’©é§å›¨î„é?
        recent_steps = training_env.episode_metrics['episode_steps'][-100:]
        avg_steps_per_episode = sum(recent_steps) / len(recent_steps)
    else:
        # é¥ç‚ºâ‚¬â‚¬é’ä¼´å¤ç¼ƒî†¾æ®‘æ¦›æ¨¿î…»éŠ?
        avg_steps_per_episode = config.experiment.max_steps_per_episode
    
    avg_step_reward = recent_episode_reward / avg_steps_per_episode if avg_steps_per_episode else 0.0
    
    # é‘¾å³°å½‡ç¼ƒæˆ ç²¶é·æ’´å¢¤æ·‡â„ƒä¼…
    num_vehicles = len(training_env.simulator.vehicles)
    num_rsus = len(training_env.simulator.rsus)
    num_uavs = len(training_env.simulator.uavs)
    state_dim = getattr(training_env.agent_env, 'state_dim', 'N/A')
    
    # é¦ƒå• æ·‡î†¼î˜²é”›æ°­æ•¹é—†å——ç•¬éå¯¸æ®‘ç»¯è¤ç²ºé–°å¶‡ç–†é™å‚›æšŸé”›å ¢æ•¤æµœå¶©TMLé¶ãƒ¥æ†¡é„å‰§ãšé”›?
    # é©å­˜å¸´æµ£è·¨æ•¤å®¸æ’î‡±éãƒ§æ®‘configç€µç¡…è–„
    
    results = {
        'algorithm': algorithm,
        'agent_type': 'single_agent',
        'timestamp': timestamp,
        'training_start_time': datetime.now().isoformat(),
        'network_topology': {
            'num_vehicles': num_vehicles,
            'num_rsus': num_rsus,
            'num_uavs': num_uavs,
        },
        'state_dim': state_dim,
        'override_scenario': override_scenario,
        'training_config': {
            'num_episodes': len(training_env.episode_rewards),
            'training_time_hours': training_time / 3600,
            'max_steps_per_episode': config.experiment.max_steps_per_episode
        },
        # é¦ƒå• å¨£è¯²å§ç»¯è¤ç²ºé–°å¶‡ç–†é™å‚›æšŸé”›åœšTMLé¶ãƒ¥æ†¡é—‡â‚¬ç‘•ä¾Šç´š
        'system_config': {
            'num_vehicles': num_vehicles,
            'num_rsus': num_rsus,
            'num_uavs': num_uavs,
            'simulation_time': config.simulation_time,
            'time_slot': config.time_slot,
            'device': str(config.device),
            'random_seed': config.random_seed,
        },
        # é¦ƒå• å¨£è¯²å§ç¼ƒæˆ ç²¶é–°å¶‡ç–†é™å‚›æšŸ
        'network_config': {
            'bandwidth': config.network.bandwidth,
            'carrier_frequency': config.communication.carrier_frequency,
            'coverage_radius': config.network.coverage_radius,
        },
        # é¦ƒå• å¨£è¯²å§é–«æ°«ä¿Šé–°å¶‡ç–†é™å‚›æšŸ
        'communication_config': {
            'vehicle_tx_power': config.communication.vehicle_tx_power,
            'rsu_tx_power': config.communication.rsu_tx_power,
            'uav_tx_power': config.communication.uav_tx_power,
            'antenna_gain_vehicle': config.communication.antenna_gain_vehicle,
            'antenna_gain_rsu': config.communication.antenna_gain_rsu,
            'antenna_gain_uav': config.communication.antenna_gain_uav,
        },
        # é¦ƒå• å¨£è¯²å§ç’ï¼„ç•»é‘³è—‰å§é™å‚›æšŸ
        'compute_config': {
            'vehicle_cpu_freq': config.compute.vehicle_cpu_freq,
            'rsu_cpu_freq': config.compute.rsu_cpu_freq,
            'uav_cpu_freq': config.compute.uav_cpu_freq,
            'vehicle_memory': getattr(config.compute, 'vehicle_memory', 4e9),
            'rsu_memory': getattr(config.compute, 'rsu_memory', 32e9),
            'uav_memory': getattr(config.compute, 'uav_memory', 16e9),
            'vehicle_static_power': config.compute.vehicle_static_power,
            'rsu_static_power': config.compute.rsu_static_power,
            'uav_static_power': getattr(config.compute, 'uav_static_power', 20.0),
        },
        # é¦ƒå• å¨£è¯²å§æµ è¯²å§Ÿéœå²ƒç¸¼ç»‰è¯²å¼¬é?
        'task_migration_config': {
            'task_arrival_rate': config.task.arrival_rate,
            'task_size_mean': sum(config.task.data_size_range) / 2,
            'task_size_std': (config.task.data_size_range[1] - config.task.data_size_range[0]) / 4,
            'task_cpu_cycles_mean': sum(config.task.compute_cycles_range) / 2,
            'task_cpu_cycles_std': (config.task.compute_cycles_range[1] - config.task.compute_cycles_range[0]) / 4,
            'task_deadline_mean': sum(config.task.deadline_range) / 2,
            'cache_capacity_rsu': config.cache.rsu_cache_capacity,
            'cache_capacity_uav': config.cache.uav_cache_capacity,
            'migration_threshold': getattr(config.migration, 'threshold', 0.8),
        },
        'episode_rewards': training_env.episode_rewards,
        'episode_metrics': training_env.episode_metrics,
        'final_performance': {
            # é»æ„ªç·µæ¶“ã‚‡î’æ¿‚æ §å§³é¸å›¨çˆ£é”›å²€æ•¤é–«æ–¾ç¬‰éš?
            'avg_episode_reward': recent_episode_reward,  # Episodeé¬è¯²îš›é”æ†‹ç´™ç’î… ç²Œé©î†½çˆ£é”›?
            'avg_step_reward': avg_step_reward,           # å§£å¿”î„éªå†²æ½æ¿‚æ §å§³é”›å î‡®å§£æ—‡ç˜æµ¼å¸®ç´š
            'avg_reward': avg_step_reward,  # éšæˆæ‚—éç…î†é”›æ°¶ç²¯ç’ã‚„å¨‡é¢â•¬er-stepé”›å œç¬Œé™îˆî‹é–æ ¦ç«´é‘·è¾¾ç´š
            
            # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°«å¨‡é¢ã„¦æ´¿ç»‹å†²ç•¾é¨å‹«é’©é§å›¨æŸŸå¨‰æ›ªç´é–¬å®å¤MovingAverage(100)é¨å‹¬å°é”ã„¥å¥–é?
            'avg_delay': _calculate_stable_delay_average(training_env),
            'avg_completion': _calculate_stable_completion_average(training_env),
            
            # é¦ƒå¹† é‚æ¿î–ƒé”›æ°­åŠé”ç‡¼vg_energyéœå®ºaw_costé”›å²€æ•¤æµœåºç¬Œç€µè§„ç˜®ç€¹ç‚ºç™æ¶“â‚¬é‘·?
            'avg_energy': _finite_mean(
                training_env.episode_metrics['total_energy'][len(training_env.episode_metrics['total_energy'])//2:]
                if training_env.episode_metrics.get('total_energy') else [],
                0.0
            ),
            'raw_cost': _calculate_raw_cost_for_training(training_env),
        }
    }
    
    print(f"é¦ƒæ³ é€å •æ³¦é¨å‹¯å¤ç¼ƒî†¼å¼¬é?")
    print(f"   ç»¯è¤ç²ºé·æ’´å¢¤: {num_vehicles}æï¹ç· , {num_rsus}RSU, {num_uavs}UAV")
    print(f"   ç¼ƒæˆ ç²¶é–°å¶‡ç–†: ç”¯ï¹€î†”{config.network.bandwidth/1e6:.0f}MHz, æ£°æˆ å·¼{config.communication.carrier_frequency/1e9:.1f}GHz")
    print(f"   æµ è¯²å§Ÿé™å‚›æšŸ: é’æ‹Œæªéœå™config.task.arrival_rate:.1f}, éç‰ˆåµé–²å¼¡sum(config.task.data_size_range)/2/1e6:.1f}MB")
    
    # é¦ƒå¹† éµæ’³åµƒéæŠ½æ•­é¬Ñ†å…˜é¸å›¨çˆ£
    final_perf = results['final_performance']
    print(f"\né¦ƒå¹† éˆâ‚¬ç¼å Ÿâ‚¬Ñ†å…˜é¸å›¨çˆ£:")
    print(f"   Raw Cost: {final_perf.get('raw_cost', 'N/A'):.4f} (= -avg_rewardé”›å±¼ç¬Œç€µè§„ç˜®ç€¹ç‚ºç™æ¶“â‚¬é‘·?")
    print(f"   Avg Reward: {final_perf.get('avg_reward', 0):.4f} (= -raw_costé”›å²ƒî†„ç¼å†§ç´­é–æ «æ´°é?")
    print(f"   Avg Delay: {final_perf.get('avg_delay', 0):.4f}s")
    print(f"   Avg Energy: {final_perf.get('avg_energy', 0):.2f}J")
    print(f"   Completion Rate: {final_perf.get('avg_completion', 0):.1%}")
    
    # æµ£è·¨æ•¤éƒå •æ£¿é´è™«æƒæµ è·ºæ‚•
    filename = get_timestamped_filename("training_results")
    filepath = f"results/single_agent/{algorithm.lower()}/{filename}"
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"é¦ƒæ‘ {algorithm}ç’î… ç²Œç¼æ’´ç‰å®¸è¹­ç¹šç€›æ¨ºåŸŒ {filepath}")
    
    return results


def plot_single_training_curves(algorithm: str, training_env: SingleAgentTrainingEnvironment):
    """ç¼æ¨ºåŸ—ç’î… ç²Œé‡èŒ¬åš - ç» â‚¬å¨²ä½·ç´­ç¼‡åº£å¢—"""
    
    # é¦ƒå¸¹ æµ£è·¨æ•¤é‚æ‰®æ®‘ç» â‚¬å¨²ä½¸å½²ç‘™å——å¯²ç»¯è¤ç²º
    from visualization.clean_charts import create_training_chart, cleanup_old_charts, plot_objective_function_breakdown
    
    # é’æ¶˜ç¼“ç» æ¥ç¡¶é©î†¼ç¶
    algorithm_dir = f"results/single_agent/{algorithm.lower()}"
    
    # å¨“å‘¯æ‚ŠéƒÑ…æ®‘éæ¤¾ç¶‘é¥æã€ƒ
    cleanup_old_charts(algorithm_dir)
    
    # é¢ç†¸åšéç¨¿ç¸¾é¥æã€ƒ
    chart_path = f"{algorithm_dir}/training_overview.png"
    create_training_chart(training_env, algorithm, chart_path)
    
    # é¦ƒå¹† é¢ç†¸åšé©î†½çˆ£é‘èŠ¥æšŸé’å—šĞ’é¥æ’…ç´™é„å‰§ãšéƒè·ºæ¬¢éŠ†ä½½å…˜é‘°æ¤¾è¢±æ¤¤è§„ç‰³è¹‡å†ªæ´°éå›©æ®‘é‰å†®å™¸ç’ï¼„å°é”›?
    objective_path = f"{algorithm_dir}/objective_analysis.png"
    plot_objective_function_breakdown(training_env, algorithm, objective_path)
    
    print(f"é¦ƒæ± {algorithm} ç’î… ç²Œé™îˆî‹é–æ §å‡¡ç€¹å±¾åš")
    print(f"   ç’î… ç²Œé¬æ˜î: {chart_path}")
    print(f"   é©î†½çˆ£é’å—˜ç€½: {objective_path}")
    
    # é¢ç†¸åšç’î… ç²Œé¬è¤ç²¨
    from visualization.clean_charts import get_summary_text
    summary = get_summary_text(training_env, algorithm)
    print(f"\n{summary}")


def compare_single_algorithms(algorithms: List[str], num_episodes: Optional[int] = None) -> Dict:
    """å§£æ—‡ç·æ¾¶æ°«é‡œé—æ›Ÿæ«¤é‘³æˆ’ç¶‹ç» æ¥ç¡¶é¨å‹¬â‚¬Ñ†å…˜"""
    # æµ£è·¨æ•¤é–°å¶‡ç–†æ¶“î… æ®‘æ¦›æ¨¿î…»éŠ?
    if num_episodes is None:
        num_episodes = config.experiment.num_episodes
    
    print("\né¦ƒæ•Ÿ å¯®â‚¬æ¿®å¬ªå´Ÿé…é¸¿å…˜æµ£æ’¶ç•»å¨‰æ›Ÿâ‚¬Ñ†å…˜å§£æ—‡ç·")
    print("=" * 60)
    
    results = {}
    
    # ç’î… ç²Œéµâ‚¬éˆå¤Œç•»å¨‰?
    for algorithm in algorithms:
        print(f"\nå¯®â‚¬æ¿®å¬­î†„ç¼?{algorithm}...")
        results[algorithm] = train_single_algorithm(algorithm, num_episodes)
    
    # é¦ƒå¸¹ é¢ç†¸åšç» â‚¬å¨²ä½ºæ®‘ç€µè§„ç˜®é¥æã€ƒ
    from visualization.clean_charts import create_comparison_chart
    timestamp = generate_timestamp()
    comparison_chart_path = f"results/single_agent_comparison_{timestamp}.png" if timestamp else "results/single_agent_comparison.png"
    create_comparison_chart(results, comparison_chart_path)
    
    # æ·‡æ¿†ç“¨å§£æ—‡ç·ç¼æ’´ç‰
    timestamp = generate_timestamp()
    comparison_results = {
        'algorithms': algorithms,
        'agent_type': 'single_agent',
        'num_episodes': num_episodes,
        'timestamp': timestamp,
        'comparison_time': datetime.now().isoformat(),
        'results': results,
        'summary': {}
    }
    
    # ç’ï¼„ç•»å§¹å›¨â‚¬è¤ç²ºç’?
    for algorithm, result in results.items():
        final_perf = result['final_performance']
        comparison_results['summary'][algorithm] = {
            'final_avg_reward': final_perf['avg_reward'],
            'final_avg_delay': final_perf['avg_delay'],
            'final_completion_rate': final_perf['avg_completion'],
            'training_time_hours': result['training_config']['training_time_hours']
        }
    
    # æµ£è·¨æ•¤éƒå •æ£¿é´è™«æƒæµ è·ºæ‚•
    comparison_filename = get_timestamped_filename("single_agent_comparison")
    with open(f"results/{comparison_filename}", "w", encoding="utf-8") as f:
        json.dump(comparison_results, f, indent=2, ensure_ascii=False)
    
    print("\né¦ƒå¹† é—æ›Ÿæ«¤é‘³æˆ’ç¶‹ç» æ¥ç¡¶å§£æ—‡ç·ç€¹å±¾åšé”›?)
    print(f"é¦ƒæ« å§£æ—‡ç·ç¼æ’´ç‰å®¸è¹­ç¹šç€›æ¨ºåŸŒ results/{comparison_filename}")
    print(f"é¦ƒæ³ ç€µè§„ç˜®é¥æã€ƒå®¸è¹­ç¹šç€›æ¨ºåŸŒ {comparison_chart_path}")
    
    return comparison_results




def main():
    """æ¶“è¯²åš±é?""
    parser = argparse.ArgumentParser(description='é—æ›Ÿæ«¤é‘³æˆ’ç¶‹ç» æ¥ç¡¶ç’î… ç²Œé‘´æ°­æ¹°')
    parser.add_argument('--algorithm', type=str, choices=['DDPG', 'TD3', 'TD3-LE', 'TD3_LE', 'TD3_LATENCY_ENERGY', 'DQN', 'PPO', 'SAC', 'CAM_TD3', 'OPTIMIZED_TD3'],
                       help='é–«å¤‹å«¨ç’î… ç²Œç» æ¥ç¡¶')
    parser.add_argument('--episodes', type=int, default=None, help=f'ç’î… ç²Œæî†½î‚¼ (æ¦›æ¨¿î…»: {config.experiment.num_episodes})')
    parser.add_argument('--eval_interval', type=int, default=None, help=f'ç’‡å‹ªåŠé—‚æ’®æ®§ (æ¦›æ¨¿î…»: {config.experiment.eval_interval})')
    parser.add_argument('--save_interval', type=int, default=None, help=f'æ·‡æ¿†ç“¨é—‚æ’®æ®§ (æ¦›æ¨¿î…»: {config.experiment.save_interval})')
    parser.add_argument('--compare', action='store_true', help='å§£æ—‡ç·éµâ‚¬éˆå¤Œç•»å¨‰?)
    parser.add_argument('--seed', type=int, default=None, help='ç‘•å—™æ´Šé—…å¿”æº€ç»‰å¶…ç“™ (æ¦›æ¨¿î…»ç’‡è¯²å½‡configé´æ «å¹†æ¾§å†¨å½‰é–²?')
    parser.add_argument('--num-vehicles', type=int, default=None, help='ç‘•å—™æ´Šæï¹ç· éä¼´å™ºé¢ã„¤ç°¬ç€¹ç‚ºç™')
    parser.add_argument('--force-offload', type=str, choices=['local', 'remote', 'local_only', 'remote_only'],
                        help='å¯®å“„åŸ—é—æ­Œæµ‡å¦¯â€³ç´¡é”›æ­­ocal/local_only é´?remote/remote_only')
    parser.add_argument('--fixed-offload-policy', type=str, 
                        choices=['random', 'greedy', 'local_only', 'rsu_only', 'round_robin', 'weighted'],
                        help='é¥å“„ç•¾é—æ­Œæµ‡ç»›æ «æšé”›å œç¬‰æµ£è·¨æ•¤é…é¸¿å…˜æµ£æ’³î„Ÿæ¶”ç‹…ç´šé”›æ­³andom/greedy/local_only/rsu_only/round_robin/weighted')
    # é¦ƒå¯ª ç€¹ç‚´æ¤‚é™îˆî‹é–æ §å¼¬é?(æ¦›æ¨¿î…»å¯®â‚¬éš?
    parser.add_argument('--realtime-vis', action='store_true', default=True, help='éšîˆœæ•¤ç€¹ç‚´æ¤‚é™îˆî‹é–?(æ¦›æ¨¿î…»å¯®â‚¬éš?')
    parser.add_argument('--no-realtime-vis', action='store_false', dest='realtime_vis', help='ç»‚ä½ºæ•¤ç€¹ç‚´æ¤‚é™îˆî‹é–?)
    parser.add_argument('--vis-port', type=int, default=5000, help='ç€¹ç‚´æ¤‚é™îˆî‹é–æ ¨æ¹‡é”â€³æ«’ç»”îˆšå½› (æ¦›æ¨¿î…»: 5000)')
    # é¦ƒå¸¹ æ¥‚æ¨¼î¬ç’î… ç²Œé™îˆî‹é–æ §å¼¬é?
    parser.add_argument('--advanced-vis', action='store_true', help='éšîˆœæ•¤æ¥‚æ¨¼î¬ç’î… ç²Œé™îˆî‹é–?Dashboard')
    # é¦ƒæ®Œ æ¾§ç‚²å·±ç¼‚æ’³ç“¨é™å‚›æšŸé”›å ¥ç²¯ç’ã‚…æƒé¢îŸ’ç´š
    parser.add_argument('--no-enhanced-cache', action='store_true', 
                       help='ç»‚ä½ºæ•¤æ¾§ç‚²å·±ç¼‚æ’³ç“¨ç»¯è¤ç²ºé”›å ¥ç²¯ç’ã‚…æƒé¢ã„¥åçä¾º1/L2 + é‘î…å®³ç»›æ «æš + RSUé—å¿ç¶”é”›?)
    # é¦ƒĞ› æ¶“ã‚‰æ¨å¨ˆç”µî…¸ç»¾å®ç´‘éç­¹ç´™Stage-1 æ£°å‹«åé–°?+ Stage-2 ç»®å‰§ç²ç’‹å†¨å®³é”›?
    parser.add_argument('--two-stage', action='store_true', help='éšîˆœæ•¤æ¶“ã‚‰æ¨å¨ˆå«çœ°ç‘™ï½ç´™æ£°å‹«åé–°?ç»®å‰§ç²ç’‹å†¨å®³é”›?)
    # é¦ƒî¥ é¸å›§ç•¾æ¶“ã‚„é‡œé—ƒèˆµî†Œé¨å‹­ç•»å¨‰?
    parser.add_argument('--stage1-alg', type=str, default=None,
                        help='é—ƒèˆµî†Œæ¶“â‚¬ç» æ¥ç¡¶é”›å¥ffloading æ¾¶è¾¾ç´šé”›æ­¨euristic|greedy|cache_first|distance_first')
    parser.add_argument('--stage2-alg', type=str, default=None,
                        help='é—ƒèˆµî†Œæµœå²€ç•»å¨‰æ›ªç´™ç¼‚æ’³ç“¨/æ©ä½ºĞ©éºÑƒåŸ—é¨å‡´Lé”›å¤›ç´°TD3|SAC|DDPG|PPO|DQN|TD3-LE')
    # é¦ƒå¹† æ¶“î…ãç’§å‹¬ç°®é’å—›å¤é‹èˆµç€¯é”›åœ¥hase 1 + Phase 2é”›? æ¦›æ¨¿î…»éšîˆœæ•¤
    parser.add_argument('--central-resource', action='store_true', default=True,
                        help='éšîˆœæ•¤æ¶“î…ãç’§å‹¬ç°®é’å—›å¤é‹èˆµç€¯é”›åœ¥hase 1éå´‡ç“¥ + Phase 2éµÑ†î”‘é”›å¤›ç´éµâ•çé˜èˆµâ‚¬?é”ã„¤ç¶”ç»Œæ´ªæ£¿ [æ¦›æ¨¿î…»éšîˆœæ•¤]')
    parser.add_argument('--no-central-resource', action='store_false', dest='central_resource',
                        help='ç»‚ä½ºæ•¤æ¶“î…ãç’§å‹¬ç°®é’å—›å¤é‹èˆµç€¯é”›å±¼å¨‡é¢ã„¦çˆ£é‘å——æ½é–â‚¬ç’§å‹¬ç°®é’å—›å¤')
    parser.add_argument('--silent-mode', action='store_true',
                        help='éšîˆœæ•¤é—ˆæ¬“ç²¯å¦¯â€³ç´¡é”›å²ƒçƒ¦æ©å›ªî†„ç¼å†ªç²¨é‰ç†·æ‚—é¨å‹ªæ°¦æµœæ“å½ç»€?)
    parser.add_argument('--resume-from', type=str,
                        help='æµ åº¡å‡¡éˆå¤‹Äé¨?(.pth é´æ «æ´°è¤°æ›å¢ ç¼‚â‚¬) ç¼Ñ…ç”»ç’î… ç²Œé”›å±½î˜²é¢ã„¥å‡¡ç€›ï¸¾ç“¥é£?)
    parser.add_argument('--resume-lr-scale', type=float, default=None,
                        help='Warm-start éšåº£æ®‘ç€›ï¸¿ç¯„éœå›©ç¼‰é€å‰§éƒ´é?(æ¦›æ¨¿î…»0.5é”›å²ƒî†•æ¶“?é™îˆ™ç¹šé£æ¬å¸«éŠ?')
    
    # é¦ƒå• é–«æ°«ä¿Šå¦¯â€³ç€·æµ¼æ¨ºå¯²é™å‚›æšŸé”›?GPPéå›§å™¯æ¾§ç‚²å·±é”›?
    parser.add_argument('--comm-enhancements', action='store_true',
                        help='éšîˆœæ•¤éµâ‚¬éˆå¤â‚¬æ°«ä¿Šå¦¯â€³ç€·æµ¼æ¨ºå¯²é”›å æ©ç›æ‹Œæƒ¤+ç»¯è¤ç²ºç»¾Ñƒå…±éµ?é”ã„¦â‚¬ä½¸ç”«ç€¹æ–¤ç´šEnable all communication model enhancements')
    parser.add_argument('--fast-fading', action='store_true',
                        help='éšîˆœæ•¤é—…å¿”æº€è¹‡î‚¥â€œé’€æ–¤ç´™Rayleigh/Riciané”›å¡ƒnable fast fading')
    parser.add_argument('--system-interference', action='store_true',
                        help='éšîˆœæ•¤ç»¯è¤ç²ºç»¾Ñƒå…±éµæ‹Œî…¸ç» ?Enable system-level interference calculation')
    parser.add_argument('--dynamic-bandwidth', action='store_true',
                        help='éšîˆœæ•¤é”ã„¦â‚¬ä½¸ç”«ç€¹è—‰åé–°?Enable dynamic bandwidth allocation')
    # é¦ƒå• å§ï½„æ°¦æ·‡ï¿ äº¾é’å—›å¤
    parser.add_argument('--channel-allocation', action='store_true',
                        help='éšîˆœæ•¤å§ï½„æ°¦æ·‡ï¿ äº¾é’å—›å¤é”›å å™ºçæˆæ‚“æ£°æˆå…±éµå¸®ç´šEnable orthogonal channel allocation')
    
    args = parser.parse_args()

    if args.seed is not None:
        os.environ['RANDOM_SEED'] = str(args.seed)
        _apply_global_seed_from_env()

    # é¦ƒå¹† æ¶“î…ãç’§å‹¬ç°®é’å—›å¤é‹èˆµç€¯é”›å ¥ç²¯ç’ã‚…æƒé¢îŸ’ç´š
    if args.central_resource:
        os.environ['CENTRAL_RESOURCE'] = '1'
        print("é¦ƒå¹† éšîˆœæ•¤æ¶“î…ãç’§å‹¬ç°®é’å—›å¤é‹èˆµç€¯é”›åœ¥hase 1 + Phase 2é”›å¡ æ¦›æ¨¿î…»å¦¯â€³ç´¡]")
    else:
        os.environ.pop('CENTRAL_RESOURCE', None)
        print("éˆ¿ç‹…ç¬  æµ£è·¨æ•¤éå›§å™¯é§å›§å¯‘ç’§å‹¬ç°®é’å—›å¤å¦¯â€³ç´¡é”›å å‡¡é–«æ°³ç¹ƒ --no-central-resource ç»‚ä½ºæ•¤æ¶“î…ãç’§å‹¬ç°®é”›?)
    
    # é¦ƒå• é–«æ°«ä¿Šå¦¯â€³ç€·æµ¼æ¨ºå¯²é–°å¶‡ç–†
    if args.comm_enhancements or args.fast_fading or args.system_interference or args.dynamic_bandwidth or args.channel_allocation:
        print("\n" + "="*70)
        print("é¦ƒå¯ª é–«æ°«ä¿Šå¦¯â€³ç€·æµ¼æ¨ºå¯²é–°å¶‡ç–†é”›?GPPéå›§å™¯æ¾§ç‚²å·±é”›?)
        print("="*70)
        
        # æ¿¡å‚›ç‰éšîˆœæ•¤æµœ?-comm-enhancementsé”›å±½å¯éšîˆœæ•¤éµâ‚¬éˆå¤‰ç´­é–?
        if args.comm_enhancements:
            config.communication.enable_fast_fading = True
            config.communication.use_system_interference = True
            config.communication.use_bandwidth_allocator = True
            config.communication.use_channel_allocation = True  # é¦ƒå• é–å‘­æƒˆæ·‡ï¿ äº¾é’å—›å¤
            config.communication.use_communication_enhancements = True
            print("é‰?éšîˆœæ•¤éµâ‚¬éˆå¤â‚¬æ°«ä¿Šå¦¯â€³ç€·æµ¼æ¨ºå¯²é”›å ç•¬é?GPPéå›§å™¯å¦¯â€³ç´¡é”›?)
        else:
            # é—æ› å«­é–°å¶‡ç–†éšå‹¯ã€æµ¼æ¨ºå¯²
            if args.fast_fading:
                config.communication.enable_fast_fading = True
                print("é‰?éšîˆœæ•¤é—…å¿”æº€è¹‡î‚¥â€œé’€æ–¤ç´™Rayleigh/Riciané’å——ç«·é”›?)
            
            if args.system_interference:
                config.communication.use_system_interference = True
                print("é‰?éšîˆœæ•¤ç»¯è¤ç²ºç»¾Ñƒå…±éµæ‹Œî…¸ç» ?)
            
            if args.dynamic_bandwidth:
                config.communication.use_bandwidth_allocator = True
                print("é‰?éšîˆœæ•¤é”ã„¦â‚¬ä½¸ç”«ç€¹è—‰åé–°å¶ˆçšŸæ´ï¹€æ«’")
            
            # é¦ƒå• å§ï½„æ°¦æ·‡ï¿ äº¾é’å—›å¤
            if args.channel_allocation:
                config.communication.use_channel_allocation = True
                print("é‰?éšîˆœæ•¤å§ï½„æ°¦æ·‡ï¿ äº¾é’å—›å¤é”›å å™ºçæˆæ‚“æ£°æˆå…±éµå¸®ç´š")
        
        # é„å‰§ãšé–°å¶‡ç–†ç’‡ï¸½å„
        print("\né–°å¶‡ç–†ç’‡ï¸½å„é”›?)
        print(f"  - è¹‡î‚¥â€œé’€? {'éšîˆœæ•¤' if config.communication.enable_fast_fading else 'ç»‚ä½ºæ•¤'}")
        print(f"  - ç»¯è¤ç²ºç»¾Ñƒå…±éµ? {'éšîˆœæ•¤' if config.communication.use_system_interference else 'ç»‚ä½ºæ•¤'}")
        print(f"  - é”ã„¦â‚¬ä½¸ç”«ç€¹è—‰åé–°? {'éšîˆœæ•¤' if config.communication.use_bandwidth_allocator else 'ç»‚ä½ºæ•¤'}")
        print(f"  - å§ï½„æ°¦æ·‡ï¿ äº¾é’å—›å¤: {'éšîˆœæ•¤' if config.communication.use_channel_allocation else 'ç»‚ä½ºæ•¤'}")
        print(f"  - æèŠ¥å°æ£°æˆ å·¼: {config.communication.carrier_frequency/1e9:.1f} GHz")
        print(f"  - ç¼‚æ «çˆœéå ¢å·¼: {config.communication.coding_efficiency}")
        if config.communication.enable_fast_fading:
            print(f"  - è¹‡î‚¥â€œé’€è—‰å¼¬é? èŸ½={config.communication.fast_fading_std}, K={config.communication.rician_k_factor}dB")
        if config.communication.use_channel_allocation:
            num_channels = int(config.communication.total_bandwidth / config.communication.channel_bandwidth)
            print(f"  - é¬è®³ä¿Šé–¬æ’´æšŸ: {num_channels}æ¶“?({config.communication.total_bandwidth/1e6:.0f}MHz / {config.communication.channel_bandwidth/1e6:.0f}MHz)")
        print("="*70 + "\n")
    
    # Toggle two-stage pipeline via environment for the simulator
    if args.two_stage:
        os.environ['TWO_STAGE_MODE'] = '1'
    # Stage1/Stage2 algorithm selections (env-based for env init)
    if args.stage1_alg:
        os.environ['STAGE1_ALG'] = args.stage1_alg
    if args.stage2_alg:
        # éä½½î†ç‘•å—™æ´Šæ¶“è¤ç•»å¨‰æ›¢â‚¬å¤‹å«¨
        if not args.algorithm:
            args.algorithm = args.stage2_alg
        else:
            # ç‘•å——å•“æ¶“æ´ªæ¨å¨ˆå…¸ç°©é–«å¤‹å«¨
            args.algorithm = args.stage2_alg

    # é¦ƒæ•¡ æ·‡î†¼î˜²é”›æ°­î„œçº­î†½ç€¯å¯¤ç°…verride_scenarioé™å‚›æšŸ
    override_scenario = None
    if args.num_vehicles is not None:
        override_scenario = {
            "num_vehicles": args.num_vehicles,
        }
        # éšå±¾æ¤‚ç’å‰§ç–†éœîˆšî•¨é™æ©€å™ºé”›å æ‚œéšåº¡å‹ç€¹ç™¸ç´š
        os.environ['TRAINING_SCENARIO_OVERRIDES'] = json.dumps(override_scenario)
        print(f"é¦ƒæµ ç‘•å—™æ´Šé™å‚›æšŸ: æï¹ç· é?= {args.num_vehicles}")
    
    enforce_mode = None
    if getattr(args, 'force_offload', None):
        if args.force_offload in ('local', 'local_only'):
            enforce_mode = 'local_only'
        elif args.force_offload in ('remote', 'remote_only'):
            enforce_mode = 'remote_only'
    
    # é’æ¶˜ç¼“ç¼æ’´ç‰é©î†¼ç¶
    os.makedirs("results/single_agent", exist_ok=True)
    
    # é¦ƒå¹† é„å‰§ãšCAMTD3ç»¯è¤ç²ºæ·‡â„ƒä¼…
    if args.algorithm and not args.compare:
        print("\n" + "="*80)
        print("é¦ƒæ®Œ CAMTD3 ç’î… ç²Œç»¯è¤ç²ºéšîˆšå§©")
        print("="*80)
        print(f"ç»¯è¤ç²ºéšå¶‡Ğ: CAMTD3 (Cache-Aware Migration with Twin Delayed DDPG)")
        print(f"æµ£è·¨æ•¤ç» æ¥ç¡¶: {args.algorithm}")
        print(f"ç»¯è¤ç²ºé‹èˆµç€¯: Phase 1 (æ¶“î…ãç’§å‹¬ç°®é’å—›å¤) + Phase 2 (æµ è¯²å§ŸéµÑ†î”‘)")
        print(f"ç’î… ç²Œæî†½æšŸ: {args.episodes}")
        if args.seed:
            print(f"é—…å¿”æº€ç»‰å¶…ç“™: {args.seed}")
        print(f"ç€¹å±¾æš£éšå¶‡Ğ: CAMTD3-{args.algorithm}")
        print("="*80 + "\n")
    
    if args.compare:
        # å§£æ—‡ç·éµâ‚¬éˆå¤Œç•»å¨‰?
        algorithms = ['DDPG', 'TD3', 'TD3-LE', 'DQN', 'PPO', 'SAC']
        compare_single_algorithms(algorithms, args.episodes)
    elif args.algorithm:
        # ç’î… ç²Œé—æ›šé‡œç» æ¥ç¡¶ - é¦ƒæ•¡ æµ¼çŠ»â‚¬æŠ©verride_scenarioé™å‚›æšŸ
        train_single_algorithm(
            args.algorithm, 
            args.episodes, 
            args.eval_interval, 
            args.save_interval,
            enable_realtime_vis=args.realtime_vis,
            vis_port=args.vis_port,
            override_scenario=override_scenario,  # é¦ƒæ•¡ é‚æ¿î–ƒé”›æ°«ç´¶é–«æ•î›«é©æ §å¼¬é?
            use_enhanced_cache=not args.no_enhanced_cache,  # é¦ƒæ®Œ æ¦›æ¨¿î…»éšîˆœæ•¤æ¾§ç‚²å·±ç¼‚æ’³ç“¨
            enforce_offload_mode=enforce_mode,
            fixed_offload_policy=getattr(args, 'fixed_offload_policy', None),  # é¦ƒå¹† é¥å“„ç•¾é—æ­Œæµ‡ç»›æ «æš
            silent_mode=args.silent_mode,
            resume_from=args.resume_from,
            resume_lr_scale=args.resume_lr_scale,
            enable_advanced_vis=args.advanced_vis  # é¦ƒå¸¹ æ¥‚æ¨¼î¬é™îˆî‹é–?
        )
    else:
        print("ç’‡é”‹å¯šç€¹?--algorithm é´æ ¦å¨‡é¢?--compare éå›§ç¹”")
        print("æµ£è·¨æ•¤ python train_single_agent.py --help éŒãƒ§æ¹…ç”¯î†¼å§ª")


if __name__ == "__main__":
    main()
    
"""

é¦ƒæ”§ ç€¹å±¾æš£éµÑ†î”‘å¨´ä½ºâ–¼é”›å å5æ¶“îˆæ¨å¨ˆç¢‰ç´š
é¦ƒæ¶ é—ƒèˆµî†Œ1: ç»¯è¤ç²ºé’æ¿†îé–?(train_single_agent.py: mainé‘èŠ¥æšŸ)
1.1 é™å‚›æšŸç‘™ï½†ç€½æ¶“åº¨å¤ç¼ƒ?
éˆ¹æº¾æ”¢ ç‘™ï½†ç€½é›æˆ’æŠ¤ç›å±½å¼¬é?
éˆ¹? éˆ¹æº¾æ”¢ algorithm = "TD3"
éˆ¹? éˆ¹æº¾æ”¢ episodes = 800  
éˆ¹? éˆ¹æº¾æ”¢ num_vehicles = 12
éˆ¹? éˆ¹æ–ºæ”¢ enhanced_cache = True (æ¦›æ¨¿î…»)
éˆ¹?
éˆ¹æº¾æ”¢ ç’å‰§ç–†é—…å¿”æº€ç»‰å¶…ç“™
éˆ¹? éˆ¹æ–ºæ”¢ æµ å·†onfigé´æ «å¹†æ¾§å†¨å½‰é–²å¿šî‡°é™æ «î’ç€›?
éˆ¹?
éˆ¹æ–ºæ”¢ é‹å‹«ç¼“é¦çƒ˜æ«™é–°å¶‡ç–† override_scenario
   éˆ¹æ–ºæ”¢ {'num_vehicles': 12, 'override_topology': True}
   
1.2 é’æ¶˜ç¼“ç’î… ç²Œéœîˆšî•¨ (SingleAgentTrainingEnvironment)
éœîˆšî•¨é’æ¿†îé–æ ¨ç¥¦ç»‹?
éˆ¹æº¾æ”¢ 1) é–«å¤‹å«¨æµ è·¨æ¹¡é£ã„§è¢«é¨?
éˆ¹? éˆ¹æº¾æ”¢ use_enhanced_cache=True
éˆ¹? éˆ¹æ–ºæ”¢ simulator = EnhancedSystemSimulator(scenario_config)
éˆ¹?
éˆ¹æº¾æ”¢ 2) é’æ¿†îé–æ ¦è±¢éªç†·æ«’ç¼å‹ªæ¬¢ (system_simulator.py)
éˆ¹? éˆ¹æº¾æ”¢ æï¹ç· é’æ¿†îé–? 12æˆå—šæº…
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ æµ£å¶‡ç–†: é—…å¿”æº€é’å——ç«·é¦ã„©äº¾ç’ºîˆ™ç¬‚
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ é–«ç†·å®³: 30-50 km/h
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ ç¼‚æ’³ç“¨: L1(200MB) + L2(300MB)
éˆ¹? éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ RSUé–®ã„§è®²: 4æ¶“î‡çŸ¾æ¸šÑƒå´Ÿé?(é¥å“„ç•¾é·æ’´å¢¤)
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ æµ£å¶‡ç–†: ç»›å¤æ£¿ç’ºæ¿†åç”¯?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç‘•å—™æ´Šé—å©‚ç·: 150m
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç¼‚æ’³ç“¨ç€¹å½’å™º: 1000MB
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ ç’ï¼„ç•»é‘³è—‰å§: 50 GHz
éˆ¹? éˆ¹?
éˆ¹? éˆ¹æ–ºæ”¢ UAVé–®ã„§è®²: 2æ¶“î…æ£¤æµœçƒ˜æº€
éˆ¹?    éˆ¹æº¾æ”¢ æµ£å¶‡ç–†: é”ã„¦â‚¬ä½¸è´°é‘¸?
éˆ¹?    éˆ¹æº¾æ”¢ æ¥‚æ¨ºå®³: 100m
éˆ¹?    éˆ¹æº¾æ”¢ ç¼‚æ’³ç“¨ç€¹å½’å™º: 200MB
éˆ¹?    éˆ¹æ–ºæ”¢ ç’ï¼„ç•»é‘³è—‰å§: 20 GHz
éˆ¹?
éˆ¹æº¾æ”¢ 3) é’æ¿†îé–æ ¬åšœé–«å‚šç°²éºÑƒåŸ—é£?
éˆ¹? éˆ¹æº¾æ”¢ AdaptiveCacheController (é…é¸¿å…˜ç¼‚æ’³ç“¨éºÑƒåŸ—)
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ é’å——çœ°L1/L2ç¼‚æ’³ç“¨ç»›æ «æš
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ é‘î…å®³æ©å€Ÿé‡œ (HeatBasedStrategy)
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ RSUé—å¿ç¶”ç¼‚æ’³ç“¨
éˆ¹? éˆ¹?
éˆ¹? éˆ¹æ–ºæ”¢ AdaptiveMigrationController (æ©ä½ºĞ©éå´‡ç“¥éºÑƒåŸ—)
éˆ¹?    éˆ¹æº¾æ”¢ ç’ç†»æµ‡é˜å——å½¶æ©å€Ÿé‡œ
éˆ¹?    éˆ¹æº¾æ”¢ æ¾¶æ°±æ·®ç‘™ï¹€å½‚é‰â€²æ¬¢
éˆ¹?    éˆ¹æ–ºæ”¢ é´æ„­æ¹°éå ¢æ³­é’å—˜ç€½
éˆ¹?
éˆ¹æ–ºæ”¢ 4) é·æ’´å¢¤æµ¼æ¨ºå¯² (FixedTopologyOptimizer)
   éˆ¹æº¾æ”¢ éè§„åµæï¹ç· éé¢ç´­é–æ ¬ç§´é™å‚›æšŸ
   éˆ¹æº¾æ”¢ num_vehicles=12 éˆ«?hidden_dim=512
   éˆ¹æº¾æ”¢ actor_lr=1e-4, critic_lr=8e-5
   éˆ¹æ–ºæ”¢ batch_size=256
   
1.3 é’æ¶˜ç¼“TD3é…é¸¿å…˜æµ£?(TD3Environment)
TD3ç» æ¥ç¡¶é’æ¿†îé–?
éˆ¹æº¾æ”¢ ç¼ƒæˆ ç²¶ç¼æ’´ç€¯
éˆ¹? éˆ¹æº¾æ”¢ Actorç¼ƒæˆ ç²¶ (ç»›æ «æšç¼ƒæˆ ç²¶)
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ æˆæ’³å†: state_dim = æï¹ç· (12è„³5) + RSU(4è„³5) + UAV(2è„³5) + éã„¥çœ¬(16) = 106ç¼?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ é—…æ„¯æ£Œç? 512 éˆ«?512 éˆ«?256
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ æˆæ’³åš­: action_dim = 3(æµ è¯²å§Ÿé’å—›å¤) + 4(RSUé–«å¤‹å«¨) + 2(UAVé–«å¤‹å«¨) + 8(éºÑƒåŸ—é™å‚›æšŸ) = 17ç¼?
éˆ¹? éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ Twin Criticç¼ƒæˆ ç²¶ (æµ å³°â‚¬è‚©ç¶‰ç¼æº?)
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ Critic1: ç’‡å‹ªåŠé˜èˆµâ‚¬?é”ã„¤ç¶”æµ å³°â‚¬?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ Critic2: é‘å¿“çš¯æ©å›¦åŠç’â€³äº¸å®¸?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ æˆæ’³å†: state(106ç¼? + action(17ç¼? éˆ«?æˆæ’³åš­: QéŠ?
éˆ¹? éˆ¹?
éˆ¹? éˆ¹æ–ºæ”¢ Targetç¼ƒæˆ ç²¶ (é©î†½çˆ£ç¼ƒæˆ ç²¶)
éˆ¹?    éˆ¹æº¾æ”¢ Target Actor: é¢ç†¸åšé©î†½çˆ£é”ã„¤ç¶”
éˆ¹?    éˆ¹æº¾æ”¢ Target Critic1 & Critic2: ç’ï¼„ç•»é©î†½çˆ£QéŠ?
éˆ¹?    éˆ¹æ–ºæ”¢ æîˆ›æ´¿é‚æ¿å¼¬é? èŸ¿=0.005
éˆ¹?
éˆ¹æº¾æ”¢ ç¼å¿›ç™é¥ç‚´æ–ç¼‚æ’³å•¿é–?
éˆ¹? éˆ¹æº¾æ”¢ ç€¹å½’å™º: 100,000é‰ï¼„ç²¡æ¥ ?
éˆ¹? éˆ¹æº¾æ”¢ éµè§„î‚¼æ¾¶Ñƒçš¬: 256
éˆ¹? éˆ¹æ–ºæ”¢ æµ¼æ¨ºå›ç»¾Ñ…ç²¡æ¥ å±½æ´–é€?(PER)
éˆ¹?    éˆ¹æº¾æ”¢ ä¼ª=0.6 (æµ¼æ¨ºå›ç»¾Ñ„å¯šé?
éˆ¹?    éˆ¹æ–ºæ”¢ å°¾=0.4éˆ«?.0 (é–²å¶ˆî›¦é¬Ñ‡å™°é?
éˆ¹?
éˆ¹æ–ºæ”¢ TD3é—è§„æ¹éˆå“„åŸ—
   éˆ¹æº¾æ”¢ ç»›æ «æšå¯¤æƒ°ç¹œé‡å­˜æŸŠ: policy_delay=2 (å§£?å§ãƒ¦æ´¿é‚ç™†ctor)
   éˆ¹æº¾æ”¢ é©î†½çˆ£ç»›æ «æšéªè™«ç²¦: target_noise=0.05
   éˆ¹æº¾æ”¢ éºãˆ¢å‚¨é£î„ï¼: exploration_noise=0.2 (é¸å›¨æšŸç›æ¿å™º)
   éˆ¹æ–ºæ”¢ å§Šîˆšå®³ç‘ä½¸å£€: gradient_clip=0.7
   
é¦ƒæ¶ é—ƒèˆµî†Œ2: Episodeå¯°î†å¹† (ç’î… ç²Œ800æ¶“çŒ pisode)
2.1 Episodeé–²å¶‡ç–†
å§£å¿é‡œEpisodeå¯®â‚¬æ¿®å¬«æ¤‚:
éˆ¹æº¾æ”¢ 1) é–²å¶‡ç–†æµ è·¨æ¹¡é£?(system_simulator.py: initialize_components)
éˆ¹? éˆ¹æº¾æ”¢ å¨“å‘¯â”–éµâ‚¬éˆå¤æ§¦é’?
éˆ¹? éˆ¹æº¾æ”¢ é–²å¶‡ç–†æï¹ç· æµ£å¶‡ç–†éœå²„â‚¬ç†·å®³
éˆ¹? éˆ¹æº¾æ”¢ å¨“å‘¯â”–ç¼‚æ’³ç“¨éå‘­î†
éˆ¹? éˆ¹æº¾æ”¢ é–²å¶‡ç–†ç¼ç†»î…¸éç‰ˆåµ
éˆ¹? éˆ¹æ–ºæ”¢ é–²å¶†æŸŠé¢ç†¸åšéå‘­î†æ´?(1000æ¶“î„å”´ç€¹?
éˆ¹?
éˆ¹æº¾æ”¢ 2) é‹å‹«ç¼“é’æ¿†îé˜èˆµâ‚¬?
éˆ¹? éˆ¹æº¾æ”¢ æï¹ç· é˜èˆµâ‚¬?(12è„³5ç¼?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ æµ£å¶‡ç–†(x,y): è¤°æç«´é–æ §åŸŒ[0,1]
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ é–«ç†·å®³: è¤°æç«´é–æ §åŸŒ[0,1]
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ æµ è¯²å§Ÿé—ƒç†·åªé—€å®å®³: è¤°æç«´é–?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ é‘³å€Ÿâ‚¬? è¤°æç«´é–?
éˆ¹? éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ RSUé˜èˆµâ‚¬?(4è„³5ç¼?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ æµ£å¶‡ç–†(x,y)
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç¼‚æ’³ç“¨é’â•ƒæ•¤éœ?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ é—ƒç†·åªç’ç†»æµ‡
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ é‘³å€Ÿâ‚¬?
éˆ¹? éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ UAVé˜èˆµâ‚¬?(2è„³5ç¼?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ æµ£å¶‡ç–†(x,y,z)
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç¼‚æ’³ç“¨é’â•ƒæ•¤éœ?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ é‘³å€Ÿâ‚¬?
éˆ¹? éˆ¹?
éˆ¹? éˆ¹æ–ºæ”¢ éã„¥çœ¬é˜èˆµâ‚¬?(16ç¼?
éˆ¹?    éˆ¹æº¾æ”¢ éªå†²æ½é—ƒç†·åªé—€å®å®³
éˆ¹?    éˆ¹æº¾æ”¢ éªå†²æ½ç¼‚æ’³ç“¨é’â•ƒæ•¤éœ?
éˆ¹?    éˆ¹æº¾æ”¢ ç»¯è¤ç²ºç’ç†»æµ‡
éˆ¹?    éˆ¹æº¾æ”¢ æµ è¯²å§Ÿç»«è¯²ç€·é’å——ç«· (4ç¼?
éˆ¹?    éˆ¹æº¾æ”¢ æµ è¯²å§Ÿç»«è¯²ç€·é—ƒç†·åªé—çŠ³ç˜® (4ç¼?
éˆ¹?    éˆ¹æ–ºæ”¢ æµ è¯²å§Ÿç»«è¯²ç€·é´î…î„›éˆ?(4ç¼?
éˆ¹?
éˆ¹æ–ºæ”¢ 3) é–²å¶‡ç–†éºÑƒåŸ—é£ã„§å§¸é¬?
   éˆ¹æº¾æ”¢ ç¼‚æ’³ç“¨éºÑƒåŸ—é£? å¨“å‘¯â”–é‘î…å®³æ©å€Ÿé‡œ
   éˆ¹æ–ºæ”¢ æ©ä½ºĞ©éºÑƒåŸ—é£? å¨“å‘¯â”–ç’ç†»æµ‡é˜å——å½¶

2.2 éƒå •æ£¿å§ãƒ¥æƒŠéœ?(å§£å¿é‡œEpisodeç»¾?00-300å§?
å§£å¿é‡œéƒå •æ£¿å§ãƒ§æ®‘éµÑ†î”‘å¨´ä½ºâ–¼:

éˆ¹å±¸æ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?
éˆ¹? å§ãƒ©î€ƒ1: TD3é–«å¤‹å«¨é”ã„¤ç¶” (td3.py: select_action)        éˆ¹?
éˆ¹æº¾æ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?
éˆ¹? æˆæ’³å†: state (106ç¼æ‘æ‚œé–²?                            éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ é“å¶…æ‚œæµ¼çŠ³æŒ±é–«æ°³ç¹ƒActorç¼ƒæˆ ç²¶                          éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ æˆæ’³åš­é˜ç†·îé”ã„¤ç¶”: action_raw (17ç¼?             éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ å¨£è¯²å§éºãˆ¢å‚¨é£î„ï¼ (æ¥‚æ¨»æŸ‰é£î„ï¼)                        éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ noise = N(0, exploration_noise)              éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ action = action_raw + noise                  éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ é”ã„¤ç¶”ç‘ä½¸å£€é’ç™§-1, 1]                              éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æ–ºæ”¢ é”ã„¤ç¶”é’å—šĞ’ (decompose_action)                    éˆ¹?
éˆ¹?    éˆ¹æº¾æ”¢ æµ è¯²å§Ÿé’å—›å¤é‹å¿“ã‚½ [0:3]                          éˆ¹?
éˆ¹?    éˆ¹? éˆ¹æ–ºæ”¢ softmax([local, rsu, uav])               éˆ¹?
éˆ¹?    éˆ¹æº¾æ”¢ RSUé–«å¤‹å«¨é‰å†®å™¸ [3:7]                           éˆ¹?
éˆ¹?    éˆ¹? éˆ¹æ–ºæ”¢ softmax(4æ¶“çŒ‚SUé¨å‹¬æ½ˆé–²?                    éˆ¹?
éˆ¹?    éˆ¹æº¾æ”¢ UAVé–«å¤‹å«¨é‰å†®å™¸ [7:9]                           éˆ¹?
éˆ¹?    éˆ¹? éˆ¹æ–ºæ”¢ softmax(2æ¶“çŒ†AVé¨å‹¬æ½ˆé–²?                    éˆ¹?
éˆ¹?    éˆ¹æ–ºæ”¢ éºÑƒåŸ—é™å‚›æšŸ [9:17]                             éˆ¹?
éˆ¹?       éˆ¹æº¾æ”¢ ç¼‚æ’³ç“¨éºÑƒåŸ— (4ç¼?                           éˆ¹?
éˆ¹?       éˆ¹? éˆ¹æº¾æ”¢ é‘î…å®³é—ƒå â‚¬è‰°çšŸé?                         éˆ¹?
éˆ¹?       éˆ¹? éˆ¹æº¾æ”¢ å¨£æ¨»å‘ç»›æ «æšé‰å†®å™¸                          éˆ¹?
éˆ¹?       éˆ¹? éˆ¹æº¾æ”¢ é—å¿ç¶”å¯®å“„å®³                              éˆ¹?
éˆ¹?       éˆ¹? éˆ¹æ–ºæ”¢ L1/L2å§£æ–¾ç·¥                             éˆ¹?
éˆ¹?       éˆ¹æ–ºæ”¢ æ©ä½ºĞ©éºÑƒåŸ— (4ç¼?                           éˆ¹?
éˆ¹?          éˆ¹æº¾æ”¢ ç’ç†»æµ‡é—ƒå â‚¬?                             éˆ¹?
éˆ¹?          éˆ¹æº¾æ”¢ é´æ„­æ¹°éå¿”åŠ…æ´?                           éˆ¹?
éˆ¹?          éˆ¹æº¾æ”¢ å¯¤æƒ°ç¹œé‰å†®å™¸                              éˆ¹?
éˆ¹?          éˆ¹æ–ºæ”¢ é‘³å€Ÿâ‚¬æ¥æ½ˆé–²?                             éˆ¹?
éˆ¹æ–ºæ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?

éˆ¹å±¸æ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?
éˆ¹? å§ãƒ©î€ƒ2: é„çŠ²çš é”ã„¤ç¶”é’æ‹Œåšœé–«å‚šç°²éºÑƒåŸ—é£?                    éˆ¹?
éˆ¹æº¾æ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?
éˆ¹? (train_single_agent.py: _build_simulator_actions)  éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ ç‘™ï½†ç€½éºÑƒåŸ—é™å‚›æšŸ (éš?ç¼æ‘å§©æµ£?                       éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ ç’‹å†ªæ•¤ map_agent_actions_to_params()             éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ çå“°-1,1]é‘¼å†¨æ´¿é„çŠ²çš é’æ¿å¿æµ£æ’³å¼¬éæ‹Œå¯–é¥?            éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ é’å—™î‡ç¼‚æ’³ç“¨é™å‚›æšŸéœå²ƒç¸¼ç»‰è¯²å¼¬é?                    éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ é‡å­˜æŸŠ AdaptiveCacheController                   éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ heat_threshold = action[0] * 50 + 50        éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ eviction_strategy_weight = sigmoid(action[1])éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ collaboration_strength = action[2] * 0.5 + 0.5éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ l1_l2_ratio = action[3] * 0.3 + 0.4         éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æ–ºæ”¢ é‡å­˜æŸŠ AdaptiveMigrationController               éˆ¹?
éˆ¹?    éˆ¹æº¾æ”¢ load_threshold = action[4] * 0.3 + 0.6      éˆ¹?
éˆ¹?    éˆ¹æº¾æ”¢ cost_sensitivity = action[5] * 0.5 + 0.5    éˆ¹?
éˆ¹?    éˆ¹æº¾æ”¢ delay_weight = action[6] * 0.4 + 0.4        éˆ¹?
éˆ¹?    éˆ¹æ–ºæ”¢ energy_weight = action[7] * 0.4 + 0.4       éˆ¹?
éˆ¹æ–ºæ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?

éˆ¹å±¸æ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?
éˆ¹? å§ãƒ©î€ƒ3: æµ è·¨æ¹¡é£ã„¦å¢½ç›å±¼ç«´å§?                             éˆ¹?
éˆ¹æº¾æ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?
éˆ¹? (system_simulator.py: run_simulation_step)         éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 3.1 é‡å­˜æŸŠæï¹ç· æµ£å¶‡ç–†                               éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ éè§„åµé–«ç†·å®³éœå±¾æŸŸéšæˆ Ğ©é”?                        éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ æ¾¶å‹­æ‚Šç’ºîˆšå½›æî„€æ‚œ                               éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ å¨£è¯²å§é—…å¿”æº€éµæ¿å§©                               éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 3.2 é¢ç†¸åšæµ è¯²å§Ÿ                                   éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ å¨‰å©ƒæ¾—æ©å›©â–¼é–²å›¨ç‰± (ä½=æï¹ç· éæ‡Šæ¤¾æ¢é”ï¼„å·¼)            éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ æ¶“çƒ˜ç˜¡æˆå—šæº…é¢ç†¸åšæµ è¯²å§Ÿ                           éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ æµ è¯²å§Ÿç»«è¯²ç€· (1-4): éè§„åµé¦çƒ˜æ«™é’å——ç«·           éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ éç‰ˆåµæ¾¶Ñƒçš¬: 0.5-2.0 MB                    éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç’ï¼„ç•»é—‡â‚¬å§¹? 500-3000 CPUé›ã„¦æ¹¡              éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ é´î…î„›éˆ? 0.5-3.0ç»‰?                      éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ å¨£è¯²å§é’æ‹Œæº…æˆå—•æ¢é”ï¿ æ§¦é’?                        éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 3.3 æµ è¯²å§Ÿé’å—›å¤æ¶“åº¤çšŸæ´?                            éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç€µè§„ç˜¡æ¶“îƒæ¢é”â€³å–…ç»›æ §åµæç•Œæ´°é?                    éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ éˆî„€æ¹´æ¾¶å‹­æ‚Š (å§’å‚œå·¼: local_pref)             éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ RSUé—æ­Œæµ‡ (å§’å‚œå·¼: rsu_pref)                éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ éè§„åµRSUé–«å¤‹å«¨é‰å†®å™¸é–«å¤‹å«¨éèœ‚ç¶‹RSU          éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ UAVé—æ­Œæµ‡ (å§’å‚œå·¼: uav_pref)                éˆ¹?
éˆ¹? éˆ¹? éˆ¹?    éˆ¹æ–ºæ”¢ éè§„åµUAVé–«å¤‹å«¨é‰å†®å™¸é–«å¤‹å«¨éèœ‚ç¶‹UAV          éˆ¹?
éˆ¹? éˆ¹? éˆ¹?                                             éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç¼‚æ’³ç“¨é›æˆ’è…‘å¦«â‚¬éŒ?                              éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ check_cache_hit_adaptive()              éˆ¹?
éˆ¹? éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ å¦«â‚¬éŒãƒ¥å”´ç€¹è§„æ§¸éšï¹€æ¹ªé‘ºå‚œå£ç¼‚æ’³ç“¨æ¶“?           éˆ¹?
éˆ¹? éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ é›æˆ’è…‘: é‘å¿“çš¯æµ¼çŠºç·­éƒè·ºæ¬¢                  éˆ¹?
éˆ¹? éˆ¹? éˆ¹?    éˆ¹æ–ºæ”¢ éˆî„æ‡¡æ¶“? é…é¸¿å…˜ç¼‚æ’³ç“¨éå´‡ç“¥                éˆ¹?
éˆ¹? éˆ¹? éˆ¹?       éˆ¹æº¾æ”¢ ç’‹å†ªæ•¤ç¼‚æ’³ç“¨éºÑƒåŸ—é£?should_cache_contentéˆ¹?
éˆ¹? éˆ¹? éˆ¹?       éˆ¹æº¾æ”¢ é©è½°ç°¬é‘î…å®³éå†²ç•¾é„îˆšæƒç¼‚æ’³ç“¨             éˆ¹?
éˆ¹? éˆ¹? éˆ¹?       éˆ¹æ–ºæ”¢ éµÑ†î”‘å¨£æ¨»å‘éœå±½å´—æµ£æ»…ç´¦ç€›?              éˆ¹?
éˆ¹? éˆ¹? éˆ¹?                                             éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ æµ è¯²å§Ÿæµ¼çŠºç·­æ¶“åº¡å†é—ƒ?                            éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ ç’ï¼„ç•»æ¶“å©…î”‘æµ¼çŠºç·­éƒè·ºæ¬¢éœå²ƒå…˜é‘°?                éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ çå—•æ¢é”â€³å§éãƒ¨å¦­éç¡…î…¸ç» æ¥…æ§¦é’?                éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æ–ºæ”¢ ç’æ¿ç¶æµ è¯²å§Ÿéå†©æšŸé¹?                        éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 3.4 æ¾¶å‹­æ‚Šç’ï¼„ç•»é—ƒç†·åª                               éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ _process_node_queues()                      éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ é–¬å¶…å·»éµâ‚¬éˆå¡•SUéœå­¶AV                        éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ ç€µè§„ç˜¡æ¶“î‡å¦­é?                             éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æº¾æ”¢ é‘¾å³°å½‡é—ƒç†·åªé—€å®å®³                         éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æº¾æ”¢ é”ã„¦â‚¬ä½½çšŸéæ‘î˜©éå—šå…˜é”?                    éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ capacity = base + boost(é—ƒç†·åªé—€å®å®³) éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æº¾æ”¢ æ¾¶å‹­æ‚Šæµ è¯²å§Ÿå®¸ãƒ¤ç¶”é–²?                      éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ work_remaining -= capacity        éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æº¾æ”¢ ç€¹å±¾åšé¨å‹ªæ¢é”?                          éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç’ï¼„ç•»æ¶“å¬­î”‘æµ¼çŠºç·­                      éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ é‡å­˜æŸŠç¼ç†»î…¸(å¯¤æƒ°ç¹œéŠ†ä½½å…˜é‘°?             éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ éå›ªî†‡ç€¹å±¾åš                          éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æ–ºæ”¢ æ¾¶å‹­æ‚Šç“’å‘®æ¹¡æµ è¯²å§Ÿ                         éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æ–ºæ”¢ é‡å­˜æŸŠé‘ºå‚œå£é˜èˆµâ‚¬?                            éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 3.5 é‘·îˆâ‚¬å‚šç°²æ©ä½ºĞ©å¦«â‚¬éŒ?                            éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ check_adaptive_migration()                  éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ ç’ï¼„ç•»éµâ‚¬éˆå¤å¦­éç¡…ç¤‹æè—‰æ´œç€›?                   éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æ–ºæ”¢ load = 0.8è„³é—ƒç†·åªç’ç†»æµ‡ + 0.2è„³ç¼‚æ’³ç“¨é’â•ƒæ•¤éœå›£æ”¤
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ é‡å­˜æŸŠæ©ä½ºĞ©éºÑƒåŸ—é£ã„¨ç¤‹æè—‰å·»é™?                 éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ é’ã‚†æŸ‡é„îˆšæƒç‘™ï¹€å½‚æ©ä½ºĞ©                        éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æº¾æ”¢ ç’ç†»æµ‡ç“’å‘´æ§‡éŠ?                          éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æº¾æ”¢ é¸ä½ºç”»éƒå •æ£¿ç“’å†²î™„                         éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æ–ºæ”¢ é´æ„­æ¹°éå ¢æ³­é’å—˜ç€½é–«æ°³ç¹ƒ                     éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æ–ºæ”¢ éµÑ†î”‘æ©ä½ºĞ©                                éˆ¹?
éˆ¹? éˆ¹?       éˆ¹æº¾æ”¢ RSUéˆ«æ‰²SU (éˆå¤Œåšæ©ä½ºĞ©)                  éˆ¹?
éˆ¹? éˆ¹?       éˆ¹? éˆ¹æº¾æ”¢ é–«å¤‹å«¨é©î†½çˆ£RSU (ç’ç†»æµ‡éˆâ‚¬æ?           éˆ¹?
éˆ¹? éˆ¹?       éˆ¹? éˆ¹æº¾æ”¢ ç’ï¼„ç•»æ©ä½ºĞ©é´æ„­æ¹°                      éˆ¹?
éˆ¹? éˆ¹?       éˆ¹? éˆ¹æº¾æ”¢ æµ¼çŠºç·­æµ è¯²å§Ÿ                          éˆ¹?
éˆ¹? éˆ¹?       éˆ¹? éˆ¹æ–ºæ”¢ é‡å­˜æŸŠç¼ç†»î…¸                          éˆ¹?
éˆ¹? éˆ¹?       éˆ¹æ–ºæ”¢ UAVéˆ«æ‰²SU (éƒçŠµåšæ©ä½ºĞ©)                  éˆ¹?
éˆ¹? éˆ¹?          éˆ¹æ–ºæ”¢ ç»«è®³æŠ€å¨´ä½ºâ–¼                          éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 3.6 é‡å­˜æŸŠç¼ç†»î…¸é¸å›¨çˆ£                               éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç»±îˆî…¸ç€¹å±¾åšæµ è¯²å§Ÿé?                            éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç»±îˆî…¸å¯¤æƒ°ç¹œ                                   éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç»±îˆî…¸é‘³å€Ÿâ‚¬?                                  éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç¼‚æ’³ç“¨é›æˆ’è…‘éœ?                                éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ æ©ä½ºĞ©é´æ„¬å§›éœ?                                éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ æµ è¯²å§Ÿç»«è¯²ç€·é’å——ç«·ç¼ç†»î…¸                           éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æ–ºæ”¢ æ©æ–¿æ´– step_stats (éˆî„î„ç¼ç†»î…¸éç‰ˆåµ)                éˆ¹?
éˆ¹æ–ºæ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?

éˆ¹å±¸æ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?
éˆ¹? å§ãƒ©î€ƒ4: ç’ï¼„ç•»æ¿‚æ §å§³éœå±¼ç¬…æ¶“â‚¬é˜èˆµâ‚¬?                        éˆ¹?
éˆ¹æº¾æ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?
éˆ¹? (train_single_agent.py: step é‚è§„ç¡¶)                 éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 4.1 é»æ„¬å½‡ç»¯è¤ç²ºé¸å›¨çˆ£                               éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ éªå†²æ½å¯¤æƒ°ç¹œ: avg_delay (ç»‰?                   éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ é¬æ˜å…˜é‘°? total_energy (é’ï¹â‚¬?                éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ æµ è¯²å§Ÿç€¹å±¾åšéœ? completion_rate                éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç¼‚æ’³ç“¨é›æˆ’è…‘éœ? cache_hit_rate                 éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ éç‰ˆåµæ¶“ãˆ ã‘éœ? data_loss_ratio                éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ æ©ä½ºĞ©é´æ„¬å§›éœ? migration_success_rate         éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 4.2 ç’‹å†ªæ•¤ç¼ç†¶ç«´æ¿‚æ §å§³ç’ï¼„ç•»é£?                        éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ unified_reward_calculator.calculate_reward()éˆ¹?
éˆ¹? éˆ¹?    éˆ¹?                                           éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ å¯¤æƒ°ç¹œé¯â•ƒç¶’: -ä¼ª è„³ log(avg_delay + è”š)       éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æ–ºæ”¢ ä¼ª=15.0, å¯®é¸¿çšŸæµ£åº¡æ¬¢æ©?                 éˆ¹?
éˆ¹? éˆ¹?    éˆ¹?                                           éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ é‘³å€Ÿâ‚¬æ¥å„µç¼ƒ? -å°¾ è„³ log(total_energy + è”š)    éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æ–ºæ”¢ å°¾=0.01, éªå® ã€€é‘³èŠ¥æ™¥                    éˆ¹?
éˆ¹? éˆ¹?    éˆ¹?                                           éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ ç€¹å±¾åšéœå›§îš›é”? +çº¬ è„³ completion_rate        éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æ–ºæ”¢ çº¬=200.0, æ¦§æ’³å§³æµ è¯²å§Ÿç€¹å±¾åš               éˆ¹?
éˆ¹? éˆ¹?    éˆ¹?                                           éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ ç¼‚æ’³ç“¨é›æˆ’è…‘æ¿‚æ §å§³: +æœª è„³ cache_hit_rate       éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æ–ºæ”¢ æœª=10.0, æ¦§æ’³å§³æ¥‚æ¨ºæ‡¡æ¶“î… å·¼                éˆ¹?
éˆ¹? éˆ¹?    éˆ¹?                                           éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ éç‰ˆåµæ¶“ãˆ ã‘é¯â•ƒç¶’: -è”š è„³ data_loss_ratio      éˆ¹?
éˆ¹? éˆ¹?    éˆ¹? éˆ¹æ–ºæ”¢ è”š=50.0, é–¬å®å¤æ¶“ãˆ å¯˜                    éˆ¹?
éˆ¹? éˆ¹?    éˆ¹?                                           éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æ–ºæ”¢ æ©ä½ºĞ©é´æ„¬å§›æ¿‚æ §å§³: +å‘³ è„³ migration_success    éˆ¹?
éˆ¹? éˆ¹?       éˆ¹æ–ºæ”¢ å‘³=5.0, æ¦§æ’³å§³éˆå¤‹æ™¥æ©ä½ºĞ©                 éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹?    éˆâ‚¬ç¼å îš›é”?= å±(éšå‹¯ã€æ¿‚æ §å§³/é¯â•ƒç¶’)                éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 4.3 é‹å‹«ç¼“æ¶“å¬©ç«´é˜èˆµâ‚¬ä½¸æ‚œé–²?(106ç¼?                   éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ æ¶“åº¡åµæ¿®å¬¬å§¸é¬ä½ºæµ‰éšå²€æ®‘ç¼æ’´ç€¯                       éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æ–ºæ”¢ 4.4 é’ã‚†æŸ‡Episodeé„îˆšæƒç¼æ’´æ½«                        éˆ¹?
éˆ¹?    éˆ¹æº¾æ”¢ æˆæƒ§åŸŒéˆâ‚¬æ¾¶Ñ„î„é?(200-300å§?                   éˆ¹?
éˆ¹?    éˆ¹æº¾æ”¢ ç»¯è¤ç²ºå®•â•‚ç° (éµâ‚¬éˆå¤å¦­éç¡…ç¹ƒæ?                     éˆ¹?
éˆ¹?    éˆ¹æ–ºæ”¢ ç€¹å±¾åšéœå›ªç¹ƒæµ£?(<20%)                          éˆ¹?
éˆ¹æ–ºæ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?

éˆ¹å±¸æ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?
éˆ¹? å§ãƒ©î€ƒ5: TD3ç€›ï¸¿ç¯„é‡å­˜æŸŠ (td3.py: update)               éˆ¹?
éˆ¹æº¾æ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 5.1 ç€›æ¨ºåç¼å¿›ç™é’æ¿æ´–é€å‰§ç´¦éæ’å°¯                       éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ buffer.add(state, action, reward, next_state, done)éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 5.2 é–²å›¨ç‰±éµè§„î‚¼éç‰ˆåµ (batch_size=256)              éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ æµ£è·¨æ•¤PERæµ¼æ¨ºå›ç»¾Ñ‡å™°é?                         éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 5.3 ç’ï¼„ç•»Criticé¹ç†·ã‘                             éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ é¢ç†¸åšé©î†½çˆ£é”ã„¤ç¶” (Target Actor)                éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ target_action = target_actor(next_state) éˆ¹?
éˆ¹? éˆ¹? éˆ¹?    + clipped_noise  # é©î†½çˆ£ç»›æ «æšéªè™«ç²¦        éˆ¹?
éˆ¹? éˆ¹? éˆ¹?                                             éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç’ï¼„ç•»é©î†½çˆ£QéŠ?(Twin Target Critics)          éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ q1_target = target_critic1(next_state, target_action)éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ q2_target = target_critic2(next_state, target_action)éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ target_q = min(q1, q2)  # é‘å¿“çš¯æ©å›¦åŠç’?   éˆ¹?
éˆ¹? éˆ¹? éˆ¹?                                             éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç’ï¼„ç•»TDé©î†½çˆ£                                 éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ y = reward + çº¬ è„³ (1-done) è„³ target_q   éˆ¹?
éˆ¹? éˆ¹? éˆ¹?                                             éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç’ï¼„ç•»è¤°æ’³å¢ QéŠ?                               éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ current_q1 = critic1(state, action)     éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ current_q2 = critic2(state, action)     éˆ¹?
éˆ¹? éˆ¹? éˆ¹?                                             éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ Criticé¹ç†·ã‘                                 éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ loss = MSE(current_q1, y) + MSE(current_q2, y)éˆ¹?
éˆ¹? éˆ¹? éˆ¹?                                             éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ é™å¶…æ‚œæµ¼çŠ³æŒ±é‡å­˜æŸŠCritic                         éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ critic_optimizer.zero_grad()             éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ loss.backward()                          éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ å§Šîˆšå®³ç‘ä½¸å£€ (norm=0.7)                     éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æ–ºæ”¢ critic_optimizer.step()                  éˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æº¾æ”¢ 5.4 å¯¤æƒ°ç¹œActoré‡å­˜æŸŠ (å§£å¼olicy_delay=2å§?        éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ ç’ï¼„ç•»Actoré¹ç†·ã‘                              éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ new_action = actor(state)                éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ actor_loss = -critic1(state, new_action).mean()éˆ¹?
éˆ¹? éˆ¹? éˆ¹?                                             éˆ¹?
éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ é™å¶…æ‚œæµ¼çŠ³æŒ±é‡å­˜æŸŠActor                          éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ actor_optimizer.zero_grad()              éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ actor_loss.backward()                    éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æº¾æ”¢ å§Šîˆšå®³ç‘ä½¸å£€                                éˆ¹?
éˆ¹? éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ actor_optimizer.step()                   éˆ¹?
éˆ¹? éˆ¹? éˆ¹?                                             éˆ¹?
éˆ¹? éˆ¹? éˆ¹æ–ºæ”¢ æîˆ›æ´¿é‚æ‰®æ´°éå›©ç¶‰ç¼?                            éˆ¹?
éˆ¹? éˆ¹?    éˆ¹æº¾æ”¢ target_actor = èŸ¿è„³actor + (1-èŸ¿)è„³target_actoréˆ¹?
éˆ¹? éˆ¹?    éˆ¹æ–ºæ”¢ target_critics = èŸ¿è„³critics + (1-èŸ¿)è„³target_criticséˆ¹?
éˆ¹? éˆ¹?                                                  éˆ¹?
éˆ¹? éˆ¹æ–ºæ”¢ 5.5 é‡å­˜æŸŠPERæµ¼æ¨ºå›ç»¾?                            éˆ¹?
éˆ¹?    éˆ¹æ–ºæ”¢ éè§„åµTDç’‡îˆšæ¨Šé‡å­˜æŸŠéé”‹æ¹°æµ¼æ¨ºå›ç»¾?                  éˆ¹?
éˆ¹æ–ºæ”¢éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹â‚¬éˆ¹?

é¦ƒæ¶ é—ƒèˆµî†Œ3: Episodeç¼æ’´æ½«æ¶“åº£ç²ºç’?
Episodeç¼æ’´æ½«éš?
éˆ¹æº¾æ”¢ ç’æ¿ç¶Episodeç¼ç†»î…¸
éˆ¹? éˆ¹æº¾æ”¢ é¬è¯²îš›é”?
éˆ¹? éˆ¹æº¾æ”¢ éªå†²æ½å¯¤æƒ°ç¹œ
éˆ¹? éˆ¹æº¾æ”¢ é¬æ˜å…˜é‘°?
éˆ¹? éˆ¹æº¾æ”¢ ç€¹å±¾åšéœ?
éˆ¹? éˆ¹æº¾æ”¢ ç¼‚æ’³ç“¨é›æˆ’è…‘éœ?
éˆ¹? éˆ¹æ–ºæ”¢ æ©ä½ºĞ©ç¼ç†»î…¸
éˆ¹?
éˆ¹æº¾æ”¢ ç›æ¿å™ºéºãˆ¢å‚¨é£î„ï¼
éˆ¹? éˆ¹æ–ºæ”¢ exploration_noise *= noise_decay (0.9997)
éˆ¹?
éˆ¹æ–ºæ”¢ éµæ’³åµƒæ©æ¶˜å®³æ·‡â„ƒä¼…
   éˆ¹æ–ºæ”¢ å§£?0æ¶“ç‹¤pisodeéµæ’³åµƒæ¶“â‚¬å¨†Â¤î‡›ç¼å—™ç²ºç’?

é¦ƒæ¶ é—ƒèˆµî†Œ4: é›ã„¦æ¹¡é¬Ñ†ç˜æµ¼?(å§£å»µval_interval=50æ¶“çŒ pisode)
ç’‡å‹ªåŠå¨´ä½ºâ–¼:
éˆ¹æº¾æ”¢ éæŠ½æ£´éºãˆ¢å‚¨é£î„ï¼
éˆ¹æº¾æ”¢ æ©æ„¯î”‘10æ¶“î…ç¥´ç’‡æ—¹pisode
éˆ¹æº¾æ”¢ ç’ï¼„ç•»éªå†²æ½é¬Ñ†å…˜é¸å›¨çˆ£
éˆ¹? éˆ¹æº¾æ”¢ éªå†²æ½æ¿‚æ §å§³
éˆ¹? éˆ¹æº¾æ”¢ éªå†²æ½å¯¤æƒ°ç¹œ
éˆ¹? éˆ¹æº¾æ”¢ éªå†²æ½é‘³å€Ÿâ‚¬?
éˆ¹? éˆ¹æ–ºæ”¢ éªå†²æ½ç€¹å±¾åšéœ?
éˆ¹æ–ºæ”¢ æ·‡æ¿†ç“¨é¬Ñ†å…˜é‡èŒ¬åš

é¦ƒæ¶ é—ƒèˆµî†Œ5: ç’î… ç²Œç¼æ’´æ½«æ¶“åºç¹šç€›?(800æ¶“çŒ pisodeç€¹å±¾åšéš?
æ·‡æ¿†ç“¨ç¼æ’´ç‰:
éˆ¹æº¾æ”¢ 1) å¦¯â€³ç€·é‰å†®å™¸
éˆ¹? éˆ¹æ–ºæ”¢ results/models/single_agent/td3/
éˆ¹?    éˆ¹æº¾æ”¢ actor_final.pth
éˆ¹?    éˆ¹æº¾æ”¢ critic1_final.pth
éˆ¹?    éˆ¹æº¾æ”¢ critic2_final.pth
éˆ¹?    éˆ¹æ–ºæ”¢ target_networks_final.pth
éˆ¹?
éˆ¹æº¾æ”¢ 2) ç’î… ç²Œéç‰ˆåµ
éˆ¹? éˆ¹æ–ºæ”¢ results/single_agent/td3/training_results_YYYYMMDD_HHMMSS.json
éˆ¹?    éˆ¹æº¾æ”¢ rewards: [...]
éˆ¹?    éˆ¹æº¾æ”¢ delays: [...]
éˆ¹?    éˆ¹æº¾æ”¢ energies: [...]
éˆ¹?    éˆ¹æº¾æ”¢ completion_rates: [...]
éˆ¹?    éˆ¹æ–ºæ”¢ cache_metrics: {...}
éˆ¹?
éˆ¹æ–ºæ”¢ 3) é™îˆî‹é–æ §æµ˜ç›?
   éˆ¹æ–ºæ”¢ results/single_agent/td3/training_chart_YYYYMMDD_HHMMSS.png
      éˆ¹æº¾æ”¢ æ¿‚æ §å§³é‡èŒ¬åš
      éˆ¹æº¾æ”¢ å¯¤æƒ°ç¹œé‡èŒ¬åš
      éˆ¹æº¾æ”¢ é‘³å€Ÿâ‚¬æ¥æ´¸ç»¾?
      éˆ¹æ–ºæ”¢ ç€¹å±¾åšéœå›¨æ´¸ç»¾?
      
é¦ƒæ”½ éç¨¿ç¸¾é¶â‚¬éˆîˆ™å¯’é?
1. Twin Delayed DDPG (TD3)
    é™å­‹riticç¼ƒæˆ ç²¶é‘å¿“çš¯QéŠè‰°ç¹ƒæµ¼æ‹Œî…¸
    å¯¤æƒ°ç¹œç»›æ «æšé‡å­˜æŸŠé»æ„°ç®ç»‹å†²ç•¾é¬?
    é©î†½çˆ£ç»›æ «æšéªè™«ç²¦é–æ §å™ºçæˆæŸŸå®¸?
2. é‘·îˆâ‚¬å‚šç°²éºÑƒåŸ—éˆå“„åŸ—
    é…é¸¿å…˜ç¼‚æ’³ç“¨éºÑƒåŸ—é”›æ°±å„¹æ´ï¹æ‹·éŸª?+ é’å——çœ°ç¼‚æ’³ç“¨
    é…é¸¿å…˜æ©ä½ºĞ©éºÑƒåŸ—é”›æ°¬î˜¿ç¼ç£‹Ğ•é™?+ é´æ„­æ¹°éå ¢æ³­
3. ç¼ç†¶ç«´æ¿‚æ §å§³é‘èŠ¥æšŸ
    æ¾¶æ°±æ´°éå›¦ç´­é–æ µç´°å¯¤æƒ°ç¹œéŠ†ä½½å…˜é‘°æ¤¼â‚¬ä½¸ç•¬é´æ„®å·¼
    ç€µè§„æšŸé¯â•ƒç¶’é”›æ°¶ä¼©éå¶†ç€¬ç»”îˆšâ‚¬ç…å¥–é?
    éªå® ã€€é‰å†®å™¸é”›æ°±â€˜æ·‡æ¿†æ‚‡æ¤¤è§„å¯šéå›§å´—ç’‹?
4. é”ã„¦â‚¬ä½ºç¶‰ç¼æ»„å«‡éµ?
    æï¹ç· ç»‰è¯²å§©å¦¯â€³ç€·é”›æ°±æ¹¡ç€¹ç‚ºäº¾ç’ºîˆšæº€é…?
    é¥å“„ç•¾RSU/UAVé”›æ°¶ç™ç’‡ä½ºç•»å¨‰æ›Ÿæ¹éå Ÿâ‚¬?
    é‘·îˆâ‚¬å‚šç°²ç’ï¼„ç•»ç’§å‹¬ç°®é’å—›å¤

"""
