
# 单智能体算法架构

<cite>
**本文档引用文件**   
- [td3.py](file://single_agent/td3.py)
- [ppo.py](file://single_agent/ppo.py)
- [dqn.py](file://single_agent/dqn.py)
- [sac.py](file://single_agent/sac.py)
- [ddpg.py](file://single_agent/ddpg.py)
- [reward_calculator.py](file://utils/reward_calculator.py)
- [td3_optimized.py](file://single_agent/td3_optimized.py)
- [td3_optimized_agent.py](file://single_agent/td3_optimized_agent.py)
</cite>

## 目录
1. [引言](#引言)
2. [项目结构](#项目结构)
3. [核心组件](#核心组件)
4. [架构概述](#架构概述)
5. [详细组件分析](#详细组件分析)
6. [依赖分析](#依赖分析)
7. [性能考量](#性能考量)
8. [故障排除指南](#故障排除指南)
9. [结论](#结论)
10. [附录](#附录)

## 引言
本文档旨在为VEC_mig_caching系统中的单智能体算法构建全面的架构文档，涵盖TD3、PPO、DQN、SAC和DDPG在简化车联网场景中的部署模式及其与多智能体系统的性能对比。文档详细说明了双Q网络、目标策略延迟更新和策略平滑正则化在单节点决策中的实现细节，分析了PPO的裁剪机制如何在车辆移动性预测任务中稳定训练过程，并阐述了DQN在离散动作空间下的经验回放优化策略以及SAC在连续控制任务中基于最大熵框架的探索优势。结合系统实际应用场景，文档对比了单智能体与多智能体在任务卸载延迟、系统吞吐量和能量消耗方面的差异，提供了单智能体训练流程图、状态-动作空间定义示例、奖励函数设计原则，并说明了其作为基线模型在实验评估中的作用。

## 项目结构
VEC_mig_caching系统采用模块化设计，将单智能体算法、多智能体算法、缓存管理、通信模型、配置管理、核心队列、决策管理、评估模块、迁移管理、模型定义、结果存储、工具集和工具函数等组件分离，确保了代码的可维护性和可扩展性。单智能体算法位于`single_agent`目录下，包括TD3、PPO、DQN、SAC和DDPG等算法的实现。多智能体算法位于`algorithms`目录下，包括MADDPG、MAPPO、MATD3、QMIX和SAC_MA等算法的实现。缓存管理、通信模型、配置管理、核心队列、决策管理、评估模块、迁移管理、模型定义、结果存储、工具集和工具函数等组件分别位于各自的目录下，确保了代码的模块化和可维护性。

```mermaid
graph TD
subgraph "算法模块"
TD3[TD3]
PPO[PPO]
DQN[DQN]
SAC[SAC]
DDPG[DDPG]
end
subgraph "多智能体算法模块"
MADDPG[MADDPG]
MAPPO[MAPPO]
MATD3[MATD3]
QMIX[QMIX]
SAC_MA[SAC_MA]
end
subgraph "系统模块"
Caching[缓存管理]
Communication[通信模型]
Config[配置管理]
Core[核心队列]
Decision[决策管理]
Evaluation[评估模块]
Migration[迁移管理]
Models[模型定义]
Results[结果存储]
Tools[工具集]
Utils[工具函数]
end
TD3 --> Caching
PPO --> Caching
DQN --> Caching
SAC --> Caching
DDPG --> Caching
MADDPG --> Caching
MAPPO --> Caching
MATD3 --> Caching
QMIX --> Caching
SAC_MA --> Caching
Caching --> Communication
Communication --> Config
Config --> Core
Core --> Decision
Decision --> Evaluation
Evaluation --> Migration
Migration --> Models
Models --> Results
Results --> Tools
Tools --> Utils
```

**Diagram sources**
- [single_agent/td3.py](file://single_agent/td3.py)
- [single_agent/ppo.py](file://single_agent/ppo.py)
- [single_agent/dqn.py](file://single_agent/dqn.py)
- [single_agent/sac.py](file://single_agent/sac.py)
- [single_agent/ddpg.py](file://single_agent/ddpg.py)
- [algorithms/maddpg.py](file://algorithms/maddpg.py)
- [algorithms/mappo.py](file://algorithms/mappo.py)
- [algorithms/matd3.py](file://algorithms/matd3.py)
- [algorithms/qmix.py](file://algorithms/qmix.py)
- [algorithms/sac_ma.py](file://algorithms/sac_ma.py)
- [caching/cache_manager.py](file://caching/cache_manager.py)
- [communication/models.py](file://communication/models.py)
- [config/algorithm_config.py](file://config/algorithm_config.py)
- [core/queue_manager.py](file://core/queue_manager.py)
- [decision/offloading_manager.py](file://decision/offloading_manager.py)
- [evaluation/performance_evaluator.py](file://evaluation/performance_evaluator.py)
- [migration/migration_manager.py](file://migration/migration_manager.py)
- [models/base_node.py](file://models/base_node.py)
- [results/single_agent/ddpg/training_results_20250918_233913.json](file://results/single_agent/ddpg/training_results_20250918_233913.json)
- [tools/performance_optimization.py](file://tools/performance_optimization.py)
- [utils/reward_calculator.py](file://utils/reward_calculator.py)

**Section sources**
- [single_agent/td3.py](file://single_agent/td3.py)
- [single_agent/ppo.py](file://single_agent/ppo.py)
- [single_agent/dqn.py](file://single_agent/dqn.py)
- [single_agent/sac.py](file://single_agent/sac.py)
- [single_agent/ddpg.py](file://single_agent/ddpg.py)
- [algorithms/maddpg.py](file://algorithms/maddpg.py)
- [algorithms/mappo.py](file://algorithms/mappo.py)
- [algorithms/matd3.py](file://algorithms/matd3.py)
- [algorithms/qmix.py](file://algorithms/qmix.py)
- [algorithms/sac_ma.py](file://algorithms/sac_ma.py)
- [caching/cache_manager.py](file://caching/cache_manager.py)
- [communication/models.py](file://communication/models.py)
- [config/algorithm_config.py](file://config/algorithm_config.py)
- [core/queue_manager.py](file://core/queue_manager.py)
- [decision/offloading_manager.py](file://decision/offloading_manager.py)
- [evaluation/performance_evaluator.py](file://evaluation/performance_evaluator.py)
- [migration/migration_manager.py](file://migration/migration_manager.py)
- [models/base_node.py](file://models/base_node.py)
- [results/single_agent/ddpg/training_results_20250918_233913.json](file://results/single_agent/ddpg/training_results_20250918_233913.json)
- [tools/performance_optimization.py](file://tools/performance_optimization.py)
- [utils/reward_calculator.py](file://utils/reward_calculator.py)

## 核心组件
单智能体算法的核心组件包括TD3、PPO、DQN、SAC和DDPG，这些算法在VEC_mig_caching系统中用于处理连续和离散动作空间下的任务卸载和资源管理问题。TD3通过双Q网络减少过估计，延迟策略更新提高稳定性，目标策略平滑化减少方差，改进的探索策略提高样本效率。PPO通过裁剪代理目标防止过大策略更新，GAE减少方差，自适应KL散度约束稳定训练过程。DQN通过深度Q网络处理离散动作空间，经验回放机制提高样本效率，目标网络稳定训练过程，ε-贪婪探索策略提高探索效率。SAC通过最大熵强化学习框架，自动温度参数调节，双Q网络减少过估计，高样本效率。DDPG通过Actor-Critic架构处理连续动作空间，经验回放机制提高样本效率，目标网络稳定训练过程，噪声探索策略提高探索效率。

**Section sources**
- [single_agent/td3.py](file://single_agent/td3.py)
- [single_agent/ppo.py](file://single_agent/ppo.py)
- [single_agent/dqn.py](file://single_agent/dqn.py)
- [single_agent/sac.py](file://single_agent/sac.py)
- [single_agent/ddpg.py](file://single_agent/ddpg.py)

## 架构概述
单智能体算法的架构设计遵循模块化原则，每个算法由环境、智能体、网络和缓冲区四个主要组件构成。环境负责构建全局状态向量，分解动作，计算奖励，执行训练步骤，保存和加载模型。智能体负责选择动作，存储经验，更新网络参数，保存和加载模型。网络负责前向传播，初始化权重，计算损失。缓冲区负责存储经验，采样经验批次，更新优先级。这种设计确保了代码的可维护性和可扩展性，同时也便于算法的调试和优化。

```mermaid
graph TD
subgraph "环境"
GetStateVector[构建全局状态向量]
DecomposeAction[分解动作]
CalculateReward[计算奖励]
TrainStep[执行训练步骤]
SaveModels[保存模型]
LoadModels[加载模型]
end
subgraph "智能体"
SelectAction[选择动作]
StoreExperience[存储经验]
Update[更新网络参数]
SaveModel[保存模型]
LoadModel[加载模型]
end
subgraph "网络"
Forward[前向传播]
InitWeights[初始化权重]
ComputeLoss[计算损失]
end
subgraph "缓冲区"
Push[存储经验]
Sample[采样经验批次]
UpdatePriorities[更新优先级]
end
GetStateVector --> SelectAction
DecomposeAction --> StoreExperience
CalculateReward --> Update
TrainStep --> Update
SaveModels --> SaveModel
LoadModels --> LoadModel
SelectAction --> Forward
StoreExperience --> Push
Update --> ComputeLoss
SaveModel --> SaveModels
LoadModel --> LoadModels
Forward --> ComputeLoss
InitWeights --> Forward
ComputeLoss --> Update
Push --> Sample
Sample --> Update
UpdatePriorities --> Update
```

**Diagram sources**
- [single_agent/td3.py](file://single_agent/td3.py)
- [single_agent/ppo.py](file://single_agent/ppo.py)
- [single_agent/dqn.py](file://single_agent/dqn.py)
- [single_agent/sac.py](file://single_agent/sac.py)
- [single_agent/ddpg.py](file://single_agent/ddpg.py)

## 详细组件分析
### TD3算法分析
TD3算法通过双Q网络减少过估计，延迟策略更新提高稳定性，目标策略平滑化减少方差，改进的探索策略提高样本效率。TD3的Actor网络采用确定性策略，Critic网络采用双Q网络，ReplayBuffer采用优先级经验回放，Agent负责选择动作，存储经验，更新网络参数，保存和加载模型。TD3的Environment负责构建全局状态向量，分解动作，计算奖励，执行训练步骤，保存和加载模型。

#### TD3 Actor网络
```mermaid
classDiagram
class TD3Actor {
+int state_dim
+int action_dim
+float max_action
+network Sequential
+_init_weights() void
+forward(state Tensor) Tensor
}
TD3Actor --> TD3Agent : "uses"
```

**Diagram sources**
- [single_agent/td3.py](file://single_agent/td3.py#L65-L97)

#### TD3 Critic网络
```mermaid
classDiagram
class TD3Critic {
+int state_dim
+int action_dim
+q1_network Sequential
+q2_network Sequential
+_init_weights() void
+forward(state Tensor, action Tensor) Tuple~Tensor, Tensor~
+q1(state Tensor, action Tensor) Tensor
}
TD3Critic --> TD3Agent : "uses"
```

**Diagram sources**
- [single_agent/td3.py](file://single_agent/td3.py#L100-L150)

#### TD3 ReplayBuffer
```mermaid
classDiagram
class TD3ReplayBuffer {
+int capacity
+int ptr
+int size
+float alpha
+states np.ndarray
+actions np.ndarray
+rewards np.ndarray
+next_states np.ndarray
+dones np.ndarray
+priorities np.ndarray
+__len__() int
+push(state np.ndarray, action np.ndarray, reward float, next_state np.ndarray, done bool) void
+sample(batch_size int, beta float) Tuple~Tensor, Tensor, Tensor, Tensor, Tensor, np.ndarray, Tensor~
+update_priorities(indices np.ndarray, priorities np.ndarray) void
}
TD3ReplayBuffer --> TD3Agent : "uses"
```

**Diagram sources**
- [single_agent/td3.py](file://single_agent/td3.py#L153-L213)

#### TD3 Agent
```mermaid
classDiagram
class TD3Agent {
+int state_dim
+int action_dim
+TD3Config config
+int optimized_batch_size
+device torch.device
+actor TD3Actor
+critic TD3Critic
+target_actor TD3Actor
+target_critic TD3Critic
+actor_optimizer Adam
+critic_optimizer Adam
+actor_lr_scheduler ExponentialLR
+critic_lr_scheduler ExponentialLR
+replay_buffer TD3ReplayBuffer
+float beta
+float beta_increment
+float exploration_noise
+int step_count
+int update_count
+actor_losses float[]
+critic_losses float[]
+select_action(state np.ndarray, training bool) np.ndarray
+store_experience(state np.ndarray, action np.ndarray, reward float, next_state np.ndarray, done bool) void
+update() Dict~str, float~
+_update_critic(states Tensor, actions Tensor, rewards Tensor, next_states Tensor, dones Tensor, weights Tensor) Tuple~float, Tensor~
+_update_actor(states Tensor) float
+soft_update(target Module, source Module, tau float) void
+hard_update(target Module, source Module) void
+save_model(filepath str) void
+load_model(filepath str) void
}
TD3Agent --> TD3Actor : "uses"
TD3Agent --> TD3Critic : "uses"
TD3Agent --> TD3ReplayBuffer : "uses"
```

**Diagram sources**
- [single_agent/td3.py](file://single_agent/td3.py#L216-L426)

#### TD3 Environment
```mermaid
classDiagram
class TD3Environment {
+TD3Config config
+int state_dim
+int action_dim
+TD3Agent agent
+int episode_count
+int step_count
+get_state_vector(node_states Dict, system_metrics Dict) np.ndarray
+decompose_action(action np.ndarray) Dict~str, np.ndarray~
+get_actions(state np.ndarray, training bool) Dict~str, np.ndarray~
+calculate_reward(system_metrics Dict) float
+train_step(state np.ndarray, action Union~np.ndarray, int~, reward float, next_state np.ndarray, done bool) Dict
+save_models(filepath str) void
+load_models(filepath str) void
+store_experience(state np.ndarray, action np.ndarray, reward float, next_state np.ndarray, done bool, log_prob float, value float) void
+update(last_value float) Dict
+get_training_stats() Dict
}
TD3Environment --> TD3Agent : "uses"
```

**Diagram sources**
- [single_agent/td3.py](file://single_agent/td3.py#L429-L546)

### PPO算法分析
PPO算法通过裁剪代理目标防止过大策略更新，GAE减少方差，自适应KL散度约束稳定训练过程。PPO的Actor网络采用随机策略，Critic网络采用价值网络，Buffer采用经验缓冲区，Agent负责选择动作，存储经验，更新网络参数，保存和加载模型。PPO的Environment负责构建全局状态向量，分解动作，计算奖励，执行训练步骤，保存和加载模型。

#### PPO Actor网络
```mermaid
classDiagram
class PPOActor {
+int state_dim
+int action_dim
+feature_layers Sequential
+mean_layer Linear
+log_std Parameter
+_init_weights() void
+forward(state Tensor) Tuple~Tensor, Tensor~
+get_action_and_logprob(state Tensor, action Tensor) Tuple~Tensor, Tensor, Tensor~
}
PPOActor --> PPOAgent : "uses"
```

**Diagram sources**
- [single_agent/ppo.py](file://single_agent/ppo.py#L58-L113)

#### PPO Critic网络
```mermaid
classDiagram
class PPOCritic {
+int state_dim
+value_network Sequential
+_init_weights() void
+forward(state Tensor) Tensor
}
PPOCritic --> PPOAgent : "uses"
```

**Diagram sources**
- [single_agent/ppo.py](file://single_agent/ppo.py#L116-L144)

#### PPO Buffer
```mermaid
classDiagram
class PPOBuffer {
+int buffer_size
+int ptr
+int size
+states np.ndarray
+actions np.ndarray
+log_probs np.ndarray
+rewards np.ndarray
+dones np.ndarray
+values np.ndarray
+advantages np.ndarray
+returns np.ndarray
+store(state np.ndarray, action np.ndarray, log_prob float, reward float, done bool, value float) void
+compute_advantages_and_returns(last_value float, gamma float, gae_lambda float, use_gae bool) void
+get_batch(batch_size int) Tuple~Tensor, Tensor, Tensor, Tensor, Tensor, Tensor~
+clear() void
}
PPOBuffer --> PPOAgent : "uses"
```

**Diagram sources**
- [single_agent/ppo.py](file://single_agent/ppo.py#L147-L232)

#### PPO Agent
```mermaid
classDiagram
class PPOAgent {
+int state_dim
+int action_dim
+PPOConfig config
+int optimized_batch_size
+device torch.device
+actor PPOActor
+critic PPOCritic
+actor_optimizer Adam
+critic_optimizer Adam
+buffer PPOBuffer
+actor_losses float[]
+critic_losses float[]
+entropy_losses float[]
+kl_divergences float[]
+int step_count
+select_action(state np.ndarray, training bool) Tuple~np.ndarray, float, float~
+store_experience(state np.ndarray, action np.ndarray, log_prob float, reward float, done bool, value float) void
+update(last_value float) Dict~str, float~
+save_model(filepath str) void
+load_model(filepath str) void
}
PPOAgent --> PPOActor : "uses"
PPOAgent --> PPOCritic : "uses"
PPOAgent --> PPOBuffer : "uses"
```

**Diagram sources**
- [single_agent/ppo.py](file://single_agent/ppo.py#L235-L407)

#### PPO Environment
```mermaid
classDiagram
class PPOEnvironment {
+PPOConfig config
+int state_dim
+int action_dim
+PPOAgent agent
+int episode_count
+int step_count
+get_state_vector(node_states Dict, system_metrics Dict) np.ndarray
+decompose_action(action np.ndarray) Dict~str, np.ndarray~
+get_actions(state np.ndarray, training bool) Tuple~Dict~str, np.ndarray~, float, float~
+calculate_reward(system_metrics Dict) float
+save_models(filepath str) void
+load_models(filepath str) void
+train_step(state np.ndarray, action Union~np.ndarray, int~, reward float, next_state np.ndarray, done bool) Dict
+store_experience(state np.ndarray, action Union~np.ndarray, int~, reward float, next_state np.ndarray, done bool, log_prob float, value float) void
+update(last_value float) Dict
+get_training_stats() Dict
}
PPOEnvironment --> PPOAgent : "uses"
```

**Diagram sources**
- [single_agent/ppo.py](file://single_agent/ppo.py#L410-L518)

### DQN算法分析
DQN算法通过深度Q网络处理离散动作空间，经验回放机制提高样本效率，目标网络稳定训练过程，ε-贪婪探索策略提高探索效率。DQN的Network采用Dueling DQN架构，ReplayBuffer采用经验回放缓冲区，Agent负责选择动作，存储经验，更新网络参数，保存和加载模型。DQN的Environment负责构建全局状态向量，分解动作，计算奖励，执行训练步骤，保存和加载模型。

#### DQN Network
```mermaid
classDiagram
    class DQNNetwork {
        +int state_dim
        +int action_dim
        +bool dueling
        +feature_layers Sequential
        +value_stream Sequential
        +advantage_stream Sequential
        +q_network