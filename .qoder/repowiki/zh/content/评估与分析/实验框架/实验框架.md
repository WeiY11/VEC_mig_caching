# 实验框架

<cite>
**本文档引用的文件**
- [evaluation.py](file://experiments/evaluation.py)
- [run_full_experiment.py](file://run_full_experiment.py)
- [test_complete_system.py](file://evaluation/test_complete_system.py)
- [experiment_summary.md](file://results/experiment_summary.md)
</cite>

## 目录
1. [引言](#引言)
2. [实验框架设计](#实验框架设计)
3. [多轮对比实验组织](#多轮对比实验组织)
4. [控制变量设置](#控制变量设置)
5. [结果聚合机制](#结果聚合机制)
6. [完整实验协调流程](#完整实验协调流程)
7. [端到端系统验证](#端到端系统验证)
8. [实际实验设计示例](#实际实验设计示例)
9. [扩展实验指导](#扩展实验指导)
10. [实验一致性保障](#实验一致性保障)

## 引言

本实验框架旨在对车联网边缘计算系统中的多智能体深度强化学习算法（如MATD3-MIG）进行全面评估。通过系统化的实验设计，框架支持多轮对比实验，能够精确控制变量，并对实验结果进行有效聚合。核心组件包括`experiments/evaluation.py`中的实验评估模块、`run_full_experiment.py`中的完整实验协调器以及`evaluation/test_complete_system.py`中的端到端系统验证器。`experiment_summary.md`文件记录了实际的实验配置和结果，为性能对比提供了数据支持。

## 实验框架设计

实验框架采用模块化设计，核心组件包括实验运行器、性能指标计算器和基线算法实现。`ExperimentRunner`类负责协调整个评估流程，调用`PerformanceMetrics`进行指标计算，并与系统仿真器交互。`PerformanceMetrics`类提供了计算系统综合性能指标的方法，包括任务时延、能耗、数据丢失率等。基线算法通过`BaselineAlgorithms`类实现，包括随机分配、贪心分配、轮询分配和负载感知分配等算法。

```mermaid
classDiagram
class ExperimentRunner {
+simulator : SystemSimulator
+metrics_calculator : PerformanceMetrics
+run_full_evaluation(num_steps, save_results)
}
class PerformanceMetrics {
+metrics_history : List
+baseline_results : Dict
+calculate_system_metrics(vehicles, rsus, uavs, cache_managers, migration_manager, simulation_time)
+run_baseline_comparison(system_simulator, num_steps)
+generate_comparison_plots(results, save_path)
+save_results_to_file(results, filename)
+print_performance_summary(results)
}
class BaselineAlgorithms {
+random_allocation(tasks, nodes)
+greedy_allocation(tasks, node_states)
+round_robin_allocation(tasks, nodes)
+load_aware_allocation(tasks, node_states)
}
class ExperimentResult {
+algorithm_name : str
+avg_delay : float
+total_energy : float
+data_loss_rate : float
+cache_hit_rate : float
+migration_success_rate : float
+task_completion_rate : float
+cpu_utilization : float
+bandwidth_utilization : float
+queue_utilization : float
+transmission_delay : float
+processing_delay : float
+waiting_delay : float
+computation_energy : float
+communication_energy : float
+migration_energy : float
}
ExperimentRunner --> PerformanceMetrics : "使用"
ExperimentRunner --> BaselineAlgorithms : "使用"
PerformanceMetrics --> ExperimentResult : "创建"
```

**图表来源**
- [experiments/evaluation.py](file://experiments/evaluation.py#L16-L531)

**本节来源**
- [experiments/evaluation.py](file://experiments/evaluation.py#L16-L531)

## 多轮对比实验组织

实验框架通过`run_full_experiment.py`中的`FullExperimentRunner`类组织多轮对比实验。该类定义了多个实验配置，包括标准配置、高负载场景和大规模场景。每个实验配置包含不同的参数，如车辆数量、RSU数量、UAV数量和任务到达率。`run_all_experiments`方法遍历所有实验配置，依次运行每个配置下的实验。

```mermaid
sequenceDiagram
participant Main as "主程序"
participant Runner as "FullExperimentRunner"
participant ExpConfig as "ExperimentConfig"
participant SingleExp as "单个实验"
Main->>Runner : run_all_experiments()
loop 遍历每个实验配置
Runner->>ExpConfig : 获取配置
Runner->>SingleExp : run_single_experiment(exp_config)
SingleExp->>SingleExp : 运行MATD3-MIG实验
SingleExp->>SingleExp : 运行基线算法实验
SingleExp-->>Runner : 返回实验结果
Runner->>Runner : 计算改进效果
Runner->>Runner : 保存结果
end
Runner-->>Main : 返回所有结果
```

**图表来源**
- [run_full_experiment.py](file://run_full_experiment.py#L151-L583)

**本节来源**
- [run_full_experiment.py](file://run_full_experiment.py#L151-L583)

## 控制变量设置

实验框架通过`ExperimentConfig`类精确控制实验变量。每个实验配置都明确定义了车辆数量、RSU数量、UAV数量、任务到达率等参数。在运行实验时，这些参数被传递给`create_test_environment`和`generate_tasks`方法，确保每次实验都在相同的条件下进行。此外，`FullExperimentRunner`类还控制了实验的轮数和每轮的长度，确保实验的可重复性。

```mermaid
flowchart TD
Start([开始实验]) --> CreateEnv["创建测试环境"]
CreateEnv --> SetParams["设置实验参数"]
SetParams --> GenerateTasks["生成测试任务"]
GenerateTasks --> RunExperiment["运行实验"]
RunExperiment --> CalculateMetrics["计算性能指标"]
CalculateMetrics --> End([结束实验])
subgraph "控制变量"
SetParams
GenerateTasks
end
```

**图表来源**
- [run_full_experiment.py](file://run_full_experiment.py#L29-L54)

**本节来源**
- [run_full_experiment.py](file://run_full_experiment.py#L29-L54)

## 结果聚合机制

实验框架通过`PerformanceMetrics`类的`print_performance_summary`和`save_results_to_file`方法对实验结果进行聚合。`print_performance_summary`方法将所有算法的性能指标汇总成一个表格，并计算MATD3-MIG相对于最佳基线的改进。`save_results_to_file`方法将结果保存为CSV和JSON格式，便于后续分析。`FullExperimentRunner`类的`generate_summary_report`方法生成详细的Markdown报告，包含性能对比表和改进效果。

```mermaid
flowchart TD
Start([开始结果聚合]) --> CollectResults["收集所有实验结果"]
CollectResults --> PrintSummary["打印性能摘要"]
PrintSummary --> GeneratePlots["生成对比图表"]
GeneratePlots --> SaveResults["保存结果到文件"]
SaveResults --> GenerateReport["生成汇总报告"]
GenerateReport --> End([结果聚合完成])
```

**图表来源**
- [experiments/evaluation.py](file://experiments/evaluation.py#L269-L531)
- [run_full_experiment.py](file://run_full_experiment.py#L484-L583)

**本节来源**
- [experiments/evaluation.py](file://experiments/evaluation.py#L269-L531)
- [run_full_experiment.py](file://run_full_experiment.py#L484-L583)

## 完整实验协调流程

`run_full_experiment.py`中的`main`函数协调了整个实验流程。它首先创建`FullExperimentRunner`实例，然后调用`run_all_experiments`方法。该方法依次运行每个实验配置，包括创建测试环境、生成任务、运行MATD3-MIG算法和基线算法、计算性能指标、生成图表和保存结果。整个流程确保了实验的完整性和一致性。

```mermaid
sequenceDiagram
participant Main as "主函数"
participant Runner as "FullExperimentRunner"
participant Sim as "系统仿真器"
participant DM as "决策制定器"
participant MM as "迁移管理器"
participant CM as "缓存管理器"
Main->>Runner : 创建实例
Main->>Runner : run_all_experiments()
Runner->>Runner : 遍历实验配置
loop 每个实验配置
Runner->>Runner : create_test_environment()
Runner->>Runner : generate_tasks()
Runner->>DM : make_offloading_decision()
DM->>MM : 管理任务迁移
DM->>CM : request_content()
Runner->>Sim : run_complete_simulation()
Runner->>Runner : calculate_system_metrics()
Runner->>Runner : generate_comparison_plots()
Runner->>Runner : save_results()
end
Runner-->>Main : 返回结果
```

**图表来源**
- [run_full_experiment.py](file://run_full_experiment.py#L546-L583)

**本节来源**
- [run_full_experiment.py](file://run_full_experiment.py#L546-L583)

## 端到端系统验证

`evaluation/test_complete_system.py`中的`CompleteSystemSimulator`类提供了端到端系统功能的验证逻辑。该类模拟了车辆、RSU和UAV的移动性，生成计算任务，并处理任务的卸载和执行。`run_simulation`方法运行完整的仿真，`simulate_time_slot`方法模拟每个时隙的事件，`process_task`方法处理单个任务。通过`test_simulator`函数，可以快速验证仿真器的正确性。

```mermaid
sequenceDiagram
participant Test as "test_simulator"
participant Sim as "CompleteSystemSimulator"
participant Mobility as "移动性模型"
participant TaskGen as "任务生成器"
participant TaskProc as "任务处理器"
Test->>Sim : 创建仿真器
Test->>Sim : run_simulation(num_time_slots=100)
Sim->>Mobility : update_mobility()
Sim->>TaskGen : generate_task()
loop 每个任务
TaskGen->>TaskProc : process_task()
TaskProc->>TaskProc : find_nearest_rsu()
TaskProc->>TaskProc : find_nearest_uav()
TaskProc->>TaskProc : check_cache_hit()
TaskProc->>TaskProc : calculate_transmission_delay()
TaskProc->>TaskProc : calculate_computation_delay()
TaskProc->>TaskProc : calculate_energy_consumption()
end
Sim-->>Test : 返回结果
```

**图表来源**
- [evaluation/test_complete_system.py](file://evaluation/test_complete_system.py#L13-L533)

**本节来源**
- [evaluation/test_complete_system.py](file://evaluation/test_complete_system.py#L13-L533)

## 实际实验设计示例

`experiment_summary.md`文件展示了实际的实验设计。实验配置包括标准配置、高负载场景和大规模场景。在每个配置下，MATD3-MIG算法与随机、贪心、轮询和负载感知等基线算法进行了性能对比。结果显示，MATD3-MIG在平均时延、总能耗、任务完成率和缓存命中率等方面均优于基线算法。例如，在标准配置下，MATD3-MIG的平均时延比随机算法降低了41.1%，总能耗降低了6.7%，任务完成率提高了30.7%，缓存命中率提高了400.0%。

```mermaid
flowchart TD
Start([开始实验设计]) --> DefineConfig["定义实验配置"]
DefineConfig --> SelectAlgorithms["选择对比算法"]
SelectAlgorithms --> SetParameters["设置实验参数"]
SetParameters --> RunExperiments["运行实验"]
RunExperiments --> AnalyzeResults["分析结果"]
AnalyzeResults --> GenerateReport["生成报告"]
GenerateReport --> End([实验设计完成])
subgraph "实际实验设计"
DefineConfig
SelectAlgorithms
SetParameters
end
```

**图表来源**
- [experiment_summary.md](file://results/experiment_summary.md#L1-L122)

**本节来源**
- [experiment_summary.md](file://results/experiment_summary.md#L1-L122)

## 扩展实验指导

要扩展实验，可以修改`run_full_experiment.py`中的`experiment_configs`列表，添加新的实验配置。例如，可以增加一个低负载场景或一个不同任务类型的场景。要新增评估维度，可以在`PerformanceMetrics`类中添加新的指标计算方法，并在`ExperimentResult`类中添加相应的字段。要修改实验参数，可以直接修改`ExperimentConfig`类的实例化参数。确保在修改后重新运行实验，并检查结果的合理性。

```mermaid
flowchart TD
Start([开始扩展实验]) --> ModifyConfig["修改实验配置"]
ModifyConfig --> AddMetrics["新增评估维度"]
AddMetrics --> AdjustParams["调整实验参数"]
AdjustParams --> RunNewExperiments["运行新实验"]
RunNewExperiments --> ValidateResults["验证结果"]
ValidateResults --> UpdateReport["更新报告"]
UpdateReport --> End([扩展实验完成])
```

**本节来源**
- [run_full_experiment.py](file://run_full_experiment.py#L151-L583)
- [experiments/evaluation.py](file://experiments/evaluation.py#L16-L531)

## 实验一致性保障

实验框架通过多种机制保障实验的一致性。首先，所有实验都使用相同的`ExperimentConfig`类来定义参数，确保变量控制的一致性。其次，`FullExperimentRunner`类在每次运行实验前都会重置系统状态，避免前一次实验的影响。此外，`PerformanceMetrics`类使用相同的指标计算方法，确保结果的可比性。最后，所有结果都保存为标准化的CSV和JSON格式，并生成详细的Markdown报告，便于复现和验证。

```mermaid
flowchart TD
Start([开始一致性保障]) --> UseConfig["使用统一配置"]
UseConfig --> ResetState["重置系统状态"]
ResetState --> StandardMetrics["使用标准指标"]
StandardMetrics --> SaveResults["保存标准化结果"]
SaveResults --> GenerateReport["生成详细报告"]
GenerateReport --> End([一致性保障完成])
```

**本节来源**
- [run_full_experiment.py](file://run_full_experiment.py#L151-L583)
- [experiments/evaluation.py](file://experiments/evaluation.py#L16-L531)