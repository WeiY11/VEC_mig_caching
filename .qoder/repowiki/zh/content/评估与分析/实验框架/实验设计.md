# 实验设计

<cite>
**本文档引用文件**  
- [evaluation.py](file://experiments/evaluation.py)
- [external_config.py](file://config/external_config.py)
- [system_config.py](file://config/system_config.py)
- [run_full_experiment.py](file://run_full_experiment.py)
- [experiment_summary.md](file://results/experiment_summary.md)
</cite>

## 目录
1. [引言](#引言)
2. [实验框架设计机制](#实验框架设计机制)
3. [实验组定义与控制变量设置](#实验组定义与控制变量设置)
4. [多算法对比实验配置](#多算法对比实验配置)
5. [实验设计模式](#实验设计模式)
6. [新增实验维度实现](#新增实验维度实现)
7. [实验可复现性与数据一致性保障](#实验可复现性与数据一致性保障)
8. [结论](#结论)

## 引言
本项目旨在通过MATD3-MIG系统实现高效的任务卸载与迁移管理，提升车联网环境下的服务质量。实验设计围绕核心目标展开，涵盖基准实验、消融实验和扩展场景实验等多种类型，确保全面评估系统性能。通过`experiments/evaluation.py`中的实验框架，结合`experiment_summary.md`的实际案例，详细展示了如何定义实验组、设置控制变量以及配置多算法对比实验。

## 实验框架设计机制

`experiments/evaluation.py`中的实验框架设计机制主要包括以下几个关键组件：

- **ExperimentResult**: 定义了实验结果的数据结构，包含算法名称、平均时延、总能耗、数据丢失率、缓存命中率、迁移成功率、任务完成率等指标。
- **BaselineAlgorithms**: 提供了多种基线算法的实现，包括随机分配、贪心分配、轮询分配和负载感知分配。
- **PerformanceMetrics**: 计算系统综合性能指标，并支持基线算法对比实验。
- **ExperimentRunner**: 负责运行完整的评估实验，包括MATD3-MIG算法和基线算法的对比。

该框架通过`PerformanceMetrics.calculate_system_metrics`方法计算系统综合性能指标，涵盖了任务时延、能耗、数据丢失率、缓存性能、迁移性能和资源利用率等多个方面。此外，`PerformanceMetrics.run_baseline_comparison`方法用于运行基线算法对比实验，生成详细的性能对比报告。

**Section sources**
- [evaluation.py](file://experiments/evaluation.py#L16-L225)

## 实验组定义与控制变量设置

实验组的定义主要通过`run_full_experiment.py`中的`ExperimentConfig`类来实现。每个实验配置都包含以下参数：

- **name**: 实验名称（如"standard"、"high_load"、"large_scale"）
- **description**: 实验描述
- **num_episodes**: 实验轮数
- **episode_length**: 每轮实验的步长
- **num_vehicles**: 车辆数量
- **num_rsus**: RSU数量
- **num_uavs**: UAV数量
- **task_arrival_rate**: 任务到达率

控制变量的设置通过`config/external_config.py`中的`ExternalConfigManager`类进行管理。该类允许在运行时调整关键参数，提高系统的灵活性。主要控制变量包括：

- **time_settings**: 时隙长度和仿真时间
- **task_generation**: 任务到达率、数据大小范围、计算密度、截止时间范围和输出比例
- **network_topology**: 车辆、RSU和UAV的数量，区域宽度和高度，RSU覆盖半径
- **compute_resources**: CPU频率范围和并行效率
- **communication**: 总带宽、发射功率
- **migration_parameters**: 迁移阈值、过载阈值、冷却期和最大迁移距离
- **cache_settings**: 缓存容量、命中阈值和预测窗口

这些参数可以通过外部配置文件`vec_system_config.json`进行修改，从而实现对实验环境的灵活控制。

**Section sources**
- [run_full_experiment.py](file://run_full_experiment.py#L29-L38)
- [external_config.py](file://config/external_config.py#L74-L246)

## 多算法对比实验配置

多算法对比实验的配置通过`run_full_experiment.py`中的`FullExperimentRunner`类实现。该类定义了多个基线算法，包括：

- **RandomAlgorithm**: 随机选择节点
- **GreedyAlgorithm**: 选择负载最低的节点
- **RoundRobinAlgorithm**: 轮询选择节点
- **LoadAwareAlgorithm**: 基于负载和距离的综合决策

这些算法与MATD3-MIG系统进行对比，以评估其性能优劣。具体配置如下：

- **MATD3-MIG**: 使用实际的MATD3-MIG系统进行任务卸载和迁移管理
- **Random**: 随机分配任务，成功率较低
- **Greedy**: 选择负载最低的节点，成功率较高
- **Round_Robin**: 轮询分配任务，负载均衡较好
- **Load_Aware**: 综合考虑负载和距离，性能较优

通过`run_full_experiment.py`中的`run_single_experiment`方法，可以运行单个实验配置，生成详细的性能对比结果。这些结果包括平均时延、总能耗、完成率、丢失率和缓存命中率等指标。

**Section sources**
- [run_full_experiment.py](file://run_full_experiment.py#L100-L200)

## 实验设计模式

### 基准实验
基准实验旨在评估不同算法在标准配置下的性能表现。通过`ExperimentConfig`类定义的标准实验配置，运行MATD3-MIG系统和多个基线算法，生成性能对比报告。基准实验的结果显示，MATD3-MIG在平均时延、总能耗、完成率和缓存命中率等方面均优于其他基线算法。

### 消融实验
消融实验用于分析各个组件对整体性能的影响。通过逐步移除或替换某些组件（如缓存策略、迁移机制），观察系统性能的变化。例如，关闭缓存功能后，缓存命中率显著下降，导致平均时延和总能耗增加。消融实验有助于理解各组件的重要性及其相互作用。

### 扩展场景实验
扩展场景实验模拟不同的网络环境和负载条件，评估系统在各种情况下的鲁棒性和适应性。通过调整车辆、RSU和UAV的数量，以及任务到达率等参数，生成大规模、高负载等复杂场景。扩展场景实验的结果表明，MATD3-MIG系统在不同场景下均能保持良好的性能表现。

**Section sources**
- [run_full_experiment.py](file://run_full_experiment.py#L200-L300)

## 新增实验维度实现

新增实验维度（如能耗权重调整）的实现步骤如下：

1. **修改配置文件**: 在`vec_system_config.json`中添加新的配置项，例如`energy_weight`。
2. **更新配置管理器**: 修改`ExternalConfigManager`类，使其能够读取和应用新的配置项。
3. **调整奖励函数**: 在`RLConfig`类中，根据新的能耗权重调整奖励函数，确保强化学习模型能够正确反映能耗的重要性。
4. **重新运行实验**: 使用更新后的配置文件重新运行实验，生成新的性能对比结果。

通过这种方式，可以灵活地引入新的实验维度，进一步优化系统性能。

**Section sources**
- [external_config.py](file://config/external_config.py#L74-L246)
- [system_config.py](file://config/system_config.py#L22-L52)

## 实验可复现性与数据一致性保障

为了确保实验的可复现性和数据一致性，采取了以下措施：

- **固定随机种子**: 在`SystemConfig`类中设置了固定的随机种子，确保每次实验的初始状态一致。
- **外部配置管理**: 通过`ExternalConfigManager`类管理外部配置，确保所有实验使用相同的参数设置。
- **结果保存**: 将实验结果保存为JSON和CSV格式，便于后续分析和验证。
- **数值稳定性检查**: 在`enhance_numerical_stability.py`中实现了数值稳定性检查，确保计算过程中不会出现异常值。

这些措施共同保证了实验结果的可靠性和可重复性，为后续研究提供了坚实的基础。

**Section sources**
- [system_config.py](file://config/system_config.py#L263-L301)
- [enhance_numerical_stability.py](file://enhance_numerical_stability.py#L201-L259)

## 结论
通过对`experiments/evaluation.py`中实验框架的设计机制进行深入解析，结合`experiment_summary.md`中的实际案例，我们详细说明了如何定义实验组、设置控制变量以及配置多算法对比实验。实验设计模式包括基准实验、消融实验和扩展场景实验，能够全面评估系统性能。新增实验维度的实现步骤清晰，确保了实验的灵活性和可扩展性。最后，通过一系列保障措施，确保了实验的可复现性和数据一致性，为后续研究提供了可靠的支持。