
# 单智能体算法

<cite>
**本文档引用的文件**   
- [td3.py](file://single_agent/td3.py)
- [ddpg.py](file://single_agent/ddpg.py)
- [dqn.py](file://single_agent/dqn.py)
- [ppo.py](file://single_agent/ppo.py)
- [sac.py](file://single_agent/sac.py)
- [uav_action_space.py](file://algorithms/uav_action_space.py)
- [train_single_agent.py](file://train_single_agent.py)
</cite>

## 目录
1. [引言](#引言)
2. [项目结构](#项目结构)
3. [核心组件](#核心组件)
4. [架构概述](#架构概述)
5. [详细组件分析](#详细组件分析)
6. [依赖分析](#依赖分析)
7. [性能考虑](#性能考虑)
8. [故障排除指南](#故障排除指南)
9. [结论](#结论)

## 引言
本文档全面文档化单智能体深度强化学习算法的实现及其在系统基准测试中的角色。重点解析TD3算法在`single_agent/td3.py`中的实现，包括双Q网络、目标策略平滑和延迟更新机制。同时对比分析DDPG的确定性策略梯度、DQN的Q-learning框架、PPO的裁剪目标函数以及SAC的熵最大化原理。说明这些算法如何作为多智能体系统的性能基线，并通过`train_single_agent.py`脚本进行独立训练和评估。提供各算法的网络架构图、动作空间适配方法（特别是与`uav_action_space.py`的集成）、超参数调优指南以及在边缘计算场景下的性能表现对比。

## 项目结构
项目结构展示了单智能体强化学习算法的实现，包括TD3、DDPG、DQN、PPO和SAC等算法。这些算法通过`train_single_agent.py`脚本进行训练和评估。`algorithms`目录包含多智能体算法和UAV专用动作空间定义。`config`、`utils`和`tools`等目录提供配置、工具和辅助功能。

```mermaid
graph TD
single_agent[单智能体算法]
--> td3[TD3]
--> ddpg[DDPG]
--> dqn[DQN]
--> ppo[PPO]
--> sac[SAC]
train_script[train_single_agent.py]
--> |训练| single_agent
algorithms[算法]
--> |包含| uav_action_space[uav_action_space.py]
train_script --> |使用| uav_action_space
```

**图源**
- [td3.py](file://single_agent/td3.py)
- [ddpg.py](file://single_agent/ddpg.py)
- [dqn.py](file://single_agent/dqn.py)
- [ppo.py](file://single_agent/ppo.py)
- [sac.py](file://single_agent/sac.py)
- [uav_action_space.py](file://algorithms/uav_action_space.py)
- [train_single_agent.py](file://train_single_agent.py)

**节源**
- [td3.py](file://single_agent/td3.py)
- [ddpg.py](file://single_agent/ddpg.py)
- [dqn.py](file://single_agent/dqn.py)
- [ppo.py](file://single_agent/ppo.py)
- [sac.py](file://single_agent/sac.py)
- [uav_action_space.py](file://algorithms/uav_action_space.py)
- [train_single_agent.py](file://train_single_agent.py)

## 核心组件
单智能体算法的核心组件包括TD3、DDPG、DQN、PPO和SAC。这些算法都实现了Actor-Critic架构，但各有特点。TD3通过双Q网络和延迟更新提高稳定性；DDPG是基础的确定性策略梯度方法；DQN处理离散动作空间；PPO使用裁剪目标函数；SAC采用最大熵框架。所有算法都通过`train_single_agent.py`脚本进行训练，并与`uav_action_space.py`集成以适配UAV特定动作。

**节源**
- [td3.py](file://single_agent/td3.py)
- [ddpg.py](file://single_agent/ddpg.py)
- [dqn.py](file://single_agent/dqn.py)
- [ppo.py](file://single_agent/ppo.py)
- [sac.py](file://single_agent/sac.py)
- [train_single_agent.py](file://train_single_agent.py)

## 架构概述
系统架构包括单智能体算法、训练脚本和UAV专用动作空间。`train_single_agent.py`作为主入口，协调各个单智能体算法的训练过程。每个算法实现都包含环境、智能体、网络和缓冲区等组件。`uav_action_space.py`提供UAV专用的动作空间定义，被所有算法共享使用。

```mermaid
graph TD
train_script[train_single_agent.py]
--> |控制| td3_env[TD3Environment]
--> |使用| td3_agent[TD3Agent]
--> |包含| td3_actor[TD3Actor]
--> |和| td3_critic[TD3Critic]
train_script --> |控制| ddpg_env[DDPGEnvironment]
--> |使用| ddpg_agent[DDPGAgent]
--> |包含| ddpg_actor[DDPGActor]
--> |和| ddpg_critic[DDPGCritic]
train_script --> |控制| dqn_env[DQNEnvironment]
--> |使用| dqn_agent[DQNAgent]
--> |包含| dqn_network[DQNNetwork]
train_script --> |控制| ppo_env[PPOEnvironment]
--> |使用| ppo_agent[PPOAgent]
--> |包含| ppo_actor[PPOActor]
--> |和| ppo_critic[PPOCritic]
train_script --> |控制| sac_env[SACEnvironment]
--> |使用| sac_agent[SACAgent]
--> |包含| sac_actor[SACActor]
--> |和| sac_critic[SACCritic]
td3_env --> |集成| uav_action_space[uav_action_space.py]
ddpg_env --> |集成| uav_action_space
dqn_env --> |集成| uav_action_space
ppo_env --> |集成| uav_action_space
sac_env --> |集成| uav_action_space
```

**图源**
- [td3.py](file://single_agent/td3.py)
- [ddpg.py](file://single_agent/ddpg.py)
- [dqn.py](file://single_agent/dqn.py)
- [ppo.py](file://single_agent/ppo.py)
- [sac.py](file://single_agent/sac.py)
- [train_single_agent.py](file://train_single_agent.py)
- [uav_action_space.py](file://algorithms/uav_action_space.py)

## 详细组件分析

### TD3算法分析
TD3算法通过双Q网络减少过估计，延迟策略更新提高稳定性，目标策略平滑化减少方差。其核心组件包括TD3Actor、TD3Critic、TD3ReplayBuffer和TD3Agent。

#### TD3网络架构
```mermaid
classDiagram
class TD3Config {
+hidden_dim : int
+actor_lr : float
+critic_lr : float
+batch_size : int
+buffer_size : int
+tau : float
+gamma : float
+policy_delay : int
+target_noise : float
+noise_clip : float
+exploration_noise : float
+noise_decay : float
+min_noise : float
+per_alpha : float
+per_beta_start : float
+per_beta_frames : int
+update_freq : int
+warmup_steps : int
}
class TD3Actor {
-max_action : float
-network : nn.Sequential
+forward(state : torch.Tensor) : torch.Tensor
+_init_weights() : void
}
class TD3Critic {
-q1_network : nn.Sequential
-q2_network : nn.Sequential
+forward(state : torch.Tensor, action : torch.Tensor) : Tuple[torch.Tensor, torch.Tensor]
+q1(state : torch.Tensor, action : torch.Tensor) : torch.Tensor
+_init_weights() : void
}
class TD3ReplayBuffer {
-capacity : int
-ptr : int
-size : int
-alpha : float
-states : np.ndarray
-actions : np.ndarray
-rewards : np.ndarray
-next_states : np.ndarray
-dones : np.ndarray
-priorities : np.ndarray
+push(state : np.ndarray, action : np.ndarray, reward : float, next_state : np.ndarray, done : bool) : void
+sample(batch_size : int, beta : float) : Tuple[torch.Tensor, ...]
+update_priorities(indices : np.ndarray, priorities : np.ndarray) : void
+__len__() : int
}
class TD3Agent {
-state_dim : int
-action_dim : int
-config : TD3Config
-optimized_batch_size : int
-device : torch.device
-actor : TD3Actor
-critic : TD3Critic
-target_actor : TD3Actor
-target_critic : TD3Critic
-actor_optimizer : optim.Adam
-critic_optimizer : optim.Adam
-actor_lr_scheduler : optim.lr_scheduler.ExponentialLR
-critic_lr_scheduler : optim.lr_scheduler.ExponentialLR
-beta : float
-beta_increment : float
-replay_buffer : TD3ReplayBuffer
-exploration_noise : float
-step_count : int
-update_count : int
-actor_losses : List[float]
-critic_losses : List[float]
+select_action(state : np.ndarray, training : bool) : np.ndarray
+store_experience(state : np.ndarray, action : np.ndarray, reward : float, next_state : np.ndarray, done : bool) : void
+update() : Dict[str, float]
+_update_critic(states : torch.Tensor, actions : torch.Tensor, rewards : torch.Tensor, next_states : torch.Tensor, dones : torch.Tensor, weights : torch.Tensor) : Tuple[float, torch.Tensor]
+_update_actor(states : torch.Tensor) : float
+soft_update(target : nn.Module, source : nn.Module, tau : float) : void
+hard_update(target : nn.Module, source : nn.Module) : void
+save_model(filepath : str) : void
+load_model(filepath : str) : void
}
class TD3Environment {
-config : TD3Config
-state_dim : int
-action_dim : int
-agent : TD3Agent
-episode_count : int
-step_count : int
+get_state_vector(node_states : Dict, system_metrics : Dict) : np.ndarray
+decompose_action(action : np.ndarray) : Dict[str, np.ndarray]
+get_actions(state : np.ndarray, training : bool) : Dict[str, np.ndarray]
+calculate_reward(system_metrics : Dict) : float
+train_step(state : np.ndarray, action : Union[np.ndarray, int], reward : float, next_state : np.ndarray, done : bool) : Dict
+save_models(filepath : str) : void
+load_models(filepath : str) : void
+store_experience(state : np.ndarray, action : np.ndarray, reward : float, next_state : np.ndarray, done : bool, log_prob : float, value : float) : void
+update(last_value : float) : Dict
+get_training_stats() : Dict
}
TD3Environment --> TD3Agent : "使用"
TD3Agent --> TD3Actor : "包含"
TD3Agent --> TD3Critic : "包含"
TD3Agent --> TD3ReplayBuffer : "使用"
TD3Actor --> TD3Config : "参数"
TD3Critic --> TD3Config : "参数"
TD3ReplayBuffer --> TD3Config : "参数"
```

**图源**
- [td3.py](file://single_agent/td3.py)

**节源**
- [td3.py](file://single_agent/td3.py)

### DDPG算法分析
DDPG算法是Actor-Critic架构的基础实现，使用经验回放和目标网络稳定训练过程。

#### DDPG网络架构
```mermaid
classDiagram
class DDPGConfig {
+hidden_dim : int
+actor_lr : float
+critic_lr : float
+batch_size : int
+buffer_size : int
+tau : float
+gamma : float
+noise_scale : float
+noise_decay : float
+min_noise : float
+update_freq : int
+warmup_steps : int
}
class DDPGActor {
-max_action : float
-network : nn.Sequential
+forward(state : torch.Tensor) : torch.Tensor
+_init_weights() : void
}
class DDPGCritic {
-state_encoder : nn.Sequential
-fusion_network : nn.Sequential
+forward(state : torch.Tensor, action : torch.Tensor) : torch.Tensor
+_init_weights() : void
}
class DDPGReplayBuffer {
-capacity : int
-ptr : int
-size : int
-states : np.ndarray
-actions : np.ndarray
-rewards : np.ndarray
-next_states : np.ndarray
-dones : np.ndarray
+push(state : np.ndarray, action : np.ndarray, reward : float, next_state : np.ndarray, done : bool) : void
+sample(batch_size : int) : Tuple[torch.Tensor, ...]
+__len__() : int
}
class DDPGAgent {
-state_dim : int
-action_dim : int
-config : DDPGConfig
-optimized_batch_size : int
-device : torch.device
-actor : DDPGActor
-critic : DDPGCritic
-target_actor : DDPGActor
-target_critic : DDPGCritic
-actor_optimizer : optim.Adam
-critic_optimizer : optim.Adam
-replay_buffer : DDPGReplayBuffer
-noise_scale : float
-step_count : int
-actor_losses : List[float]
-critic_losses : List[float]
+select_action(state : np.ndarray, training : bool) : np.ndarray
+store_experience(state : np.ndarray, action : np.ndarray, reward : float, next_state : np.ndarray, done : bool) : void
+update() : Dict[str, float]
+_update_critic(states : torch.Tensor, actions : torch.Tensor, rewards : torch.Tensor, next_states : torch.Tensor, dones : torch.Tensor) : float
+_update_actor(states : torch.Tensor) : float
+soft_update(target : nn.Module, source : nn.Module, tau : float) : void
+hard_update(target : nn.Module, source : nn.Module) : void
+save_model(filepath : str) : void
+load_model(filepath : str) : void
}
class DDPGEnvironment {
-config : DDPGConfig
-state_dim : int
-action_dim : int
-agent : DDPGAgent
-episode_count : int
-step_count : int
+get_state_vector(node_states : Dict, system_metrics : Dict) : np.ndarray
+decompose_action(action : np.ndarray) : Dict[str, np.ndarray]
+get_actions(state : np.ndarray, training : bool) : Dict[str, np.ndarray]
+calculate_reward(system_metrics : Dict) : float
+train_step(state : np.ndarray, action : Union[np.ndarray, int], reward : float, next_state : np.ndarray, done : bool) : Dict
+save_models(filepath : str) : void
+load_models(filepath : str) : void
+store_experience(state : np.ndarray, action : np.ndarray, reward : float, next_state : np.ndarray, done : bool, log_prob : float, value : float) : void
+update(last_value : float) : Dict
+get_training_stats() : Dict
}
DDPGEnvironment --> DDPGAgent : "使用"
DDPGAgent --> DDPGActor : "包含"
DDPGAgent --> DDPGCritic : "包含"
DDPGAgent --> DDPGReplayBuffer : "使用"
DDPGActor --> DDPGConfig : "参数"
DDPGCritic --> DDPGConfig : "参数"
DDPGReplayBuffer --> DDPGConfig : "参数"
```

**图源**
- [ddpg.py](file://single_agent/ddpg.py)

**节源**
- [ddpg.py](file://single_agent/ddpg.py)

### DQN算法分析
DQN算法处理离散动作空间，使用经验回放和目标网络稳定训练过程。

#### DQN网络架构
```mermaid
classDiagram
class DQNConfig {
+hidden_dim : int
+lr : float
+batch_size : int
+buffer_size : int
+target_update_freq : int
+gamma : float
+epsilon : float
+epsilon_decay : float
+min_epsilon : float
+update_freq : int
+warmup_steps : int
+double_dqn : bool
+dueling_dqn : bool
}
class DQNNetwork {
-state_dim : int
-action_dim : int
-dueling : bool
-feature_layers : nn.Sequential
-value_stream : nn.Sequential
-advantage_stream : nn.Sequential
-q_network : nn.Sequential
+forward(state : torch.Tensor) : torch.Tensor
+_init_weights() : void
}
class DQNReplayBuffer {
-capacity : int
-ptr : int
-size : int
-states : np.ndarray
-actions : np.ndarray
-rewards : np.ndarray
-next_states : np.ndarray
-dones : np.ndarray
+push(state : np.ndarray, action : int, reward : float, next_state : np.ndarray, done : bool) : void
+sample(batch_size : int) : Tuple[torch.Tensor, ...]
+__len__() : int
}
class DQNAgent {
-state_dim : int
-action_dim : int
-config : DQNConfig
-optimized_batch_size : int
-device : torch.device
-q_network : DQNNetwork
-target_q_network : DQNNetwork
-optimizer : optim.Adam
-replay_buffer : DQNReplayBuffer
-epsilon : float
-step_count : int
-update_count : int
-losses : List[float]
-q_values : List[float]
+select_action(state : np.ndarray, training : bool) : int
+store_experience(state : np.ndarray, action : int, reward : float, next_state : np.ndarray, done : bool) : void
+update() : Dict[str, float]
+_compute_loss(states : torch.Tensor, actions : torch.Tensor, rewards : torch.Tensor, next_states : torch.Tensor, dones : torch.Tensor) : torch.Tensor
+hard_update(target : nn.Module, source : nn.Module) : void
+save_model(filepath : str) : void
+load_model(filepath : str) : void
}
class DQNEnvironment {
-config : DQNConfig
-state_dim : int
-action_dim : int
-agent : DQNAgent
-action_map : Dict[int, Dict[str, int]]
-episode_count : int
-step_count : int
+_build_action_map() : Dict[int, Dict[str, int]]
+get_state_vector(node_states : Dict, system_metrics : Dict) : np.ndarray
+decompose_action(action_idx : int) : Dict[str, int]
+get_actions(state : np.ndarray, training : bool) : Dict[str, int]
+calculate_reward(system_metrics : Dict) : float
+train_step(state : np.ndarray, action : Union[np.ndarray, int], reward : float, next_state : np.ndarray, done : bool) : Dict
+save_models(filepath : str) : void
+load_models(filepath : str) : void
+store_experience(state : np.ndarray, action : Union[np.ndarray, int], reward : float, next_state : np.ndarray, done : bool, log_prob : float, value : float) : void
+update(last_value : float) : Dict
+get_training_stats() : Dict
}
DQNEnvironment --> DQNAgent : "使用"
DQNAgent --> DQNNetwork : "包含"
DQNAgent --> DQNReplayBuffer : "使用"
DQNNetwork --> DQNConfig : "参数"
DQNReplayBuffer --> DQNConfig : "参数"
```

**图源**
- [dqn.py](file://single_agent/dqn.py)

**节源**
- [dqn.py](file://single_agent/dqn.py)

### PPO算法分析
PPO算法使用策略梯度方法处理连续动作空间，通过裁剪代理目标防止过大策略更新。

#### PPO网络架构
```mermaid
classDiagram
    class PPOConfig {
        +hidden_dim: int
        +actor_lr: float
        +critic_lr: float
        +clip_ratio: float
        +entropy_coef: float
        +value_coef: float
        +max_grad_norm: float
        +batch_size: int
        +buffer_size: int
        +ppo_epochs: int
        +gamma: float
        +gae_lambda: float
        +normalize_advantages: bool
        +use_gae: bool
        +target_kl: float
    }
    
    class PPOActor {
        -action_dim: int
        -feature_layers: nn.Sequential
        -mean_layer: nn.Linear
        -log_std: nn.Parameter
        +forward(state: torch.Tensor): Tuple[torch.Tensor, torch.Tensor]
        +get_action_and_logprob(state: torch.Tensor, action: Optional[torch.Tensor]): Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
        +_init_weights(): void
    }
    
    class PPOCritic {
        -value_network: nn.Sequential
        +forward(state: torch.Tensor): torch.Tensor
        +_init_weights(): void
    }
    
    class PPOBuffer {
        -buffer_size: int
        -ptr: int
        -size: int
        -states: np.ndarray
        -actions: np.ndarray
        -log_probs: np.ndarray
        -rewards: np.ndarray
        -dones: np.ndarray
        -values: np.ndarray
        -advantages: np.ndarray
        -returns: np.ndarray
        +store(state: np.ndarray, action: np.ndarray, log_prob: float, reward: float, done: bool, value: float): void
        +compute_advantages_and_returns(last_value: float, gamma: float, gae_lambda: float, use_gae: bool): void
        +get_batch(batch_size: int): Tuple[torch.Tensor, ...]
        +clear(): void
    }
    
    class PPOAgent {
        -