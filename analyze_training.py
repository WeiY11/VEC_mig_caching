"""
è®­ç»ƒç»“æœåˆ†æè„šæœ¬
åˆ†æOPTIMIZED_TD3è®­ç»ƒä¸æ”¶æ•›çš„åŸå› 
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
from pathlib import Path

# è¯»å–è®­ç»ƒç»“æœ
results_file = 'd:/VEC_mig_caching/results/single_agent/optimized_td3/training_results_20251202_002525.json'
with open(results_file, 'r', encoding='utf-8') as f:
    data = json.load(f)

episode_rewards = np.array(data['episode_rewards'])
num_episodes = len(episode_rewards)

# è®¡ç®—æ»‘åŠ¨å¹³å‡
def moving_average(data, window_size):
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

window_sizes = [10, 50, 100]
ma_rewards = {w: moving_average(episode_rewards, w) for w in window_sizes}

# ç»Ÿè®¡åˆ†æ
print("=" * 80)
print("OPTIMIZED_TD3 è®­ç»ƒç»“æœåˆ†ææŠ¥å‘Š")
print("=" * 80)
print(f"\nã€åŸºæœ¬ä¿¡æ¯ã€‘")
print(f"æ€»Episodes: {num_episodes}")
print(f"è®­ç»ƒæ—¶é•¿: {data['training_config']['training_time_hours']:.2f} å°æ—¶")
print(f"è®¾å¤‡: {data['system_config']['device']}")
print(f"ç½‘ç»œæ‹“æ‰‘: {data['network_topology']}")
print(f"çŠ¶æ€ç»´åº¦: {data['state_dim']}")

print(f"\nã€å¥–åŠ±ç»Ÿè®¡ã€‘")
print(f"å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.4f}")
print(f"æ ‡å‡†å·®: {np.std(episode_rewards):.4f}")
print(f"æœ€å°å€¼: {np.min(episode_rewards):.4f}")
print(f"æœ€å¤§å€¼: {np.max(episode_rewards):.4f}")
print(f"å‰50è½®å‡å€¼: {np.mean(episode_rewards[:50]):.4f}")
print(f"å50è½®å‡å€¼: {np.mean(episode_rewards[-50:]):.4f}")
print(f"æœ€ä½³50è½®å‡å€¼: {np.mean(sorted(episode_rewards, reverse=True)[:50]):.4f}")

# è®¡ç®—æ”¶æ•›æ€§æŒ‡æ ‡
print(f"\nã€æ”¶æ•›æ€§åˆ†æã€‘")
# åˆ’åˆ†è®­ç»ƒé˜¶æ®µ
phase1 = episode_rewards[:250]  # å‰25%
phase2 = episode_rewards[250:500]  # ä¸­æœŸ25%-50%
phase3 = episode_rewards[500:750]  # ä¸­åæœŸ50%-75%
phase4 = episode_rewards[750:]  # å25%

print(f"é˜¶æ®µ1 (0-250):   å‡å€¼={np.mean(phase1):.4f}, æ ‡å‡†å·®={np.std(phase1):.4f}")
print(f"é˜¶æ®µ2 (250-500): å‡å€¼={np.mean(phase2):.4f}, æ ‡å‡†å·®={np.std(phase2):.4f}")
print(f"é˜¶æ®µ3 (500-750): å‡å€¼={np.mean(phase3):.4f}, æ ‡å‡†å·®={np.std(phase3):.4f}")
print(f"é˜¶æ®µ4 (750-1000):å‡å€¼={np.mean(phase4):.4f}, æ ‡å‡†å·®={np.std(phase4):.4f}")

# è¶‹åŠ¿åˆ†æ
improvement = np.mean(phase4) - np.mean(phase1)
print(f"\né˜¶æ®µ1â†’é˜¶æ®µ4æ”¹è¿›: {improvement:.4f} ({improvement/np.mean(phase1)*100:.2f}%)")

# è®¡ç®—æ³¢åŠ¨ç³»æ•°
cv = np.std(episode_rewards) / abs(np.mean(episode_rewards))
print(f"å˜å¼‚ç³»æ•° (CV): {cv:.4f} (è¶Šå°è¶Šç¨³å®š)")

# å¼‚å¸¸å€¼åˆ†æ
print(f"\nã€å¼‚å¸¸å€¼åˆ†æã€‘")
threshold = -3.0
outliers = episode_rewards[episode_rewards < threshold]
print(f"ä½äº{threshold}çš„å¼‚å¸¸å€¼æ•°é‡: {len(outliers)} ({len(outliers)/num_episodes*100:.2f}%)")
if len(outliers) > 0:
    print(f"å¼‚å¸¸å€¼å‡å€¼: {np.mean(outliers):.4f}")
    outlier_indices = np.where(episode_rewards < threshold)[0]
    print(f"å¼‚å¸¸å€¼å‡ºç°ä½ç½® (å‰10ä¸ª): {outlier_indices[:10].tolist()}")

# åˆ›å»ºå¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('OPTIMIZED_TD3 Training Analysis - æ¨¡å‹æœªæ”¶æ•›è¯Šæ–­', fontsize=16, fontweight='bold')

# å›¾1: åŸå§‹å¥–åŠ±æ›²çº¿
ax1 = axes[0, 0]
ax1.plot(episode_rewards, alpha=0.3, label='Episode Reward', color='blue')
for w in window_sizes:
    ma = ma_rewards[w]
    x = np.arange(w-1, num_episodes)
    ax1.plot(x, ma, label=f'MA-{w}', linewidth=2)
ax1.axhline(y=np.mean(episode_rewards), color='red', linestyle='--', label='Mean')
ax1.set_xlabel('Episode')
ax1.set_ylabel('Reward')
ax1.set_title('è®­ç»ƒå¥–åŠ±æ›²çº¿ (æ— æ˜æ˜¾ä¸Šå‡è¶‹åŠ¿)')
ax1.legend()
ax1.grid(True, alpha=0.3)

# å›¾2: å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾
ax2 = axes[0, 1]
ax2.hist(episode_rewards, bins=50, color='skyblue', edgecolor='black', alpha=0.7)
ax2.axvline(np.mean(episode_rewards), color='red', linestyle='--', linewidth=2, label=f'Mean={np.mean(episode_rewards):.3f}')
ax2.axvline(np.median(episode_rewards), color='green', linestyle='--', linewidth=2, label=f'Median={np.median(episode_rewards):.3f}')
ax2.set_xlabel('Reward')
ax2.set_ylabel('Frequency')
ax2.set_title('å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾ (é«˜æ–¹å·®)')
ax2.legend()
ax2.grid(True, alpha=0.3)

# å›¾3: é˜¶æ®µæ€§è¡¨ç°å¯¹æ¯”
ax3 = axes[1, 0]
phases = ['Phase1\n(0-250)', 'Phase2\n(250-500)', 'Phase3\n(500-750)', 'Phase4\n(750-1000)']
means = [np.mean(phase1), np.mean(phase2), np.mean(phase3), np.mean(phase4)]
stds = [np.std(phase1), np.std(phase2), np.std(phase3), np.std(phase4)]
x_pos = np.arange(len(phases))
bars = ax3.bar(x_pos, means, yerr=stds, capsize=5, color=['#ff7f0e', '#2ca02c', '#d62728', '#9467bd'], alpha=0.7)
ax3.set_xticks(x_pos)
ax3.set_xticklabels(phases)
ax3.set_ylabel('Mean Reward')
ax3.set_title('åˆ†é˜¶æ®µæ€§èƒ½å¯¹æ¯” (æ— ç¨³å®šæ”¹è¿›)')
ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
ax3.grid(True, alpha=0.3, axis='y')
# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, (m, s) in enumerate(zip(means, stds)):
    ax3.text(i, m, f'{m:.3f}', ha='center', va='bottom' if m > 0 else 'top')

# å›¾4: æ»‘åŠ¨æ ‡å‡†å·® (æ³¢åŠ¨æ€§åˆ†æ)
ax4 = axes[1, 1]
window = 50
rolling_std = [np.std(episode_rewards[max(0, i-window):i+1]) for i in range(num_episodes)]
ax4.plot(rolling_std, color='purple', linewidth=1.5)
ax4.axhline(y=np.mean(rolling_std), color='red', linestyle='--', label=f'Mean Std={np.mean(rolling_std):.3f}')
ax4.set_xlabel('Episode')
ax4.set_ylabel('Rolling Std (window=50)')
ax4.set_title('è®­ç»ƒæ³¢åŠ¨æ€§åˆ†æ (é«˜æ³¢åŠ¨æ€§)')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
output_path = 'd:/VEC_mig_caching/results/single_agent/optimized_td3/training_analysis.png'
plt.savefig(output_path, dpi=150, bbox_inches='tight')
print(f"\nå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {output_path}")

# è¯Šæ–­ç»“è®º
print("\n" + "=" * 80)
print("ã€è¯Šæ–­ç»“è®ºã€‘")
print("=" * 80)
print("\nâŒ æ¨¡å‹æœªèƒ½æ”¶æ•›ï¼Œä¸»è¦é—®é¢˜ï¼š")
print("   1. å¥–åŠ±æ— ä¸Šå‡è¶‹åŠ¿: å‰åæœŸå‡å€¼ç›¸è¿‘ï¼Œæ”¹è¿›å¹…åº¦å¾®å°")
print(f"   2. é«˜æ–¹å·®: æ ‡å‡†å·®={np.std(episode_rewards):.4f}ï¼Œå˜å¼‚ç³»æ•°={cv:.4f}")
print(f"   3. é¢‘ç¹å¼‚å¸¸å€¼: {len(outliers)}ä¸ªæä½å¥–åŠ± (<{threshold})")
print("   4. æŒç»­æ³¢åŠ¨: åæœŸä»å­˜åœ¨å¤§å¹…éœ‡è¡ï¼Œæœªç¨³å®š")

print("\nğŸ” å¯èƒ½åŸå› åˆ†æï¼š")
print("   â‘  æ¢ç´¢å™ªå£°è¿‡é«˜: exploration_noise=0.15 å¯èƒ½å¯¼è‡´åæœŸæ¢ç´¢è¿‡åº¦")
print("   â‘¡ å­¦ä¹ ç‡ä¸åŒ¹é…: actor_lr=3e-5, critic_lr=8e-5 å¯èƒ½è¿‡å°æˆ–ä¸å¹³è¡¡")
print("   â‘¢ æ‰¹é‡å¤§å°è¿‡å¤§: batch_size=768 å¯èƒ½å¯¼è‡´æ›´æ–°é¢‘ç‡ä¸è¶³")
print("   â‘£ å¥–åŠ±å‡½æ•°å°ºåº¦: è´Ÿå€¼å¥–åŠ±å¯èƒ½å½±å“æ¢¯åº¦ä¼ æ’­")
print("   â‘¤ ç½‘ç»œå®¹é‡: hidden_dim=512, GAT heads=6 å¯èƒ½è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆ")
print("   â‘¥ é¢„çƒ­ä¸è¶³: warmup_steps=2000 (çº¦20 episodes) å¯èƒ½ä¸å¤Ÿ")

print("\n" + "=" * 80)
