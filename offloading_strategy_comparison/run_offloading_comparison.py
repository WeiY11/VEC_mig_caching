#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸è½½ç­–ç•¥å¯¹æ¯”å®éªŒæ¡†æ¶ (Offloading Strategy Comparison)

ã€å®éªŒç›®çš„ã€‘
å¯¹æ¯”æ ¸å¿ƒå¸è½½ç­–ç•¥åœ¨ä¸åŒå‚æ•°ä¸‹çš„æ€§èƒ½ï¼š

ã€å¯¹æ¯”ç­–ç•¥ã€‘ï¼ˆ6ç§ï¼‰
åŸºå‡†ç­–ç•¥ï¼ˆ2ç§ï¼‰ï¼š
1. LocalOnly  - çº¯æœ¬åœ°è®¡ç®—ï¼ˆä¸‹é™åŸºå‡†ï¼‰
2. RSUOnly    - ä»…åŸºç«™å¸è½½ï¼ˆä¼ ç»ŸMECï¼‰

å¯å‘å¼ç­–ç•¥ï¼ˆ2ç§ï¼‰ï¼š
3. LoadBalance - è´Ÿè½½å‡è¡¡ï¼ˆæœ€ä½³å¯å‘å¼ï¼‰
4. Random      - éšæœºé€‰æ‹©ï¼ˆå¯¹ç…§ç»„ï¼‰

æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆ2ç§ï¼‰ï¼š
5. TD3         - å®Œæ•´TD3ç­–ç•¥ï¼ˆä¸»è¦è´¡çŒ®ï¼‰
6. TD3-NoMig   - æ— è¿ç§»TD3ï¼ˆæ¶ˆèå®éªŒï¼‰

ã€æ‰«æå‚æ•°ã€‘
1. è½¦è¾†æ•°é‡ (num_vehicles): 8, 12, 16, 20, 24
2. ä»»åŠ¡åˆ°è¾¾ç‡ (task_arrival_rate): 0.3, 0.5, 0.7, 0.9, 1.1
3. é€šä¿¡å¸¦å®½ (bandwidth_mhz): 10, 20, 30, 40, 50
4. ä»»åŠ¡æ•°æ®å¤§å° (data_size_mb): 0.5, 1.0, 1.5, 2.0, 2.5
5. è®¡ç®—èµ„æº (cpu_frequency_ghz): 1.5, 2.0, 2.5, 3.0, 3.5

ã€è¯„ä¼°æŒ‡æ ‡ã€‘
- å¹³å‡åŠ æƒæˆæœ¬ = 2.0Ã—æ—¶å»¶(s) + 1.2Ã—èƒ½è€—(kJ)
  å…¶ä¸­ï¼šèƒ½è€—ä»Jè½¬æ¢ä¸ºkJï¼ˆé™¤ä»¥1000ï¼‰
- ä»»åŠ¡å®Œæˆç‡
- å¹³å‡æ—¶å»¶ (ç§’)
- æ€»èƒ½è€— (ç„¦è€³)
"""

import sys
import json
import time
import argparse
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
parent_dir = Path(__file__).parent.parent
sys.path.insert(0, str(parent_dir))

from config import config
from single_agent.td3 import TD3Agent
from train_single_agent import SingleAgentTrainingEnvironment
from utils.unified_reward_calculator import UnifiedRewardCalculator
from offloading_strategies import create_offloading_strategy


class OffloadingComparisonExperiment:
    """å¸è½½ç­–ç•¥å¯¹æ¯”å®éªŒ"""
    
    def __init__(self, output_dir: str = "results/offloading_comparison"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å¥–åŠ±è®¡ç®—å™¨ï¼ˆç”¨äºè®¡ç®—åŠ æƒæˆæœ¬ï¼‰
        self.reward_calculator = UnifiedRewardCalculator(algorithm="general")
        
        # æƒé‡ï¼ˆä»é…ç½®è·å–ï¼‰
        self.weight_delay = config.rl.reward_weight_delay
        self.weight_energy = config.rl.reward_weight_energy
        
        # ğŸ”§ æ–°å¢ï¼šç¯å¢ƒç¼“å­˜æœºåˆ¶ï¼ˆå‡å°‘é‡å¤åˆå§‹åŒ–ï¼‰
        self._env_cache = {}  # key: scenario_hash, value: env
        self._env_reuse_count = 0
        
        print(f"[INIT] åŠ æƒæˆæœ¬ = {self.weight_delay}Â·æ—¶å»¶ + {self.weight_energy}Â·(èƒ½è€—/1000)")
        print(f"[INIT] ä½¿ç”¨TD3å½’ä¸€åŒ–æ–¹å¼ï¼šèƒ½è€—å•ä½ä»Jè½¬æ¢ä¸ºkJ")
        print(f"[INIT] ç¯å¢ƒç¼“å­˜å·²å¯ç”¨ï¼Œå°†å‡å°‘é‡å¤åˆå§‹åŒ–å¼€é”€")
    
    def load_td3_agent(self, num_vehicles: int = 12) -> TD3Agent:
        """
        åŠ è½½è®­ç»ƒå¥½çš„TD3æ¨¡å‹
        
        Args:
            num_vehicles: è½¦è¾†æ•°é‡ï¼ˆç”¨äºé€‰æ‹©å¯¹åº”çš„æ¨¡å‹ï¼‰
        
        Returns:
            TD3æ™ºèƒ½ä½“
        """
        # ğŸ”§ é‡è¦ä¿®å¤ï¼šTD3æ¨¡å‹æ˜¯ç”¨12è¾†è½¦è®­ç»ƒçš„ï¼Œæ‰€ä»¥çŠ¶æ€ç»´åº¦å¿…é¡»æ˜¯98
        # æ— è®ºå®é™…è¯„ä¼°æ—¶æœ‰å¤šå°‘è¾†è½¦ï¼Œæ¨¡å‹çš„è¾“å…¥ç»´åº¦éƒ½æ˜¯å›ºå®šçš„
        TRAINED_NUM_VEHICLES = 12  # TD3è®­ç»ƒæ—¶çš„è½¦è¾†æ•°
        state_dim = TRAINED_NUM_VEHICLES * 5 + 4 * 5 + 2 * 5 + 8  # å¿…é¡»æ˜¯98ç»´
        action_dim = 16
        
        # å¯¼å…¥TD3Config
        from single_agent.td3 import TD3Config
        import os
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥åŒ¹é…è®­ç»ƒæ—¶çš„é…ç½®
        # è¿™äº›å€¼æ¥è‡ªå›ºå®šæ‹“æ‰‘ä¼˜åŒ–å™¨ï¼ˆè§train_single_agent.pyï¼‰
        os.environ['TD3_HIDDEN_DIM'] = '400'  # è®­ç»ƒæ—¶ä½¿ç”¨çš„éšè—å±‚ç»´åº¦
        os.environ['TD3_ACTOR_LR'] = '1e-4'
        os.environ['TD3_CRITIC_LR'] = '8e-5'
        os.environ['TD3_BATCH_SIZE'] = '256'
        
        # åˆ›å»ºé…ç½®ï¼ˆä¼šè¯»å–ç¯å¢ƒå˜é‡ï¼‰
        td3_config = TD3Config()
        
        # åˆ›å»ºTD3æ™ºèƒ½ä½“ï¼ˆä½¿ç”¨å›ºå®šçš„98ç»´è¾“å…¥ï¼‰
        agent = TD3Agent(state_dim, action_dim, td3_config)
        
        # å°è¯•åŠ è½½æ¨¡å‹
        # æ³¨æ„ï¼šTD3Agent.load_model()ä¼šè‡ªåŠ¨æ·»åŠ _td3.pthåç¼€
        # æ‰€ä»¥è·¯å¾„ä¼ å…¥æ—¶ä¸è¦å¸¦.pthåç¼€
        model_paths = [
            f"../results/single_agent/td3/{num_vehicles}/best_model",  # ä»çˆ¶ç›®å½•çš„ç»“æœä¸­åŠ è½½
            f"results/single_agent/td3/{num_vehicles}/best_model",  # å…¼å®¹åŸæœ‰ä½ç½®
            f"../models/td3/{num_vehicles}/best_model",  # å¦ä¸€ä¸ªå¯èƒ½çš„ä½ç½®
        ]
        
        model_loaded = False
        for model_path in model_paths:
            # æ£€æŸ¥å®é™…æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆåŠ ä¸Šåç¼€ï¼‰
            actual_file = Path(f"{model_path}_td3.pth")
            if actual_file.exists():
                try:
                    agent.load_model(model_path)
                    print(f"[LOAD] æˆåŠŸåŠ è½½TD3æ¨¡å‹: {actual_file}")
                    model_loaded = True
                    break
                except Exception as e:
                    print(f"[WARN] å°è¯•åŠ è½½ {actual_file} å¤±è´¥: {e}")
        
        if not model_loaded:
            print(f"[WARN] æœªæ‰¾åˆ°TD3æ¨¡å‹")
            print(f"[WARN] å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„TD3ç­–ç•¥ï¼ˆå¯èƒ½æ€§èƒ½è¾ƒå·®ï¼‰")
        
        return agent
    
    def evaluate_strategy(
        self,
        strategy,
        num_episodes: int = 50,
        max_steps: int = 100,
        **env_params
    ) -> Dict:
        """
        è¯„ä¼°å•ä¸ªç­–ç•¥çš„æ€§èƒ½
        
        Args:
            strategy: å¸è½½ç­–ç•¥å®ä¾‹
            num_episodes: è¯„ä¼°è½®æ¬¡
            max_steps: æ¯è½®æœ€å¤§æ­¥æ•°
            **env_params: ç¯å¢ƒå‚æ•°ï¼ˆå¯è¦†ç›–é»˜è®¤é…ç½®ï¼‰
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        # ğŸ”§ ä¿®å¤ï¼šæ„å»ºå‚æ•°è¦†ç›–å­—å…¸
        override_scenario = {}
        if env_params:
            # æ˜ å°„ç¯å¢ƒå‚æ•°åˆ°scenarioé…ç½®
            if 'num_vehicles' in env_params:
                override_scenario['num_vehicles'] = env_params['num_vehicles']
            if 'task_arrival_rate' in env_params:
                override_scenario['task_arrival_rate'] = env_params['task_arrival_rate']
            if 'bandwidth_mhz' in env_params:
                override_scenario['bandwidth'] = env_params['bandwidth_mhz']
            if 'data_size_mb' in env_params:
                # æ•°æ®å¤§å°èŒƒå›´ï¼ˆMBè½¬æ¢ä¸ºbytesï¼‰
                size_mb = env_params['data_size_mb']
                override_scenario['data_size_range'] = (size_mb * 0.8 * 1e6, size_mb * 1.2 * 1e6)
            if 'cpu_frequency_ghz' in env_params:
                freq_hz = env_params['cpu_frequency_ghz'] * 1e9
                override_scenario['computation_capacity'] = freq_hz / 1e6  # è½¬æ¢ä¸ºMIPSç­‰æ•ˆå€¼
        
        # ğŸ”§ æ–°å¢ï¼šç¯å¢ƒç¼“å­˜æœºåˆ¶
        # ç”Ÿæˆåœºæ™¯å“ˆå¸Œå€¼ç”¨äºç¼“å­˜
        import hashlib
        scenario_str = json.dumps(override_scenario, sort_keys=True) if override_scenario else "default"
        scenario_hash = hashlib.md5(scenario_str.encode()).hexdigest()[:16]  # ä½¿ç”¨æ›´é•¿çš„å“ˆå¸Œé¿å…ç¢°æ’
        
        # å°è¯•ä»ç¼“å­˜è·å–ç¯å¢ƒ
        if scenario_hash in self._env_cache:
            env = self._env_cache[scenario_hash]
            self._env_reuse_count += 1
            if self._env_reuse_count <= 3:  # åªæ‰“å°å‰å‡ æ¬¡
                print(f"  [CACHE] å¤ç”¨ç¯å¢ƒ (hash={scenario_hash})")
        else:
            # åˆ›å»ºæ–°ç¯å¢ƒå¹¶ç¼“å­˜
            env = SingleAgentTrainingEnvironment("TD3", override_scenario=override_scenario if override_scenario else None)
            self._env_cache[scenario_hash] = env
            print(f"  [NEW] åˆ›å»ºæ–°ç¯å¢ƒ (hash={scenario_hash})")
        
        # å®‰å…¨æ³¨å…¥ç¯å¢ƒå¼•ç”¨
        if hasattr(strategy, 'update_environment'):
            strategy.update_environment(env)
        
        # è¯„ä¼°æŒ‡æ ‡
        episode_costs = []
        episode_delays = []
        episode_energies = []
        episode_completion_rates = []
        
        for ep in range(num_episodes):
            state = env.reset_environment()
            # å®‰å…¨è°ƒç”¨ç­–ç•¥çš„resetæ–¹æ³•
            if hasattr(strategy, 'reset'):
                strategy.reset()
            
            episode_reward = 0
            episode_delay = 0
            episode_energy = 0
            completed_tasks = 0
            total_tasks = 0
            # è®°å½•å®é™…æ­¥æ•°
            actual_steps = 0
            
            for step in range(max_steps):
                # é€‰æ‹©åŠ¨ä½œ
                action = strategy.select_action(state)
                
                # æ„å»ºåŠ¨ä½œå­—å…¸
                actions_dict = env._build_actions_from_vector(action)
                
                # è°ƒè¯•ï¼šç¬¬ä¸€è½®ç¬¬ä¸€æ­¥æ‰“å°åŠ¨ä½œ
                if ep == 0 and step == 0:
                    sim_actions = env._build_simulator_actions(actions_dict)
                    print(f"\n[DEBUG] {strategy.name} åŠ¨ä½œ:")
                    print(f"  åŸå§‹actionå‰3ç»´: {action[:3]}")
                    if sim_actions and 'vehicle_offload_pref' in sim_actions:
                        prefs = sim_actions['vehicle_offload_pref']
                        print(f"  å¸è½½æ¦‚ç‡: local={prefs['local']:.4f}, rsu={prefs['rsu']:.4f}, uav={prefs['uav']:.4f}")
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = env.step(action, state, actions_dict)
                
                # ç´¯ç§¯å¥–åŠ±ï¼ˆreward = -costï¼‰
                episode_reward += reward
                
                state = next_state
                actual_steps = step + 1  # æ›´æ–°å®é™…æ­¥æ•°
                
                if done:
                    break
            
            # ğŸ”§ ä¿®å¤ï¼šæŒ‰å®é™…æ­¥æ•°è®¡ç®—åŠ æƒæˆæœ¬
            # æ³¨æ„ï¼šä½¿ç”¨TD3çš„å½’ä¸€åŒ–æ–¹å¼ï¼Œèƒ½è€—éœ€è¦é™¤ä»¥1000ï¼ˆJè½¬kJï¼‰
            avg_reward_per_step = episode_reward / max(actual_steps, 1)
            # reward = -(2.0*delay + 1.2*energy/1000)ï¼Œæ‰€ä»¥cost = -reward
            weighted_cost = -avg_reward_per_step  # è½¬æ¢ä¸ºæˆæœ¬
            
            # ğŸ”§ ä¿®å¤ï¼šä»ç¯å¢ƒè·å–å®é™…çš„æ—¶å»¶ã€èƒ½è€—å’Œå®Œæˆç‡
            avg_delay = 0.0
            avg_energy = 0.0
            completion_rate = 0.0
            # actual_steps å·²ç»åœ¨å¾ªç¯ä¸­æ›´æ–°ï¼Œä¸éœ€è¦é‡å¤å®šä¹‰
            
            if hasattr(env, 'simulator'):
                try:
                    # æ–¹æ³•1ï¼šä»statsç›´æ¥è·å–ï¼ˆæœ€å‡†ç¡®ï¼‰
                    if hasattr(env.simulator, 'stats'):
                        stats = env.simulator.stats
                        
                        # è·å–å¹³å‡æ—¶å»¶
                        total_delay = stats.get('total_delay', 0.0)
                        completed_tasks = stats.get('completed_tasks', 0)
                        if completed_tasks > 0:
                            avg_delay = total_delay / completed_tasks
                        
                        # è·å–æ€»èƒ½è€—ï¼ˆä»statsä¸­çš„æ­£ç¡®å­—æ®µï¼‰
                        total_energy = stats.get('total_energy', 0.0)
                        # æŒ‰å®é™…æ­¥æ•°å½’ä¸€åŒ–
                        avg_energy = total_energy / max(actual_steps, 1)
                        
                        # è·å–çœŸå®å®Œæˆç‡
                        total_tasks = stats.get('total_tasks', 0)
                        if total_tasks > 0:
                            completion_rate = completed_tasks / total_tasks
                        else:
                            completion_rate = 0.0  # æ— ä»»åŠ¡æ—¶å®Œæˆç‡ä¸º0
                    
                    # æ–¹æ³•2ï¼šå¦‚æœstatsä¸å¯ç”¨ï¼Œä»vehiclesè·å–ï¼ˆlistä¸æ˜¯dictï¼‰
                    elif hasattr(env.simulator, 'vehicles') and isinstance(env.simulator.vehicles, list):
                        total_delay = 0.0
                        task_count = 0
                        for vehicle in env.simulator.vehicles:  # ğŸ”§ ä¿®å¤ï¼švehiclesæ˜¯list
                            if isinstance(vehicle, dict) and 'completed_tasks' in vehicle:
                                for task in vehicle.get('completed_tasks', []):
                                    if isinstance(task, dict):
                                        comp_time = task.get('completion_time', 0)
                                        gen_time = task.get('generation_time', 0)
                                        if comp_time > gen_time:
                                            total_delay += comp_time - gen_time
                                            task_count += 1
                        
                        if task_count > 0:
                            avg_delay = total_delay / task_count
                            # ä¼°ç®—èƒ½è€—ï¼ˆåŸºäºé»˜è®¤å€¼ï¼‰
                            avg_energy = task_count * 15.0 / max(actual_steps, 1)
                            completion_rate = 0.9  # åˆç†çš„é»˜è®¤å€¼
                        else:
                            # å®Œå…¨æ²¡æœ‰æ•°æ®æ—¶çš„å…œåº•
                            avg_delay = weighted_cost / (self.weight_delay + self.weight_energy * 0.5)
                            avg_energy = weighted_cost / (self.weight_energy + self.weight_delay * 0.5) * 100
                            completion_rate = 0.8  # å¤‡ç”¨é»˜è®¤å€¼
                    else:
                        # å…œåº•ä¼°ç®—
                        avg_delay = weighted_cost / (self.weight_delay + self.weight_energy * 0.5)
                        avg_energy = weighted_cost / (self.weight_energy + self.weight_delay * 0.5) * 100
                        completion_rate = 0.8  # å¤‡ç”¨é»˜è®¤å€¼
                        print(f"  âš ï¸ [{strategy.name}] æ— æ³•è·å–çœŸå®æŒ‡æ ‡ï¼Œä½¿ç”¨ä¼°ç®—å€¼")
                        
                except Exception as e:
                    # å¼‚å¸¸å¤„ç†
                    print(f"  âš ï¸ [{strategy.name}] æŒ‡æ ‡é‡‡é›†å¼‚å¸¸: {e}ï¼Œä½¿ç”¨å¤‡ç”¨ä¼°ç®—")
                    avg_delay = weighted_cost / (self.weight_delay + self.weight_energy * 0.5)
                    avg_energy = weighted_cost / (self.weight_energy + self.weight_delay * 0.5) * 100
                    completion_rate = 0.8  # å¤‡ç”¨é»˜è®¤å€¼
            else:
                # æ²¡æœ‰simulatoræ—¶çš„å¤‡ç”¨ä¼°ç®—
                print(f"  âš ï¸ [{strategy.name}] ç¯å¢ƒæ²¡æœ‰simulatorï¼Œä½¿ç”¨ä¼°ç®—å€¼")
                avg_delay = weighted_cost / (self.weight_delay + self.weight_energy * 0.5)
                avg_energy = weighted_cost / (self.weight_energy + self.weight_delay * 0.5) * 100
                completion_rate = 0.5
            
            # é‡æ–°è®¡ç®—åŠ æƒæˆæœ¬ï¼Œç¡®ä¿ä½¿ç”¨TD3å½’ä¸€åŒ–æ–¹å¼
            actual_weighted_cost = 2.0 * avg_delay + 1.2 * (avg_energy / 1000.0)
            
            episode_costs.append(actual_weighted_cost)
            episode_delays.append(avg_delay)
            episode_energies.append(avg_energy)  # ä½¿ç”¨å¹³å‡èƒ½è€—ï¼ˆJï¼‰
            episode_completion_rates.append(completion_rate)
            
            if (ep + 1) % 10 == 0 or ep == 0:
                recent_cost = np.mean(episode_costs[-10:]) if episode_costs else 0
                recent_delay = np.mean(episode_delays[-10:]) if episode_delays else 0
                recent_energy = np.mean(episode_energies[-10:]) if episode_energies else 0
                recent_completion = np.mean(episode_completion_rates[-10:]) if episode_completion_rates else 0
                
                print(f"  Episode {ep+1}/{num_episodes}: "
                      f"Cost={recent_cost:.2f}, "
                      f"Delay={recent_delay:.4f}s, "
                      f"Energy={recent_energy:.2f}J, "
                      f"Completion={recent_completion*100:.1f}%")
        
        # ğŸ”§ æ”¹è¿›ï¼šè¿”å›æ›´è¯¦ç»†çš„è¯„ä¼°ç»“æœ
        return {
            'strategy_name': strategy.name,
            'num_episodes': num_episodes,
            'max_steps': max_steps,
            
            # æ ¸å¿ƒæŒ‡æ ‡
            'avg_weighted_cost': float(np.mean(episode_costs)),
            'std_weighted_cost': float(np.std(episode_costs)),
            'avg_delay': float(np.mean(episode_delays)),
            'std_delay': float(np.std(episode_delays)),
            'avg_energy': float(np.mean(episode_energies)),
            'std_energy': float(np.std(episode_energies)),
            'avg_completion_rate': float(np.mean(episode_completion_rates)),
            'std_completion_rate': float(np.std(episode_completion_rates)),
            
            # åŸå§‹æ•°æ®ï¼ˆç”¨äºåç»­åˆ†æï¼‰
            'episode_costs': [float(c) for c in episode_costs],
            'episode_delays': [float(d) for d in episode_delays],
            'episode_energies': [float(e) for e in episode_energies],
            
            # ç¯å¢ƒå‚æ•°å’Œå…ƒæ•°æ®
            'env_params': env_params,
            'override_scenario': override_scenario if override_scenario else {},
            'scenario_hash': scenario_hash,
            
            # ç»Ÿè®¡ä¿¡æ¯
            'min_cost': float(np.min(episode_costs)),
            'max_cost': float(np.max(episode_costs)),
            'median_cost': float(np.median(episode_costs))
        }
    
    def run_vehicle_sweep(
        self,
        strategies: List,
        vehicle_counts: List[int] = [8, 12, 16, 20, 24],
        num_episodes: int = 50
    ) -> Dict:
        """
        å®éªŒ1: è½¦è¾†æ•°é‡å˜åŒ–å®éªŒ
        
        Args:
            strategies: ç­–ç•¥åˆ—è¡¨
            vehicle_counts: è½¦è¾†æ•°é‡åˆ—è¡¨
            num_episodes: æ¯ä¸ªé…ç½®çš„è¯„ä¼°è½®æ¬¡
        
        Returns:
            å®éªŒç»“æœ
        """
        print("\n" + "="*70)
        print("å®éªŒ1: è½¦è¾†æ•°é‡å˜åŒ–å¯¹æ¯”")
        print("="*70)
        
        results = {}
        
        for strategy in strategies:
            print(f"\n[{strategy.name}]")
            strategy_results = []
            
            for num_vehicles in vehicle_counts:
                print(f"\n  è¯„ä¼° N={num_vehicles} è¾†è½¦...")
                
                # ğŸ”§ ä¿®å¤ï¼šç›´æ¥é€šè¿‡env_paramsä¼ é€’å‚æ•°ï¼Œæ— éœ€ä¿®æ”¹å…¨å±€config
                result = self.evaluate_strategy(
                    strategy,
                    num_episodes=num_episodes,
                    num_vehicles=num_vehicles
                )
                strategy_results.append(result)
                
                # ä½¿ç”¨TD3å½’ä¸€åŒ–æ–¹å¼è®¡ç®—åŠ æƒæˆæœ¬
                calculated_cost = 2.0 * result['avg_delay'] + 1.2 * (result['avg_energy'] / 1000.0)
                print(f"    â†’ åŠ æƒæˆæœ¬: {calculated_cost:.2f} "
                      f"(æ—¶å»¶: {result['avg_delay']:.4f}s, èƒ½è€—: {result['avg_energy']:.2f}J)")
            
            results[strategy.name] = strategy_results
        
        return {
            'experiment': 'vehicle_sweep',
            'parameter': 'num_vehicles',
            'values': vehicle_counts,
            'results': results
        }
    
    def run_task_rate_sweep(
        self,
        strategies: List,
        task_rates: List[float] = [0.3, 0.5, 0.7, 0.9, 1.1],
        num_episodes: int = 50
    ) -> Dict:
        """
        å®éªŒ2: ä»»åŠ¡åˆ°è¾¾ç‡å˜åŒ–å®éªŒ
        
        Args:
            strategies: ç­–ç•¥åˆ—è¡¨
            task_rates: ä»»åŠ¡åˆ°è¾¾ç‡åˆ—è¡¨ï¼ˆä»»åŠ¡/ç§’/è½¦è¾†ï¼‰
            num_episodes: è¯„ä¼°è½®æ¬¡
        
        Returns:
            å®éªŒç»“æœ
        """
        print("\n" + "="*70)
        print("å®éªŒ2: ä»»åŠ¡åˆ°è¾¾ç‡å˜åŒ–å¯¹æ¯”")
        print("="*70)
        
        results = {}
        
        for strategy in strategies:
            print(f"\n[{strategy.name}]")
            strategy_results = []
            
            for rate in task_rates:
                print(f"\n  è¯„ä¼°ä»»åŠ¡ç‡={rate:.1f} tasks/s/vehicle...")
                
                # ğŸ”§ ä¿®å¤ï¼šç›´æ¥é€šè¿‡env_paramsä¼ é€’å‚æ•°
                result = self.evaluate_strategy(
                    strategy,
                    num_episodes=num_episodes,
                    task_arrival_rate=rate
                )
                strategy_results.append(result)
                
                # ä½¿ç”¨TD3å½’ä¸€åŒ–æ–¹å¼è®¡ç®—åŠ æƒæˆæœ¬
                calculated_cost = 2.0 * result['avg_delay'] + 1.2 * (result['avg_energy'] / 1000.0)
                print(f"    â†’ åŠ æƒæˆæœ¬: {calculated_cost:.2f} "
                      f"(æ—¶å»¶: {result['avg_delay']:.4f}s, èƒ½è€—: {result['avg_energy']:.2f}J)")
            
            results[strategy.name] = strategy_results
        
        return {
            'experiment': 'task_rate_sweep',
            'parameter': 'task_arrival_rate',
            'values': task_rates,
            'results': results
        }
    
    def run_bandwidth_sweep(
        self,
        strategies: List,
        bandwidths: List[int] = [10, 20, 30, 40, 50],
        num_episodes: int = 50
    ) -> Dict:
        """
        å®éªŒ3: é€šä¿¡å¸¦å®½å˜åŒ–å®éªŒ
        
        Args:
            strategies: ç­–ç•¥åˆ—è¡¨
            bandwidths: å¸¦å®½åˆ—è¡¨ï¼ˆMHzï¼‰
            num_episodes: è¯„ä¼°è½®æ¬¡
        
        Returns:
            å®éªŒç»“æœ
        """
        print("\n" + "="*70)
        print("å®éªŒ3: é€šä¿¡å¸¦å®½å˜åŒ–å¯¹æ¯”")
        print("="*70)
        
        results = {}
        
        for strategy in strategies:
            print(f"\n[{strategy.name}]")
            strategy_results = []
            
            for bw in bandwidths:
                print(f"\n  è¯„ä¼°å¸¦å®½={bw} MHz...")
                
                result = self.evaluate_strategy(
                    strategy,
                    num_episodes=num_episodes,
                    bandwidth_mhz=bw
                )
                strategy_results.append(result)
                
                # ä½¿ç”¨TD3å½’ä¸€åŒ–æ–¹å¼è®¡ç®—åŠ æƒæˆæœ¬
                calculated_cost = 2.0 * result['avg_delay'] + 1.2 * (result['avg_energy'] / 1000.0)
                print(f"    â†’ åŠ æƒæˆæœ¬: {calculated_cost:.2f} "
                      f"(æ—¶å»¶: {result['avg_delay']:.4f}s, èƒ½è€—: {result['avg_energy']:.2f}J)")
            
            results[strategy.name] = strategy_results
        
        return {
            'experiment': 'bandwidth_sweep',
            'parameter': 'bandwidth_mhz',
            'values': bandwidths,
            'results': results
        }
    
    def run_data_size_sweep(
        self,
        strategies: List,
        data_sizes: List[float] = [0.5, 1.0, 1.5, 2.0, 2.5],
        num_episodes: int = 50
    ) -> Dict:
        """
        å®éªŒ4: ä»»åŠ¡æ•°æ®å¤§å°å˜åŒ–å®éªŒ
        
        Args:
            strategies: ç­–ç•¥åˆ—è¡¨
            data_sizes: æ•°æ®å¤§å°åˆ—è¡¨ï¼ˆMBï¼‰
            num_episodes: è¯„ä¼°è½®æ¬¡
        
        Returns:
            å®éªŒç»“æœ
        """
        print("\n" + "="*70)
        print("å®éªŒ4: ä»»åŠ¡æ•°æ®å¤§å°å˜åŒ–å¯¹æ¯”")
        print("="*70)
        
        results = {}
        
        for strategy in strategies:
            print(f"\n[{strategy.name}]")
            strategy_results = []
            
            for size in data_sizes:
                print(f"\n  è¯„ä¼°æ•°æ®å¤§å°={size:.1f} MB...")
                
                result = self.evaluate_strategy(
                    strategy,
                    num_episodes=num_episodes,
                    data_size_mb=size
                )
                strategy_results.append(result)
                
                # ä½¿ç”¨TD3å½’ä¸€åŒ–æ–¹å¼è®¡ç®—åŠ æƒæˆæœ¬
                calculated_cost = 2.0 * result['avg_delay'] + 1.2 * (result['avg_energy'] / 1000.0)
                print(f"    â†’ åŠ æƒæˆæœ¬: {calculated_cost:.2f} "
                      f"(æ—¶å»¶: {result['avg_delay']:.4f}s, èƒ½è€—: {result['avg_energy']:.2f}J)")
            
            results[strategy.name] = strategy_results
        
        return {
            'experiment': 'data_size_sweep',
            'parameter': 'data_size_mb',
            'values': data_sizes,
            'results': results
        }
    
    def run_cpu_frequency_sweep(
        self,
        strategies: List,
        cpu_frequencies: List[float] = [1.5, 2.0, 2.5, 3.0, 3.5],
        num_episodes: int = 50
    ) -> Dict:
        """
        å®éªŒ5: è®¡ç®—èµ„æºï¼ˆCPUé¢‘ç‡ï¼‰å˜åŒ–å®éªŒ
        
        Args:
            strategies: ç­–ç•¥åˆ—è¡¨
            cpu_frequencies: CPUé¢‘ç‡åˆ—è¡¨ï¼ˆGHzï¼‰
            num_episodes: è¯„ä¼°è½®æ¬¡
        
        Returns:
            å®éªŒç»“æœ
        """
        print("\n" + "="*70)
        print("å®éªŒ5: è®¡ç®—èµ„æºï¼ˆCPUé¢‘ç‡ï¼‰å˜åŒ–å¯¹æ¯”")
        print("="*70)
        
        results = {}
        
        for strategy in strategies:
            print(f"\n[{strategy.name}]")
            strategy_results = []
            
            for freq in cpu_frequencies:
                print(f"\n  è¯„ä¼°CPUé¢‘ç‡={freq:.1f} GHz...")
                
                result = self.evaluate_strategy(
                    strategy,
                    num_episodes=num_episodes,
                    cpu_frequency_ghz=freq
                )
                strategy_results.append(result)
                
                # ä½¿ç”¨TD3å½’ä¸€åŒ–æ–¹å¼è®¡ç®—åŠ æƒæˆæœ¬
                calculated_cost = 2.0 * result['avg_delay'] + 1.2 * (result['avg_energy'] / 1000.0)
                print(f"    â†’ åŠ æƒæˆæœ¬: {calculated_cost:.2f} "
                      f"(æ—¶å»¶: {result['avg_delay']:.4f}s, èƒ½è€—: {result['avg_energy']:.2f}J)")
            
            results[strategy.name] = strategy_results
        
        return {
            'experiment': 'cpu_frequency_sweep',
            'parameter': 'cpu_frequency_ghz',
            'values': cpu_frequencies,
            'results': results
        }
    
    def save_results(self, results: Dict, filename: str):
        """ä¿å­˜å®éªŒç»“æœ"""
        output_file = self.output_dir / filename
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\n[SAVED] {output_file}")


def main():
    parser = argparse.ArgumentParser(description="å¸è½½ç­–ç•¥å¯¹æ¯”å®éªŒ")
    parser.add_argument('--mode', type=str, default='all',
                        choices=['all', 'vehicle', 'task_rate', 'bandwidth', 'data_size', 'cpu'],
                        help="å®éªŒæ¨¡å¼")
    parser.add_argument('--episodes', type=int, default=50,
                        help="æ¯ä¸ªé…ç½®çš„è¯„ä¼°è½®æ¬¡")
    parser.add_argument('--quick', action='store_true',
                        help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆå‡å°‘å‚æ•°ç‚¹å’Œè½®æ¬¡ï¼‰")
    parser.add_argument('--train-td3', action='store_true',
                        help="åœ¨å¯¹æ¯”å®éªŒå‰å…ˆè®­ç»ƒTD3æ¨¡å‹")
    parser.add_argument('--td3-episodes', type=int, default=200,
                        help="TD3è®­ç»ƒè½®æ¬¡ï¼ˆé»˜è®¤200ï¼‰")
    
    args = parser.parse_args()
    
    print("="*70)
    print("å¸è½½ç­–ç•¥å¯¹æ¯”å®éªŒ")
    print("="*70)
    print(f"æ¨¡å¼: {args.mode}")
    print(f"è½®æ¬¡: {args.episodes}")
    print(f"å¿«é€Ÿæµ‹è¯•: {args.quick}")
    print(f"è®­ç»ƒTD3: {args.train_td3}")
    if args.train_td3:
        print(f"TD3è®­ç»ƒè½®æ¬¡: {args.td3_episodes}")
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = OffloadingComparisonExperiment()
    
    # å¦‚æœéœ€è¦ï¼Œå…ˆè®­ç»ƒTD3æ¨¡å‹
    if args.train_td3:
        print("\n" + "="*70)
        print("æ­¥éª¤1: è®­ç»ƒTD3æ¨¡å‹")
        print("="*70)
        
        # å¯¼å…¥è®­ç»ƒæ¨¡å—
        import subprocess
        import sys
        
        # è®¾ç½®è½¦è¾†æ•°é‡ï¼ˆä½¿ç”¨æ ‡å‡†é…ç½®ï¼‰
        num_vehicles = 12
        
        print(f"\n[TD3è®­ç»ƒ] å¼€å§‹è®­ç»ƒTD3æ¨¡å‹ (è½¦è¾†æ•°={num_vehicles}, è½®æ¬¡={args.td3_episodes})")
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤
        train_cmd = [
            sys.executable,  # Pythonè§£é‡Šå™¨
            str(Path(__file__).parent.parent / "train_single_agent.py"),  # ä½¿ç”¨ç»å¯¹è·¯å¾„
            "--algorithm", "TD3",
            "--num-vehicles", str(num_vehicles),
            "--episodes", str(args.td3_episodes),
            # ä¸ä½¿ç”¨realtime-viså‚æ•°ï¼Œé»˜è®¤å°±æ˜¯å…³é—­çš„
        ]
        
        # æ‰§è¡Œè®­ç»ƒ
        try:
            subprocess.run(train_cmd, check=True, cwd=Path(__file__).parent)
            print("\n[TD3è®­ç»ƒ] âœ… TD3æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        except subprocess.CalledProcessError as e:
            print(f"\n[TD3è®­ç»ƒ] âŒ TD3è®­ç»ƒå¤±è´¥: {e}")
            print("[TD3è®­ç»ƒ] å°†ç»§ç»­æ‰§è¡Œå¯¹æ¯”å®éªŒï¼ˆä¸å«HybridDRLï¼‰")
        except Exception as e:
            print(f"\n[TD3è®­ç»ƒ] âŒ TD3è®­ç»ƒå‡ºé”™: {e}")
            print("[TD3è®­ç»ƒ] å°†ç»§ç»­æ‰§è¡Œå¯¹æ¯”å®éªŒï¼ˆä¸å«HybridDRLï¼‰")
        
        print("\n" + "="*70)
        print("æ­¥éª¤2: è¿è¡Œå¯¹æ¯”å®éªŒ")
        print("="*70)
    
    # åˆ›å»ºç­–ç•¥å®ä¾‹
    print("\n[INIT] åˆå§‹åŒ–ç­–ç•¥...")
    
    # æ ¸å¿ƒå¯¹æ¯”ç­–ç•¥ï¼ˆæ ¹æ®è®ºæ–‡éœ€æ±‚ç²¾ç®€ï¼‰
    strategies = [
        # åŸºå‡†ç­–ç•¥
        create_offloading_strategy("LocalOnly"),  # çº¯æœ¬åœ°è®¡ç®—åŸºå‡†
        create_offloading_strategy("RSUOnly", selection_mode="load_balance"),  # ä¼ ç»ŸMECåŸºå‡†
        
        # å¯å‘å¼ç­–ç•¥
        create_offloading_strategy("LoadBalance"),  # è´Ÿè½½å‡è¡¡ï¼ˆæœ€ä½³å¯å‘å¼ï¼‰
        create_offloading_strategy("Random"),  # éšæœºç­–ç•¥ï¼ˆå¯¹ç…§ç»„ï¼‰
    ]
    
    # åŠ è½½TD3æ¨¡å‹
    try:
        td3_agent = experiment.load_td3_agent(num_vehicles=12)
        
        # 1. å®Œæ•´TD3ç­–ç•¥
        td3_strategy = create_offloading_strategy("HybridDRL", td3_agent=td3_agent)
        td3_strategy.name = "TD3"  # ç®€åŒ–åç§°
        strategies.append(td3_strategy)
        
        # 2. æ— è¿ç§»çš„TD3ç­–ç•¥ï¼ˆæ¶ˆèå®éªŒï¼‰
        from offloading_strategies import HybridDRLStrategy
        
        class NoMigrationTD3Strategy(HybridDRLStrategy):
            """æ— è¿ç§»çš„TD3ç­–ç•¥ï¼ˆæ¶ˆèå®éªŒï¼‰"""
            def __init__(self, td3_agent):
                super().__init__(td3_agent)
                self.name = "TD3-NoMig"  # TD3 without Migration
                
            def select_action(self, state: np.ndarray) -> np.ndarray:
                """ä½¿ç”¨TD3ä½†ç¦ç”¨è¿ç§»"""
                # å…ˆé€‚é…çŠ¶æ€ç»´åº¦ï¼ˆä½¿ç”¨çˆ¶ç±»çš„æ–¹æ³•ï¼‰
                adapted_state = self._adapt_state_dimension(state)
                
                # è·å–TD3çš„åŸå§‹åŠ¨ä½œ
                action = self.td3_agent.select_action(adapted_state, training=False)
                
                # ä¿®æ”¹è¿ç§»ç›¸å…³çš„æ§åˆ¶å‚æ•°
                # action[9:16] æ˜¯æ§åˆ¶å‚æ•°ï¼Œå…¶ä¸­åŒ…å«è¿ç§»é˜ˆå€¼ç­‰
                # å°†è¿ç§»æ¦‚ç‡è®¾ç½®ä¸º-5ï¼ˆç»è¿‡sigmoidåæ¥è¿‘0ï¼‰
                action[10] = -5.0  # è¿ç§»é˜ˆå€¼è®¾ä¸ºæä½å€¼ï¼Œç¦ç”¨è¿ç§»
                action[11] = -5.0  # è¿ç§»ç‡è®¾ä¸ºæä½å€¼
                
                return action
        
        no_mig_strategy = NoMigrationTD3Strategy(td3_agent)
        strategies.append(no_mig_strategy)
        
        print(f"[INFO] æˆåŠŸåŠ è½½TD3ç­–ç•¥ï¼ˆå«æ¶ˆèç‰ˆæœ¬ï¼‰ï¼Œå…±{len(strategies)}ç§ç­–ç•¥")
        print(f"[INFO] ç­–ç•¥åˆ—è¡¨: {[s.name for s in strategies]}")
        
    except Exception as e:
        print(f"[WARN] æ— æ³•åŠ è½½TD3ç­–ç•¥: {e}")
        print(f"[INFO] å°†å¯¹æ¯”{len(strategies)}ç§åŸºç¡€ç­–ç•¥")
    
    # è°ƒæ•´å‚æ•°ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
    if args.quick:
        args.episodes = 20
        vehicle_counts = [8, 12, 16]
        task_rates = [0.5, 0.7, 0.9]
        bandwidths = [10, 20, 30]
        data_sizes = [0.5, 1.0, 1.5]
        cpu_frequencies = [1.5, 2.0, 2.5]
    else:
        vehicle_counts = [8, 12, 16, 20, 24]
        task_rates = [0.3, 0.5, 0.7, 0.9, 1.1]
        bandwidths = [10, 20, 30, 40, 50]
        data_sizes = [0.5, 1.0, 1.5, 2.0, 2.5]
        cpu_frequencies = [1.5, 2.0, 2.5, 3.0, 3.5]
    
    # è¿è¡Œå®éªŒ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    all_results = {}
    
    if args.mode in ['all', 'vehicle']:
        result = experiment.run_vehicle_sweep(strategies, vehicle_counts, args.episodes)
        all_results['vehicle_sweep'] = result
        experiment.save_results(result, f'vehicle_sweep_{timestamp}.json')
    
    if args.mode in ['all', 'task_rate']:
        result = experiment.run_task_rate_sweep(strategies, task_rates, args.episodes)
        all_results['task_rate_sweep'] = result
        experiment.save_results(result, f'task_rate_sweep_{timestamp}.json')
    
    if args.mode in ['all', 'bandwidth']:
        result = experiment.run_bandwidth_sweep(strategies, bandwidths, args.episodes)
        all_results['bandwidth_sweep'] = result
        experiment.save_results(result, f'bandwidth_sweep_{timestamp}.json')
    
    if args.mode in ['all', 'data_size']:
        result = experiment.run_data_size_sweep(strategies, data_sizes, args.episodes)
        all_results['data_size_sweep'] = result
        experiment.save_results(result, f'data_size_sweep_{timestamp}.json')
    
    if args.mode in ['all', 'cpu']:
        result = experiment.run_cpu_frequency_sweep(strategies, cpu_frequencies, args.episodes)
        all_results['cpu_frequency_sweep'] = result
        experiment.save_results(result, f'cpu_frequency_sweep_{timestamp}.json')
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    if args.mode == 'all':
        experiment.save_results(all_results, f'all_experiments_{timestamp}.json')
    
    # ğŸ”§ æ–°å¢ï¼šæ‰“å°å®éªŒæ€»ç»“
    print("\n" + "="*70)
    print("å®éªŒå®Œæˆï¼")
    print("="*70)
    
    # ç¯å¢ƒç¼“å­˜ç»Ÿè®¡
    if hasattr(experiment, '_env_cache'):
        print(f"\n[æ€§èƒ½ç»Ÿè®¡]")
        print(f"  ç¯å¢ƒç¼“å­˜æ•°: {len(experiment._env_cache)}")
        print(f"  ç¯å¢ƒå¤ç”¨æ¬¡æ•°: {experiment._env_reuse_count}")
        cache_efficiency = (experiment._env_reuse_count / max(1, experiment._env_reuse_count + len(experiment._env_cache))) * 100
        print(f"  ç¼“å­˜å‘½ä¸­ç‡: {cache_efficiency:.1f}%")
    
    # å®éªŒè§„æ¨¡ç»Ÿè®¡
    total_evaluations = 0
    for exp_name, exp_data in all_results.items():
        if 'results' in exp_data:
            num_strategies = len(exp_data['results'])
            num_params = len(exp_data.get('values', []))
            total_evaluations += num_strategies * num_params
    
    if total_evaluations > 0:
        print(f"\n[å®éªŒè§„æ¨¡]")
        print(f"  æ€»è¯„ä¼°æ¬¡æ•°: {total_evaluations}")
        print(f"  å®éªŒç»´åº¦: {len(all_results)}")
        print(f"  ç­–ç•¥æ•°é‡: {len(strategies)}")
    
    print("\n[è¾“å‡ºæ–‡ä»¶]")
    print(f"  ç»“æœç›®å½•: {experiment.output_dir}")
    print("="*70)


if __name__ == "__main__":
    main()

