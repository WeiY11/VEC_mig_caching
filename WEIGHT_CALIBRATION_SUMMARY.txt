================================================================================
权重校准总结 - 2024-12-02
================================================================================

【诊断结果】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
问题：修复归一化目标后，系统性能正常但奖励仍偏低

实际系统性能（500 episodes）：
  • 平均延迟: 1.51s (98.8%低于目标2.0s) ✅
  • 平均能耗: 925J (100%低于目标1500J) ✅
  • 完成率: 正常

奖励组成分析：
  • 归一化延迟成本: 0.0002 ≈ 0
  • 归一化能耗成本: 0.0000 = 0
  • 估算基础成本: 0.0002
  • 实际奖励均值: -3.6646
  • 差值: 3.66 ← 这部分来自辅助惩罚项！

根本原因：
  辅助惩罚项（cache、queue、remote_reject、offload等）权重过高，
  主导了总奖励，掩盖了核心优化目标（delay + energy）


【校准方案】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

✅ 策略1: 平衡核心权重
  • reward_weight_delay: 0.5 (保持不变)
  • reward_weight_energy: 0.3 → 0.5 (提升67%，平衡权重)

✅ 策略2: 大幅降低辅助惩罚项权重
  • reward_penalty_dropped: 0.01 → 0.005 (-50%)
  • reward_weight_completion_gap: 0.05 → 0.02 (-60%)
  • reward_weight_loss_ratio: 0.1 → 0.05 (-50%)
  • reward_weight_cache_pressure: 0.1 → 0.05 (-50%)
  • reward_weight_cache_bonus: 0.3 → 0.15 (-50%)
  • reward_weight_queue_overload: 0.05 → 0.02 (-60%)
  • reward_weight_cache: 0.2 → 0.1 (-50%)
  • reward_weight_migration: 0.1 → 0.05 (-50%)
  • reward_weight_joint: 0.05 → 0.02 (-60%)
  • reward_weight_remote_reject: 0.15 → 0.08 (-47%)
  • reward_weight_offload_bonus: 0.1 → 0.05 (-50%)

✅ 策略3: 微调归一化目标（产生适度学习信号）
  当前问题：98.8%的episode延迟低于目标 → 几乎无梯度信号
  
  调整策略：降低目标到60分位数，让40%的episode产生正向成本
  
  • latency_target: 2.0 → 1.6s (60分位数≈1.56s)
  • latency_upper_tolerance: 3.0 → 2.5s
  • energy_target: 1500 → 1000J (60分位数≈955J)
  • energy_upper_tolerance: 3000 → 1800J

理论依据：
  √ 60/40比例产生适度梯度信号
  √ 避免所有episode都"太好"导致无学习信号
  √ 避免所有episode都"太差"导致惩罚饱和


【预期效果】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

预期奖励范围: -1.5 到 -2.5 (vs 优化前-1.87)

成分占比预估：
  • 核心成本 (delay+energy): ~40-50% (当前<1%)
  • 辅助惩罚项: ~50-60% (当前>99%)

训练稳定性：
  • 变异系数: <0.2 (当前0.31)
  • 前后期改进: >5% (当前-2.97%)

性能目标：
  • 与优化前持平或略优: -1.5 到 -2.0
  • 收敛性改善


【修改文件清单】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. config/system_config.py (Lines 236-277)
   - 调整14个奖励权重
   - 调整4个归一化目标值

2. train_single_agent.py (Lines 349-358)
   - 更新update_reward_targets()默认值

3. analyze_reward_components.py (新建)
   - 诊断脚本，分析奖励组成


【下一步】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

启动重新训练：
  python train_single_agent.py --algorithm OPTIMIZED_TD3 --num-episodes 500

监控关键指标：
  • Episode奖励: 目标范围 -1.5 到 -2.5
  • 延迟: 保持在1.5s左右
  • 能耗: 保持在900-1000J
  • 完成率: 保持99%+

训练完成后分析：
  python analyze_optimized_results.py
  python analyze_reward_components.py


【风险与备选方案】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

风险1: 辅助惩罚降低过多，系统性能退化
  → 观察完成率和丢包率，若显著下降则回调部分惩罚权重

风险2: 归一化目标过低，产生过多惩罚
  → 若平均奖励<-5.0，则回调到1.8s/1200J

风险3: 学习信号仍不足
  → 进一步降低归一化目标到50分位数（1.5s/900J）

================================================================================
