"""
MATD3æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³• - å¯¹åº”è®ºæ–‡æ ¸å¿ƒç®—æ³•
å¤šæ™ºèƒ½ä½“Twin Delayed DDPGç®—æ³•å®ç°
"""
# æ€§èƒ½ä¼˜åŒ– - å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰
try:
    from tools.performance_optimization import OPTIMIZED_BATCH_SIZES
except ImportError:
    OPTIMIZED_BATCH_SIZES = {'MATD3': 256}  # é»˜è®¤å€¼

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional
from collections import deque
import random

from config import config


class Actor(nn.Module):
    """Actorç½‘ç»œ - ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(Actor, self).__init__()
        
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        
        # åˆå§‹åŒ–æƒé‡
        self.fc3.weight.data.uniform_(-3e-3, 3e-3)
        self.fc3.bias.data.uniform_(-3e-3, 3e-3)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = torch.tanh(self.fc3(x))  # è¾“å‡ºèŒƒå›´[-1, 1]
        return x


class Critic(nn.Module):
    """Criticç½‘ç»œ - ä»·å€¼ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(Critic, self).__init__()
        
        # Q1ç½‘ç»œ
        self.fc1_q1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2_q1 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3_q1 = nn.Linear(hidden_dim, 1)
        
        # Q2ç½‘ç»œ (Twinç½‘ç»œ)
        self.fc1_q2 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2_q2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3_q2 = nn.Linear(hidden_dim, 1)
    
    def forward(self, state, action):
        x = torch.cat([state, action], dim=-1)
        
        # Q1
        q1 = F.relu(self.fc1_q1(x))
        q1 = F.relu(self.fc2_q1(q1))
        q1 = self.fc3_q1(q1)
        
        # Q2
        q2 = F.relu(self.fc1_q2(x))
        q2 = F.relu(self.fc2_q2(q2))
        q2 = self.fc3_q2(q2)
        
        return q1, q2
    
    def q1(self, state, action):
        x = torch.cat([state, action], dim=-1)
        q1 = F.relu(self.fc1_q1(x))
        q1 = F.relu(self.fc2_q1(q1))
        q1 = self.fc3_q1(q1)
        return q1


class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size: int):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done
    
    def __len__(self):
        return len(self.buffer)


class MATD3Agent:
    """
    å¤šæ™ºèƒ½ä½“TD3ç®—æ³•å®ç°
    """
    
    def __init__(self, agent_id: str, state_dim: int, action_dim: int):
        self.agent_id = agent_id
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # ç½‘ç»œå‚æ•°
        self.hidden_dim = config.rl.hidden_dim
        self.actor_lr = config.rl.actor_lr
        self.critic_lr = config.rl.critic_lr
        self.tau = config.rl.tau
        self.gamma = config.rl.gamma
        
        # ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å° - å¯¹åº”è®ºæ–‡ä¸­çš„æ‰¹é‡å¤„ç†å‚æ•°
        self.optimized_batch_size = OPTIMIZED_BATCH_SIZES.get('MATD3', config.rl.batch_size)
        print(f"ğŸš€ {agent_id} ä½¿ç”¨ä¼˜åŒ–æ‰¹æ¬¡å¤§å°: {self.optimized_batch_size}")
        
        # å™ªå£°å‚æ•°
        self.policy_noise = config.rl.policy_noise
        self.noise_clip = config.rl.noise_clip
        self.policy_delay = config.rl.policy_delay
        
        # ç½‘ç»œåˆå§‹åŒ– - ä½¿ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.actor = Actor(state_dim, action_dim, self.hidden_dim).to(self.device)
        self.actor_target = Actor(state_dim, action_dim, self.hidden_dim).to(self.device)
        self.critic = Critic(state_dim, action_dim, self.hidden_dim).to(self.device)
        self.critic_target = Critic(state_dim, action_dim, self.hidden_dim).to(self.device)
        
        # å¤åˆ¶ç›®æ ‡ç½‘ç»œæƒé‡
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.critic_lr)
        
        # ç»éªŒå›æ”¾
        self.replay_buffer = ReplayBuffer(config.rl.buffer_size)
        
        # è®­ç»ƒè®¡æ•°
        self.total_it = 0
    
    def select_action(self, state: np.ndarray, add_noise: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ - ä½¿ç”¨GPUåŠ é€Ÿ"""
        state_tensor = torch.FloatTensor(state.reshape(1, -1)).to(self.device)
        
        with torch.no_grad():
            action_tensor = self.actor(state_tensor)
            action = action_tensor.cpu().data.numpy().flatten()
        
        if add_noise:
            noise = np.random.normal(0, config.rl.exploration_noise, size=self.action_dim)
            action = action + noise
            action = np.clip(action, -1, 1)
        
        return action
    
    def train(self, batch_size: Optional[int] = None):
        """è®­ç»ƒæ™ºèƒ½ä½“ - ä½¿ç”¨ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°"""
        # ä½¿ç”¨ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°ï¼Œæé«˜GPUåˆ©ç”¨ç‡
        batch_size_val = batch_size if batch_size is not None else self.optimized_batch_size
        if len(self.replay_buffer) < batch_size_val:
            return {}
        
        self.total_it += 1
        
        # é‡‡æ ·ç»éªŒ
        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size_val)
        
        # ç§»åŠ¨åˆ°GPUè¿›è¡ŒåŠ é€Ÿè®¡ç®—
        state = torch.FloatTensor(state).to(self.device)
        action = torch.FloatTensor(action).to(self.device)
        reward = torch.FloatTensor(reward).unsqueeze(1).to(self.device)
        next_state = torch.FloatTensor(next_state).to(self.device)
        done = torch.FloatTensor(done).unsqueeze(1).to(self.device)
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            # ç›®æ ‡åŠ¨ä½œåŠ å™ªå£°
            noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)
            next_action = (self.actor_target(next_state) + noise).clamp(-1, 1)
            
            # è®¡ç®—ç›®æ ‡Qå€¼ (Twinç½‘ç»œå–æœ€å°å€¼)
            target_q1, target_q2 = self.critic_target(next_state, next_action)
            target_q = torch.min(target_q1, target_q2)
            target_q = reward + (1 - done) * self.gamma * target_q
        
        # æ›´æ–°Criticç½‘ç»œ
        current_q1, current_q2 = self.critic(state, action)
        
        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # å»¶è¿Ÿæ›´æ–°Actorç½‘ç»œ
        if self.total_it % self.policy_delay == 0:
            # æ›´æ–°Actorç½‘ç»œ
            actor_loss = -self.critic.q1(state, self.actor(state)).mean()
            
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self._soft_update(self.actor_target, self.actor, self.tau)
            self._soft_update(self.critic_target, self.critic, self.tau)
            
            return {
                'critic_loss': critic_loss.item(),
                'actor_loss': actor_loss.item()
            }
        else:
            return {
                'critic_loss': critic_loss.item(),
                'actor_loss': 0.0
            }
    
    def _soft_update(self, target, source, tau):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
    
    def store_transition(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
        }, filepath)
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])


class MATD3Environment:
    """
    MATD3å¤šæ™ºèƒ½ä½“ç¯å¢ƒ
    ç®¡ç†å¤šä¸ªæ™ºèƒ½ä½“çš„è®­ç»ƒè¿‡ç¨‹
    """
    
    def __init__(self):
        # æ™ºèƒ½ä½“é…ç½®
        self.num_agents = config.rl.num_agents
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç»´åº¦ (ç®€åŒ–è®¾è®¡)
        self.state_dim = 20  # èŠ‚ç‚¹çŠ¶æ€ç‰¹å¾ç»´åº¦
        self.action_dim = 10  # å†³ç­–åŠ¨ä½œç»´åº¦
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agents: Dict[str, MATD3Agent] = {}
        agent_types = ['vehicle_agent', 'rsu_agent', 'uav_agent']
        
        for i, agent_type in enumerate(agent_types):
            if i < self.num_agents:
                self.agents[agent_type] = MATD3Agent(
                    agent_id=agent_type,
                    state_dim=self.state_dim,
                    action_dim=self.action_dim
                )
        
        # å¥–åŠ±æƒé‡ - å¯¹åº”è®ºæ–‡ç›®æ ‡å‡½æ•°
        self.reward_weights = {
            'delay': config.rl.reward_weight_delay,     # Ï‰_T
            'energy': config.rl.reward_weight_energy,   # Ï‰_E  
            'loss': config.rl.reward_weight_loss        # Ï‰_D
        }
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_losses = []
    
    def get_state_vector(self, node_states: Dict, system_metrics: Dict) -> Dict[str, np.ndarray]:
        """
        æ„å»ºæ™ºèƒ½ä½“çŠ¶æ€å‘é‡ - æ”¹è¿›ç‰ˆæœ¬ï¼ŒåŒ…å«æ›´ä¸°å¯Œçš„çŠ¶æ€ä¿¡æ¯
        æ¯ä¸ªæ™ºèƒ½ä½“è§‚å¯Ÿä¸åŒçš„çŠ¶æ€ç‰¹å¾
        """
        states = {}
        
        # è½¦è¾†æ™ºèƒ½ä½“çŠ¶æ€ - å…³æ³¨æœ¬åœ°è®¡ç®—å’Œå¸è½½å†³ç­–
        if 'vehicle_agent' in self.agents:
            # åŸºç¡€ç³»ç»ŸçŠ¶æ€
            vehicle_features = [
                np.clip(system_metrics.get('avg_task_delay', 0.0) / 2.0, 0, 1),  # å½’ä¸€åŒ–æ—¶å»¶
                np.clip(system_metrics.get('total_energy_consumption', 0.0) / 500.0, 0, 1),  # å½’ä¸€åŒ–èƒ½è€—
                system_metrics.get('data_loss_rate', 0.0),  # æ•°æ®ä¸¢å¤±ç‡
                system_metrics.get('task_completion_rate', 0.0),  # ä»»åŠ¡å®Œæˆç‡
            ]
            
            # è½¦è¾†ç‰¹å®šçŠ¶æ€
            vehicle_nodes = [s for s in node_states.values() if hasattr(s, 'node_type') and s.node_type.value == 'vehicle']
            if vehicle_nodes:
                avg_vehicle_load = np.mean([s.load_factor for s in vehicle_nodes])
                avg_vehicle_queue = np.mean([len(getattr(s, 'task_queue', [])) for s in vehicle_nodes])
                vehicle_count = len(vehicle_nodes)
            else:
                avg_vehicle_load = 0.0
                avg_vehicle_queue = 0.0
                vehicle_count = 0
            
            vehicle_features.extend([
                np.clip(avg_vehicle_load, 0, 1),  # å¹³å‡è½¦è¾†è´Ÿè½½
                np.clip(avg_vehicle_queue / 10.0, 0, 1),  # å¹³å‡é˜Ÿåˆ—é•¿åº¦
                np.clip(vehicle_count / 20.0, 0, 1),  # è½¦è¾†æ•°é‡æ¯”ä¾‹
            ])
            
            # RSUå¯ç”¨æ€§çŠ¶æ€
            rsu_nodes = [s for s in node_states.values() if hasattr(s, 'node_type') and s.node_type.value == 'rsu']
            if rsu_nodes:
                avg_rsu_load = np.mean([s.load_factor for s in rsu_nodes])
                rsu_availability = sum(1 for s in rsu_nodes if s.load_factor < 0.8) / len(rsu_nodes)
            else:
                avg_rsu_load = 0.0
                rsu_availability = 0.0
            
            vehicle_features.extend([
                np.clip(avg_rsu_load, 0, 1),  # RSUå¹³å‡è´Ÿè½½
                rsu_availability,  # RSUå¯ç”¨æ€§
                system_metrics.get('cache_hit_rate', 0.0),  # ç¼“å­˜å‘½ä¸­ç‡
            ])
            
            # UAVå¯ç”¨æ€§çŠ¶æ€
            uav_nodes = [s for s in node_states.values() if hasattr(s, 'node_type') and s.node_type.value == 'uav']
            if uav_nodes:
                avg_uav_battery = np.mean([getattr(s, 'battery_level', 1.0) for s in uav_nodes])
                avg_uav_load = np.mean([s.load_factor for s in uav_nodes])
                uav_availability = sum(1 for s in uav_nodes if getattr(s, 'battery_level', 1.0) > 0.3) / len(uav_nodes)
            else:
                avg_uav_battery = 1.0
                avg_uav_load = 0.0
                uav_availability = 0.0
            
            vehicle_features.extend([
                avg_uav_battery,  # UAVå¹³å‡ç”µé‡
                np.clip(avg_uav_load, 0, 1),  # UAVå¹³å‡è´Ÿè½½
                uav_availability,  # UAVå¯ç”¨æ€§
            ])
            
            # ç½‘ç»œçŠ¶æ€
            vehicle_features.extend([
                system_metrics.get('avg_bandwidth_utilization', 0.0),  # å¸¦å®½åˆ©ç”¨ç‡
                system_metrics.get('migration_success_rate', 0.0),  # ä½¿ç”¨çœŸå®çš„è¿ç§»æˆåŠŸç‡
                np.random.uniform(0.8, 1.0),  # ä¿¡é“è´¨é‡ï¼ˆç®€åŒ–ï¼‰
                np.clip(system_metrics.get('system_load_ratio', 0.0), 0, 1),  # ç³»ç»Ÿè´Ÿè½½æ¯”
            ])
            
            # ç¡®ä¿ç»´åº¦æ­£ç¡®
            while len(vehicle_features) < self.state_dim:
                vehicle_features.append(0.0)
            
            states['vehicle_agent'] = np.array(vehicle_features[:self.state_dim], dtype=np.float32)
        
        # RSUæ™ºèƒ½ä½“çŠ¶æ€
        if 'rsu_agent' in self.agents:
            # å®‰å…¨è®¡ç®—RSUå¹³å‡è´Ÿè½½ï¼Œé¿å…ç©ºåˆ—è¡¨å¯¼è‡´çš„Noneè¿”å›å€¼
            rsu_load_factors = [s.load_factor for s in node_states.values() if s.node_type.value == 'rsu']
            avg_rsu_load = np.mean(rsu_load_factors) if rsu_load_factors else 0.0
            
            rsu_features = [
                len([s for s in node_states.values() if s.node_type.value == 'rsu']) / 10.0,  # RSUæ•°é‡
                float(avg_rsu_load),  # å¹³å‡è´Ÿè½½
                system_metrics.get('cache_hit_rate', 0.0),  # ç¼“å­˜å‘½ä¸­ç‡
                system_metrics.get('migration_success_rate', 0.0),  # è¿ç§»æˆåŠŸç‡
            ]
            
            while len(rsu_features) < self.state_dim:
                rsu_features.append(0.0)
            
            states['rsu_agent'] = np.array(rsu_features[:self.state_dim], dtype=np.float32)
        
        # UAVæ™ºèƒ½ä½“çŠ¶æ€
        if 'uav_agent' in self.agents:
            # å®‰å…¨è®¡ç®—UAVå¹³å‡ç”µé‡å’Œè´Ÿè½½ï¼Œé¿å…ç©ºåˆ—è¡¨å¯¼è‡´çš„Noneè¿”å›å€¼
            uav_battery_levels = [getattr(s, 'battery_level', 1.0) for s in node_states.values() if s.node_type.value == 'uav']
            avg_uav_battery = np.mean(uav_battery_levels) if uav_battery_levels else 1.0
            
            uav_load_factors = [s.load_factor for s in node_states.values() if s.node_type.value == 'uav']
            avg_uav_load = np.mean(uav_load_factors) if uav_load_factors else 0.0
            
            uav_features = [
                len([s for s in node_states.values() if s.node_type.value == 'uav']) / 5.0,  # UAVæ•°é‡
                float(avg_uav_battery),  # å¹³å‡ç”µé‡
                float(avg_uav_load),  # å¹³å‡è´Ÿè½½
                system_metrics.get('avg_bandwidth_utilization', 0.0),  # å¸¦å®½åˆ©ç”¨ç‡
            ]
            
            while len(uav_features) < self.state_dim:
                uav_features.append(0.0)
            
            states['uav_agent'] = np.array(uav_features[:self.state_dim], dtype=np.float32)
        
        return states
    
    def calculate_rewards(self, prev_metrics: Dict, current_metrics: Dict) -> Dict[str, float]:
        """
        è®¡ç®—æ™ºèƒ½ä½“å¥–åŠ± - ä½¿ç”¨æ ‡å‡†åŒ–å¥–åŠ±å‡½æ•°
        ç¡®ä¿å¥–åŠ±è®¡ç®—çš„ä¸€è‡´æ€§ï¼Œä¸¥æ ¼å¯¹åº”è®ºæ–‡ç›®æ ‡å‡½æ•°
        """
        from utils.standardized_reward import calculate_standardized_reward
        
        rewards = {}
        
        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“è®¡ç®—æ ‡å‡†åŒ–çš„åŸºç¡€å¥–åŠ±
        for agent_id in self.agents.keys():
            # ä½¿ç”¨æ ‡å‡†åŒ–å¥–åŠ±è®¡ç®—å‡½æ•°
            base_reward = calculate_standardized_reward(current_metrics, agent_id)
            
            # è®¡ç®—æ€§èƒ½å˜åŒ–å¥–åŠ± (å¯é€‰çš„é¢å¤–å¥–åŠ±)
            change_bonus = self._calculate_performance_change_bonus(
                prev_metrics, current_metrics, agent_id)
            
            # ç»„åˆæœ€ç»ˆå¥–åŠ±
            final_reward = base_reward + change_bonus
            
            # ç¡®ä¿å¥–åŠ±åœ¨åˆç†èŒƒå›´å†…
            rewards[agent_id] = np.clip(final_reward, -10.0, 5.0)
        
        return rewards
    
    def _calculate_performance_change_bonus(self, prev_metrics: Dict, 
                                          current_metrics: Dict, 
                                          agent_id: str) -> float:
        """
        è®¡ç®—æ€§èƒ½å˜åŒ–å¥–åŠ± - å¥–åŠ±æ€§èƒ½æ”¹å–„
        
        Args:
            prev_metrics: å‰ä¸€æ­¥çš„ç³»ç»ŸæŒ‡æ ‡
            current_metrics: å½“å‰æ­¥çš„ç³»ç»ŸæŒ‡æ ‡  
            agent_id: æ™ºèƒ½ä½“ID
            
        Returns:
            æ€§èƒ½å˜åŒ–å¥–åŠ±
        """
        # è®¡ç®—å…³é”®æŒ‡æ ‡çš„å˜åŒ–
        delay_change = (prev_metrics.get('avg_task_delay', 0.0) - 
                       current_metrics.get('avg_task_delay', 0.0))
        energy_change = (prev_metrics.get('total_energy_consumption', 0.0) - 
                        current_metrics.get('total_energy_consumption', 0.0))
        loss_change = (prev_metrics.get('data_loss_rate', 0.0) - 
                      current_metrics.get('data_loss_rate', 0.0))
        
        # å½’ä¸€åŒ–å˜åŒ– (æ”¹å–„ä¸ºæ­£ï¼Œæ¶åŒ–ä¸ºè´Ÿ)
        delay_bonus = np.tanh(delay_change / 0.1) * 0.1   # å»¶è¿Ÿå‡å°‘å¥–åŠ±
        energy_bonus = np.tanh(energy_change / 50.0) * 0.1  # èƒ½è€—å‡å°‘å¥–åŠ±
        loss_bonus = np.tanh(loss_change / 0.05) * 0.1    # ä¸¢å¤±ç‡å‡å°‘å¥–åŠ±
        
        # æ™ºèƒ½ä½“ç‰¹å®šçš„å˜åŒ–å¥–åŠ±æƒé‡
        if agent_id == 'vehicle_agent':
            # è½¦è¾†æ™ºèƒ½ä½“æ›´å…³æ³¨æœ¬åœ°å¤„ç†æ•ˆç‡
            return 0.8 * delay_bonus + 0.6 * energy_bonus + 0.4 * loss_bonus
        elif agent_id == 'rsu_agent':
            # RSUæ™ºèƒ½ä½“æ›´å…³æ³¨æ•´ä½“ç³»ç»Ÿæ€§èƒ½
            return 0.6 * delay_bonus + 0.4 * energy_bonus + 0.8 * loss_bonus
        elif agent_id == 'uav_agent':
            # UAVæ™ºèƒ½ä½“æ›´å…³æ³¨èƒ½æ•ˆ
            return 0.5 * delay_bonus + 0.9 * energy_bonus + 0.3 * loss_bonus
        else:
            # é»˜è®¤æƒé‡
            return 0.6 * delay_bonus + 0.6 * energy_bonus + 0.6 * loss_bonus
    
    def train_step(self, states: Dict[str, np.ndarray], actions: Dict[str, np.ndarray],
                  rewards: Dict[str, float], next_states: Dict[str, np.ndarray],
                  dones: Dict[str, bool]) -> Dict[str, Dict]:
        """è®­ç»ƒæ‰€æœ‰æ™ºèƒ½ä½“"""
        training_info = {}
        
        for agent_id, agent in self.agents.items():
            if agent_id in states and agent_id in actions:
                # å­˜å‚¨ç»éªŒ
                agent.store_transition(
                    states[agent_id],
                    actions[agent_id],
                    rewards.get(agent_id, 0.0),
                    next_states[agent_id],
                    dones.get(agent_id, False)
                )
                
                # è®­ç»ƒæ™ºèƒ½ä½“
                train_info = agent.train()
                training_info[agent_id] = train_info
        
        return training_info
    
    def get_actions(self, states: Dict[str, np.ndarray], training: bool = True) -> Dict[str, np.ndarray]:
        """è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œ"""
        actions = {}
        
        for agent_id, agent in self.agents.items():
            if agent_id in states:
                action = agent.select_action(states[agent_id], add_noise=training)
                actions[agent_id] = action
        
        return actions
    
    def save_models(self, directory: str):
        """ä¿å­˜æ‰€æœ‰æ™ºèƒ½ä½“æ¨¡å‹"""
        import os
        os.makedirs(directory, exist_ok=True)
        
        for agent_id, agent in self.agents.items():
            filepath = os.path.join(directory, f"{agent_id}_model.pth")
            agent.save_model(filepath)
    
    def load_models(self, directory: str):
        """åŠ è½½æ‰€æœ‰æ™ºèƒ½ä½“æ¨¡å‹"""
        import os
        
        for agent_id, agent in self.agents.items():
            filepath = os.path.join(directory, f"{agent_id}_model.pth")
            if os.path.exists(filepath):
                agent.load_model(filepath)
    
    def reset_hidden_states(self):
        """é‡ç½®éšè—çŠ¶æ€ (ä¸ºTD3ä¸éœ€è¦ï¼Œä½†ä¿æŒæ¥å£å…¼å®¹æ€§)"""
        pass
    
    def get_global_state(self, states: Dict[str, np.ndarray]) -> np.ndarray:
        """è·å–å…¨å±€çŠ¶æ€"""
        global_state = []
        for agent_id in sorted(states.keys()):
            if agent_id in states:
                global_state.append(states[agent_id])
        return np.concatenate(global_state) if global_state else np.array([])
    
    def store_experience(self, states: Dict, actions: Dict, log_probs: Dict,
                        rewards: Dict, dones: Dict, global_state: Optional[np.ndarray] = None):
        """å­˜å‚¨ç»éªŒ (ä¸ºTD3ä¸éœ€è¦log_probsï¼Œä½†ä¿æŒæ¥å£å…¼å®¹æ€§)"""
        # å¯¹äºTD3ï¼Œç›´æ¥åœ¨train_stepä¸­å­˜å‚¨ç»éªŒ
        pass
    
    def update(self):
        """æ›´æ–°æ‰€æœ‰æ™ºèƒ½ä½“ (ä¸ºTD3çš„æ›´æ–°åœ¨train_stepä¸­æ‰§è¡Œ)"""
        return {}