"""
PPO (Proximal Policy Optimization) å•æ™ºèƒ½ä½“ç®—æ³•å®ç°
ä¸“é—¨é€‚é…MATD3-MIGç³»ç»Ÿçš„VECç¯å¢ƒ

ä¸»è¦ç‰¹ç‚¹:
1. ç­–ç•¥æ¢¯åº¦æ–¹æ³•å¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´
2. è£å‰ªä»£ç†ç›®æ ‡é˜²æ­¢è¿‡å¤§ç­–ç•¥æ›´æ–°
3. GAE (Generalized Advantage Estimation) å‡å°‘æ–¹å·®
4. è‡ªé€‚åº”KLæ•£åº¦çº¦æŸ

å¯¹åº”è®ºæ–‡: Proximal Policy Optimization Algorithms
"""
# æ€§èƒ½ä¼˜åŒ– - å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰
try:
    from tools.performance_optimization import OPTIMIZED_BATCH_SIZES
except ImportError:
    OPTIMIZED_BATCH_SIZES = {'PPO': 64}  # é»˜è®¤å€¼

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from torch.distributions import Normal
from collections import deque
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass

from config import config


@dataclass
class PPOConfig:
    """ğŸ”§ PPOç®—æ³•é…ç½® - ä¿®å¤ç‰ˆï¼ˆè§£å†³æ•°æ®é‡ä¸è¶³é—®é¢˜ï¼‰"""
    # ç½‘ç»œç»“æ„
    hidden_dim: int = 256      # ğŸ”§ é™å›256ï¼Œé¿å…è¿‡æ‹Ÿåˆ
    actor_lr: float = 3e-4     # ğŸ”§ æé«˜åˆ°3e-4ï¼ŒåŠ å¿«å­¦ä¹ 
    critic_lr: float = 1e-3    # ğŸ”§ æé«˜åˆ°1e-3ï¼Œæ›´å¿«çš„ä»·å€¼å­¦ä¹ 
    
    # PPOå‚æ•° - å…³é”®è°ƒæ•´
    clip_ratio: float = 0.2
    entropy_coef: float = 0.05  # ğŸ”§ ä»0.01æé«˜åˆ°0.05ï¼Œå¢å¼ºæ¢ç´¢ï¼
    value_coef: float = 0.5
    max_grad_norm: float = 0.5  # ğŸ”§ æ”¶ç´§æ¢¯åº¦è£å‰ª
    
    # è®­ç»ƒå‚æ•° - å…³é”®ä¿®å¤
    batch_size: int = 256      # ğŸ”§ å¢å¤§batchï¼Œæé«˜æ¢¯åº¦ç¨³å®šæ€§
    buffer_size: int = 10000   # ğŸ”§ å¤§å¹…å¢åŠ ï¼50ä¸ªepisodeçš„æ•°æ®
    ppo_epochs: int = 15       # ğŸ”§ ä»10å¢è‡³15ï¼Œå……åˆ†åˆ©ç”¨æ•°æ®
    gamma: float = 0.99
    gae_lambda: float = 0.95
    
    # æ›´æ–°æ§åˆ¶ - å…³é”®æ–°å¢
    update_frequency: int = 20  # ğŸ”§ æ¯20ä¸ªepisodeæ›´æ–°ä¸€æ¬¡ï¼ˆ4000æ­¥ï¼‰
    min_buffer_size: int = 2000  # ğŸ”§ æœ€å°‘2000æ­¥æ‰æ›´æ–°
    
    # å…¶ä»–å‚æ•°
    normalize_advantages: bool = True
    use_gae: bool = True
    target_kl: float = 0.015  # ğŸ”§ ç•¥å¾®æ”¾å®½KLçº¦æŸ


class PPOActor(nn.Module):
    """PPO Actorç½‘ç»œ - éšæœºç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(PPOActor, self).__init__()
        
        self.action_dim = action_dim
        
        # å…±äº«ç‰¹å¾å±‚
        self.feature_layers = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()
        )
        
        # åŠ¨ä½œå‡å€¼å±‚
        self.mean_layer = nn.Linear(hidden_dim, action_dim)
        
        # åŠ¨ä½œæ ‡å‡†å·®å±‚ (å¯å­¦ä¹ çš„å‚æ•°)
        self.log_std = nn.Parameter(torch.zeros(action_dim))
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.feature_layers:
            if isinstance(layer, nn.Linear):
                nn.init.orthogonal_(layer.weight, gain=int(np.sqrt(2)))
                nn.init.constant_(layer.bias, 0.0)
        
        nn.init.orthogonal_(self.mean_layer.weight, gain=1)  # ä½¿ç”¨æ•´æ•°gain
        nn.init.constant_(self.mean_layer.bias, 0.0)
    
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        features = self.feature_layers(state)
        
        # åŠ¨ä½œåˆ†å¸ƒå‚æ•°
        mean = self.mean_layer(features)
        std = torch.exp(self.log_std.clamp(-20, 2))
        
        return mean, std
    
    def get_action_and_logprob(self, state: torch.Tensor, action: Optional[torch.Tensor] = None):
        """è·å–åŠ¨ä½œå’Œå¯¹æ•°æ¦‚ç‡"""
        mean, std = self.forward(state)
        dist = Normal(mean, std)
        
        if action is None:
            action = dist.sample()
        
        log_prob = dist.log_prob(action).sum(dim=-1)
        entropy = dist.entropy().sum(dim=-1)
        
        return action, log_prob, entropy


class PPOCritic(nn.Module):
    """PPO Criticç½‘ç»œ - ä»·å€¼ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, hidden_dim: int = 256):
        super(PPOCritic, self).__init__()
        
        self.value_network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.value_network:
            if isinstance(layer, nn.Linear):
                nn.init.orthogonal_(layer.weight, gain=int(np.sqrt(2)))
                nn.init.constant_(layer.bias, 0.0)
        
        # æœ€åä¸€å±‚ä½¿ç”¨å•ä½å¢ç›Š
        nn.init.orthogonal_(self.value_network[-1].weight, gain=1)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        return self.value_network(state)


class PPOBuffer:
    """PPOç»éªŒç¼“å†²åŒº"""
    
    def __init__(self, buffer_size: int, state_dim: int, action_dim: int):
        self.buffer_size = buffer_size
        self.ptr = 0
        self.size = 0
        
        # ç¼“å†²åŒºæ•°æ®
        self.states = np.zeros((buffer_size, state_dim), dtype=np.float32)
        self.actions = np.zeros((buffer_size, action_dim), dtype=np.float32)
        self.log_probs = np.zeros(buffer_size, dtype=np.float32)
        self.rewards = np.zeros(buffer_size, dtype=np.float32)
        self.dones = np.zeros(buffer_size, dtype=np.float32)
        self.values = np.zeros(buffer_size, dtype=np.float32)
        self.advantages = np.zeros(buffer_size, dtype=np.float32)
        self.returns = np.zeros(buffer_size, dtype=np.float32)
    
    def store(self, state: np.ndarray, action: np.ndarray, log_prob: float,
              reward: float, done: bool, value: float):
        """å­˜å‚¨ä¸€æ­¥ç»éªŒ"""
        if self.size < self.buffer_size:
            self.size += 1
        
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.log_probs[self.ptr] = log_prob
        self.rewards[self.ptr] = reward
        self.dones[self.ptr] = float(done)
        self.values[self.ptr] = value
        
        self.ptr = (self.ptr + 1) % self.buffer_size
    
    def compute_advantages_and_returns(self, last_value: float, gamma: float, gae_lambda: float, use_gae: bool = True):
        """è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥"""
        if use_gae:
            # ä½¿ç”¨GAEè®¡ç®—ä¼˜åŠ¿å‡½æ•°
            advantages = np.zeros(self.size, dtype=np.float32)
            last_gae = 0
            
            for t in reversed(range(self.size)):
                if t == self.size - 1:
                    next_value = last_value
                    next_done = 0
                else:
                    next_value = self.values[t + 1]
                    next_done = self.dones[t + 1]
                
                delta = self.rewards[t] + gamma * next_value * (1 - next_done) - self.values[t]
                last_gae = delta + gamma * gae_lambda * (1 - next_done) * last_gae
                advantages[t] = last_gae
            
            returns = advantages + self.values[:self.size]
        else:
            # ç›´æ¥è®¡ç®—å›æŠ¥
            returns = np.zeros(self.size, dtype=np.float32)
            running_return = last_value
            
            for t in reversed(range(self.size)):
                if self.dones[t]:
                    running_return = 0
                running_return = self.rewards[t] + gamma * running_return
                returns[t] = running_return
            
            advantages = returns - self.values[:self.size]
        
        self.advantages[:self.size] = advantages
        self.returns[:self.size] = returns
    
    def get_batch(self, batch_size: int):
        """è·å–è®­ç»ƒæ‰¹æ¬¡"""
        indices = np.random.choice(self.size, batch_size, replace=False)
        
        return (
            torch.FloatTensor(self.states[indices]),
            torch.FloatTensor(self.actions[indices]),
            torch.FloatTensor(self.log_probs[indices]),
            torch.FloatTensor(self.advantages[indices]),
            torch.FloatTensor(self.returns[indices]),
            torch.FloatTensor(self.values[indices])
        )
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.ptr = 0
        self.size = 0


class PPOAgent:
    """PPOæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, config: PPOConfig):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config
        
        # æ€§èƒ½ä¼˜åŒ– - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        self.optimized_batch_size = OPTIMIZED_BATCH_SIZES.get('PPO', config.batch_size)
        self.config.batch_size = self.optimized_batch_size
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºç½‘ç»œ
        self.actor = PPOActor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.critic = PPOCritic(state_dim, config.hidden_dim).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)
        
        # ç»éªŒç¼“å†²åŒº
        self.buffer = PPOBuffer(config.buffer_size, state_dim, action_dim)
        
        # è®­ç»ƒç»Ÿè®¡
        self.actor_losses = []
        self.critic_losses = []
        self.entropy_losses = []
        self.kl_divergences = []
        
        # å…¶ä»–å‚æ•°
        self.step_count = 0
    
    def select_action(self, state: np.ndarray, training: bool = True):
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action, log_prob, _ = self.actor.get_action_and_logprob(state_tensor)
            value = self.critic(state_tensor)
        
        action = torch.clamp(action, -1.0, 1.0)  # é™åˆ¶åŠ¨ä½œèŒƒå›´
        
        return action.cpu().numpy()[0], log_prob.cpu().item(), value.cpu().item()
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, log_prob: float,
                        reward: float, done: bool, value: float):
        """å­˜å‚¨ç»éªŒ"""
        self.buffer.store(state, action, log_prob, reward, done, value)
        self.step_count += 1
    
    def update(self, last_value: float = 0.0, force_update: bool = False) -> Dict[str, float]:
        """PPOæ›´æ–°"""
        # ğŸ”§ ä¿®å¤ï¼šéœ€è¦è¶³å¤Ÿæ•°æ®æ‰æ›´æ–°
        if not force_update and self.buffer.size < self.config.min_buffer_size:
            return {'message': 'Waiting for more data', 'buffer_size': self.buffer.size}
        
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥
        self.buffer.compute_advantages_and_returns(
            last_value, self.config.gamma, self.config.gae_lambda, self.config.use_gae
        )
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿å‡½æ•°
        if self.config.normalize_advantages and self.buffer.size > 1:
            advantages = self.buffer.advantages[:self.buffer.size]
            self.buffer.advantages[:self.buffer.size] = (
                (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            )
        
        # PPOæ›´æ–°å¾ªç¯
        actor_losses = []
        critic_losses = []
        entropy_losses = []
        kl_divs = []
        
        for epoch in range(self.config.ppo_epochs):
            # è·å–è®­ç»ƒæ‰¹æ¬¡
            batch_states, batch_actions, batch_old_log_probs, batch_advantages, batch_returns, batch_old_values = \
                self.buffer.get_batch(self.config.batch_size)
            
            batch_states = batch_states.to(self.device)
            batch_actions = batch_actions.to(self.device)
            batch_old_log_probs = batch_old_log_probs.to(self.device)
            batch_advantages = batch_advantages.to(self.device)
            batch_returns = batch_returns.to(self.device)
            
            # è®¡ç®—æ–°çš„ç­–ç•¥åˆ†å¸ƒ
            _, new_log_probs, entropy = self.actor.get_action_and_logprob(batch_states, batch_actions)
            new_values = self.critic(batch_states).squeeze()
            
            # è®¡ç®—æ¯”ç‡
            ratio = torch.exp(new_log_probs - batch_old_log_probs)
            
            # PPOè£å‰ªæŸå¤±
            surr1 = ratio * batch_advantages
            surr2 = torch.clamp(ratio, 1.0 - self.config.clip_ratio, 1.0 + self.config.clip_ratio) * batch_advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            
            # ç†µæŸå¤±
            entropy_loss = -entropy.mean()
            
            # æ€»ActoræŸå¤±
            total_actor_loss = actor_loss + self.config.entropy_coef * entropy_loss
            
            # æ›´æ–°Actor
            self.actor_optimizer.zero_grad()
            total_actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.config.max_grad_norm)
            self.actor_optimizer.step()
            
            # CriticæŸå¤±
            critic_loss = F.mse_loss(new_values, batch_returns)
            
            # æ›´æ–°Critic
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.config.max_grad_norm)
            self.critic_optimizer.step()
            
            # è®¡ç®—KLæ•£åº¦
            kl_div = (batch_old_log_probs - new_log_probs).mean()
            
            # è®°å½•æŸå¤±
            actor_losses.append(actor_loss.item())
            critic_losses.append(critic_loss.item())
            entropy_losses.append(entropy_loss.item())
            kl_divs.append(kl_div.item())
            
            # æ—©åœæ£€æŸ¥ (å¦‚æœKLæ•£åº¦è¿‡å¤§)
            if kl_div > self.config.target_kl * 4:
                break
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.buffer.clear()
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        avg_actor_loss = float(np.mean(actor_losses))
        avg_critic_loss = float(np.mean(critic_losses))
        avg_entropy_loss = float(np.mean(entropy_losses))
        avg_kl_div = float(np.mean(kl_divs))
        
        self.actor_losses.append(avg_actor_loss)
        self.critic_losses.append(avg_critic_loss)
        self.entropy_losses.append(avg_entropy_loss)
        self.kl_divergences.append(avg_kl_div)
        
        return {
            'actor_loss': avg_actor_loss,
            'critic_loss': avg_critic_loss,
            'entropy_loss': avg_entropy_loss,
            'kl_divergence': avg_kl_div,
            'ppo_epochs': len(actor_losses)
        }
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'step_count': self.step_count
        }, f"{filepath}_ppo.pth")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(f"{filepath}_ppo.pth", map_location=self.device)
        
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.step_count = checkpoint['step_count']


class PPOEnvironment:
    """PPOè®­ç»ƒç¯å¢ƒ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, num_vehicles: int = 12, num_rsus: int = 4, num_uavs: int = 2):
        from single_agent.common_state_action import UnifiedStateActionSpace
        
        self.config = PPOConfig()
        self.num_vehicles = num_vehicles
        self.num_rsus = num_rsus
        self.num_uavs = num_uavs
        
        # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€çš„çŠ¶æ€/åŠ¨ä½œç»´åº¦è®¡ç®—
        self.local_state_dim, self.global_state_dim, self.state_dim = \
            UnifiedStateActionSpace.calculate_state_dim(num_vehicles, num_rsus, num_uavs)
        self.action_dim = UnifiedStateActionSpace.calculate_action_dim(num_rsus, num_uavs)
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agent = PPOAgent(self.state_dim, self.action_dim, self.config)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_count = 0
        self.step_count = 0
        
        print(f"PPOç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼ˆä¼˜åŒ–ç‰ˆv2.0ï¼‰")
        print(f"ç½‘ç»œæ‹“æ‰‘: {num_vehicles}è¾†è½¦ + {num_rsus}ä¸ªRSU + {num_uavs}ä¸ªUAV")
        print(f"çŠ¶æ€ç»´åº¦: {self.state_dim} = å±€éƒ¨{self.local_state_dim} + å…¨å±€{self.global_state_dim}")
        print(f"åŠ¨ä½œç»´åº¦: {self.action_dim} (åŠ¨æ€é€‚é…: 3+{num_rsus}+{num_uavs}+8)")
        print(f"ç½‘ç»œå®¹é‡: hidden_dim={self.config.hidden_dim}")
        print(f"ä¼˜åŒ–ç‰¹æ€§: ç»Ÿä¸€çŠ¶æ€ç©ºé—´, åŠ¨æ€æ‹“æ‰‘é€‚é…, å…¨å±€çŠ¶æ€")
        print(f"Actorå­¦ä¹ ç‡: {self.config.actor_lr}")
        print(f"Criticå­¦ä¹ ç‡: {self.config.critic_lr}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        print(f"ç¼“å†²åŒºå¤§å°: {self.config.buffer_size}")
        print(f"æ›´æ–°é¢‘ç‡: æ¯{self.config.update_frequency}ä¸ªepisode")
        print(f"æœ€å°‘æ•°æ®é‡: {self.config.min_buffer_size}æ­¥")
        print(f"PPOè½®æ¬¡: {self.config.ppo_epochs}")
        print(f"ç†µç³»æ•°: {self.config.entropy_coef} (å¢å¼ºæ¢ç´¢)")
    
    def get_state_vector(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """ğŸ”§ ä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨ç»Ÿä¸€çš„çŠ¶æ€å‘é‡æ„å»º"""
        from single_agent.common_state_action import UnifiedStateActionSpace
        return UnifiedStateActionSpace.build_state_vector(
            node_states, system_metrics,
            self.num_vehicles, self.num_rsus, self.num_uavs,
            self.state_dim
        )
    
    def decompose_action(self, action: np.ndarray) -> Dict[str, np.ndarray]:
        """å°†å…¨å±€åŠ¨ä½œåˆ†è§£ä¸ºå„èŠ‚ç‚¹åŠ¨ä½œ"""
        from single_agent.common_state_action import UnifiedStateActionSpace
        if not isinstance(action, np.ndarray):
            action = np.array(action, dtype=np.float32)
        return UnifiedStateActionSpace.decompose_action(action, self.num_rsus, self.num_uavs, self.action_dim)
    
    def get_actions(self, state: np.ndarray, training: bool = True):
        """è·å–åŠ¨ä½œ"""
        action, log_prob, value = self.agent.select_action(state, training)
        return self.decompose_action(action), log_prob, value
    
    def calculate_reward(self, system_metrics: Dict,
                       cache_metrics: Optional[Dict] = None,
                       migration_metrics: Optional[Dict] = None) -> float:
        """
        ä½¿ç”¨ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨
        """
        from utils.unified_reward_calculator import calculate_unified_reward
        return calculate_unified_reward(system_metrics, cache_metrics, migration_metrics, algorithm="general")
    
    def save_models(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(filepath, exist_ok=True)
        self.agent.save_model(filepath)
        print(f"âœ“ PPOæ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_models(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        self.agent.load_model(filepath)
        print(f"âœ“ PPOæ¨¡å‹å·²åŠ è½½: {filepath}")
    
    def train_step(self, state: np.ndarray, action: Union[np.ndarray, int], reward: float,
                   next_state: np.ndarray, done: bool) -> Dict:
        """å ä½ç¬¦train_stepæ–¹æ³• - PPOä¸ä½¿ç”¨æ­¤æ–¹æ³•"""
        # PPOä¸ä½¿ç”¨train_stepï¼Œè€Œæ˜¯ä½¿ç”¨store_experienceå’Œupdate
        # è¿™é‡Œæä¾›ç©ºå®ç°ä»¥ä¿æŒæ¥å£ç»Ÿä¸€
        return {'message': 'PPO does not use train_step method'}
    
    def store_experience(self, state: np.ndarray, action: Union[np.ndarray, int], reward: float,
                        next_state: np.ndarray, done: bool, log_prob: float = 0.0, value: float = 0.0):
        """å­˜å‚¨ç»éªŒåˆ°ç¼“å†²åŒº - æ”¯æŒç»Ÿä¸€æ¥å£"""
        # ç¡®ä¿actionæ˜¯numpyæ•°ç»„
        if isinstance(action, int):
            action = np.array([action], dtype=np.float32)
        elif not isinstance(action, np.ndarray):
            action = np.array(action, dtype=np.float32)
        
        # PPO Agentçš„store_experienceå‚æ•°é¡ºåº: (state, action, log_prob, reward, done, value)
        # æ³¨æ„: PPOä¸ä½¿ç”¨next_stateå‚æ•°
        self.agent.store_experience(state, action, log_prob, reward, done, value)
        self.step_count += 1
    
    def update(self, last_value: float = 0.0) -> Dict:
        """PPOæ›´æ–° (åœ¨episodeç»“æŸåè°ƒç”¨) - æ”¯æŒç»Ÿä¸€æ¥å£"""
        return self.agent.update(last_value)
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return {
            'actor_loss_avg': float(np.mean(self.agent.actor_losses[-10:])) if self.agent.actor_losses else 0.0,
            'critic_loss_avg': float(np.mean(self.agent.critic_losses[-10:])) if self.agent.critic_losses else 0.0,
            'entropy_loss_avg': float(np.mean(self.agent.entropy_losses[-10:])) if self.agent.entropy_losses else 0.0,
            'kl_divergence_avg': float(np.mean(self.agent.kl_divergences[-10:])) if self.agent.kl_divergences else 0.0,
            'buffer_size': self.agent.buffer.size,
            'step_count': self.step_count
        }
