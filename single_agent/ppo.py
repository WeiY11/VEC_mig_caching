"""
PPO (Proximal Policy Optimization) å•æ™ºèƒ½ä½“ç®—æ³•å®ç°
ä¸“é—¨é€‚é…MATD3-MIGç³»ç»Ÿçš„VECç¯å¢ƒ

ä¸»è¦ç‰¹ç‚¹:
1. ç­–ç•¥æ¢¯åº¦æ–¹æ³•å¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´
2. è£å‰ªä»£ç†ç›®æ ‡é˜²æ­¢è¿‡å¤§ç­–ç•¥æ›´æ–°
3. GAE (Generalized Advantage Estimation) å‡å°‘æ–¹å·®
4. è‡ªé€‚åº”KLæ•£åº¦çº¦æŸ

å¯¹åº”è®ºæ–‡: Proximal Policy Optimization Algorithms
"""
# æ€§èƒ½ä¼˜åŒ– - å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰
try:
    from tools.performance_optimization import OPTIMIZED_BATCH_SIZES
except ImportError:
    OPTIMIZED_BATCH_SIZES = {'PPO': 64}  # é»˜è®¤å€¼

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from torch.distributions import Normal
from collections import deque
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass

from config import config


@dataclass
class PPOConfig:
    """PPOç®—æ³•é…ç½®"""
    # ç½‘ç»œç»“æ„
    hidden_dim: int = 256
    actor_lr: float = 3e-4
    critic_lr: float = 1e-3
    
    # PPOå‚æ•°
    clip_ratio: float = 0.2
    entropy_coef: float = 0.01
    value_coef: float = 0.5
    max_grad_norm: float = 0.5
    
    # è®­ç»ƒå‚æ•°
    batch_size: int = 64
    buffer_size: int = 2048
    ppo_epochs: int = 10
    gamma: float = 0.99
    gae_lambda: float = 0.95
    
    # å…¶ä»–å‚æ•°
    normalize_advantages: bool = True
    use_gae: bool = True
    target_kl: float = 0.01  # è‡ªé€‚åº”KLæ•£åº¦çº¦æŸ


class PPOActor(nn.Module):
    """PPO Actorç½‘ç»œ - éšæœºç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(PPOActor, self).__init__()
        
        self.action_dim = action_dim
        
        # å…±äº«ç‰¹å¾å±‚
        self.feature_layers = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()
        )
        
        # åŠ¨ä½œå‡å€¼å±‚
        self.mean_layer = nn.Linear(hidden_dim, action_dim)
        
        # åŠ¨ä½œæ ‡å‡†å·®å±‚ (å¯å­¦ä¹ çš„å‚æ•°)
        self.log_std = nn.Parameter(torch.zeros(action_dim))
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.feature_layers:
            if isinstance(layer, nn.Linear):
                nn.init.orthogonal_(layer.weight, gain=int(np.sqrt(2)))
                nn.init.constant_(layer.bias, 0.0)
        
        nn.init.orthogonal_(self.mean_layer.weight, gain=1)  # ä½¿ç”¨æ•´æ•°gain
        nn.init.constant_(self.mean_layer.bias, 0.0)
    
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        features = self.feature_layers(state)
        
        # åŠ¨ä½œåˆ†å¸ƒå‚æ•°
        mean = self.mean_layer(features)
        std = torch.exp(self.log_std.clamp(-20, 2))
        
        return mean, std
    
    def get_action_and_logprob(self, state: torch.Tensor, action: Optional[torch.Tensor] = None):
        """è·å–åŠ¨ä½œå’Œå¯¹æ•°æ¦‚ç‡"""
        mean, std = self.forward(state)
        dist = Normal(mean, std)
        
        if action is None:
            action = dist.sample()
        
        log_prob = dist.log_prob(action).sum(dim=-1)
        entropy = dist.entropy().sum(dim=-1)
        
        return action, log_prob, entropy


class PPOCritic(nn.Module):
    """PPO Criticç½‘ç»œ - ä»·å€¼ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, hidden_dim: int = 256):
        super(PPOCritic, self).__init__()
        
        self.value_network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.value_network:
            if isinstance(layer, nn.Linear):
                nn.init.orthogonal_(layer.weight, gain=int(np.sqrt(2)))
                nn.init.constant_(layer.bias, 0.0)
        
        # æœ€åä¸€å±‚ä½¿ç”¨å•ä½å¢ç›Š
        nn.init.orthogonal_(self.value_network[-1].weight, gain=1)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        return self.value_network(state)


class PPOBuffer:
    """PPOç»éªŒç¼“å†²åŒº"""
    
    def __init__(self, buffer_size: int, state_dim: int, action_dim: int):
        self.buffer_size = buffer_size
        self.ptr = 0
        self.size = 0
        
        # ç¼“å†²åŒºæ•°æ®
        self.states = np.zeros((buffer_size, state_dim), dtype=np.float32)
        self.actions = np.zeros((buffer_size, action_dim), dtype=np.float32)
        self.log_probs = np.zeros(buffer_size, dtype=np.float32)
        self.rewards = np.zeros(buffer_size, dtype=np.float32)
        self.dones = np.zeros(buffer_size, dtype=np.float32)
        self.values = np.zeros(buffer_size, dtype=np.float32)
        self.advantages = np.zeros(buffer_size, dtype=np.float32)
        self.returns = np.zeros(buffer_size, dtype=np.float32)
    
    def store(self, state: np.ndarray, action: np.ndarray, log_prob: float,
              reward: float, done: bool, value: float):
        """å­˜å‚¨ä¸€æ­¥ç»éªŒ"""
        if self.size < self.buffer_size:
            self.size += 1
        
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.log_probs[self.ptr] = log_prob
        self.rewards[self.ptr] = reward
        self.dones[self.ptr] = float(done)
        self.values[self.ptr] = value
        
        self.ptr = (self.ptr + 1) % self.buffer_size
    
    def compute_advantages_and_returns(self, last_value: float, gamma: float, gae_lambda: float, use_gae: bool = True):
        """è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥"""
        if use_gae:
            # ä½¿ç”¨GAEè®¡ç®—ä¼˜åŠ¿å‡½æ•°
            advantages = np.zeros(self.size, dtype=np.float32)
            last_gae = 0
            
            for t in reversed(range(self.size)):
                if t == self.size - 1:
                    next_value = last_value
                    next_done = 0
                else:
                    next_value = self.values[t + 1]
                    next_done = self.dones[t + 1]
                
                delta = self.rewards[t] + gamma * next_value * (1 - next_done) - self.values[t]
                last_gae = delta + gamma * gae_lambda * (1 - next_done) * last_gae
                advantages[t] = last_gae
            
            returns = advantages + self.values[:self.size]
        else:
            # ç›´æ¥è®¡ç®—å›æŠ¥
            returns = np.zeros(self.size, dtype=np.float32)
            running_return = last_value
            
            for t in reversed(range(self.size)):
                if self.dones[t]:
                    running_return = 0
                running_return = self.rewards[t] + gamma * running_return
                returns[t] = running_return
            
            advantages = returns - self.values[:self.size]
        
        self.advantages[:self.size] = advantages
        self.returns[:self.size] = returns
    
    def get_batch(self, batch_size: int):
        """è·å–è®­ç»ƒæ‰¹æ¬¡"""
        indices = np.random.choice(self.size, batch_size, replace=False)
        
        return (
            torch.FloatTensor(self.states[indices]),
            torch.FloatTensor(self.actions[indices]),
            torch.FloatTensor(self.log_probs[indices]),
            torch.FloatTensor(self.advantages[indices]),
            torch.FloatTensor(self.returns[indices]),
            torch.FloatTensor(self.values[indices])
        )
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.ptr = 0
        self.size = 0


class PPOAgent:
    """PPOæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, config: PPOConfig):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config
        
        # æ€§èƒ½ä¼˜åŒ– - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        self.optimized_batch_size = OPTIMIZED_BATCH_SIZES.get('PPO', config.batch_size)
        self.config.batch_size = self.optimized_batch_size
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºç½‘ç»œ
        self.actor = PPOActor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.critic = PPOCritic(state_dim, config.hidden_dim).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)
        
        # ç»éªŒç¼“å†²åŒº
        self.buffer = PPOBuffer(config.buffer_size, state_dim, action_dim)
        
        # è®­ç»ƒç»Ÿè®¡
        self.actor_losses = []
        self.critic_losses = []
        self.entropy_losses = []
        self.kl_divergences = []
        
        # å…¶ä»–å‚æ•°
        self.step_count = 0
    
    def select_action(self, state: np.ndarray, training: bool = True):
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action, log_prob, _ = self.actor.get_action_and_logprob(state_tensor)
            value = self.critic(state_tensor)
        
        action = torch.clamp(action, -1.0, 1.0)  # é™åˆ¶åŠ¨ä½œèŒƒå›´
        
        return action.cpu().numpy()[0], log_prob.cpu().item(), value.cpu().item()
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, log_prob: float,
                        reward: float, done: bool, value: float):
        """å­˜å‚¨ç»éªŒ"""
        self.buffer.store(state, action, log_prob, reward, done, value)
        self.step_count += 1
    
    def update(self, last_value: float = 0.0) -> Dict[str, float]:
        """PPOæ›´æ–°"""
        if self.buffer.size < self.config.batch_size:
            return {}
        
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥
        self.buffer.compute_advantages_and_returns(
            last_value, self.config.gamma, self.config.gae_lambda, self.config.use_gae
        )
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿å‡½æ•°
        if self.config.normalize_advantages and self.buffer.size > 1:
            advantages = self.buffer.advantages[:self.buffer.size]
            self.buffer.advantages[:self.buffer.size] = (
                (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            )
        
        # PPOæ›´æ–°å¾ªç¯
        actor_losses = []
        critic_losses = []
        entropy_losses = []
        kl_divs = []
        
        for epoch in range(self.config.ppo_epochs):
            # è·å–è®­ç»ƒæ‰¹æ¬¡
            batch_states, batch_actions, batch_old_log_probs, batch_advantages, batch_returns, batch_old_values = \
                self.buffer.get_batch(self.config.batch_size)
            
            batch_states = batch_states.to(self.device)
            batch_actions = batch_actions.to(self.device)
            batch_old_log_probs = batch_old_log_probs.to(self.device)
            batch_advantages = batch_advantages.to(self.device)
            batch_returns = batch_returns.to(self.device)
            
            # è®¡ç®—æ–°çš„ç­–ç•¥åˆ†å¸ƒ
            _, new_log_probs, entropy = self.actor.get_action_and_logprob(batch_states, batch_actions)
            new_values = self.critic(batch_states).squeeze()
            
            # è®¡ç®—æ¯”ç‡
            ratio = torch.exp(new_log_probs - batch_old_log_probs)
            
            # PPOè£å‰ªæŸå¤±
            surr1 = ratio * batch_advantages
            surr2 = torch.clamp(ratio, 1.0 - self.config.clip_ratio, 1.0 + self.config.clip_ratio) * batch_advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            
            # ç†µæŸå¤±
            entropy_loss = -entropy.mean()
            
            # æ€»ActoræŸå¤±
            total_actor_loss = actor_loss + self.config.entropy_coef * entropy_loss
            
            # æ›´æ–°Actor
            self.actor_optimizer.zero_grad()
            total_actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.config.max_grad_norm)
            self.actor_optimizer.step()
            
            # CriticæŸå¤±
            critic_loss = F.mse_loss(new_values, batch_returns)
            
            # æ›´æ–°Critic
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.config.max_grad_norm)
            self.critic_optimizer.step()
            
            # è®¡ç®—KLæ•£åº¦
            kl_div = (batch_old_log_probs - new_log_probs).mean()
            
            # è®°å½•æŸå¤±
            actor_losses.append(actor_loss.item())
            critic_losses.append(critic_loss.item())
            entropy_losses.append(entropy_loss.item())
            kl_divs.append(kl_div.item())
            
            # æ—©åœæ£€æŸ¥ (å¦‚æœKLæ•£åº¦è¿‡å¤§)
            if kl_div > self.config.target_kl * 4:
                break
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.buffer.clear()
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        avg_actor_loss = float(np.mean(actor_losses))
        avg_critic_loss = float(np.mean(critic_losses))
        avg_entropy_loss = float(np.mean(entropy_losses))
        avg_kl_div = float(np.mean(kl_divs))
        
        self.actor_losses.append(avg_actor_loss)
        self.critic_losses.append(avg_critic_loss)
        self.entropy_losses.append(avg_entropy_loss)
        self.kl_divergences.append(avg_kl_div)
        
        return {
            'actor_loss': avg_actor_loss,
            'critic_loss': avg_critic_loss,
            'entropy_loss': avg_entropy_loss,
            'kl_divergence': avg_kl_div,
            'ppo_epochs': len(actor_losses)
        }
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'step_count': self.step_count
        }, f"{filepath}_ppo.pth")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(f"{filepath}_ppo.pth", map_location=self.device)
        
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.step_count = checkpoint['step_count']


class PPOEnvironment:
    """PPOè®­ç»ƒç¯å¢ƒ"""
    
    def __init__(self):
        self.config = PPOConfig()
        
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—çŠ¶æ€ç»´åº¦ï¼Œä¸TD3ä¿æŒä¸€è‡´
        self.state_dim = 130  # è½¦è¾†60 + RSU54 + UAV16 = 130ç»´
        self.action_dim = 30  # æ•´åˆæ‰€æœ‰èŠ‚ç‚¹åŠ¨ä½œ
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agent = PPOAgent(self.state_dim, self.action_dim, self.config)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_count = 0
        self.step_count = 0
        
        print(f"âœ“ PPOç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ çŠ¶æ€ç»´åº¦: {self.state_dim}")
        print(f"âœ“ åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"âœ“ ç¼“å†²åŒºå¤§å°: {self.config.buffer_size}")
        print(f"âœ“ PPOè½®æ¬¡: {self.config.ppo_epochs}")
    
    def get_state_vector(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """æ„å»ºå…¨å±€çŠ¶æ€å‘é‡"""
        # åŸºç¡€ç³»ç»ŸçŠ¶æ€
        base_state = np.array([
            system_metrics.get('avg_task_delay', 0.0) / 1.0,
            system_metrics.get('total_energy_consumption', 0.0) / 1000.0,
            system_metrics.get('data_loss_rate', 0.0),
            system_metrics.get('cache_hit_rate', 0.0),
            system_metrics.get('migration_success_rate', 0.0),
        ])
        
        # èŠ‚ç‚¹ç‰¹å®šçŠ¶æ€ (ç®€åŒ–å®ç°)
        node_states_flat = np.random.randn(self.state_dim - len(base_state))
        
        return np.concatenate([base_state, node_states_flat])
    
    def decompose_action(self, action: np.ndarray) -> Dict[str, np.ndarray]:
        """å°†å…¨å±€åŠ¨ä½œåˆ†è§£ä¸ºå„èŠ‚ç‚¹åŠ¨ä½œ"""
        actions = {}
        start_idx = 0
        
        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“ç±»å‹åˆ†é…åŠ¨ä½œ
        for agent_type in ['vehicle_agent', 'rsu_agent', 'uav_agent']:
            end_idx = start_idx + 10  # æ¯ä¸ªæ™ºèƒ½ä½“10ä¸ªåŠ¨ä½œç»´åº¦
            actions[agent_type] = action[start_idx:end_idx]
            start_idx = end_idx
        
        return actions
    
    def get_actions(self, state: np.ndarray, training: bool = True):
        """è·å–åŠ¨ä½œ"""
        action, log_prob, value = self.agent.select_action(state, training)
        return self.decompose_action(action), log_prob, value
    
    def calculate_reward(self, system_metrics: Dict) -> float:
        """è®¡ç®—å¥–åŠ± - ä½¿ç”¨æ ‡å‡†åŒ–å¥–åŠ±å‡½æ•°"""
        from utils.standardized_reward import calculate_standardized_reward
        return calculate_standardized_reward(system_metrics, agent_type='single_agent')
    
    def save_models(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(filepath, exist_ok=True)
        self.agent.save_model(filepath)
        print(f"âœ“ PPOæ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_models(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        self.agent.load_model(filepath)
        print(f"âœ“ PPOæ¨¡å‹å·²åŠ è½½: {filepath}")
    
    def train_step(self, state: np.ndarray, action: Union[np.ndarray, int], reward: float,
                   next_state: np.ndarray, done: bool) -> Dict:
        """å ä½ç¬¦train_stepæ–¹æ³• - PPOä¸ä½¿ç”¨æ­¤æ–¹æ³•"""
        # PPOä¸ä½¿ç”¨train_stepï¼Œè€Œæ˜¯ä½¿ç”¨store_experienceå’Œupdate
        # è¿™é‡Œæä¾›ç©ºå®ç°ä»¥ä¿æŒæ¥å£ç»Ÿä¸€
        return {'message': 'PPO does not use train_step method'}
    
    def store_experience(self, state: np.ndarray, action: Union[np.ndarray, int], reward: float,
                        next_state: np.ndarray, done: bool, log_prob: float = 0.0, value: float = 0.0):
        """å­˜å‚¨ç»éªŒåˆ°ç¼“å†²åŒº - æ”¯æŒç»Ÿä¸€æ¥å£"""
        # ç¡®ä¿actionæ˜¯numpyæ•°ç»„
        if isinstance(action, int):
            action = np.array([action], dtype=np.float32)
        elif not isinstance(action, np.ndarray):
            action = np.array(action, dtype=np.float32)
        
        # PPO Agentçš„store_experienceå‚æ•°é¡ºåº: (state, action, log_prob, reward, done, value)
        # æ³¨æ„: PPOä¸ä½¿ç”¨next_stateå‚æ•°
        self.agent.store_experience(state, action, log_prob, reward, done, value)
        self.step_count += 1
    
    def update(self, last_value: float = 0.0) -> Dict:
        """PPOæ›´æ–° (åœ¨episodeç»“æŸåè°ƒç”¨) - æ”¯æŒç»Ÿä¸€æ¥å£"""
        return self.agent.update(last_value)
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return {
            'actor_loss_avg': float(np.mean(self.agent.actor_losses[-10:])) if self.agent.actor_losses else 0.0,
            'critic_loss_avg': float(np.mean(self.agent.critic_losses[-10:])) if self.agent.critic_losses else 0.0,
            'entropy_loss_avg': float(np.mean(self.agent.entropy_losses[-10:])) if self.agent.entropy_losses else 0.0,
            'kl_divergence_avg': float(np.mean(self.agent.kl_divergences[-10:])) if self.agent.kl_divergences else 0.0,
            'buffer_size': self.agent.buffer.size,
            'step_count': self.step_count
        }