"""
DQN (Deep Q-Network) å•æ™ºèƒ½ä½“ç®—æ³•å®ç°
ä¸“é—¨é€‚é…MATD3-MIGç³»ç»Ÿçš„VECç¯å¢ƒ

ä¸»è¦ç‰¹ç‚¹:
1. æ·±åº¦Qç½‘ç»œå¤„ç†ç¦»æ•£åŠ¨ä½œç©ºé—´
2. ç»éªŒå›æ”¾æœºåˆ¶æé«˜æ ·æœ¬æ•ˆç‡
3. ç›®æ ‡ç½‘ç»œç¨³å®šè®­ç»ƒè¿‡ç¨‹
4. Îµ-è´ªå©ªæ¢ç´¢ç­–ç•¥

å¯¹åº”è®ºæ–‡: Human-level control through deep reinforcement learning
"""
# æ€§èƒ½ä¼˜åŒ– - å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰
try:
    from tools.performance_optimization import OPTIMIZED_BATCH_SIZES
except ImportError:
    OPTIMIZED_BATCH_SIZES = {'DQN': 32}  # é»˜è®¤å€¼

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
from collections import deque
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass

from config import config


@dataclass
class DQNConfig:
    """DQNç®—æ³•é…ç½®"""
    # ç½‘ç»œç»“æ„
    hidden_dim: int = 256
    lr: float = 1e-4
    
    # è®­ç»ƒå‚æ•° - ğŸ”§ ä¿®å¤ï¼šå¢åŠ æ‰¹æ¬¡å¤§å°è§£å†³æ¢¯åº¦ä¼°è®¡é—®é¢˜
    batch_size: int = 128  # ä»32å¢åŠ åˆ°128ï¼Œè§£å†³è¯Šæ–­å‘ç°çš„æ‰¹æ¬¡è¿‡å°é—®é¢˜
    buffer_size: int = 50000
    target_update_freq: int = 1000
    gamma: float = 0.99
    
    # æ¢ç´¢å‚æ•°
    epsilon: float = 1.0
    epsilon_decay: float = 0.995
    min_epsilon: float = 0.05
    
    # è®­ç»ƒé¢‘ç‡
    update_freq: int = 4
    warmup_steps: int = 1000
    
    # DQNå˜ç§é€‰æ‹©
    double_dqn: bool = True
    dueling_dqn: bool = True


class DQNNetwork(nn.Module):
    """DQNç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256, dueling: bool = True):
        super(DQNNetwork, self).__init__()
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.dueling = dueling
        
        # å…±äº«ç‰¹å¾å±‚
        self.feature_layers = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        if dueling:
            # Dueling DQNæ¶æ„
            # ä»·å€¼æµ
            self.value_stream = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Linear(hidden_dim // 2, 1)
            )
            
            # ä¼˜åŠ¿æµ
            self.advantage_stream = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Linear(hidden_dim // 2, action_dim)
            )
        else:
            # æ ‡å‡†DQNæ¶æ„
            self.q_network = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, action_dim)
            )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        def init_layer(layer):
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.0)
        
        self.feature_layers.apply(init_layer)
        
        if self.dueling:
            self.value_stream.apply(init_layer)
            self.advantage_stream.apply(init_layer)
        else:
            self.q_network.apply(init_layer)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        features = self.feature_layers(state)
        
        if self.dueling:
            # Dueling DQN: Q(s,a) = V(s) + A(s,a) - mean(A(s,:))
            values = self.value_stream(features)
            advantages = self.advantage_stream(features)
            
            # å‡å»ä¼˜åŠ¿çš„å‡å€¼ä»¥ç¡®ä¿å¯è¯†åˆ«æ€§
            q_values = values + advantages - advantages.mean(dim=1, keepdim=True)
            
            return q_values
        else:
            # æ ‡å‡†DQN
            return self.q_network(features)


class DQNReplayBuffer:
    """DQNç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int, state_dim: int):
        self.capacity = capacity
        self.ptr = 0
        self.size = 0
        
        # é¢„åˆ†é…å†…å­˜
        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros(capacity, dtype=np.int32)
        self.rewards = np.zeros(capacity, dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros(capacity, dtype=np.float32)
    
    def push(self, state: np.ndarray, action: int, reward: float, 
             next_state: np.ndarray, done: bool):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.next_states[self.ptr] = next_state
        self.dones[self.ptr] = float(done)
        
        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    
    def sample(self, batch_size: int) -> Tuple[torch.Tensor, ...]:
        """é‡‡æ ·ç»éªŒæ‰¹æ¬¡"""
        indices = np.random.choice(self.size, batch_size, replace=False)
        
        batch_states = torch.FloatTensor(self.states[indices])
        batch_actions = torch.LongTensor(self.actions[indices])
        batch_rewards = torch.FloatTensor(self.rewards[indices])
        batch_next_states = torch.FloatTensor(self.next_states[indices])
        batch_dones = torch.FloatTensor(self.dones[indices])
        
        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones
    
    def __len__(self):
        return self.size


class DQNAgent:
    """DQNæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, config: DQNConfig):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config
        
        # æ€§èƒ½ä¼˜åŒ– - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        self.optimized_batch_size = OPTIMIZED_BATCH_SIZES.get('DQN', config.batch_size)
        self.config.batch_size = self.optimized_batch_size
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºç½‘ç»œ
        self.q_network = DQNNetwork(
            state_dim, action_dim, config.hidden_dim, config.dueling_dqn
        ).to(self.device)
        
        self.target_q_network = DQNNetwork(
            state_dim, action_dim, config.hidden_dim, config.dueling_dqn
        ).to(self.device)
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
        self.hard_update(self.target_q_network, self.q_network)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=config.lr)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = DQNReplayBuffer(config.buffer_size, state_dim)
        
        # æ¢ç´¢å‚æ•°
        self.epsilon = config.epsilon
        self.step_count = 0
        self.update_count = 0
        
        # è®­ç»ƒç»Ÿè®¡
        self.losses = []
        self.q_values = []
    
    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """é€‰æ‹©åŠ¨ä½œ - Îµ-è´ªå©ªç­–ç•¥"""
        if training and random.random() < self.epsilon:
            # éšæœºæ¢ç´¢
            return random.randrange(self.action_dim)
        else:
            # è´ªå©ªé€‰æ‹©
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                q_values = self.q_network(state_tensor)
                self.q_values.append(q_values.max().item())
                return q_values.argmax().item()
    
    def store_experience(self, state: np.ndarray, action: int, reward: float,
                        next_state: np.ndarray, done: bool):
        """å­˜å‚¨ç»éªŒ"""
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def update(self) -> Dict[str, float]:
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        if len(self.replay_buffer) < self.config.batch_size:
            return {}
        
        self.step_count += 1
        
        # é¢„çƒ­æœŸä¸æ›´æ–°
        if self.step_count < self.config.warmup_steps:
            return {}
        
        # æ›´æ–°é¢‘ç‡æ§åˆ¶
        if self.step_count % self.config.update_freq != 0:
            return {}
        
        self.update_count += 1
        
        # é‡‡æ ·ç»éªŒæ‰¹æ¬¡
        batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = \
            self.replay_buffer.sample(self.config.batch_size)
        
        batch_states = batch_states.to(self.device)
        batch_actions = batch_actions.to(self.device)
        batch_rewards = batch_rewards.to(self.device)
        batch_next_states = batch_next_states.to(self.device)
        batch_dones = batch_dones.to(self.device)
        
        # è®¡ç®—æŸå¤±å¹¶æ›´æ–°
        loss = self._compute_loss(batch_states, batch_actions, batch_rewards, 
                                batch_next_states, batch_dones)
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10.0)
        self.optimizer.step()
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.update_count % self.config.target_update_freq == 0:
            self.hard_update(self.target_q_network, self.q_network)
        
        # è¡°å‡æ¢ç´¢ç‡
        self.epsilon = max(self.config.min_epsilon, 
                          self.epsilon * self.config.epsilon_decay)
        
        self.losses.append(loss.item())
        
        return {
            'loss': loss.item(),
            'epsilon': self.epsilon,
            'q_value_avg': float(np.mean(self.q_values[-100:])) if self.q_values else 0.0
        }
    
    def _compute_loss(self, states: torch.Tensor, actions: torch.Tensor, 
                     rewards: torch.Tensor, next_states: torch.Tensor, 
                     dones: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—DQNæŸå¤±"""
        # å½“å‰Qå€¼
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            if self.config.double_dqn:
                # Double DQN
                next_actions = self.q_network(next_states).argmax(dim=1)
                next_q_values = self.target_q_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
            else:
                # æ ‡å‡†DQN
                next_q_values = self.target_q_network(next_states).max(dim=1)[0]
            
            target_q_values = rewards + (1 - dones) * self.config.gamma * next_q_values
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(current_q_values, target_q_values)
        
        return loss
    
    def hard_update(self, target: nn.Module, source: nn.Module):
        """ç¡¬æ›´æ–°ç½‘ç»œå‚æ•°"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(param.data)
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_q_network_state_dict': self.target_q_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'step_count': self.step_count,
            'update_count': self.update_count
        }, f"{filepath}_dqn.pth")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(f"{filepath}_dqn.pth", map_location=self.device)
        
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_q_network.load_state_dict(checkpoint['target_q_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']
        self.step_count = checkpoint['step_count']
        self.update_count = checkpoint['update_count']


class DQNEnvironment:
    """DQNè®­ç»ƒç¯å¢ƒ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, num_vehicles: int = 12, num_rsus: int = 4, num_uavs: int = 2):
        from single_agent.common_state_action import UnifiedStateActionSpace
        
        self.config = DQNConfig()
        self.num_vehicles = num_vehicles
        self.num_rsus = num_rsus
        self.num_uavs = num_uavs
        
        # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€çš„çŠ¶æ€ç»´åº¦è®¡ç®—
        self.local_state_dim, self.global_state_dim, self.state_dim = \
            UnifiedStateActionSpace.calculate_state_dim(num_vehicles, num_rsus, num_uavs)
        
        # DQNä½¿ç”¨ç¦»æ•£åŠ¨ä½œï¼šç®€åŒ–ä¸º125ä¸ªç»„åˆï¼ˆ5^3ï¼‰
        self.action_dim = 125
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agent = DQNAgent(self.state_dim, self.action_dim, self.config)
        
        # åŠ¨ä½œæ˜ å°„
        self.action_map = self._build_action_map()
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_count = 0
        self.step_count = 0
        
        print(f"DQNç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼ˆä¼˜åŒ–ç‰ˆv2.0ï¼‰")
        print(f"ç½‘ç»œæ‹“æ‰‘: {num_vehicles}è¾†è½¦ + {num_rsus}ä¸ªRSU + {num_uavs}ä¸ªUAV")
        print(f"çŠ¶æ€ç»´åº¦: {self.state_dim} = å±€éƒ¨{self.local_state_dim} + å…¨å±€{self.global_state_dim}")
        print(f"åŠ¨ä½œç»´åº¦: {self.action_dim} (ç¦»æ•£)")
        print(f"Double DQN: {self.config.double_dqn}")
        print(f"Dueling DQN: {self.config.dueling_dqn}")
        print(f"ä¼˜åŒ–ç‰¹æ€§: ç»Ÿä¸€çŠ¶æ€ç©ºé—´, åŠ¨æ€æ‹“æ‰‘é€‚é…, å…¨å±€çŠ¶æ€")
    
    def _build_action_map(self) -> Dict[int, Dict[str, int]]:
        """æ„å»ºç¦»æ•£åŠ¨ä½œæ˜ å°„"""
        action_map = {}
        action_idx = 0
        
        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹ç±»å‹çš„5ä¸ªåŠ¨ä½œé€‰æ‹©åˆ›å»ºç»„åˆ
        for vehicle_action in range(5):
            for rsu_action in range(5):
                for uav_action in range(5):
                    action_map[action_idx] = {
                        'vehicle_agent': vehicle_action,
                        'rsu_agent': rsu_action,
                        'uav_agent': uav_action
                    }
                    action_idx += 1
        
        return action_map
    
    def get_state_vector(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """ä½¿ç”¨ç»Ÿä¸€æ–¹æ³•æ„å»ºçŠ¶æ€å‘é‡"""
        from single_agent.common_state_action import UnifiedStateActionSpace
        return UnifiedStateActionSpace.build_state_vector(
            node_states,
            system_metrics,
            self.num_vehicles,
            self.num_rsus,
            self.num_uavs,
            self.state_dim
        )
    
    def decompose_action(self, action_idx: int) -> Dict[str, int]:
        """å°†ç¦»æ•£åŠ¨ä½œç´¢å¼•è½¬æ¢ä¸ºå„èŠ‚ç‚¹åŠ¨ä½œ"""
        return self.action_map[action_idx]
    
    def get_actions(self, state: np.ndarray, training: bool = True) -> Dict[str, int]:
        """è·å–åŠ¨ä½œ"""
        discrete_action = self.agent.select_action(state, training)
        return self.decompose_action(discrete_action)
    
    def calculate_reward(self, system_metrics: Dict,
                       cache_metrics: Optional[Dict] = None,
                       migration_metrics: Optional[Dict] = None) -> float:
        """
        ä½¿ç”¨ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨
        """
        from utils.unified_reward_calculator import calculate_unified_reward
        return calculate_unified_reward(system_metrics, cache_metrics, migration_metrics, algorithm="general")
    
    def train_step(self, state: np.ndarray, action: int, reward: float,
                   next_state: np.ndarray, done: bool) -> Dict:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        # DQNéœ€è¦æ•´æ•°åŠ¨ä½œï¼Œå¦‚æœæ˜¯numpyæ•°ç»„åˆ™è½¬æ¢
        if isinstance(action, np.ndarray):
            if action.size == 1:
                action_int = int(action.item())
            else:
                action_int = int(action[0])
        else:
            action_int = int(action)
        
        # å­˜å‚¨ç»éªŒ
        self.agent.store_experience(state, action_int, reward, next_state, done)
        
        # æ›´æ–°ç½‘ç»œ
        training_info = self.agent.update()
        
        self.step_count += 1
        
        return training_info
    
    def save_models(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(filepath, exist_ok=True)
        self.agent.save_model(filepath)
        print(f"âœ“ DQNæ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_models(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        self.agent.load_model(filepath)
        print(f"âœ“ DQNæ¨¡å‹å·²åŠ è½½: {filepath}")
    
    def store_experience(self, state: np.ndarray, action: Union[np.ndarray, int], reward: float,
                        next_state: np.ndarray, done: bool, log_prob: float = 0.0, value: float = 0.0):
        """å­˜å‚¨ç»éªŒåˆ°ç¼“å†²åŒº - æ”¯æŒPPOå…¼å®¹æ€§"""
        # DQNéœ€è¦æ•´æ•°åŠ¨ä½œï¼Œå¦‚æœæ˜¯numpyæ•°ç»„åˆ™è½¬æ¢
        if isinstance(action, np.ndarray):
            if action.size == 1:
                action_int = int(action.item())
            else:
                action_int = int(action[0])
        else:
            action_int = int(action)
        
        # DQNåªä½¿ç”¨å‰5ä¸ªå‚æ•°ï¼Œlog_probå’Œvalueè¢«å¿½ç•¥
        self.agent.store_experience(state, action_int, reward, next_state, done)
        self.step_count += 1
    
    def update(self, last_value: float = 0.0) -> Dict:
        """æ›´æ–°ç½‘ç»œå‚æ•° - æ”¯æŒPPOå…¼å®¹æ€§"""
        # DQNä¸ä½¿ç”¨last_valueå‚æ•°
        return self.agent.update()
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return {
            'loss_avg': float(np.mean(self.agent.losses[-100:])) if self.agent.losses else 0.0,
            'q_value_avg': float(np.mean(self.agent.q_values[-100:])) if self.agent.q_values else 0.0,
            'epsilon': self.agent.epsilon,
            'buffer_size': len(self.agent.replay_buffer),
            'step_count': self.step_count,
            'update_count': self.agent.update_count,
            'double_dqn': self.config.double_dqn,
            'dueling_dqn': self.config.dueling_dqn
        }