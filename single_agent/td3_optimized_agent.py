"""
ä¼˜åŒ–çš„TD3æ™ºèƒ½ä½“å®ç°
ä¿®å¤äº†åŸå§‹å®ç°ä¸­çš„å…³é”®é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, Tuple
from collections import deque

from single_agent.td3_optimized import OptimizedTD3Actor, OptimizedTD3Critic, OptimizedTD3Config


class OptimizedTD3ReplayBuffer:
    """ä¼˜åŒ–çš„TD3ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int, state_dim: int, action_dim: int, alpha: float = 0.6):
        self.capacity = capacity
        self.ptr = 0
        self.size = 0
        self.alpha = alpha
        
        # é¢„åˆ†é…å†…å­˜
        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)
        self.rewards = np.zeros((capacity, 1), dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros((capacity, 1), dtype=np.float32)
        
        # ä¼˜å…ˆçº§ç›¸å…³
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.max_priority = 1.0
    
    def __len__(self):
        return self.size
    
    def push(self, state: np.ndarray, action: np.ndarray, reward: float, next_state: np.ndarray, done: bool):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.next_states[self.ptr] = next_state
        self.dones[self.ptr] = done
        
        # æ–°ç»éªŒä½¿ç”¨æœ€å¤§ä¼˜å…ˆçº§
        self.priorities[self.ptr] = self.max_priority
        
        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    
    def sample(self, batch_size: int, beta: float = 0.4):
        """é‡‡æ ·ç»éªŒ"""
        if self.size < batch_size:
            return None
        
        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        priorities = self.priorities[:self.size]
        
        # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
        if np.any(np.isnan(priorities)) or np.any(np.isinf(priorities)):
            print(f"è­¦å‘Š: å‘ç°NaNæˆ–Infä¼˜å…ˆçº§å€¼ï¼Œé‡ç½®ä¸ºé»˜è®¤å€¼")
            priorities = np.ones_like(priorities)
            self.priorities[:self.size] = priorities
            self.max_priority = 1.0
        
        # ç¡®ä¿ä¼˜å…ˆçº§ä¸ºæ­£å€¼
        priorities = np.maximum(priorities, 1e-8)
        
        probs = priorities ** self.alpha
        probs_sum = probs.sum()
        
        # æ£€æŸ¥æ¦‚ç‡å’Œæ˜¯å¦æœ‰æ•ˆ
        if probs_sum <= 0 or np.isnan(probs_sum) or np.isinf(probs_sum):
            print(f"è­¦å‘Š: æ¦‚ç‡å’Œæ— æ•ˆ ({probs_sum})ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ")
            probs = np.ones_like(probs) / len(probs)
        else:
            probs /= probs_sum
        
        # é‡‡æ ·ç´¢å¼•
        indices = np.random.choice(self.size, batch_size, p=probs)
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        weights = (self.size * probs[indices]) ** (-beta)
        max_weight = weights.max()
        
        # æ£€æŸ¥æƒé‡æ˜¯å¦æœ‰æ•ˆ
        if max_weight <= 0 or np.isnan(max_weight) or np.isinf(max_weight):
            weights = np.ones_like(weights)
        else:
            weights /= max_weight
        
        return (
            self.states[indices],
            self.actions[indices],
            self.rewards[indices],
            self.next_states[indices],
            self.dones[indices],
            indices,
            weights
        )
    
    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """æ›´æ–°ä¼˜å…ˆçº§"""
        # ç¡®ä¿prioritiesæ˜¯ä¸€ç»´æ•°ç»„
        if priorities.ndim > 1:
            priorities = priorities.flatten()
        
        # æ£€æŸ¥å¹¶å¤„ç†NaN/Infå€¼
        if np.any(np.isnan(priorities)) or np.any(np.isinf(priorities)):
            print(f"è­¦å‘Š: æ›´æ–°ä¼˜å…ˆçº§æ—¶å‘ç°NaNæˆ–Infå€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼æ›¿æ¢")
            priorities = np.where(np.isnan(priorities) | np.isinf(priorities), 1.0, priorities)
        
        # ç¡®ä¿ä¼˜å…ˆçº§ä¸ºæ­£å€¼
        priorities = np.maximum(priorities, 1e-8)
        
        self.priorities[indices] = priorities
        valid_max = np.max(priorities[np.isfinite(priorities)])
        self.max_priority = max(self.max_priority, valid_max)


class OptimizedTD3Agent:
    """ä¼˜åŒ–çš„TD3æ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, config: OptimizedTD3Config):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºç½‘ç»œ
        self.actor = OptimizedTD3Actor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.critic = OptimizedTD3Critic(state_dim, action_dim, config.hidden_dim).to(self.device)
        
        # ç›®æ ‡ç½‘ç»œ
        self.target_actor = OptimizedTD3Actor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.target_critic = OptimizedTD3Critic(state_dim, action_dim, config.hidden_dim).to(self.device)
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
        self.hard_update(self.target_actor, self.actor)
        self.hard_update(self.target_critic, self.critic)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.actor_lr_scheduler = optim.lr_scheduler.ExponentialLR(self.actor_optimizer, gamma=0.9995)
        self.critic_lr_scheduler = optim.lr_scheduler.ExponentialLR(self.critic_optimizer, gamma=0.9995)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.beta = 0.4
        self.beta_increment = (1.0 - 0.4) / 500000
        self.replay_buffer = OptimizedTD3ReplayBuffer(config.buffer_size, state_dim, action_dim)
        
        # è®­ç»ƒç»Ÿè®¡
        self.total_it = 0
        self.update_count = 0
        self.exploration_noise = config.exploration_noise
        self.initial_exploration_noise = config.exploration_noise  # è®°å½•åˆå§‹å™ªå£°
        
        # æŸå¤±è®°å½•
        self.actor_losses = deque(maxlen=1000)
        self.critic_losses = deque(maxlen=1000)
        
        # é¿å…å±€éƒ¨æœ€ä¼˜çš„å‚æ•°
        self.episode_count = 0  # è¿½è¸ªepisodeæ•°
        self.recent_rewards = deque(maxlen=50)  # è¿½è¸ªæœ€è¿‘50ä¸ªepisodeçš„å¥–åŠ±
        self.last_improvement_episode = 0  # ä¸Šæ¬¡æ”¹å–„çš„episode
        self.exploration_reset_interval = 100  # æ¯100ä¸ªepisodeé‡å¯ä¸€æ¬¡æ¢ç´¢
        
        # ğŸ”¥ æ–°å¢ï¼šæå‰ç»ˆæ­¢å’Œå™ªå£°é€€ç«æœºåˆ¶ (600è½®åå¯ç”¨)
        self.early_stop_episode = 600  # åœ¨600è½®åå¼€å§‹æ£€æµ‹æ˜¯å¦æå‰ç»ˆæ­¢
        self.noise_annealing_start = 600  # åœ¨600è½®åå¼€å§‹å™ªå£°é€€ç«
        self.noise_annealing_rate = 0.995  # å™ªå£°é€€ç«ç‡ (æ¯ä¸ªepisodeä¹˜ä»¥0.995)
        self.reward_std_threshold = 0.5  # å¥–åŠ±æ–¹å·®é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è®¤ä¸ºæ”¶æ•›
        
        print(f"âœ“ ä¼˜åŒ–TD3æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ ç½‘ç»œéšè—ç»´åº¦: {config.hidden_dim}")
        print(f"âœ“ ç¼“å†²åŒºå¤§å°: {config.buffer_size}")
        print(f"âœ“ å¯ç”¨é¿å…å±€éƒ¨æœ€ä¼˜æœºåˆ¶ (å‘¨æœŸé‡å¯é—´éš”: {self.exploration_reset_interval}ep)")
    
    def select_action(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state.reshape(1, -1)).to(self.device)
        
        with torch.no_grad():
            action = self.actor(state_tensor).cpu().data.numpy().flatten()
        
        if training:
            # æ·»åŠ æ¢ç´¢å™ªå£°
            noise = np.random.normal(0, self.exploration_noise, size=self.action_dim)
            action = action + noise
            action = np.clip(action, -1.0, 1.0)
            
            # å™ªå£°è¡°å‡
            self.exploration_noise = max(
                self.config.min_noise,
                self.exploration_noise * self.config.noise_decay
            )
        
        return action
    
    def set_episode_count(self, episode: int, recent_reward: float = None):
        """
        æ›´æ–°episodeè®¡æ•°ï¼Œå®ç°è‡ªé€‚åº”æ¢ç´¢é‡å¯æœºåˆ¶
        """
        self.episode_count = episode
        
        if recent_reward is not None:
            self.recent_rewards.append(recent_reward)
        
        # ğŸ”¥ æ–°å¢ï¼šå™ªå£°é€€ç« (600è½®å)
        if episode >= self.noise_annealing_start:
            # é€æ¸é™ä½å™ªå£°ï¼Œä½†ä¿æŒæœ€å°å™ªå£°
            self.exploration_noise = max(
                self.config.min_noise,
                self.exploration_noise * self.noise_annealing_rate
            )
            if episode % 50 == 0:  # æ¯50è½®æŠ¥å‘Šä¸€æ¬¡
                print(f"ğŸ’¨ Episode {episode}: å™ªå£°é€€ç« -> {self.exploration_noise:.4f}")
        
        # å‘¨æœŸæ€§é‡å¯æ¢ç´¢ï¼šæ¯100ä¸ªepisodeæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦é™·å…¥å±€éƒ¨æœ€ä¼˜
        if episode % self.exploration_reset_interval == 0 and episode > 100:
            # è®¡ç®—æœ€è¿‘50ä¸ªepisodeçš„å¹³å‡å¥–åŠ±
            if len(self.recent_rewards) >= 30:
                recent_avg = np.mean(list(self.recent_rewards)[-30:])
                earlier_avg = np.mean(list(self.recent_rewards)[:30])
                
                # å¦‚æœæ²¡æœ‰æ˜¾è‘—æ”¹å–„ï¼Œé‡å¯æ¢ç´¢
                improvement_ratio = (earlier_avg - recent_avg) / (abs(earlier_avg) + 1e-6)
                
                if improvement_ratio < 0.05:  # æ”¹å–„å°‘äº5%
                    # é‡å¯å™ªå£°
                    old_noise = self.exploration_noise
                    self.exploration_noise = self.initial_exploration_noise * 0.5  # é‡å¯ä¸ºåˆå§‹å€¼çš„50%
                    print(f"ğŸ”„ Episode {episode}: æ£€æµ‹åˆ°å±€éƒ¨æœ€ä¼˜,é‡å¯æ¢ç´¢")
                    print(f"   (æ”¹å–„ç‡: {improvement_ratio*100:.2f}% < 5%)")
                    print(f"   æ¢ç´¢å™ªå£°: {old_noise:.4f} â†’ {self.exploration_noise:.4f}")
                    print(f"   æœ€è¿‘å¹³å‡å¥–åŠ±: {recent_avg:.4f} (æ—©æœŸ: {earlier_avg:.4f})")
    
    def check_early_stopping(self) -> bool:
        """
        ğŸ”¥ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦åº”è¯¥æå‰ç»ˆæ­¢è®­ç»ƒ (600è½®å)
        åŸºäºæœ€è¿‘50ä¸ªepisodeå¥–åŠ±æ–¹å·®åˆ¤æ–­æ”¶æ•›
        """
        if self.episode_count < self.early_stop_episode:
            return False
        
        if len(self.recent_rewards) < 50:
            return False
        
        # è®¡ç®—æœ€è¿‘50ä¸ªepisodeå¥–åŠ±çš„æ ‡å‡†å·®
        reward_std = np.std(list(self.recent_rewards))
        
        if reward_std < self.reward_std_threshold:
            print(f"âœ… Episode {self.episode_count}: è®­ç»ƒæ”¶æ•›ï¼Œæå‰ç»ˆæ­¢")
            print(f"   å¥–åŠ±æ ‡å‡†å·®: {reward_std:.4f} < {self.reward_std_threshold}")
            print(f"   æœ€è¿‘50è½®å¹³å‡å¥–åŠ±: {np.mean(list(self.recent_rewards)):.4f}")
            return True
        
        return False
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, reward: float,
                        next_state: np.ndarray, done: bool):
        """å­˜å‚¨ç»éªŒ"""
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def update(self) -> Dict[str, float]:
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        if len(self.replay_buffer) < self.config.batch_size:
            return {}
        
        self.total_it += 1
        
        # æ›´æ–°beta
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        # é‡‡æ ·ç»éªŒ
        batch = self.replay_buffer.sample(self.config.batch_size, self.beta)
        if batch is None:
            return {}
        
        states, actions, rewards, next_states, dones, indices, weights = batch
        
        # è½¬æ¢ä¸ºtensor
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.FloatTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        weights = torch.FloatTensor(weights).to(self.device)
        
        # æ›´æ–°Critic
        critic_loss, td_errors = self._update_critic(states, actions, rewards, next_states, dones, weights)
        
        # æ›´æ–°ä¼˜å…ˆçº§
        td_errors_np = td_errors.cpu().data.numpy()
        
        # æ£€æŸ¥TDè¯¯å·®æ˜¯å¦åŒ…å«NaN/Inf
        if np.any(np.isnan(td_errors_np)) or np.any(np.isinf(td_errors_np)):
            print(f"è­¦å‘Š: TDè¯¯å·®åŒ…å«NaNæˆ–Infå€¼ï¼Œä½¿ç”¨é»˜è®¤ä¼˜å…ˆçº§")
            priorities = np.ones_like(td_errors_np) * 1e-6
        else:
            priorities = np.abs(td_errors_np) + 1e-6
        
        self.replay_buffer.update_priorities(indices, priorities)
        
        # å»¶è¿Ÿç­–ç•¥æ›´æ–°
        actor_loss = 0.0
        if self.total_it % self.config.policy_delay == 0:
            actor_loss = self._update_actor(states)
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self.soft_update(self.target_actor, self.actor, self.config.tau)
            self.soft_update(self.target_critic, self.critic, self.config.tau)
        
        self.update_count += 1
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if self.update_count % 1000 == 0:
            self.actor_lr_scheduler.step()
            self.critic_lr_scheduler.step()
        
        return {
            'critic_loss': critic_loss,
            'actor_loss': actor_loss,
            'exploration_noise': self.exploration_noise,
            'buffer_size': len(self.replay_buffer),
            'update_count': self.update_count,
            'beta': self.beta
        }
    
    def _update_critic(self, states: torch.Tensor, actions: torch.Tensor, 
                      rewards: torch.Tensor, next_states: torch.Tensor, 
                      dones: torch.Tensor, weights: torch.Tensor) -> Tuple[float, torch.Tensor]:
        """æ›´æ–°Criticç½‘ç»œ"""
        with torch.no_grad():
            # ç›®æ ‡ç­–ç•¥å¹³æ»‘åŒ–
            next_actions = self.target_actor(next_states)
            
            # æ·»åŠ è£å‰ªå™ªå£°
            noise = torch.randn_like(next_actions) * self.config.target_noise
            noise = torch.clamp(noise, -self.config.noise_clip, self.config.noise_clip)
            next_actions = torch.clamp(next_actions + noise, -1.0, 1.0)
            
            # è®¡ç®—ç›®æ ‡Qå€¼ (å–ä¸¤ä¸ªQç½‘ç»œçš„æœ€å°å€¼)
            target_q1, target_q2 = self.target_critic(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2)
            target_q = rewards + (1 - dones) * self.config.gamma * target_q
        
        # å½“å‰Qå€¼
        current_q1, current_q2 = self.critic(states, actions)
        
        # æ£€æŸ¥Qå€¼æ˜¯å¦åŒ…å«NaN
        if torch.any(torch.isnan(current_q1)) or torch.any(torch.isnan(current_q2)):
            print("è­¦å‘Š: å½“å‰Qå€¼åŒ…å«NaN")
            current_q1 = torch.nan_to_num(current_q1, nan=0.0)
            current_q2 = torch.nan_to_num(current_q2, nan=0.0)
        
        if torch.any(torch.isnan(target_q)):
            print("è­¦å‘Š: ç›®æ ‡Qå€¼åŒ…å«NaN")
            target_q = torch.nan_to_num(target_q, nan=0.0)
        
        # TDè¯¯å·®
        td_error1 = target_q - current_q1
        td_error2 = target_q - current_q2
        
        # æ£€æŸ¥TDè¯¯å·®æ˜¯å¦åŒ…å«NaN
        if torch.any(torch.isnan(td_error1)) or torch.any(torch.isnan(td_error2)):
            print("è­¦å‘Š: TDè¯¯å·®åŒ…å«NaN")
            td_error1 = torch.nan_to_num(td_error1, nan=0.0)
            td_error2 = torch.nan_to_num(td_error2, nan=0.0)
        
        # åŠ æƒæŸå¤±
        critic_loss1 = (weights * td_error1.pow(2)).mean()
        critic_loss2 = (weights * td_error2.pow(2)).mean()
        critic_loss = critic_loss1 + critic_loss2
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦åŒ…å«NaN
        if torch.isnan(critic_loss):
            print("è­¦å‘Š: CriticæŸå¤±ä¸ºNaNï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°")
            return 0.0, torch.zeros_like(td_error1).detach()
        
        # æ›´æ–°Critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)
        self.critic_optimizer.step()
        
        self.critic_losses.append(critic_loss.item())
        
        # è¿”å›å¹³å‡TDè¯¯å·®ç”¨äºä¼˜å…ˆçº§æ›´æ–°
        td_errors = (td_error1 + td_error2) / 2
        
        return critic_loss.item(), td_errors.detach()
    
    def _update_actor(self, states: torch.Tensor) -> float:
        """æ›´æ–°Actorç½‘ç»œ"""
        # è®¡ç®—ç­–ç•¥æŸå¤± (åªä½¿ç”¨Q1ç½‘ç»œ)
        actions = self.actor(states)
        actor_loss = -self.critic.q1(states, actions).mean()
        
        # æ›´æ–°Actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
        self.actor_optimizer.step()
        
        self.actor_losses.append(actor_loss.item())
        
        return actor_loss.item()
    
    def soft_update(self, target: nn.Module, source: nn.Module, tau: float):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
    
    def hard_update(self, target: nn.Module, source: nn.Module):
        """ç¡¬æ›´æ–°ç½‘ç»œå‚æ•°"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(param.data)
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'target_actor_state_dict': self.target_actor.state_dict(),
            'target_critic_state_dict': self.target_critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'exploration_noise': self.exploration_noise,
            'update_count': self.update_count,
            'total_it': self.total_it,
        }, os.path.join(filepath, 'optimized_td3_agent.pth'))
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        import os
        checkpoint = torch.load(os.path.join(filepath, 'optimized_td3_agent.pth'), 
                               map_location=self.device)
        
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.target_actor.load_state_dict(checkpoint['target_actor_state_dict'])
        self.target_critic.load_state_dict(checkpoint['target_critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        
        self.exploration_noise = checkpoint.get('exploration_noise', self.config.exploration_noise)
        self.update_count = checkpoint.get('update_count', 0)
        self.total_it = checkpoint.get('total_it', 0)