"""
TD3 (Twin Delayed Deep Deterministic Policy Gradient) å•æ™ºèƒ½ä½“ç®—æ³•å®ç°
ä¸“é—¨é€‚é…MATD3-MIGç³»ç»Ÿçš„VECç¯å¢ƒ

ä¸»è¦ç‰¹ç‚¹:
1. Twin Criticç½‘ç»œå‡å°‘è¿‡ä¼°è®¡
2. å»¶è¿Ÿç­–ç•¥æ›´æ–°æé«˜ç¨³å®šæ€§
3. ç›®æ ‡ç­–ç•¥å¹³æ»‘åŒ–å‡å°‘æ–¹å·®
4. æ”¹è¿›çš„æ¢ç´¢ç­–ç•¥

å¯¹åº”è®ºæ–‡: Addressing Function Approximation Error in Actor-Critic Methods
"""
# æ€§èƒ½ä¼˜åŒ– - å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰
try:
    from tools.performance_optimization import OPTIMIZED_BATCH_SIZES
except ImportError:
    OPTIMIZED_BATCH_SIZES = {'TD3': 128}  # é»˜è®¤å€¼

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
from collections import deque
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass

from config import config


@dataclass
class TD3Config:
    """TD3ç®—æ³•é…ç½® - ğŸ¯ ä¼˜åŒ–ç‰ˆv2.0ï¼ˆå‡å°‘æ”¶æ•›åæŒ¯è¡ï¼‰"""
    # ç½‘ç»œç»“æ„
    hidden_dim: int = 400  
    actor_lr: float = 1e-4  # ğŸ”§ æé«˜Actorå­¦ä¹ ç‡ï¼Œå¢å¼ºç­–ç•¥æ›´æ–°åŠ›åº¦
    critic_lr: float = 8e-5  # ğŸ”§ é€‚åº¦æé«˜Criticå­¦ä¹ ç‡ï¼Œè¿½è¸ªæ›´ç²¾ç¡®
    
    # è®­ç»ƒå‚æ•°
    batch_size: int = 256
    buffer_size: int = 100000
    tau: float = 0.005  # ğŸ”§ å›è°ƒè‡³ç¨³å®šå€¼ï¼Œå¹³è¡¡ç›®æ ‡ç½‘ç»œè·Ÿéšé€Ÿåº¦
    gamma: float = 0.99  
    
    # TD3ç‰¹æœ‰å‚æ•°
    policy_delay: int = 2  # ğŸ”§ ç¼©çŸ­ç­–ç•¥å»¶è¿Ÿï¼Œå‡å°‘ç­–ç•¥è½åç°è±¡
    target_noise: float = 0.05
    noise_clip: float = 0.2
    
    # æ¢ç´¢å‚æ•°
    exploration_noise: float = 0.2
    noise_decay: float = 0.9997  # ğŸ”§ æ”¾æ…¢å™ªå£°è¡°å‡ï¼Œé¿å…åæœŸæ¢ç´¢ä¸è¶³
    min_noise: float = 0.05  # ğŸ”§ æé«˜æœ€å°å™ªå£°ï¼Œä¿æŒé•¿æœŸæ¢ç´¢
    
    # ğŸ”§ æ–°å¢ï¼šæ¢¯åº¦è£å‰ªé˜²æ­¢è¿‡æ‹Ÿåˆ
    gradient_clip_norm: float = 0.7  # ğŸ”§ æ”¾å®½æ¢¯åº¦è£å‰ªï¼Œå…è®¸é€‚åº¦æ›´æ–°
    use_gradient_clip: bool = True   # å¯ç”¨æ¢¯åº¦è£å‰ª
    
    def __post_init__(self):
        """ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼Œç”¨äºå›ºå®šæ‹“æ‰‘ä¼˜åŒ–"""
        import os
        
        # è¯»å–å›ºå®šæ‹“æ‰‘ä¼˜åŒ–å™¨è®¾ç½®çš„ç¯å¢ƒå˜é‡
        if 'TD3_HIDDEN_DIM' in os.environ:
            self.hidden_dim = int(os.environ['TD3_HIDDEN_DIM'])
            print(f"[TD3Config] ä»ç¯å¢ƒå˜é‡è¯»å– hidden_dim: {self.hidden_dim}")
            
        if 'TD3_ACTOR_LR' in os.environ:
            self.actor_lr = float(os.environ['TD3_ACTOR_LR'])
            print(f"[TD3Config] ä»ç¯å¢ƒå˜é‡è¯»å– actor_lr: {self.actor_lr}")
            
        if 'TD3_CRITIC_LR' in os.environ:
            self.critic_lr = float(os.environ['TD3_CRITIC_LR'])
            print(f"[TD3Config] ä»ç¯å¢ƒå˜é‡è¯»å– critic_lr: {self.critic_lr}")
            
        if 'TD3_BATCH_SIZE' in os.environ:
            self.batch_size = int(os.environ['TD3_BATCH_SIZE'])
            print(f"[TD3Config] ä»ç¯å¢ƒå˜é‡è¯»å– batch_size: {self.batch_size}")
            
        if 'TD3_TAU' in os.environ:
            self.tau = float(os.environ['TD3_TAU'])
            print(f"[TD3Config] ä»ç¯å¢ƒå˜é‡è¯»å– tau: {self.tau}")
            
        if 'TD3_EXPLORATION_NOISE' in os.environ:
            self.exploration_noise = float(os.environ['TD3_EXPLORATION_NOISE'])
            print(f"[TD3Config] ä»ç¯å¢ƒå˜é‡è¯»å– exploration_noise: {self.exploration_noise}")
            
        if 'TD3_POLICY_DELAY' in os.environ:
            self.policy_delay = int(os.environ['TD3_POLICY_DELAY'])
            print(f"[TD3Config] ä»ç¯å¢ƒå˜é‡è¯»å– policy_delay: {self.policy_delay}")
            
        if 'TD3_GRADIENT_CLIP' in os.environ:
            self.gradient_clip_norm = float(os.environ['TD3_GRADIENT_CLIP'])
            print(f"[TD3Config] ä»ç¯å¢ƒå˜é‡è¯»å– gradient_clip_norm: {self.gradient_clip_norm}")
    
    # PER å‚æ•°ï¼ˆä¼˜åŒ–ä»¥å‡å°‘ä½è´¨é‡æ ·æœ¬å½±å“ï¼‰
    per_alpha: float = 0.6  # ğŸ”§ å›è°ƒä¼˜å…ˆçº§æŒ‡æ•°ï¼Œå‡è½»æ—©æœŸè¿‡åº¦å…³æ³¨
    per_beta_start: float = 0.4  # ğŸ”§ å›è°ƒISèµ·ç‚¹ï¼Œå¹³è¡¡æ ·æœ¬æƒé‡
    per_beta_frames: int = 400000  # ğŸ”§ æ”¾ç¼“betaå¢é•¿ï¼Œç¨³å®šå­¦ä¹ 

    # åæœŸç¨³å®šç­–ç•¥å‚æ•°
    late_stage_start_updates: int = 90000  # ğŸ”§ çº¦ç­‰äº800è½®æ›´æ–°æ­¥
    late_stage_tau: float = 0.003
    late_stage_policy_delay: int = 3
    late_stage_noise_floor: float = 0.03
    td_error_clip: float = 4.0
    
    # è®­ç»ƒé¢‘ç‡
    update_freq: int = 1
    warmup_steps: int = 1000


class TD3Actor(nn.Module):
    """TD3 Actorç½‘ç»œ - ç¡®å®šæ€§ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256, max_action: float = 1.0):
        super(TD3Actor, self).__init__()
        
        self.max_action = max_action
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.0)
        
        # æœ€åä¸€å±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
        nn.init.uniform_(self.network[-2].weight, -3e-3, 3e-3)
        nn.init.uniform_(self.network[-2].bias, -3e-3, 3e-3)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        return self.max_action * self.network(state)


class TD3Critic(nn.Module):
    """TD3 Twin Criticç½‘ç»œ - åŒQç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(TD3Critic, self).__init__()
        
        # Q1ç½‘ç»œ
        self.q1_network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Q2ç½‘ç»œ
        self.q2_network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for network in [self.q1_network, self.q2_network]:
            for layer in network:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    nn.init.constant_(layer.bias, 0.0)
            
            # æœ€åä¸€å±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
            nn.init.uniform_(network[-1].weight, -3e-3, 3e-3)
            nn.init.uniform_(network[-1].bias, -3e-3, 3e-3)
    
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­ - è¿”å›ä¸¤ä¸ªQå€¼"""
        sa = torch.cat([state, action], dim=1)
        
        q1 = self.q1_network(sa)
        q2 = self.q2_network(sa)
        
        return q1, q2
    
    def q1(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """åªè¿”å›Q1å€¼ (ç”¨äºç­–ç•¥æ›´æ–°)"""
        sa = torch.cat([state, action], dim=1)
        return self.q1_network(sa)


class TD3ReplayBuffer:
    """TD3 Prioritized Experience Replay ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int, state_dim: int, action_dim: int, alpha: float = 0.6):
        self.capacity = capacity
        self.ptr = 0
        self.size = 0
        self.alpha = alpha
        
        # é¢„åˆ†é…å†…å­˜
        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)
        self.rewards = np.zeros(capacity, dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros(capacity, dtype=np.float32)
        # ä¼˜å…ˆçº§æ•°ç»„
        self.priorities = np.zeros(capacity, dtype=np.float32)
    
    def __len__(self):
        return self.size
    
    def push(self, state: np.ndarray, action: np.ndarray, reward: float, next_state: np.ndarray, done: bool):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        max_prio = self.priorities.max() if self.size > 0 else 1.0
        
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.next_states[self.ptr] = next_state
        self.dones[self.ptr] = float(done)
        self.priorities[self.ptr] = max_prio
        
        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    
    def sample(self, batch_size: int, beta: float):
        """æŒ‰ä¼˜å…ˆçº§é‡‡æ ·ç»éªŒ, è¿”å›æ ·æœ¬åŠé‡è¦æ€§æƒé‡å’Œç´¢å¼•"""
        if self.size == self.capacity:
            prios = self.priorities
        else:
            prios = self.priorities[:self.size]
        probs = prios ** self.alpha
        probs /= probs.sum()
        indices = np.random.choice(self.size, batch_size, p=probs)
        
        weights = (self.size * probs[indices]) ** (-beta)
        weights /= weights.max()  # å½’ä¸€åŒ–åˆ°[0,1]
        weights = weights.astype(np.float32)
        
        batch_states = torch.FloatTensor(self.states[indices])
        batch_actions = torch.FloatTensor(self.actions[indices])
        batch_rewards = torch.FloatTensor(self.rewards[indices]).unsqueeze(1)
        batch_next_states = torch.FloatTensor(self.next_states[indices])
        batch_dones = torch.FloatTensor(self.dones[indices]).unsqueeze(1)
        weights_tensor = torch.FloatTensor(weights).unsqueeze(1)
        
        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, indices, weights_tensor
    
    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """æ ¹æ®æ–°çš„TDè¯¯å·®æ›´æ–°ä¼˜å…ˆçº§"""
        self.priorities[indices] = priorities


class TD3Agent:
    """TD3æ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, config: TD3Config):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config
        
        # æ€§èƒ½ä¼˜åŒ– - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        self.optimized_batch_size = OPTIMIZED_BATCH_SIZES.get('TD3', config.batch_size)
        self.config.batch_size = self.optimized_batch_size
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºç½‘ç»œ
        self.actor = TD3Actor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.critic = TD3Critic(state_dim, action_dim, config.hidden_dim).to(self.device)
        
        # ç›®æ ‡ç½‘ç»œ
        self.target_actor = TD3Actor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.target_critic = TD3Critic(state_dim, action_dim, config.hidden_dim).to(self.device)
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
        self.hard_update(self.target_actor, self.actor)
        self.hard_update(self.target_critic, self.critic)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)
        # ğŸ”§ æš‚æ—¶ç¦ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œé¿å…çŸ­æœŸè®­ç»ƒä¸­å­¦ä¹ ç‡è¿‡å¿«è¡°å‡
        # self.actor_lr_scheduler = optim.lr_scheduler.ExponentialLR(self.actor_optimizer, gamma=0.995)
        # self.critic_lr_scheduler = optim.lr_scheduler.ExponentialLR(self.critic_optimizer, gamma=0.995)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        # PER betaå‚æ•°
        self.beta = config.per_beta_start
        self.beta_increment = (1.0 - config.per_beta_start) / max(1, config.per_beta_frames)
        self.replay_buffer = TD3ReplayBuffer(config.buffer_size, state_dim, action_dim, alpha=config.per_alpha)
        
        # æ¢ç´¢å™ªå£°
        self.exploration_noise = config.exploration_noise
        self.step_count = 0
        self.update_count = 0
        self.late_stage_applied = False
        
        # è®­ç»ƒç»Ÿè®¡
        self.actor_losses = []
        self.critic_losses = []
    
    def select_action(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.actor(state_tensor).cpu().numpy()[0]
        
        # æ·»åŠ æ¢ç´¢å™ªå£°
        if training:
            noise = np.random.normal(0, self.exploration_noise, size=action.shape)
            action = np.clip(action + noise, -1.0, 1.0)
        
        return action
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, reward: float,
                        next_state: np.ndarray, done: bool):
        """å­˜å‚¨ç»éªŒ"""
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def update(self) -> Dict[str, float]:
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        if len(self.replay_buffer) < self.config.batch_size:
            return {}
        
        self.step_count += 1
        
        # é¢„çƒ­æœŸä¸æ›´æ–°
        if self.step_count < self.config.warmup_steps:
            return {}
        
        self.update_count += 1
        
        # é‡‡æ ·ç»éªŒæ‰¹æ¬¡ (å«ç´¢å¼•ä¸ISæƒé‡)
        batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, indices, weights = \
            self.replay_buffer.sample(self.config.batch_size, self.beta)
        # æ›´æ–°beta
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        batch_states = batch_states.to(self.device)
        batch_actions = batch_actions.to(self.device)
        batch_rewards = batch_rewards.to(self.device)
        batch_next_states = batch_next_states.to(self.device)
        batch_dones = batch_dones.to(self.device)
        weights = weights.to(self.device)

        # æ›´æ–°Criticå¹¶è·å–TDè¯¯å·®
        critic_loss, td_errors = self._update_critic(batch_states, batch_actions, batch_rewards, 
                                        batch_next_states, batch_dones, weights)
        # æ ¹æ®TDè¯¯å·®æ›´æ–°ä¼˜å…ˆçº§
        self.replay_buffer.update_priorities(indices, td_errors.detach().cpu().numpy() + 1e-6)

        training_info = {'critic_loss': critic_loss}
        
        # åæœŸç¨³å®šç­–ç•¥ï¼šåŠ¨æ€è°ƒæ•´
        if not self.late_stage_applied and self.update_count >= self.config.late_stage_start_updates:
            self._apply_late_stage_strategy()
            self.late_stage_applied = True

        # å»¶è¿Ÿç­–ç•¥æ›´æ–°
        if self.update_count % self.config.policy_delay == 0:
            # æ›´æ–°Actor
            actor_loss = self._update_actor(batch_states)
            training_info['actor_loss'] = actor_loss
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self.soft_update(self.target_actor, self.actor, self.config.tau)
            self.soft_update(self.target_critic, self.critic, self.config.tau)
        
        # è¡°å‡å™ªå£°
        self.exploration_noise = max(self.config.min_noise, 
                                   self.exploration_noise * self.config.noise_decay)
        
        training_info['exploration_noise'] = self.exploration_noise
        
        return training_info

    def _apply_late_stage_strategy(self):
        """åº”ç”¨åæœŸç¨³å®šç­–ç•¥ï¼Œé˜²æ­¢å¥–åŠ±å´©æºƒ"""
        print("ğŸ”§ å¯ç”¨åæœŸç¨³å®šç­–ç•¥ï¼šè°ƒæ•´tau/policy_delay/å™ªå£°ä¸‹é™/TDè¯¯å·®è£å‰ª")
        self.config.tau = self.config.late_stage_tau
        self.config.policy_delay = self.config.late_stage_policy_delay
        self.config.min_noise = max(self.config.min_noise, self.config.late_stage_noise_floor)
        # é™åˆ¶ç°æœ‰å™ªå£°ä¸ä½äºæ–°ä¸‹é™
        self.exploration_noise = max(self.exploration_noise, self.config.min_noise)
    
    def _update_critic(self, states: torch.Tensor, actions: torch.Tensor, 
                      rewards: torch.Tensor, next_states: torch.Tensor, 
                      dones: torch.Tensor, weights: torch.Tensor) -> Tuple[float, torch.Tensor]:
        """æ›´æ–°Criticç½‘ç»œ"""
        with torch.no_grad():
            # ç›®æ ‡ç­–ç•¥å¹³æ»‘åŒ–
            next_actions = self.target_actor(next_states)
            
            # æ·»åŠ è£å‰ªå™ªå£°
            noise = torch.randn_like(next_actions) * self.config.target_noise
            noise = torch.clamp(noise, -self.config.noise_clip, self.config.noise_clip)
            next_actions = torch.clamp(next_actions + noise, -1.0, 1.0)
            
            # è®¡ç®—ç›®æ ‡Qå€¼ (å–ä¸¤ä¸ªQç½‘ç»œçš„æœ€å°å€¼)
            target_q1, target_q2 = self.target_critic(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2)
            target_q = rewards + (1 - dones) * self.config.gamma * target_q
        
        # å½“å‰Qå€¼
        current_q1, current_q2 = self.critic(states, actions)
        
        # CriticæŸå¤± (ä¸¤ä¸ªQç½‘ç»œçš„æŸå¤±ä¹‹å’Œ)
        # TDè¯¯å·®
        td_errors = (current_q1 - target_q)
        # åŠ æƒMSEæŸå¤±
        critic_loss = (weights * td_errors.pow(2)).mean() + (weights * (current_q2 - target_q).pow(2)).mean()
        
        # æ›´æ–°Critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        # TDè¯¯å·®è£å‰ªï¼Œé˜²æ­¢æç«¯å€¼ä¸»å¯¼PER
        if self.config.td_error_clip is not None:
            td_errors = td_errors.clamp(-self.config.td_error_clip, self.config.td_error_clip)
        # ğŸ”§ ä½¿ç”¨é…ç½®çš„æ¢¯åº¦è£å‰ªå‚æ•°
        if self.config.use_gradient_clip:
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.config.gradient_clip_norm)
        self.critic_optimizer.step()
        
        self.critic_losses.append(critic_loss.item())
        return critic_loss.item(), td_errors.abs().squeeze()
    
    def _update_actor(self, states: torch.Tensor) -> float:
        """æ›´æ–°Actorç½‘ç»œ"""
        # è®¡ç®—ç­–ç•¥æŸå¤± (åªä½¿ç”¨Q1ç½‘ç»œ)
        actions = self.actor(states)
        actor_loss = -self.critic.q1(states, actions).mean()
        
        # æ›´æ–°Actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        # ğŸ”§ ä½¿ç”¨é…ç½®çš„æ¢¯åº¦è£å‰ªå‚æ•°
        if self.config.use_gradient_clip:
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.config.gradient_clip_norm)
        self.actor_optimizer.step()
        
        self.actor_losses.append(actor_loss.item())
        # ğŸ”§ æš‚æ—¶ç¦ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
        # self.actor_lr_scheduler.step()
        # self.critic_lr_scheduler.step()
        return actor_loss.item()
    
    def soft_update(self, target: nn.Module, source: nn.Module, tau: float):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
    
    def hard_update(self, target: nn.Module, source: nn.Module):
        """ç¡¬æ›´æ–°ç½‘ç»œå‚æ•°"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(param.data)
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'target_actor_state_dict': self.target_actor.state_dict(),
            'target_critic_state_dict': self.target_critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'exploration_noise': self.exploration_noise,
            'step_count': self.step_count,
            'update_count': self.update_count
        }, f"{filepath}_td3.pth")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(f"{filepath}_td3.pth", map_location=self.device)
        
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.target_actor.load_state_dict(checkpoint['target_actor_state_dict'])
        self.target_critic.load_state_dict(checkpoint['target_critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.exploration_noise = checkpoint['exploration_noise']
        self.step_count = checkpoint['step_count']
        self.update_count = checkpoint['update_count']


class TD3Environment:
    """TD3è®­ç»ƒç¯å¢ƒ"""
    
    def __init__(self, num_vehicles: int = 12, num_rsus: int = 4, num_uavs: int = 2):
        self.config = TD3Config()
        self.num_vehicles = num_vehicles
        self.num_rsus = num_rsus
        self.num_uavs = num_uavs
        
        # ğŸ”§ ä¼˜åŒ–åçš„çŠ¶æ€ç»´åº¦ï¼šæ‰€æœ‰èŠ‚ç‚¹ç»Ÿä¸€ä¸º5ç»´ + å…¨å±€çŠ¶æ€8ç»´
        # è½¦è¾†çŠ¶æ€: NÃ—5ç»´ + RSUçŠ¶æ€: MÃ—5ç»´ + UAVçŠ¶æ€: KÃ—5ç»´ + å…¨å±€: 8ç»´
        self.local_state_dim = num_vehicles * 5 + num_rsus * 5 + num_uavs * 5
        self.global_state_dim = 8
        self.state_dim = self.local_state_dim + self.global_state_dim
        
        # ğŸ”§ ä¼˜åŒ–åçš„åŠ¨ä½œç©ºé—´ï¼šåŠ¨æ€é€‚é…ç½‘ç»œæ‹“æ‰‘
        # 3(ä»»åŠ¡åˆ†é…) + num_rsus(RSUé€‰æ‹©) + num_uavs(UAVé€‰æ‹©) + 7(æ§åˆ¶å‚æ•°)
        self.action_dim = 3 + num_rsus + num_uavs + 7
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agent = TD3Agent(self.state_dim, self.action_dim, self.config)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_count = 0
        self.step_count = 0
        
        print(f"TD3ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
        print(f"ç½‘ç»œæ‹“æ‰‘: {num_vehicles}è¾†è½¦ + {num_rsus}ä¸ªRSU + {num_uavs}ä¸ªUAV")
        print(f"çŠ¶æ€ç»´åº¦: {self.state_dim} = å±€éƒ¨{self.local_state_dim} ({num_vehicles}Ã—5 + {num_rsus}Ã—5 + {num_uavs}Ã—5) + å…¨å±€{self.global_state_dim}")
        print(f"åŠ¨ä½œç»´åº¦: {self.action_dim} (åŠ¨æ€é€‚é…: 3+{num_rsus}+{num_uavs}+7)")
        print(f"ç­–ç•¥å»¶è¿Ÿæ›´æ–°: {self.config.policy_delay}")
        print(f"ä¼˜åŒ–ç‰¹æ€§: ç§»é™¤æ§åˆ¶å‚æ•°å†—ä½™, æ·»åŠ å…¨å±€çŠ¶æ€, ç»Ÿä¸€å½’ä¸€åŒ–")
    
    def get_state_vector(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """
        ğŸ”§ ä¼˜åŒ–ç‰ˆçŠ¶æ€å‘é‡æ„å»º
        çŠ¶æ€ç»„æˆ: è½¦è¾†(NÃ—5) + RSU(MÃ—5) + UAV(KÃ—5) + å…¨å±€(8) ç»´
        """
        state_components = []
        
        # ========== 1. å±€éƒ¨èŠ‚ç‚¹çŠ¶æ€ ==========
        
        # è½¦è¾†çŠ¶æ€ (NÃ—5ç»´)
        for i in range(self.num_vehicles):
            vehicle_key = f'vehicle_{i}'
            if vehicle_key in node_states:
                vehicle_state = node_states[vehicle_key][:5]  # åªå–å‰5ç»´
                valid_state = [float(v) if np.isfinite(v) else 0.5 for v in vehicle_state]
                state_components.extend(valid_state)
            else:
                state_components.extend([0.5, 0.5, 0.0, 0.0, 0.0])
        
        # RSUçŠ¶æ€ (MÃ—5ç»´) - ç»Ÿä¸€ä¸º5ç»´
        for i in range(self.num_rsus):
            rsu_key = f'rsu_{i}'
            if rsu_key in node_states:
                rsu_state = node_states[rsu_key][:5]  # åªå–å‰5ç»´
                valid_state = [float(v) if np.isfinite(v) else 0.5 for v in rsu_state]
                state_components.extend(valid_state)
            else:
                state_components.extend([0.5, 0.5, 0.0, 0.0, 0.0])
        
        # UAVçŠ¶æ€ (KÃ—5ç»´) - ç»Ÿä¸€ä¸º5ç»´
        for i in range(self.num_uavs):
            uav_key = f'uav_{i}'
            if uav_key in node_states:
                uav_state = node_states[uav_key][:5]  # åªå–å‰5ç»´
                valid_state = [float(v) if np.isfinite(v) else 0.5 for v in uav_state]
                state_components.extend(valid_state)
            else:
                state_components.extend([0.5, 0.5, 0.5, 0.0, 0.0])
        
        # ========== 2. å…¨å±€ç³»ç»ŸçŠ¶æ€ (8ç»´) ==========
        global_state = self._build_global_state(node_states, system_metrics)
        state_components.extend(global_state)
        
        # ========== 3. æœ€ç»ˆå¤„ç† ==========
        state_vector = np.array(state_components[:self.state_dim], dtype=np.float32)
        
        # ç»´åº¦ä¸è¶³æ—¶è¡¥é½
        if len(state_vector) < self.state_dim:
            padding_needed = self.state_dim - len(state_vector)
            state_vector = np.pad(state_vector, (0, padding_needed), mode='constant', constant_values=0.5)
        
        # æ•°å€¼å®‰å…¨æ£€æŸ¥
        state_vector = np.nan_to_num(state_vector, nan=0.5, posinf=1.0, neginf=0.0)
        state_vector = np.clip(state_vector, 0.0, 1.0)  # ç¡®ä¿æ‰€æœ‰å€¼åœ¨[0,1]
        
        return state_vector
    
    def _build_global_state(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """
        æ„å»ºå…¨å±€ç³»ç»ŸçŠ¶æ€ï¼ˆ8ç»´ï¼‰
        æä¾›ç³»ç»Ÿçº§åˆ«çš„æ•´ä½“ä¿¡æ¯ï¼Œè¾…åŠ©æ™ºèƒ½ä½“è¿›è¡Œå…¨å±€åè°ƒå†³ç­–
        """
        # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„é˜Ÿåˆ—ä¿¡æ¯ï¼ˆä»å±€éƒ¨çŠ¶æ€ä¸­æå–ï¼‰
        all_queues = []
        for i in range(self.num_vehicles):
            v_state = node_states.get(f'vehicle_{i}')
            if v_state is not None and len(v_state) > 3:
                all_queues.append(v_state[3])  # é˜Ÿåˆ—ç»´åº¦
        for i in range(self.num_rsus):
            r_state = node_states.get(f'rsu_{i}')
            if r_state is not None and len(r_state) > 3:
                all_queues.append(r_state[3])
        
        # è®¡ç®—å…¨å±€æŒ‡æ ‡
        avg_queue = np.mean(all_queues) if all_queues else 0.0
        congestion_ratio = len([q for q in all_queues if q > 0.5]) / max(1, len(all_queues))
        
        # ä»system_metricsè·å–ç³»ç»Ÿçº§æŒ‡æ ‡
        completion_rate = system_metrics.get('task_completion_rate', 0.5)
        avg_energy = system_metrics.get('total_energy_consumption', 0.0) / max(1, self.num_vehicles + self.num_rsus + self.num_uavs)
        cache_hit_rate = system_metrics.get('cache_hit_rate', 0.0)
        
        # æ„å»ºå…¨å±€çŠ¶æ€å‘é‡
        global_state = np.array([
            np.clip(avg_queue, 0.0, 1.0),           # å¹³å‡é˜Ÿåˆ—å ç”¨ç‡
            np.clip(congestion_ratio, 0.0, 1.0),    # æ‹¥å¡èŠ‚ç‚¹æ¯”ä¾‹
            np.clip(completion_rate, 0.0, 1.0),     # ä»»åŠ¡å®Œæˆç‡
            np.clip(avg_energy / 1000.0, 0.0, 1.0), # å¹³å‡èƒ½è€—
            np.clip(cache_hit_rate, 0.0, 1.0),      # ç¼“å­˜å‘½ä¸­ç‡
            0.0,  # episodeè¿›åº¦ï¼ˆéœ€è¦ä»å¤–éƒ¨ä¼ å…¥ï¼‰
            np.clip(len([q for q in all_queues if q > 0]) / max(1, len(all_queues)), 0.0, 1.0),  # æ´»è·ƒèŠ‚ç‚¹æ¯”ä¾‹
            np.clip(sum(all_queues) / max(1, len(all_queues)), 0.0, 1.0)  # ç½‘ç»œæ€»è´Ÿè½½
        ], dtype=np.float32)
        
        return global_state
    
    def decompose_action(self, action: np.ndarray) -> Dict[str, np.ndarray]:
        """
        ğŸ”§ ä¼˜åŒ–ç‰ˆåŠ¨ä½œåˆ†è§£ï¼šåŠ¨æ€é€‚é…ç½‘ç»œæ‹“æ‰‘
        åŠ¨ä½œç©ºé—´ï¼š3(åˆ†é…) + num_rsus(RSUé€‰æ‹©) + num_uavs(UAVé€‰æ‹©) + 7(æ§åˆ¶)
        """
        actions = {}
        
        # ç¡®ä¿actioné•¿åº¦è¶³å¤Ÿ
        if len(action) < self.action_dim:
            action = np.pad(action, (0, self.action_dim - len(action)), mode='constant')
        
        # åŠ¨æ€åˆ†è§£åŠ¨ä½œ
        idx = 0
        
        # 1. ä»»åŠ¡åˆ†é…åå¥½ï¼ˆ3ç»´ï¼‰
        task_allocation = action[idx:idx+3]
        idx += 3
        
        # 2. RSUé€‰æ‹©æƒé‡ï¼ˆnum_rsusç»´ï¼‰
        rsu_selection = action[idx:idx+self.num_rsus]
        idx += self.num_rsus
        
        # 3. UAVé€‰æ‹©æƒé‡ï¼ˆnum_uavsç»´ï¼‰
        uav_selection = action[idx:idx+self.num_uavs]
        idx += self.num_uavs
        
        # 4. æ§åˆ¶å‚æ•°ï¼ˆ7ç»´ï¼‰
        control_params = action[idx:idx+7]
        
        # æ„å»ºvehicle_agentçš„å®Œæ•´åŠ¨ä½œï¼ˆç”¨äºä»¿çœŸå™¨ï¼‰
        actions['vehicle_agent'] = np.concatenate([
            task_allocation,   # 3ç»´
            rsu_selection,     # num_rsusç»´
            uav_selection,     # num_uavsç»´
            control_params     # 7ç»´
        ])
        
        # RSUå’ŒUAV agentçš„åŠ¨ä½œï¼ˆç”¨äºé€‰æ‹©æ¦‚ç‡è®¡ç®—ï¼‰
        actions['rsu_agent'] = rsu_selection
        actions['uav_agent'] = uav_selection
        
        return actions
    
    def get_actions(self, state: np.ndarray, training: bool = True) -> Dict[str, np.ndarray]:
        """è·å–åŠ¨ä½œ"""
        global_action = self.agent.select_action(state, training)
        return self.decompose_action(global_action)
    
    def calculate_reward(self, system_metrics: Dict, 
                       cache_metrics: Optional[Dict] = None,
                       migration_metrics: Optional[Dict] = None) -> float:
        """
        ä½¿ç”¨ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨
        """
        from utils.unified_reward_calculator import calculate_unified_reward
        return calculate_unified_reward(system_metrics, cache_metrics, migration_metrics, algorithm="general")
    
    def train_step(self, state: np.ndarray, action: Union[np.ndarray, int], reward: float,
                   next_state: np.ndarray, done: bool) -> Dict:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        # TD3éœ€è¦numpyæ•°ç»„ï¼Œå¦‚æœæ˜¯æ•´æ•°åˆ™è½¬æ¢
        if isinstance(action, int):
            action = np.array([action], dtype=np.float32)
        elif not isinstance(action, np.ndarray):
            action = np.array(action, dtype=np.float32)
        
        # å­˜å‚¨ç»éªŒ
        self.agent.store_experience(state, action, reward, next_state, done)
        
        # æ›´æ–°ç½‘ç»œ
        training_info = self.agent.update()
        
        self.step_count += 1
        
        return training_info
    
    def save_models(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(filepath, exist_ok=True)
        self.agent.save_model(filepath)
        print(f"âœ“ TD3æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_models(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        self.agent.load_model(filepath)
        print(f"âœ“ TD3æ¨¡å‹å·²åŠ è½½: {filepath}")
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, reward: float,
                        next_state: np.ndarray, done: bool, log_prob: float = 0.0, value: float = 0.0):
        """å­˜å‚¨ç»éªŒåˆ°ç¼“å†²åŒº - æ”¯æŒPPOå…¼å®¹æ€§"""
        # TD3åªä½¿ç”¨å‰5ä¸ªå‚æ•°ï¼Œlog_probå’Œvalueè¢«å¿½ç•¥
        self.agent.store_experience(state, action, reward, next_state, done)
        self.step_count += 1
    
    def update(self, last_value: float = 0.0) -> Dict:
        """æ›´æ–°ç½‘ç»œå‚æ•° - æ”¯æŒPPOå…¼å®¹æ€§"""
        # TD3ä¸ä½¿ç”¨last_valueå‚æ•°
        return self.agent.update()
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return {
            'actor_loss_avg': float(np.mean(self.agent.actor_losses[-100:])) if self.agent.actor_losses else 0.0,
            'critic_loss_avg': float(np.mean(self.agent.critic_losses[-100:])) if self.agent.critic_losses else 0.0,
            'exploration_noise': self.agent.exploration_noise,
            'buffer_size': len(self.agent.replay_buffer),
            'step_count': self.step_count,
            'update_count': self.agent.update_count,
            'policy_delay': self.config.policy_delay
        }