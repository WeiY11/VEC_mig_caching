"""
TD3 (Twin Delayed Deep Deterministic Policy Gradient) å•æ™ºèƒ½ä½“ç®—æ³•å®ç°
ä¸“é—¨é€‚é…MATD3-MIGç³»ç»Ÿçš„VECç¯å¢ƒ

ä¸»è¦ç‰¹ç‚¹:
1. Twin Criticç½‘ç»œå‡å°‘è¿‡ä¼°è®¡
2. å»¶è¿Ÿç­–ç•¥æ›´æ–°æé«˜ç¨³å®šæ€§
3. ç›®æ ‡ç­–ç•¥å¹³æ»‘åŒ–å‡å°‘æ–¹å·®
4. æ”¹è¿›çš„æ¢ç´¢ç­–ç•¥

å¯¹åº”è®ºæ–‡: Addressing Function Approximation Error in Actor-Critic Methods
"""
# æ€§èƒ½ä¼˜åŒ– - å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰
try:
    from tools.performance_optimization import OPTIMIZED_BATCH_SIZES
except ImportError:
    OPTIMIZED_BATCH_SIZES = {'TD3': 128}  # é»˜è®¤å€¼

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
from collections import deque
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass

from config import config


@dataclass
class TD3Config:
    """TD3ç®—æ³•é…ç½® (å·²è°ƒä¼˜)"""
    # ç½‘ç»œç»“æ„
    hidden_dim: int = 400  # æå‡å®¹é‡
    actor_lr: float = 5e-5  # ğŸ”§ ä»1e-4é™ä½åˆ°5e-5 - é˜²æ­¢è¿‡æ‹Ÿåˆ
    critic_lr: float = 1e-4  # ä¿æŒCriticå­¦ä¹ ç‡
    
    # è®­ç»ƒå‚æ•°
    batch_size: int = 256
    buffer_size: int = 100000
    tau: float = 0.003  # ğŸ”§ ä»0.005é™ä½åˆ°0.003 - æ›´æ…¢çš„ç›®æ ‡ç½‘ç»œæ›´æ–°
    gamma: float = 0.99  # æŠ˜æ‰£å› å­
    
    # TD3ç‰¹æœ‰å‚æ•° - æŠ—è¿‡æ‹Ÿåˆä¼˜åŒ–
    policy_delay: int = 4   # ğŸ”§ ä»2å¢åŠ åˆ°4 - æ›´å°‘çš„ç­–ç•¥æ›´æ–°é¢‘ç‡
    target_noise: float = 0.05  # ğŸ”§ ä»0.1é™ä½åˆ°0.05 - å‡å°‘æ¢ç´¢å™ªå£°
    noise_clip: float = 0.2  # ğŸ”§ ä»0.3é™ä½åˆ°0.2 - é™åˆ¶å™ªå£°å¹…åº¦
    
    # æ¢ç´¢å‚æ•°
    exploration_noise: float = 0.2
    noise_decay: float = 0.9998  # ğŸ”§ å‡ç¼“å™ªå£°è¡°å‡ï¼Œä¿æŒæ›´é•¿æ—¶é—´çš„æ¢ç´¢
    min_noise: float = 0.05
    
    # ğŸ”§ æ–°å¢ï¼šæ¢¯åº¦è£å‰ªé˜²æ­¢è¿‡æ‹Ÿåˆ
    gradient_clip_norm: float = 1.0  # æ¢¯åº¦è£å‰ªé˜ˆå€¼
    use_gradient_clip: bool = True   # å¯ç”¨æ¢¯åº¦è£å‰ª
    
    # PER å‚æ•°
    per_alpha: float = 0.6  # 0 è¡¨ç¤ºUniform, 1 è¡¨ç¤ºå®Œå…¨ä¾èµ–TDè¯¯å·®
    per_beta_start: float = 0.4  # ISæƒé‡çš„åˆå§‹beta
    per_beta_frames: int = 500000  # betaä»åˆå§‹å€¼çº¿æ€§å¢åŠ åˆ°1.0æ‰€éœ€çš„æ­¥æ•°
    
    # è®­ç»ƒé¢‘ç‡
    update_freq: int = 1
    warmup_steps: int = 1000  # ğŸ”§ å‡å°‘é¢„çƒ­æ­¥æ•°ï¼Œç¡®ä¿çŸ­æœŸè®­ç»ƒèƒ½å¼€å§‹å­¦ä¹ 


class TD3Actor(nn.Module):
    """TD3 Actorç½‘ç»œ - ç¡®å®šæ€§ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256, max_action: float = 1.0):
        super(TD3Actor, self).__init__()
        
        self.max_action = max_action
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.0)
        
        # æœ€åä¸€å±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
        nn.init.uniform_(self.network[-2].weight, -3e-3, 3e-3)
        nn.init.uniform_(self.network[-2].bias, -3e-3, 3e-3)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        return self.max_action * self.network(state)


class TD3Critic(nn.Module):
    """TD3 Twin Criticç½‘ç»œ - åŒQç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(TD3Critic, self).__init__()
        
        # Q1ç½‘ç»œ
        self.q1_network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Q2ç½‘ç»œ
        self.q2_network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for network in [self.q1_network, self.q2_network]:
            for layer in network:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    nn.init.constant_(layer.bias, 0.0)
            
            # æœ€åä¸€å±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
            nn.init.uniform_(network[-1].weight, -3e-3, 3e-3)
            nn.init.uniform_(network[-1].bias, -3e-3, 3e-3)
    
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­ - è¿”å›ä¸¤ä¸ªQå€¼"""
        sa = torch.cat([state, action], dim=1)
        
        q1 = self.q1_network(sa)
        q2 = self.q2_network(sa)
        
        return q1, q2
    
    def q1(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """åªè¿”å›Q1å€¼ (ç”¨äºç­–ç•¥æ›´æ–°)"""
        sa = torch.cat([state, action], dim=1)
        return self.q1_network(sa)


class TD3ReplayBuffer:
    """TD3 Prioritized Experience Replay ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int, state_dim: int, action_dim: int, alpha: float = 0.6):
        self.capacity = capacity
        self.ptr = 0
        self.size = 0
        self.alpha = alpha
        
        # é¢„åˆ†é…å†…å­˜
        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)
        self.rewards = np.zeros(capacity, dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros(capacity, dtype=np.float32)
        # ä¼˜å…ˆçº§æ•°ç»„
        self.priorities = np.zeros(capacity, dtype=np.float32)
    
    def __len__(self):
        return self.size
    
    def push(self, state: np.ndarray, action: np.ndarray, reward: float, next_state: np.ndarray, done: bool):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        max_prio = self.priorities.max() if self.size > 0 else 1.0
        
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.next_states[self.ptr] = next_state
        self.dones[self.ptr] = float(done)
        self.priorities[self.ptr] = max_prio
        
        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    
    def sample(self, batch_size: int, beta: float):
        """æŒ‰ä¼˜å…ˆçº§é‡‡æ ·ç»éªŒ, è¿”å›æ ·æœ¬åŠé‡è¦æ€§æƒé‡å’Œç´¢å¼•"""
        if self.size == self.capacity:
            prios = self.priorities
        else:
            prios = self.priorities[:self.size]
        probs = prios ** self.alpha
        probs /= probs.sum()
        indices = np.random.choice(self.size, batch_size, p=probs)
        
        weights = (self.size * probs[indices]) ** (-beta)
        weights /= weights.max()  # å½’ä¸€åŒ–åˆ°[0,1]
        weights = weights.astype(np.float32)
        
        batch_states = torch.FloatTensor(self.states[indices])
        batch_actions = torch.FloatTensor(self.actions[indices])
        batch_rewards = torch.FloatTensor(self.rewards[indices]).unsqueeze(1)
        batch_next_states = torch.FloatTensor(self.next_states[indices])
        batch_dones = torch.FloatTensor(self.dones[indices]).unsqueeze(1)
        weights_tensor = torch.FloatTensor(weights).unsqueeze(1)
        
        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, indices, weights_tensor
    
    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """æ ¹æ®æ–°çš„TDè¯¯å·®æ›´æ–°ä¼˜å…ˆçº§"""
        self.priorities[indices] = priorities


class TD3Agent:
    """TD3æ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, config: TD3Config):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config
        
        # æ€§èƒ½ä¼˜åŒ– - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        self.optimized_batch_size = OPTIMIZED_BATCH_SIZES.get('TD3', config.batch_size)
        self.config.batch_size = self.optimized_batch_size
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºç½‘ç»œ
        self.actor = TD3Actor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.critic = TD3Critic(state_dim, action_dim, config.hidden_dim).to(self.device)
        
        # ç›®æ ‡ç½‘ç»œ
        self.target_actor = TD3Actor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.target_critic = TD3Critic(state_dim, action_dim, config.hidden_dim).to(self.device)
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
        self.hard_update(self.target_actor, self.actor)
        self.hard_update(self.target_critic, self.critic)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)
        # ğŸ”§ æš‚æ—¶ç¦ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œé¿å…çŸ­æœŸè®­ç»ƒä¸­å­¦ä¹ ç‡è¿‡å¿«è¡°å‡
        # self.actor_lr_scheduler = optim.lr_scheduler.ExponentialLR(self.actor_optimizer, gamma=0.995)
        # self.critic_lr_scheduler = optim.lr_scheduler.ExponentialLR(self.critic_optimizer, gamma=0.995)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        # PER betaå‚æ•°
        self.beta = config.per_beta_start
        self.beta_increment = (1.0 - config.per_beta_start) / max(1, config.per_beta_frames)
        self.replay_buffer = TD3ReplayBuffer(config.buffer_size, state_dim, action_dim, alpha=config.per_alpha)
        
        # æ¢ç´¢å™ªå£°
        self.exploration_noise = config.exploration_noise
        self.step_count = 0
        self.update_count = 0
        
        # è®­ç»ƒç»Ÿè®¡
        self.actor_losses = []
        self.critic_losses = []
    
    def select_action(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.actor(state_tensor).cpu().numpy()[0]
        
        # æ·»åŠ æ¢ç´¢å™ªå£°
        if training:
            noise = np.random.normal(0, self.exploration_noise, size=action.shape)
            action = np.clip(action + noise, -1.0, 1.0)
        
        return action
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, reward: float,
                        next_state: np.ndarray, done: bool):
        """å­˜å‚¨ç»éªŒ"""
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def update(self) -> Dict[str, float]:
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        if len(self.replay_buffer) < self.config.batch_size:
            return {}
        
        self.step_count += 1
        
        # é¢„çƒ­æœŸä¸æ›´æ–°
        if self.step_count < self.config.warmup_steps:
            return {}
        
        self.update_count += 1
        
        # é‡‡æ ·ç»éªŒæ‰¹æ¬¡ (å«ç´¢å¼•ä¸ISæƒé‡)
        batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, indices, weights = \
            self.replay_buffer.sample(self.config.batch_size, self.beta)
        # æ›´æ–°beta
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        batch_states = batch_states.to(self.device)
        batch_actions = batch_actions.to(self.device)
        batch_rewards = batch_rewards.to(self.device)
        batch_next_states = batch_next_states.to(self.device)
        batch_dones = batch_dones.to(self.device)
        weights = weights.to(self.device)

        # æ›´æ–°Criticå¹¶è·å–TDè¯¯å·®
        critic_loss, td_errors = self._update_critic(batch_states, batch_actions, batch_rewards, 
                                        batch_next_states, batch_dones, weights)
        # æ ¹æ®TDè¯¯å·®æ›´æ–°ä¼˜å…ˆçº§
        self.replay_buffer.update_priorities(indices, td_errors.detach().cpu().numpy() + 1e-6)

        training_info = {'critic_loss': critic_loss}
        
        # å»¶è¿Ÿç­–ç•¥æ›´æ–°
        if self.update_count % self.config.policy_delay == 0:
            # æ›´æ–°Actor
            actor_loss = self._update_actor(batch_states)
            training_info['actor_loss'] = actor_loss
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self.soft_update(self.target_actor, self.actor, self.config.tau)
            self.soft_update(self.target_critic, self.critic, self.config.tau)
        
        # è¡°å‡å™ªå£°
        self.exploration_noise = max(self.config.min_noise, 
                                   self.exploration_noise * self.config.noise_decay)
        
        training_info['exploration_noise'] = self.exploration_noise
        
        return training_info
    
    def _update_critic(self, states: torch.Tensor, actions: torch.Tensor, 
                      rewards: torch.Tensor, next_states: torch.Tensor, 
                      dones: torch.Tensor, weights: torch.Tensor) -> Tuple[float, torch.Tensor]:
        """æ›´æ–°Criticç½‘ç»œ"""
        with torch.no_grad():
            # ç›®æ ‡ç­–ç•¥å¹³æ»‘åŒ–
            next_actions = self.target_actor(next_states)
            
            # æ·»åŠ è£å‰ªå™ªå£°
            noise = torch.randn_like(next_actions) * self.config.target_noise
            noise = torch.clamp(noise, -self.config.noise_clip, self.config.noise_clip)
            next_actions = torch.clamp(next_actions + noise, -1.0, 1.0)
            
            # è®¡ç®—ç›®æ ‡Qå€¼ (å–ä¸¤ä¸ªQç½‘ç»œçš„æœ€å°å€¼)
            target_q1, target_q2 = self.target_critic(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2)
            target_q = rewards + (1 - dones) * self.config.gamma * target_q
        
        # å½“å‰Qå€¼
        current_q1, current_q2 = self.critic(states, actions)
        
        # CriticæŸå¤± (ä¸¤ä¸ªQç½‘ç»œçš„æŸå¤±ä¹‹å’Œ)
        # TDè¯¯å·®
        td_errors = (current_q1 - target_q)
        # åŠ æƒMSEæŸå¤±
        critic_loss = (weights * td_errors.pow(2)).mean() + (weights * (current_q2 - target_q).pow(2)).mean()
        
        # æ›´æ–°Critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        # ğŸ”§ ä½¿ç”¨é…ç½®çš„æ¢¯åº¦è£å‰ªå‚æ•°
        if self.config.use_gradient_clip:
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.config.gradient_clip_norm)
        self.critic_optimizer.step()
        
        self.critic_losses.append(critic_loss.item())
        return critic_loss.item(), td_errors.abs().squeeze()
    
    def _update_actor(self, states: torch.Tensor) -> float:
        """æ›´æ–°Actorç½‘ç»œ"""
        # è®¡ç®—ç­–ç•¥æŸå¤± (åªä½¿ç”¨Q1ç½‘ç»œ)
        actions = self.actor(states)
        actor_loss = -self.critic.q1(states, actions).mean()
        
        # æ›´æ–°Actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        # ğŸ”§ ä½¿ç”¨é…ç½®çš„æ¢¯åº¦è£å‰ªå‚æ•°
        if self.config.use_gradient_clip:
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.config.gradient_clip_norm)
        self.actor_optimizer.step()
        
        self.actor_losses.append(actor_loss.item())
        # ğŸ”§ æš‚æ—¶ç¦ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
        # self.actor_lr_scheduler.step()
        # self.critic_lr_scheduler.step()
        return actor_loss.item()
    
    def soft_update(self, target: nn.Module, source: nn.Module, tau: float):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
    
    def hard_update(self, target: nn.Module, source: nn.Module):
        """ç¡¬æ›´æ–°ç½‘ç»œå‚æ•°"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(param.data)
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'target_actor_state_dict': self.target_actor.state_dict(),
            'target_critic_state_dict': self.target_critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'exploration_noise': self.exploration_noise,
            'step_count': self.step_count,
            'update_count': self.update_count
        }, f"{filepath}_td3.pth")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(f"{filepath}_td3.pth", map_location=self.device)
        
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.target_actor.load_state_dict(checkpoint['target_actor_state_dict'])
        self.target_critic.load_state_dict(checkpoint['target_critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.exploration_noise = checkpoint['exploration_noise']
        self.step_count = checkpoint['step_count']
        self.update_count = checkpoint['update_count']


class TD3Environment:
    """TD3è®­ç»ƒç¯å¢ƒ"""
    
    def __init__(self):
        self.config = TD3Config()
        
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—çŠ¶æ€ç»´åº¦
        # è½¦è¾†çŠ¶æ€: 12Ã—5=60ç»´ + RSUçŠ¶æ€: 6Ã—9=54ç»´ + UAVçŠ¶æ€: 2Ã—8=16ç»´ = 130ç»´
        self.state_dim = 130  # æ­£ç¡®çš„çŠ¶æ€ç»´åº¦
        # ğŸ¤– æ‰©å±•åŠ¨ä½œç©ºé—´: 11ç»´åŸæœ‰ + 7ç»´ç¼“å­˜è¿ç§»æ§åˆ¶ = 18ç»´
        self.action_dim = 18  # æ”¯æŒè‡ªé€‚åº”ç¼“å­˜è¿ç§»æ§åˆ¶
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agent = TD3Agent(self.state_dim, self.action_dim, self.config)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_count = 0
        self.step_count = 0
        
        print(f"âœ“ TD3ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ çŠ¶æ€ç»´åº¦: {self.state_dim}")
        print(f"âœ“ åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"âœ“ ç­–ç•¥å»¶è¿Ÿæ›´æ–°: {self.config.policy_delay}")
    
    def get_state_vector(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """
        ğŸ”§ ä¿®å¤ï¼šæ„å»ºå‡†ç¡®çš„130ç»´çŠ¶æ€å‘é‡ï¼ŒåŸºäºæ­£ç¡®çš„ç¼“å­˜è®¡ç®—
        çŠ¶æ€ç»„æˆ: è½¦è¾†60ç»´ + RSU54ç»´ + UAV16ç»´ = 130ç»´
        """
        state_components = []
        
        # 1. è½¦è¾†çŠ¶æ€ (12Ã—5=60ç»´)  
        for i in range(12):
            vehicle_key = f'vehicle_{i}'
            if vehicle_key in node_states:
                vehicle_state = node_states[vehicle_key]
                # ç¡®ä¿æ•°å€¼æœ‰æ•ˆæ€§
                valid_state = []
                for val in vehicle_state[:5]:
                    if np.isfinite(val):
                        valid_state.append(float(val))
                    else:
                        valid_state.append(0.5)
                state_components.extend(valid_state)
                
                # è¡¥é½åˆ°5ç»´
                while len(state_components) % 5 != 0:
                    state_components.append(0.0)
            else:
                # é»˜è®¤è½¦è¾†çŠ¶æ€: [ä½ç½®x, ä½ç½®y, é€Ÿåº¦, é˜Ÿåˆ—, èƒ½è€—]
                state_components.extend([0.5, 0.5, 0.0, 0.0, 0.0])
        
        # 2. RSUçŠ¶æ€ (6Ã—9=54ç»´) - ğŸ”§ ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ç¼“å­˜è®¡ç®—
        for i in range(6):
            rsu_key = f'rsu_{i}'
            if rsu_key in node_states:
                rsu_state = node_states[rsu_key]
                # ç¡®ä¿æ•°å€¼æœ‰æ•ˆæ€§å’Œç»´åº¦æ­£ç¡®
                valid_rsu_state = []
                for j, val in enumerate(rsu_state[:9]):
                    if np.isfinite(val):
                        # å¯¹ç¼“å­˜åˆ©ç”¨ç‡(ç¬¬2ç»´)è¿›è¡Œç‰¹æ®Šæ£€æŸ¥
                        if j == 2:  # ç¼“å­˜åˆ©ç”¨ç‡ç»´åº¦
                            valid_rsu_state.append(min(1.0, max(0.0, float(val))))
                        else:
                            valid_rsu_state.append(float(val))
                    else:
                        valid_rsu_state.append(0.5 if j < 2 else 0.0)
                
                # è¡¥é½åˆ°9ç»´
                while len(valid_rsu_state) < 9:
                    valid_rsu_state.append(0.0)
                
                state_components.extend(valid_rsu_state)
            else:
                # é»˜è®¤RSUçŠ¶æ€: [ä½ç½®x, ä½ç½®y, ç¼“å­˜åˆ©ç”¨ç‡, é˜Ÿåˆ—, èƒ½è€—, ç¼“å­˜å‚æ•°4ç»´]
                state_components.extend([0.5, 0.5, 0.0, 0.0, 0.0, 0.7, 0.35, 0.05, 0.3])
        
        # 3. UAVçŠ¶æ€ (2Ã—8=16ç»´) - ğŸ”§ ä¼˜åŒ–æ•°å€¼ç¨³å®šæ€§
        for i in range(2):
            uav_key = f'uav_{i}'
            if uav_key in node_states:
                uav_state = node_states[uav_key]
                # ç¡®ä¿æ•°å€¼æœ‰æ•ˆæ€§
                valid_uav_state = []
                for j, val in enumerate(uav_state[:8]):
                    if np.isfinite(val):
                        # å¯¹ç¼“å­˜åˆ©ç”¨ç‡(ç¬¬3ç»´)è¿›è¡Œç‰¹æ®Šå¤„ç†
                        if j == 3:  # ç¼“å­˜åˆ©ç”¨ç‡ç»´åº¦
                            valid_uav_state.append(min(1.0, max(0.0, float(val))))
                        else:
                            valid_uav_state.append(float(val))
                    else:
                        valid_uav_state.append(0.5 if j < 3 else 0.0)
                
                # è¡¥é½åˆ°8ç»´
                while len(valid_uav_state) < 8:
                    valid_uav_state.append(0.0)
                
                state_components.extend(valid_uav_state)
            else:
                # é»˜è®¤UAVçŠ¶æ€: [ä½ç½®x, ä½ç½®y, ä½ç½®z, ç¼“å­˜åˆ©ç”¨ç‡, èƒ½è€—, è¿ç§»å‚æ•°3ç»´]
                state_components.extend([0.5, 0.5, 0.5, 0.0, 0.0, 0.75, 1.0, 0.3])
        
        # ç¡®ä¿çŠ¶æ€å‘é‡æ­£å¥½æ˜¯130ç»´
        state_vector = np.array(state_components[:130], dtype=np.float32)
        
        # å¦‚æœç»´åº¦ä¸è¶³130ï¼Œè¡¥é½
        if len(state_vector) < 130:
            padding_needed = 130 - len(state_vector)
            state_vector = np.pad(state_vector, (0, padding_needed), mode='constant', constant_values=0.5)
        
        # æ•°å€¼å®‰å…¨æ£€æŸ¥
        state_vector = np.nan_to_num(state_vector, nan=0.5, posinf=1.0, neginf=0.0)
        
        return state_vector
    
    def decompose_action(self, action: np.ndarray) -> Dict[str, np.ndarray]:
        """
        å°†å…¨å±€åŠ¨ä½œåˆ†è§£ä¸ºå„èŠ‚ç‚¹åŠ¨ä½œ
        ğŸ¤– æ›´æ–°æ”¯æŒ18ç»´åŠ¨ä½œç©ºé—´ï¼š
        - vehicle_agent: 18ç»´ (11ç»´åŸæœ‰ + 7ç»´ç¼“å­˜è¿ç§»æ§åˆ¶)
        """
        actions = {}
        
        # ç¡®ä¿actioné•¿åº¦è¶³å¤Ÿ
        if len(action) < 18:
            action = np.pad(action, (0, 18-len(action)), mode='constant')
        
        # ğŸ¤– vehicle_agent è·å¾—æ‰€æœ‰18ç»´åŠ¨ä½œ
        # å‰11ç»´ï¼šä»»åŠ¡åˆ†é…(3) + RSUé€‰æ‹©(6) + UAVé€‰æ‹©(2)
        # å7ç»´ï¼šç¼“å­˜æ§åˆ¶(4) + è¿ç§»æ§åˆ¶(3)
        actions['vehicle_agent'] = action[:18]
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä»vehicle_agentä¸­æå–RSUå’ŒUAVé€‰æ‹©
        # è®­ç»ƒæ¡†æ¶éœ€è¦ä»rsu_agentå’Œuav_agentè·å–é€‰æ‹©æ¦‚ç‡
        actions['rsu_agent'] = action[3:9]   # RSUé€‰æ‹©ï¼ˆ6ç»´ï¼‰
        actions['uav_agent'] = action[9:11]  # UAVé€‰æ‹©ï¼ˆ2ç»´ï¼‰
        
        return actions
    
    def get_actions(self, state: np.ndarray, training: bool = True) -> Dict[str, np.ndarray]:
        """è·å–åŠ¨ä½œ"""
        global_action = self.agent.select_action(state, training)
        return self.decompose_action(global_action)
    
    def calculate_reward(self, system_metrics: Dict, 
                       cache_metrics: Optional[Dict] = None,
                       migration_metrics: Optional[Dict] = None) -> float:
        """
        ä½¿ç”¨ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨
        """
        from utils.unified_reward_calculator import calculate_unified_reward
        return calculate_unified_reward(system_metrics, cache_metrics, migration_metrics, algorithm="general")
    
    def train_step(self, state: np.ndarray, action: Union[np.ndarray, int], reward: float,
                   next_state: np.ndarray, done: bool) -> Dict:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        # TD3éœ€è¦numpyæ•°ç»„ï¼Œå¦‚æœæ˜¯æ•´æ•°åˆ™è½¬æ¢
        if isinstance(action, int):
            action = np.array([action], dtype=np.float32)
        elif not isinstance(action, np.ndarray):
            action = np.array(action, dtype=np.float32)
        
        # å­˜å‚¨ç»éªŒ
        self.agent.store_experience(state, action, reward, next_state, done)
        
        # æ›´æ–°ç½‘ç»œ
        training_info = self.agent.update()
        
        self.step_count += 1
        
        return training_info
    
    def save_models(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(filepath, exist_ok=True)
        self.agent.save_model(filepath)
        print(f"âœ“ TD3æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_models(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        self.agent.load_model(filepath)
        print(f"âœ“ TD3æ¨¡å‹å·²åŠ è½½: {filepath}")
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, reward: float,
                        next_state: np.ndarray, done: bool, log_prob: float = 0.0, value: float = 0.0):
        """å­˜å‚¨ç»éªŒåˆ°ç¼“å†²åŒº - æ”¯æŒPPOå…¼å®¹æ€§"""
        # TD3åªä½¿ç”¨å‰5ä¸ªå‚æ•°ï¼Œlog_probå’Œvalueè¢«å¿½ç•¥
        self.agent.store_experience(state, action, reward, next_state, done)
        self.step_count += 1
    
    def update(self, last_value: float = 0.0) -> Dict:
        """æ›´æ–°ç½‘ç»œå‚æ•° - æ”¯æŒPPOå…¼å®¹æ€§"""
        # TD3ä¸ä½¿ç”¨last_valueå‚æ•°
        return self.agent.update()
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return {
            'actor_loss_avg': float(np.mean(self.agent.actor_losses[-100:])) if self.agent.actor_losses else 0.0,
            'critic_loss_avg': float(np.mean(self.agent.critic_losses[-100:])) if self.agent.critic_losses else 0.0,
            'exploration_noise': self.agent.exploration_noise,
            'buffer_size': len(self.agent.replay_buffer),
            'step_count': self.step_count,
            'update_count': self.agent.update_count,
            'policy_delay': self.config.policy_delay
        }