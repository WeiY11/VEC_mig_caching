"""
GATè·¯ç”±å™¨ - å›¾æ³¨æ„åŠ›ç½‘ç»œç”¨äºååŒç¼“å­˜

åŸºäºGraph Attention Networks (GAT)çš„è·¯ç”±ç‰¹å¾æå–å™¨ï¼Œæ˜¾å¼å»ºæ¨¡ï¼š
1. è½¦è¾†-RSUçš„æ³¨æ„åŠ›å…³ç³»ï¼ˆå¸è½½å†³ç­–ï¼‰
2. RSU-RSUçš„ååŒç¼“å­˜å…³ç³»ï¼ˆå†…å®¹å…±äº«ï¼‰

ä¸»è¦ç‰¹ç‚¹ï¼š
- å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
- è¾¹ç‰¹å¾èåˆï¼ˆè·ç¦»ã€ä¿¡å·å¼ºåº¦ã€å¸¦å®½ï¼‰
- åŠ¨æ€æ‹“æ‰‘æ”¯æŒï¼ˆé‚»æ¥çŸ©é˜µæ©ç ï¼‰
- è¾“å‡ºååŒç¼“å­˜æ¦‚ç‡çŸ©é˜µ

ä½œè€…ï¼šVEC_mig_caching Team
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Optional, Dict


class GATLayer(nn.Module):
    """
    å•å±‚å›¾æ³¨æ„åŠ›
    
    å®ç°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œèåˆè¾¹ç‰¹å¾
    
    âœ¨ ä¼˜åŒ–ç‚¹ï¼š
    1. æ·»åŠ æ®‹å·®è¿æ¥
    2. æ·»åŠ LayerNormç¨³å®šè®­ç»ƒ
    3. æ”¹è¿›åˆå§‹åŒ–ç­–ç•¥
    """
    
    def __init__(
        self,
        in_features: int,
        out_features: int,
        num_heads: int = 4,
        edge_feature_dim: int = 8,
        dropout: float = 0.1,
        concat: bool = True,
        use_residual: bool = True,
    ):
        """
        Args:
            in_features: è¾“å…¥ç‰¹å¾ç»´åº¦
            out_features: è¾“å‡ºç‰¹å¾ç»´åº¦ï¼ˆæ¯ä¸ªå¤´ï¼‰
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            edge_feature_dim: è¾¹ç‰¹å¾ç»´åº¦
            dropout: Dropoutç‡
            concat: æ˜¯å¦æ‹¼æ¥å¤šå¤´è¾“å‡ºï¼ˆå¦åˆ™å¹³å‡ï¼‰
            use_residual: æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥
        """
        super(GATLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.num_heads = num_heads
        self.edge_feature_dim = edge_feature_dim
        self.concat = concat
        self.use_residual = use_residual
        
        # æ¯ä¸ªå¤´çš„å˜æ¢çŸ©é˜µ
        self.W = nn.Parameter(torch.FloatTensor(num_heads, in_features, out_features))
        
        # æ³¨æ„åŠ›æƒé‡å‚æ•° [num_heads, 2*out_features + edge_feature_dim]
        self.a = nn.Parameter(torch.FloatTensor(num_heads, 2 * out_features + edge_feature_dim, 1))
        
        # è¾¹ç‰¹å¾åµŒå…¥
        self.edge_embedding = nn.Linear(edge_feature_dim, edge_feature_dim)
        
        self.leakyrelu = nn.LeakyReLU(0.2)
        self.dropout = nn.Dropout(dropout)
        
        # âœ¨ æ·»åŠ LayerNormæå‡ç¨³å®šæ€§
        if concat:
            self.layer_norm = nn.LayerNorm(out_features * num_heads)
            # æ®‹å·®æŠ•å½±ï¼ˆå¦‚æœç»´åº¦ä¸åŒ¹é…ï¼‰
            if use_residual and in_features != out_features * num_heads:
                self.residual_proj = nn.Linear(in_features, out_features * num_heads)
            else:
                self.residual_proj = None
        else:
            self.layer_norm = nn.LayerNorm(out_features)
            if use_residual and in_features != out_features:
                self.residual_proj = nn.Linear(in_features, out_features)
            else:
                self.residual_proj = None
        
        self._init_weights()
    
    def _init_weights(self):
        """âœ¨ æ”¹è¿›çš„åˆå§‹åŒ–ç­–ç•¥"""
        # Xavieråˆå§‹åŒ–å˜æ¢çŸ©é˜µ
        nn.init.xavier_uniform_(self.W, gain=1.414)  # ä½¿ç”¨æ›´å¤§çš„gain
        # Xavieråˆå§‹åŒ–æ³¨æ„åŠ›å‚æ•°
        nn.init.xavier_uniform_(self.a, gain=1.414)
    
    def forward(
        self,
        h_source: torch.Tensor,
        h_target: torch.Tensor,
        edge_features: Optional[torch.Tensor] = None,
        adjacency_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            h_source: [batch_size, num_source, in_features] æºèŠ‚ç‚¹ç‰¹å¾
            h_target: [batch_size, num_target, in_features] ç›®æ ‡èŠ‚ç‚¹ç‰¹å¾
            edge_features: [batch_size, num_source, num_target, edge_feature_dim] è¾¹ç‰¹å¾
            adjacency_mask: [batch_size, num_source, num_target] é‚»æ¥æ©ç ï¼ˆTrueè¡¨ç¤ºå­˜åœ¨è¾¹ï¼‰
            
        Returns:
            h_out: [batch_size, num_target, out_features*num_heads] ï¼ˆconcat=Trueï¼‰
                   æˆ– [batch_size, num_target, out_features] ï¼ˆconcat=Falseï¼‰
        """
        batch_size = h_source.size(0)
        num_source = h_source.size(1)
        num_target = h_target.size(1)
        
        # çº¿æ€§å˜æ¢ [batch_size, num_heads, num_nodes, out_features]
        Wh_source = torch.einsum('bni,hio->bhno', h_source, self.W)
        Wh_target = torch.einsum('bni,hio->bhno', h_target, self.W)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        # æ‰©å±•ç»´åº¦è¿›è¡Œå¹¿æ’­
        Wh_source_expanded = Wh_source.unsqueeze(3)  # [batch, heads, num_source, 1, out]
        Wh_target_expanded = Wh_target.unsqueeze(2)  # [batch, heads, 1, num_target, out]
        
        # æ‹¼æ¥æºå’Œç›®æ ‡ç‰¹å¾ [batch, heads, num_source, num_target, 2*out]
        concat_features = torch.cat([
            Wh_source_expanded.expand(-1, -1, -1, num_target, -1),
            Wh_target_expanded.expand(-1, -1, num_source, -1, -1),
        ], dim=-1)
        
        # èåˆè¾¹ç‰¹å¾
        if edge_features is not None:
            edge_emb = self.edge_embedding(edge_features)  # [batch, num_source, num_target, edge_dim]
            edge_emb = edge_emb.unsqueeze(1).expand(-1, self.num_heads, -1, -1, -1)  # [batch, heads, src, tgt, edge_dim]
            concat_features = torch.cat([concat_features, edge_emb], dim=-1)
        else:
            # é›¶è¾¹ç‰¹å¾
            zero_edge = torch.zeros(
                batch_size, self.num_heads, num_source, num_target, self.edge_feature_dim,
                device=h_source.device
            )
            concat_features = torch.cat([concat_features, zero_edge], dim=-1)
        
        # è®¡ç®—æ³¨æ„åŠ›logits [batch, heads, num_source, num_target]
        e = torch.einsum('bhsti,hio->bhst', concat_features, self.a).squeeze(-1)
        e = self.leakyrelu(e)
        
        # åº”ç”¨é‚»æ¥æ©ç 
        if adjacency_mask is not None:
            # Ensure boolean
            if adjacency_mask.dtype != torch.bool:
                adjacency_mask = adjacency_mask > 0.5
                
            # adjacency_mask: [batch, num_source, num_target]
            adjacency_mask = adjacency_mask.unsqueeze(1).expand(-1, self.num_heads, -1, -1)
            e = e.masked_fill(~adjacency_mask, float('-inf'))
        
        # Softmaxå½’ä¸€åŒ–ï¼ˆå¯¹æºèŠ‚ç‚¹ç»´åº¦ï¼‰
        attention = F.softmax(e, dim=2)  # [batch, heads, num_source, num_target]
        attention = self.dropout(attention)
        
        # åŠ æƒèšåˆæºèŠ‚ç‚¹ç‰¹å¾
        # attention: [batch, heads, num_source, num_target]
        # Wh_source: [batch, heads, num_source, out]
        # ç»“æœ: [batch, heads, num_target, out]
        h_prime = torch.einsum('bhst,bhso->bhto', attention, Wh_source)
        
        # å¤šå¤´è¾“å‡ºå¤„ç†
        if self.concat:
            # æ‹¼æ¥æ‰€æœ‰å¤´ [batch, num_target, heads*out]
            h_out = h_prime.permute(0, 2, 1, 3).reshape(batch_size, num_target, -1)
        else:
            # å¹³å‡æ‰€æœ‰å¤´ [batch, num_target, out]
            h_out = h_prime.mean(dim=1)
        
        # âœ¨ æ·»åŠ æ®‹å·®è¿æ¥
        if self.use_residual:
            if self.residual_proj is not None:
                # ç»´åº¦ä¸åŒ¹é…ï¼Œéœ€è¦æŠ•å½±
                residual = self.residual_proj(h_target)
            else:
                # ç»´åº¦åŒ¹é…ï¼Œç›´æ¥ç›¸åŠ 
                residual = h_target
            h_out = h_out + residual
        
        # âœ¨ LayerNormç¨³å®šè®­ç»ƒ
        h_out = self.layer_norm(h_out)
        
        return h_out


class VehicleRSUAttention(nn.Module):
    """
    è½¦è¾†-RSUæ³¨æ„åŠ›æ¨¡å—
    
    å»ºæ¨¡è½¦è¾†åˆ°RSUçš„å¸è½½å†³ç­–ï¼Œè€ƒè™‘è·ç¦»ã€ä¿¡å·è´¨é‡ã€RSUè´Ÿè½½ç­‰å› ç´ 
    """
    
    def __init__(
        self,
        vehicle_feature_dim: int = 5,
        rsu_feature_dim: int = 5,
        hidden_dim: int = 128,
        num_heads: int = 4,
        edge_feature_dim: int = 8,
    ):
        super(VehicleRSUAttention, self).__init__()
        
        # èŠ‚ç‚¹ç‰¹å¾æŠ•å½±
        self.vehicle_proj = nn.Linear(vehicle_feature_dim, hidden_dim)
        self.rsu_proj = nn.Linear(rsu_feature_dim, hidden_dim)
        
        # GATå±‚
        self.gat_layer = GATLayer(
            in_features=hidden_dim,
            out_features=hidden_dim // num_heads,
            num_heads=num_heads,
            edge_feature_dim=edge_feature_dim,
            concat=True,
        )
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, hidden_dim)
    
    def forward(
        self,
        vehicle_features: torch.Tensor,
        rsu_features: torch.Tensor,
        edge_features: Optional[torch.Tensor] = None,
        adjacency_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Args:
            vehicle_features: [batch_size, num_vehicles, vehicle_feature_dim]
            rsu_features: [batch_size, num_rsus, rsu_feature_dim]
            edge_features: [batch_size, num_vehicles, num_rsus, edge_feature_dim]
            adjacency_mask: [batch_size, num_vehicles, num_rsus]
            
        Returns:
            vehicle_representations: [batch_size, num_vehicles, hidden_dim]
        """
        # æŠ•å½±èŠ‚ç‚¹ç‰¹å¾
        h_vehicles = F.relu(self.vehicle_proj(vehicle_features))
        h_rsus = F.relu(self.rsu_proj(rsu_features))
        
        # GATæ³¨æ„åŠ›ï¼ˆRSU -> Vehiclesï¼‰
        # æ³¨æ„ï¼šedge_featuresæ˜¯[batch, V, R, dim]ï¼ŒGATæœŸæœ›[batch, src, tgt, dim]
        # è¿™é‡Œsrc=RSU, tgt=Vehicleï¼Œæ‰€ä»¥éœ€è¦è½¬ç½®
        if edge_features is not None:
            edge_features_t = edge_features.permute(0, 2, 1, 3)
        else:
            edge_features_t = None
            
        if adjacency_mask is not None:
            adjacency_mask_t = adjacency_mask.permute(0, 2, 1)
        else:
            adjacency_mask_t = None
            
        h_out = self.gat_layer(h_rsus, h_vehicles, edge_features_t, adjacency_mask_t)
        
        # è¾“å‡ºæŠ•å½±
        vehicle_representations = self.output_proj(h_out)
        
        return vehicle_representations


class RSURSUCollaborativeAttention(nn.Module):
    """
    RSU-RSUååŒç¼“å­˜æ³¨æ„åŠ›æ¨¡å—
    
    å»ºæ¨¡RSUä¹‹é—´çš„å†…å®¹åä½œå…³ç³»ï¼Œè¾“å‡ºååŒç¼“å­˜å†³ç­–æ¦‚ç‡
    """
    
    def __init__(
        self,
        rsu_feature_dim: int = 5,
        hidden_dim: int = 128,
        num_heads: int = 4,
        edge_feature_dim: int = 8,
    ):
        super(RSURSUCollaborativeAttention, self).__init__()
        
        # RSUç‰¹å¾æŠ•å½±
        self.rsu_proj = nn.Linear(rsu_feature_dim, hidden_dim)
        
        # ç‰©ç†é‚»æ¥æ³¨æ„åŠ›
        self.physical_gat = GATLayer(
            in_features=hidden_dim,
            out_features=hidden_dim // num_heads,
            num_heads=num_heads,
            edge_feature_dim=edge_feature_dim,
            concat=True,
        )
        
        # ç¼“å­˜å†…å®¹ç›¸ä¼¼åº¦æ³¨æ„åŠ›ï¼ˆself-attentionï¼‰
        self.content_gat = GATLayer(
            in_features=hidden_dim,
            out_features=hidden_dim // num_heads,
            num_heads=num_heads,
            edge_feature_dim=edge_feature_dim,
            concat=True,
        )
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )
        
        # ååŒç¼“å­˜æ¦‚ç‡é¢„æµ‹å¤´
        self.collab_cache_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid(),
        )
    
    def forward(
        self,
        rsu_features: torch.Tensor,
        physical_adjacency: Optional[torch.Tensor] = None,
        cache_similarity: Optional[torch.Tensor] = None,
        edge_features: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            rsu_features: [batch_size, num_rsus, rsu_feature_dim]
            physical_adjacency: [batch_size, num_rsus, num_rsus] ç‰©ç†é‚»æ¥æ©ç 
            cache_similarity: [batch_size, num_rsus, num_rsus] ç¼“å­˜ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆå¯é€‰ï¼‰
            edge_features: [batch_size, num_rsus, num_rsus, edge_feature_dim] è¾¹ç‰¹å¾
            
        Returns:
            rsu_representations: [batch_size, num_rsus, hidden_dim]
            collab_cache_probs: [batch_size, num_rsus, 1] ååŒç¼“å­˜æ¦‚ç‡
        """
        # æŠ•å½±RSUç‰¹å¾
        h_rsus = F.relu(self.rsu_proj(rsu_features))
        
        # ç‰©ç†é‚»æ¥æ³¨æ„åŠ›ï¼ˆä½¿ç”¨è¾¹ç‰¹å¾ï¼‰
        h_physical = self.physical_gat(h_rsus, h_rsus, edge_features, physical_adjacency)
        
        # ç¼“å­˜å†…å®¹ç›¸ä¼¼åº¦æ³¨æ„åŠ›ï¼ˆä½¿ç”¨cache_similarityä½œä¸ºé¢å¤–æ©ç ï¼‰
        # æ³¨ï¼šä¸ä½¿ç”¨è¾¹ç‰¹å¾ï¼Œå› ä¸ºè¿™æ˜¯åŸºäºå†…å®¹çš„æ³¨æ„åŠ›
        h_content = self.content_gat(h_rsus, h_rsus, None, cache_similarity)
        
        # èåˆä¸¤è·¯æ³¨æ„åŠ›
        h_fused = self.fusion(torch.cat([h_physical, h_content], dim=-1))
        
        # é¢„æµ‹ååŒç¼“å­˜æ¦‚ç‡
        collab_cache_probs = self.collab_cache_head(h_fused)
        
        return h_fused, collab_cache_probs


class GATRouterActor(nn.Module):
    """
    åŸºäºGATçš„Actorç½‘ç»œ
    
    æ›¿æ¢GraphFeatureExtractorï¼Œæ˜¾å¼å»ºæ¨¡è½¦è¾†-RSUå’ŒRSU-RSUçš„æ³¨æ„åŠ›å…³ç³»
    """
    
    def __init__(
        self,
        num_vehicles: int,
        num_rsus: int,
        num_uavs: int,
        vehicle_feature_dim: int = 5,
        rsu_feature_dim: int = 6,  # ğŸ”§ ä¿®å¤: RSUç°åœ¨6ç»´ï¼ˆ+cpu_frequencyï¼‰
        uav_feature_dim: int = 6,  # ğŸ”§ ä¿®å¤: UAVç°åœ¨6ç»´ï¼ˆ+cpu_frequencyï¼‰
        global_feature_dim: int = 8,
        hidden_dim: int = 128,
        num_heads: int = 4,
        edge_feature_dim: int = 8,
        central_state_dim: int = 0,
    ):
        super(GATRouterActor, self).__init__()
        self.num_vehicles = num_vehicles
        self.num_rsus = num_rsus
        self.num_uavs = num_uavs
        self.vehicle_feature_dim = vehicle_feature_dim
        self.rsu_feature_dim = rsu_feature_dim  # ğŸ”§ æ–°å¢: ä¿å­˜ç»´åº¦
        self.uav_feature_dim = uav_feature_dim  # ğŸ”§ æ–°å¢: ä¿å­˜ç»´åº¦
        # å…¨å±€ç‰¹å¾ç»´åº¦ = åŸºç¡€å…¨å±€ç‰¹å¾ + ä¸­å¤®çŠ¶æ€ç‰¹å¾
        self.actual_global_dim = global_feature_dim + central_state_dim
        
        # è½¦è¾†-RSUæ³¨æ„åŠ›
        self.vehicle_rsu_attention = VehicleRSUAttention(
            vehicle_feature_dim, rsu_feature_dim, hidden_dim, num_heads, edge_feature_dim
        )
        
        # RSU-RSUååŒç¼“å­˜æ³¨æ„åŠ›
        self.rsu_rsu_attention = RSURSUCollaborativeAttention(
            rsu_feature_dim, hidden_dim, num_heads, edge_feature_dim
        )
        
        # UAVç‰¹å¾ç¼–ç ï¼ˆç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨MLPï¼‰
        self.uav_encoder = nn.Sequential(
            nn.Linear(uav_feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )
        
        # å…¨å±€ç‰¹å¾ç¼–ç ï¼ˆåŒ…å«ä¸­å¤®çŠ¶æ€ï¼‰
        self.global_encoder = nn.Sequential(
            nn.Linear(self.actual_global_dim, hidden_dim),
            nn.ReLU(),
        )
        
        # æœ€ç»ˆèåˆå±‚
        self.final_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 4, hidden_dim * 2),  # vehicle + rsu + uav + global
            nn.ReLU(),
            nn.Linear(hidden_dim * 2, hidden_dim),
        )
        
        # è®°å½•æœ€åä¸€æ¬¡çš„ååŒç¼“å­˜æ¦‚ç‡ï¼ˆä¾›å¤–éƒ¨è®¿é—®ï¼‰
        self.last_collab_cache_probs = None
    
    def forward(
        self,
        state: torch.Tensor,
        adjacency_info: Optional[Dict[str, torch.Tensor]] = None,
    ) -> torch.Tensor:
        """
        Args:
            state: [batch_size, state_dim] å®Œæ•´çŠ¶æ€å‘é‡
            adjacency_info: é‚»æ¥ä¿¡æ¯å­—å…¸ï¼ˆå¯é€‰ï¼‰ï¼ŒåŒ…å«ï¼š
                - 'vehicle_rsu_mask': [batch, num_vehicles, num_rsus]
                - 'rsu_rsu_mask': [batch, num_rsus, num_rsus]
                - 'edge_features': ...
                
        Returns:
            global_representation: [batch_size, hidden_dim] å…¨å±€è¡¨ç¤º
        """
        batch_size = state.size(0)
        
        # è§£æçŠ¶æ€å‘é‡
        idx = 0
        vehicle_features = state[:, idx:idx + self.num_vehicles * self.vehicle_feature_dim].view(batch_size, self.num_vehicles, self.vehicle_feature_dim)
        idx += self.num_vehicles * self.vehicle_feature_dim
        
        # ğŸ”§ ä¿®å¤: ä½¿ç”¨å®é™…ç»´åº¦ï¼ˆRSU=6, UAV=6ï¼‰
        rsu_features = state[:, idx:idx + self.num_rsus * self.rsu_feature_dim].view(batch_size, self.num_rsus, self.rsu_feature_dim)
        idx += self.num_rsus * self.rsu_feature_dim
        
        uav_features = state[:, idx:idx + self.num_uavs * self.uav_feature_dim].view(batch_size, self.num_uavs, self.uav_feature_dim)
        idx += self.num_uavs * self.uav_feature_dim
        
        # å‡è®¾å‰©ä½™ä¸ºå…¨å±€ç‰¹å¾
        global_features = state[:, idx:]
        
        # âœ¨ å¦‚æœæ²¡æœ‰æä¾›é‚»æ¥ä¿¡æ¯ï¼Œåˆ™åŠ¨æ€æ„å»º
        if adjacency_info is None:
            adjacency_info = self._build_adjacency_info(state)
        
        # è½¦è¾†-RSUæ³¨æ„åŠ›
        vehicle_repr = self.vehicle_rsu_attention(
            vehicle_features,
            rsu_features,
            edge_features=adjacency_info.get('vehicle_rsu_edge_features'),
            adjacency_mask=adjacency_info.get('vehicle_rsu_mask'),
        )
        vehicle_repr_pooled = vehicle_repr.mean(dim=1)  # [batch, hidden_dim]
        
        # RSU-RSUååŒç¼“å­˜æ³¨æ„åŠ›
        rsu_repr, collab_cache_probs = self.rsu_rsu_attention(
            rsu_features,
            physical_adjacency=adjacency_info.get('rsu_rsu_mask'),
            cache_similarity=adjacency_info.get('cache_similarity'),
            edge_features=adjacency_info.get('rsu_rsu_edge_features'),
        )
        rsu_repr_pooled = rsu_repr.mean(dim=1)  # [batch, hidden_dim]
        self.last_collab_cache_probs = collab_cache_probs  # ç¼“å­˜ååŒç¼“å­˜æ¦‚ç‡
        
        # UAVç¼–ç 
        uav_repr = self.uav_encoder(uav_features).mean(dim=1)  # [batch, hidden_dim]
        
        # å…¨å±€ç‰¹å¾ç¼–ç 
        global_repr = self.global_encoder(global_features)  # [batch, hidden_dim]
        
        # èåˆæ‰€æœ‰è¡¨ç¤º
        fused_repr = self.final_fusion(torch.cat([
            vehicle_repr_pooled, rsu_repr_pooled, uav_repr, global_repr
        ], dim=-1))
        
        return fused_repr
    
    def get_collab_cache_probs(self) -> Optional[torch.Tensor]:
        """è·å–æœ€åä¸€æ¬¡forwardçš„ååŒç¼“å­˜æ¦‚ç‡"""
        return self.last_collab_cache_probs
    
    def _build_adjacency_info(self, state: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ä»çŠ¶æ€å‘é‡æ„å»ºåŠ¨æ€é‚»æ¥ä¿¡æ¯
        
        Args:
            state: [batch_size, state_dim] çŠ¶æ€å‘é‡
            
        Returns:
            adjacency_info: åŒ…å«é‚»æ¥æ©ç å’Œè¾¹ç‰¹å¾çš„å­—å…¸
        """
        batch_size = state.size(0)
        device = state.device
        
        # è§£æä½ç½®å’Œè´Ÿè½½ä¿¡æ¯
        idx = 0
        vehicle_features = state[:, idx:idx + self.num_vehicles * self.vehicle_feature_dim].view(batch_size, self.num_vehicles, self.vehicle_feature_dim)
        idx += self.num_vehicles * self.vehicle_feature_dim
        
        # ğŸ”§ ä¿®å¤: ä½¿ç”¨å®é™…ç»´åº¦ï¼ˆRSU=6, UAV=6ï¼‰
        rsu_features = state[:, idx:idx + self.num_rsus * self.rsu_feature_dim].view(batch_size, self.num_rsus, self.rsu_feature_dim)
        idx += self.num_rsus * self.rsu_feature_dim
        
        uav_features = state[:, idx:idx + self.num_uavs * self.uav_feature_dim].view(batch_size, self.num_uavs, self.uav_feature_dim)
        
        # æå–ä½ç½®ä¿¡æ¯ (å‰2ç»´æ˜¯ä½ç½®)
        vehicle_pos = vehicle_features[:, :, :2]  # [batch, num_vehicles, 2]
        rsu_pos = rsu_features[:, :, :2]  # [batch, num_rsus, 2]
        
        # è®¡ç®—è½¦è¾†-RSUè·ç¦»çŸ©é˜µ
        # vehicle_pos: [batch, num_vehicles, 2] -> [batch, num_vehicles, 1, 2]
        # rsu_pos: [batch, num_rsus, 2] -> [batch, 1, num_rsus, 2]
        v_expanded = vehicle_pos.unsqueeze(2)  # [batch, num_vehicles, 1, 2]
        r_expanded = rsu_pos.unsqueeze(1)  # [batch, 1, num_rsus, 2]
        
        # æ¬§æ°è·ç¦»
        vehicle_rsu_dist = torch.sqrt(torch.sum((v_expanded - r_expanded) ** 2, dim=-1) + 1e-8)  # [batch, num_vehicles, num_rsus]
        
        # è®¡ç®—RSU-RSUè·ç¦»çŸ©é˜µ
        r_expanded_i = rsu_pos.unsqueeze(2)  # [batch, num_rsus, 1, 2]
        r_expanded_j = rsu_pos.unsqueeze(1)  # [batch, 1, num_rsus, 2]
        rsu_rsu_dist = torch.sqrt(torch.sum((r_expanded_i - r_expanded_j) ** 2, dim=-1) + 1e-8)  # [batch, num_rsus, num_rsus]
        
        # æ„å»ºé‚»æ¥æ©ç ï¼ˆåŸºäºè·ç¦»é˜ˆå€¼ï¼‰
        # âœ¨ ä¼˜åŒ–ï¼šæ ¹æ®ä¿¡å·å¼ºåº¦åŠ¨æ€è°ƒæ•´è¦†ç›–èŒƒå›´
        vehicle_rsu_coverage = 500.0  # RSUåŸºç¡€è¦†ç›–èŒƒå›´500m
        rsu_rsu_collaboration_range = 1500.0  # RSUåä½œèŒƒå›´1500m
        
        # âœ¨ åŠ¨æ€è¦†ç›–ï¼šåŸºäºä¿¡å·è´¨é‡çš„è½¯æ©ç 
        # è®¡ç®—ä¿¡å·å¼ºåº¦æƒé‡
        signal_strength = 1.0 / (1.0 + vehicle_rsu_dist / 100.0)
        # è½¯æ©ç ï¼šä¿¡å·å¼ºåº¦ > 0.3
        vehicle_rsu_mask = signal_strength > 0.3  # [batch, num_vehicles, num_rsus]
        rsu_rsu_mask = rsu_rsu_dist <= rsu_rsu_collaboration_range  # [batch, num_rsus, num_rsus]
        
        # æ„å»ºè½¦è¾†-RSUè¾¹ç‰¹å¾ [batch, num_vehicles, num_rsus, edge_dim=8]
        # ç‰¹å¾åŒ…æ‹¬ï¼šè·ç¦»(å½’ä¸€åŒ–), ä¿¡å·å¼ºåº¦, RSUè´Ÿè½½, ç¼“å­˜åˆ©ç”¨ç‡, é˜Ÿåˆ—é•¿åº¦ç­‰
        vehicle_rsu_edge_features = torch.zeros(batch_size, self.num_vehicles, self.num_rsus, 8, device=device)
        
        # è·ç¦»å½’ä¸€åŒ– (0-1)
        vehicle_rsu_edge_features[:, :, :, 0] = torch.clamp(vehicle_rsu_dist / 1000.0, 0.0, 1.0)
        
        # ä¿¡å·å¼ºåº¦ä¼°è®¡ï¼ˆåŸºäºè·ç¦»ï¼Œç®€åŒ–çš„è·¯å¾„æŸè€—æ¨¡å‹ï¼‰
        # SINR = -32.4 - 20*log10(d_km) - 20*log10(f_GHz) + tx_power + antenna_gain
        # ç®€åŒ–ä¸º: signal_strength = 1 / (1 + distance/100)
        vehicle_rsu_edge_features[:, :, :, 1] = 1.0 / (1.0 + vehicle_rsu_dist / 100.0)
        
        # å¸¦å®½ä¼°è®¡ï¼ˆåŸºäºè´Ÿè½½ï¼Œå‡è®¾å‡åˆ†ï¼‰
        # rsu_features[:, :, 3] æ˜¯é˜Ÿåˆ—é•¿åº¦/è´Ÿè½½
        rsu_load = rsu_features[:, :, 3].unsqueeze(1).expand(-1, self.num_vehicles, -1)  # [batch, num_vehicles, num_rsus]
        vehicle_rsu_edge_features[:, :, :, 2] = torch.clamp(1.0 - rsu_load, 0.1, 1.0)  # å¯ç”¨å¸¦å®½
        
        # RSUç¼“å­˜åˆ©ç”¨ç‡ (rsu_features[:, :, 2])
        rsu_cache = rsu_features[:, :, 2].unsqueeze(1).expand(-1, self.num_vehicles, -1)
        vehicle_rsu_edge_features[:, :, :, 3] = rsu_cache
        
        # RSUèƒ½è€—çŠ¶æ€ (rsu_features[:, :, 4])
        rsu_energy = rsu_features[:, :, 4].unsqueeze(1).expand(-1, self.num_vehicles, -1)
        vehicle_rsu_edge_features[:, :, :, 4] = rsu_energy
        
        # ä¼ è¾“å»¶è¿Ÿä¼°è®¡ï¼ˆåŸºäºè·ç¦»å’Œå¸¦å®½ï¼‰
        # delay = distance / speed_of_light + data_size / bandwidth
        propagation_delay = vehicle_rsu_dist / 300.0  # å½’ä¸€åŒ–åˆ°msçº§åˆ«
        vehicle_rsu_edge_features[:, :, :, 5] = torch.clamp(propagation_delay / 10.0, 0.0, 1.0)
        
        # é“¾è·¯è´¨é‡ï¼ˆç»¼åˆæŒ‡æ ‡ï¼‰
        link_quality = vehicle_rsu_edge_features[:, :, :, 1] * vehicle_rsu_edge_features[:, :, :, 2]  # ä¿¡å·*å¸¦å®½
        vehicle_rsu_edge_features[:, :, :, 6] = link_quality
        
        # æ˜¯å¦åœ¨è¦†ç›–èŒƒå›´å†…ï¼ˆäºŒå€¼ç‰¹å¾ï¼‰
        vehicle_rsu_edge_features[:, :, :, 7] = vehicle_rsu_mask.float()
        
        # æ„å»ºRSU-RSUè¾¹ç‰¹å¾ [batch, num_rsus, num_rsus, edge_dim=8]
        rsu_rsu_edge_features = torch.zeros(batch_size, self.num_rsus, self.num_rsus, 8, device=device)
        
        # è·ç¦»å½’ä¸€åŒ–
        rsu_rsu_edge_features[:, :, :, 0] = torch.clamp(rsu_rsu_dist / 2000.0, 0.0, 1.0)
        
        # å›ä¼ å¸¦å®½ï¼ˆå‡è®¾æœ‰çº¿å›ä¼ ï¼Œå¸¦å®½å›ºå®šï¼‰
        rsu_rsu_edge_features[:, :, :, 1] = 0.9  # é«˜å¸¦å®½æœ‰çº¿å›ä¼ 
        
        # è´Ÿè½½ç›¸ä¼¼åº¦ï¼ˆç”¨äºç¼“å­˜åä½œï¼‰
        rsu_load_i = rsu_features[:, :, 3].unsqueeze(2)  # [batch, num_rsus, 1]
        rsu_load_j = rsu_features[:, :, 3].unsqueeze(1)  # [batch, 1, num_rsus]
        load_diff = torch.abs(rsu_load_i - rsu_load_j)
        rsu_rsu_edge_features[:, :, :, 2] = 1.0 - torch.clamp(load_diff, 0.0, 1.0)  # è´Ÿè½½ç›¸ä¼¼åº¦
        
        # ç¼“å­˜ç›¸ä¼¼åº¦ï¼ˆç”¨äºååŒç¼“å­˜ï¼‰
        rsu_cache_i = rsu_features[:, :, 2].unsqueeze(2)
        rsu_cache_j = rsu_features[:, :, 2].unsqueeze(1)
        cache_similarity = 1.0 - torch.abs(rsu_cache_i - rsu_cache_j)
        rsu_rsu_edge_features[:, :, :, 3] = cache_similarity
        
        # å›ä¼ å»¶è¿Ÿï¼ˆåŸºäºè·ç¦»ï¼‰
        backhaul_delay = rsu_rsu_dist / 200000.0  # å…‰çº¤é€Ÿåº¦çº¦2e5 km/s
        rsu_rsu_edge_features[:, :, :, 4] = torch.clamp(backhaul_delay / 5.0, 0.0, 1.0)
        
        # æ˜¯å¦ç‰©ç†ç›¸é‚»
        rsu_rsu_edge_features[:, :, :, 5] = rsu_rsu_mask.float()
        
        # åä½œæ½œåŠ›ï¼ˆç»¼åˆè´Ÿè½½å·®å’Œç¼“å­˜ç›¸ä¼¼åº¦ï¼‰
        collaboration_potential = (rsu_rsu_edge_features[:, :, :, 2] + rsu_rsu_edge_features[:, :, :, 3]) / 2.0
        rsu_rsu_edge_features[:, :, :, 6] = collaboration_potential
        
        # é¢„ç•™å­—æ®µ
        rsu_rsu_edge_features[:, :, :, 7] = 0.0
        
        return {
            'vehicle_rsu_mask': vehicle_rsu_mask,
            'vehicle_rsu_edge_features': vehicle_rsu_edge_features,
            'rsu_rsu_mask': rsu_rsu_mask,
            'rsu_rsu_edge_features': rsu_rsu_edge_features,
            'cache_similarity': cache_similarity,  # ç”¨äºå†…å®¹ç›¸ä¼¼åº¦æ³¨æ„åŠ›
        }
