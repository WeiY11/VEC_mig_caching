#!/usr/bin/env python3
"""
Dual-stage controller environment wrapper.

Stage 1: Offloading head (algorithm A) produces offloading preferences
         - 3 logits for [local, rsu, uav]
         - K logits for RSU selection
         - M logits for UAV selection
Stage 2: Base RL env (algorithm B) produces the full action vector; we keep
         the cache/migration control (last 8 dims) and optionally other parts.

This wrapper combines both into a single action dict compatible with the
existing SingleAgentTrainingEnvironment._build_simulator_actions.

Notes:
- Stage 1 here is a heuristic policy (several variants). RL stage 1 can be
  added later by swapping the policy implementation.
- Training is applied to Stage 2 RL env only. Stage 1 is stateless.
"""

from typing import Any, Dict, Optional, Tuple
import numpy as np


class _HeuristicOffloadPolicy:
    """ğŸ§  æ™ºèƒ½å¤šå› ç´ å¯å‘å¼å¸è½½ç­–ç•¥
    
    ç­–ç•¥ç±»å‹:
      - 'smart': æ™ºèƒ½ç»¼åˆè¯„åˆ†ï¼ˆè·ç¦»+é˜Ÿåˆ—+ç¼“å­˜+èƒ½è€—ï¼‰â­æ¨è
      - 'delay_optimal': å»¶è¿Ÿä¼˜å…ˆï¼ˆæœ€å°åŒ–ä¼ è¾“+é˜Ÿåˆ—+è®¡ç®—æ—¶å»¶ï¼‰
      - 'energy_optimal': èƒ½è€—ä¼˜å…ˆï¼ˆæœ€å°åŒ–ä¼ è¾“èƒ½è€—+è®¡ç®—èƒ½è€—ï¼‰
      - 'cache_aware': ç¼“å­˜æ„ŸçŸ¥ï¼ˆå¼ºçƒˆåå¥½ç¼“å­˜å‘½ä¸­ï¼‰
      - 'load_balance': è´Ÿè½½å‡è¡¡ï¼ˆé¿å…çƒ­ç‚¹ï¼Œå‡åŒ€åˆ†é…ï¼‰
      - 'heuristic': ç»å…¸å¯å‘å¼ï¼ˆé˜Ÿåˆ—ä¼˜å…ˆï¼‰[æ—§ç‰ˆ]
      - 'greedy': è´ªå©ªæœ€è¿‘èŠ‚ç‚¹ [æ—§ç‰ˆ]
    
    å¯é…ç½®æƒé‡ï¼ˆä»…'smart'ç­–ç•¥ï¼‰:
      - weight_delay: å»¶è¿Ÿæƒé‡ï¼ˆé»˜è®¤2.0ï¼‰
      - weight_energy: èƒ½è€—æƒé‡ï¼ˆé»˜è®¤1.0ï¼‰
      - weight_cache: ç¼“å­˜æƒé‡ï¼ˆé»˜è®¤3.0ï¼‰
      - weight_queue: é˜Ÿåˆ—æƒé‡ï¼ˆé»˜è®¤1.5ï¼‰
    """

    def __init__(self, strategy: str = 'smart', **kwargs):
        self.strategy = (strategy or 'smart').lower()
        
        # æ™ºèƒ½ç­–ç•¥çš„å¯é…ç½®æƒé‡
        self.weight_delay = kwargs.get('weight_delay', 2.0)
        self.weight_energy = kwargs.get('weight_energy', 1.0)
        self.weight_cache = kwargs.get('weight_cache', 3.0)
        self.weight_queue = kwargs.get('weight_queue', 1.5)
        
        # è·ç¦»é˜ˆå€¼ï¼ˆç±³ï¼‰
        self.near_distance = 200.0
        self.far_distance = 400.0

    @staticmethod
    def _logit_from_probs(probs: np.ndarray) -> np.ndarray:
        eps = 1e-6
        p = np.clip(probs.astype(np.float32), eps, 1.0 - eps)
        return np.log(p)
    
    def _get_vehicle_position(self, simulator, vehicle_idx: int):
        """è·å–è½¦è¾†ä½ç½®"""
        if hasattr(simulator, 'vehicles') and vehicle_idx < len(simulator.vehicles):
            return simulator.vehicles[vehicle_idx].get('position', (0, 0))
        return (0, 0)
    
    def _calculate_distance(self, simulator, pos1, pos2):
        """è®¡ç®—è·ç¦»"""
        if hasattr(simulator, 'calculate_distance'):
            return simulator.calculate_distance(pos1, pos2)
        # ç®€å•æ¬§æ°è·ç¦»
        import math
        return math.sqrt((pos1[0]-pos2[0])**2 + (pos1[1]-pos2[1])**2)
    
    def _estimate_delay(self, distance: float, queue_len: int, has_cache: bool, node_type: str) -> float:
        """ä¼°ç®—å»¶è¿Ÿï¼ˆç§’ï¼‰"""
        # ä¼ è¾“å»¶è¿Ÿï¼ˆåŸºäºè·ç¦»ï¼‰
        base_rate = 80e6 if node_type == 'rsu' else 45e6  # bps
        attenuation = 1.0 + distance / 800.0
        tx_delay = (1e6 * 8) / (base_rate / attenuation)  # å‡è®¾1MBæ•°æ®
        
        # é˜Ÿåˆ—ç­‰å¾…
        wait_delay = queue_len * 0.15 if node_type == 'rsu' else queue_len * 0.22
        
        # è®¡ç®—å»¶è¿Ÿ
        comp_delay = 0.0 if has_cache else (0.2 if node_type == 'rsu' else 0.3)
        
        return tx_delay + wait_delay + comp_delay
    
    def _estimate_energy(self, distance: float, node_type: str) -> float:
        """ä¼°ç®—èƒ½è€—ï¼ˆç„¦è€³ï¼‰"""
        tx_power = 0.18 if node_type == 'rsu' else 0.12  # W
        base_rate = 80e6 if node_type == 'rsu' else 45e6
        attenuation = 1.0 + distance / 800.0
        tx_time = (1e6 * 8) / (base_rate / attenuation)
        return tx_power * tx_time

    def decide(self, simulator, vehicle_idx: int) -> Dict[str, np.ndarray]:
        """ğŸ§  æ™ºèƒ½å†³ç­–ï¼šé€‰æ‹©æœ€ä¼˜å¸è½½ç›®æ ‡"""
        num_rsus = len(simulator.rsus)
        num_uavs = len(simulator.uavs)
        
        # è·å–è½¦è¾†ä½ç½®
        vehicle_pos = self._get_vehicle_position(simulator, vehicle_idx)
        
        # ========== ç­–ç•¥è·¯ç”± ==========
        if self.strategy == 'smart':
            return self._decide_smart(simulator, vehicle_pos, num_rsus, num_uavs)
        elif self.strategy == 'delay_optimal':
            return self._decide_delay_optimal(simulator, vehicle_pos, num_rsus, num_uavs)
        elif self.strategy == 'energy_optimal':
            return self._decide_energy_optimal(simulator, vehicle_pos, num_rsus, num_uavs)
        elif self.strategy == 'cache_aware':
            return self._decide_cache_aware(simulator, vehicle_pos, num_rsus, num_uavs)
        elif self.strategy == 'load_balance':
            return self._decide_load_balance(simulator, vehicle_pos, num_rsus, num_uavs)
        else:
            # æ—§ç‰ˆå¯å‘å¼
            return self._decide_legacy(simulator, num_rsus, num_uavs)
    
    def _decide_smart(self, simulator, vehicle_pos, num_rsus: int, num_uavs: int) -> Dict[str, np.ndarray]:
        """â­ æ™ºèƒ½ç»¼åˆè¯„åˆ†ç­–ç•¥"""
        rsu_scores = []
        uav_scores = []
        
        # è¯„ä¼°æ‰€æœ‰RSU
        for i, rsu in enumerate(simulator.rsus):
            rsu_pos = rsu.get('position', (0, 0))
            dist = self._calculate_distance(simulator, vehicle_pos, rsu_pos)
            queue_len = len(rsu.get('computation_queue', []))
            cache_size = len(rsu.get('cache', {}))
            has_cache = cache_size > 0
            
            # å¤šå› ç´ è¯„åˆ†ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            delay_score = self._estimate_delay(dist, queue_len, has_cache, 'rsu')
            energy_score = self._estimate_energy(dist, 'rsu')
            cache_bonus = -self.weight_cache if has_cache else 0.0
            queue_penalty = self.weight_queue * queue_len * 0.1
            
            total_score = (self.weight_delay * delay_score + 
                          self.weight_energy * energy_score + 
                          cache_bonus + queue_penalty)
            
            rsu_scores.append((total_score, i, dist))
        
        # è¯„ä¼°æ‰€æœ‰UAV
        for j, uav in enumerate(simulator.uavs):
            uav_pos = uav.get('position', (0, 0))
            dist = self._calculate_distance(simulator, vehicle_pos, uav_pos)
            queue_len = len(uav.get('computation_queue', []))
            
            delay_score = self._estimate_delay(dist, queue_len, False, 'uav')
            energy_score = self._estimate_energy(dist, 'uav')
            queue_penalty = self.weight_queue * queue_len * 0.1
            
            total_score = (self.weight_delay * delay_score + 
                          self.weight_energy * energy_score + 
                          queue_penalty)
            
            uav_scores.append((total_score, j, dist))
        
        # ç”Ÿæˆlogits
        rsu_logits = self._scores_to_logits(rsu_scores, num_rsus)
        uav_logits = self._scores_to_logits(uav_scores, num_uavs)
        
        # ä¸‰è·¯é€‰æ‹©logits (local/RSU/UAV)
        local_score = 5.0  # æœ¬åœ°åŸºå‡†
        rsu_best = min(rsu_scores, key=lambda x: x[0])[0] if rsu_scores else 999
        uav_best = min(uav_scores, key=lambda x: x[0])[0] if uav_scores else 999
        
        # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡ï¼ˆè¶Šå°è¶Šå¥½â†’è¶Šå¤§æ¦‚ç‡ï¼‰
        scores = np.array([local_score, rsu_best, uav_best])
        probs = np.exp(-scores / 2.0)  # æ¸©åº¦å‚æ•°=2.0
        probs = probs / probs.sum()
        three_logits = np.log(probs + 1e-6)
        
        return {
            'head3_logits': three_logits.astype(np.float32),
            'rsu_logits': rsu_logits,
            'uav_logits': uav_logits,
        }
    
    def _decide_delay_optimal(self, simulator, vehicle_pos, num_rsus: int, num_uavs: int) -> Dict[str, np.ndarray]:
        """â±ï¸ å»¶è¿Ÿä¼˜å…ˆç­–ç•¥"""
        rsu_scores = []
        for i, rsu in enumerate(simulator.rsus):
            dist = self._calculate_distance(simulator, vehicle_pos, rsu.get('position', (0, 0)))
            queue = len(rsu.get('computation_queue', []))
            cache = len(rsu.get('cache', {})) > 0
            delay = self._estimate_delay(dist, queue, cache, 'rsu')
            rsu_scores.append((delay, i, dist))
        
        uav_scores = []
        for j, uav in enumerate(simulator.uavs):
            dist = self._calculate_distance(simulator, vehicle_pos, uav.get('position', (0, 0)))
            queue = len(uav.get('computation_queue', []))
            delay = self._estimate_delay(dist, queue, False, 'uav')
            uav_scores.append((delay, j, dist))
        
        return self._build_logits_from_scores(rsu_scores, uav_scores, num_rsus, num_uavs)
    
    def _decide_energy_optimal(self, simulator, vehicle_pos, num_rsus: int, num_uavs: int) -> Dict[str, np.ndarray]:
        """âš¡ èƒ½è€—ä¼˜å…ˆç­–ç•¥"""
        rsu_scores = []
        for i, rsu in enumerate(simulator.rsus):
            dist = self._calculate_distance(simulator, vehicle_pos, rsu.get('position', (0, 0)))
            energy = self._estimate_energy(dist, 'rsu')
            rsu_scores.append((energy, i, dist))
        
        uav_scores = []
        for j, uav in enumerate(simulator.uavs):
            dist = self._calculate_distance(simulator, vehicle_pos, uav.get('position', (0, 0)))
            energy = self._estimate_energy(dist, 'uav')
            uav_scores.append((energy, j, dist))
        
        return self._build_logits_from_scores(rsu_scores, uav_scores, num_rsus, num_uavs)
    
    def _decide_cache_aware(self, simulator, vehicle_pos, num_rsus: int, num_uavs: int) -> Dict[str, np.ndarray]:
        """ğŸ“¦ ç¼“å­˜æ„ŸçŸ¥ç­–ç•¥"""
        rsu_scores = []
        for i, rsu in enumerate(simulator.rsus):
            dist = self._calculate_distance(simulator, vehicle_pos, rsu.get('position', (0, 0)))
            cache_size = len(rsu.get('cache', {}))
            # ç¼“å­˜è¶Šå¤šåˆ†æ•°è¶Šä½ï¼ˆè¶Šå¥½ï¼‰
            score = -cache_size * 10.0 + dist * 0.01
            rsu_scores.append((score, i, dist))
        
        uav_scores = []
        for j, uav in enumerate(simulator.uavs):
            dist = self._calculate_distance(simulator, vehicle_pos, uav.get('position', (0, 0)))
            score = dist * 0.01  # UAVæ— ç¼“å­˜
            uav_scores.append((score, j, dist))
        
        return self._build_logits_from_scores(rsu_scores, uav_scores, num_rsus, num_uavs)
    
    def _decide_load_balance(self, simulator, vehicle_pos, num_rsus: int, num_uavs: int) -> Dict[str, np.ndarray]:
        """âš–ï¸ è´Ÿè½½å‡è¡¡ç­–ç•¥"""
        rsu_scores = []
        for i, rsu in enumerate(simulator.rsus):
            dist = self._calculate_distance(simulator, vehicle_pos, rsu.get('position', (0, 0)))
            queue = len(rsu.get('computation_queue', []))
            # é˜Ÿåˆ—è¶Šé•¿æƒ©ç½šè¶Šé‡
            score = queue * 5.0 + dist * 0.001
            rsu_scores.append((score, i, dist))
        
        uav_scores = []
        for j, uav in enumerate(simulator.uavs):
            dist = self._calculate_distance(simulator, vehicle_pos, uav.get('position', (0, 0)))
            queue = len(uav.get('computation_queue', []))
            score = queue * 5.0 + dist * 0.001
            uav_scores.append((score, j, dist))
        
        return self._build_logits_from_scores(rsu_scores, uav_scores, num_rsus, num_uavs)
    
    def _decide_legacy(self, simulator, num_rsus: int, num_uavs: int) -> Dict[str, np.ndarray]:
        """æ—§ç‰ˆç®€å•å¯å‘å¼"""
        rsu_logits = np.full((num_rsus,), -6.0, dtype=np.float32)
        if num_rsus > 0:
            chosen = None
            best = None
            for i, rsu in enumerate(simulator.rsus):
                q = len(rsu.get('computation_queue', []))
                if best is None or q < best:
                    best = q
                    chosen = i
            if chosen is not None:
                rsu_logits[chosen] = 6.0
        
        uav_logits = np.full((num_uavs,), -6.0, dtype=np.float32)
        if num_uavs > 0:
            chosen = None
            best = None
            for j, uav in enumerate(simulator.uavs):
                q = len(uav.get('computation_queue', []))
                if best is None or q < best:
                    best = q
                    chosen = j
            if chosen is not None:
                uav_logits[chosen] = 6.0
        
        three_logits = np.array([0.0, 1.0 if num_rsus > 0 else -2.0, 
                                 0.5 if num_uavs > 0 else -2.0], dtype=np.float32)
        
        return {
            'head3_logits': three_logits,
            'rsu_logits': rsu_logits,
            'uav_logits': uav_logits,
        }
    
    def _scores_to_logits(self, scores: list, num_nodes: int) -> np.ndarray:
        """å°†è¯„åˆ†è½¬æ¢ä¸ºlogitsï¼ˆåˆ†æ•°è¶Šä½è¶Šå¥½ï¼‰"""
        logits = np.full((num_nodes,), -6.0, dtype=np.float32)
        if scores:
            # æ‰¾åˆ°æœ€ä½³èŠ‚ç‚¹
            best_idx = min(scores, key=lambda x: x[0])[1]
            logits[best_idx] = 6.0
            
            # ç»™æ¬¡ä¼˜èŠ‚ç‚¹ä¸€äº›æ¦‚ç‡
            sorted_scores = sorted(scores, key=lambda x: x[0])
            if len(sorted_scores) > 1:
                second_best = sorted_scores[1][1]
                logits[second_best] = 2.0
        
        return logits
    
    def _build_logits_from_scores(self, rsu_scores, uav_scores, num_rsus, num_uavs):
        """é€šç”¨çš„logitsæ„å»º"""
        rsu_logits = self._scores_to_logits(rsu_scores, num_rsus)
        uav_logits = self._scores_to_logits(uav_scores, num_uavs)
        
        local_score = 5.0
        rsu_best = min(rsu_scores, key=lambda x: x[0])[0] if rsu_scores else 999
        uav_best = min(uav_scores, key=lambda x: x[0])[0] if uav_scores else 999
        
        scores = np.array([local_score, rsu_best, uav_best])
        probs = np.exp(-scores / 2.0)
        probs = probs / probs.sum()
        three_logits = np.log(probs + 1e-6)
        
        return {
            'head3_logits': three_logits.astype(np.float32),
            'rsu_logits': rsu_logits,
            'uav_logits': uav_logits,
        }


class DualStageControllerEnv:
    """ğŸ”§ æ”¹è¿›çš„ä¸¤é˜¶æ®µæ§åˆ¶å™¨ï¼šåˆ†ç¦»åŠ¨ä½œç©ºé—´
    
    Stage 1ï¼ˆå¯å‘å¼ï¼‰ï¼šæ§åˆ¶å¸è½½å†³ç­–ï¼ˆå‰10ç»´ï¼‰
    Stage 2ï¼ˆRLï¼‰ï¼šåªå­¦ä¹ ç¼“å­˜/è¿ç§»æ§åˆ¶ï¼ˆ8ç»´ï¼‰
    
    å…³é”®æ”¹è¿›ï¼š
    1. base_env åªè¾“å‡º8ç»´åŠ¨ä½œï¼ˆç¼“å­˜/è¿ç§»ï¼‰
    2. Stage 1 å¯å‘å¼ç‹¬ç«‹ç”Ÿæˆå¸è½½å†³ç­–
    3. é¿å…ä¿¡ç”¨åˆ†é…æ··ä¹±ï¼šRLåªä¸ºå®ƒæ§åˆ¶çš„éƒ¨åˆ†è´Ÿè´£
    """

    def __init__(self, base_env: Any, simulator: Any, stage1_strategy: str = 'heuristic'):
        self.base = base_env  # Stage-2 RL env (TD3/SAC/...) implementing same interface
        self.simulator = simulator
        self.stage1 = _HeuristicOffloadPolicy(stage1_strategy)
        # åŠ¨ä½œç»´åº¦ä¿æŒä¸baseä¸€è‡´
        self.action_dim = getattr(self.base, 'action_dim', 18)
        self.config = getattr(self.base, 'config', None)
        # ä¿å­˜è¦†ç›–åçš„åŠ¨ä½œç”¨äºè®­ç»ƒï¼ˆç¡®ä¿è®­ç»ƒ-æ‰§è¡Œä¸€è‡´æ€§ï¼‰
        self._last_covered_action = None
        
        print(f"ğŸ§  DualStageControllerEnvåˆå§‹åŒ–:")
        print(f"   Stage 1 (å¯å‘å¼): å¸è½½å†³ç­– [{stage1_strategy}]")
        print(f"   Stage 2 (RL): ç¼“å­˜/è¿ç§»æ§åˆ¶")
        print(f"   âš ï¸  è®­ç»ƒç­–ç•¥: ç½‘ç»œå­¦ä¹ å®Œæ•´åŠ¨ä½œï¼Œä½†å‰10ç»´ä¼šè¢«è¦†ç›–")

    # ---- Policy interface passthrough with patching ----
    def get_actions(self, state: np.ndarray, training: bool = True):
        act = self.base.get_actions(state, training=training)
        # Normalize to a dict with 'vehicle_agent'
        if isinstance(act, dict):
            actions_dict = dict(act)
            vehicle_vec = actions_dict.get('vehicle_agent')
            if vehicle_vec is None:
                # fallback: construct from base vector if exists
                base_vec = None
            else:
                base_vec = np.array(vehicle_vec, dtype=np.float32)
        else:
            # tuple or ndarray; try first element or itself
            base_arr = act[0] if isinstance(act, (tuple, list)) else act
            base_vec = np.array(base_arr, dtype=np.float32)
            actions_dict = {
                'vehicle_agent': base_vec,
            }

        num_rsus = len(getattr(self.simulator, 'rsus', []))
        num_uavs = len(getattr(self.simulator, 'uavs', []))
        # Ensure base_vec sized
        if base_vec is None:
            base_vec = np.zeros(max(3 + num_rsus + num_uavs + 8, getattr(self.base, 'action_dim', 18)), dtype=np.float32)

        vec = base_vec.copy()

        # Stage-1 decision -> overwrite offloading segments
        policy = self.stage1.decide(self.simulator, 0)  # vehicle_idx not used for head3
        # For RSU/UAV logits, we need vehicle index; approximate with 0. OK for many vehicles since per-step repeats
        head3 = policy['head3_logits']
        rsu_logits = policy['rsu_logits']
        uav_logits = policy['uav_logits']

        # Place logits into vector segments
        rsu_start = 3
        rsu_end = rsu_start + num_rsus
        uav_end = rsu_end + num_uavs
        if vec.size < uav_end + 8:
            padded = np.zeros(uav_end + 8, dtype=np.float32)
            padded[:vec.size] = vec
            vec = padded

        vec[:3] = head3
        if num_rsus > 0:
            vec[rsu_start:rsu_end] = rsu_logits
        if num_uavs > 0:
            vec[rsu_end:uav_end] = uav_logits

        # ğŸ”§ ä¿å­˜è¦†ç›–åçš„åŠ¨ä½œç”¨äºè®­ç»ƒï¼ˆç¡®ä¿è®­ç»ƒ-æ‰§è¡Œä¸€è‡´ï¼‰
        self._last_covered_action = vec.copy()

        actions_dict['vehicle_agent'] = vec
        actions_dict['rsu_agent'] = vec[rsu_start:rsu_end]
        actions_dict['uav_agent'] = vec[rsu_end:uav_end]
        return actions_dict

    # ---- Training methods proxy to Stage-2 RL env ----
    def train_step(self, state, action, reward, next_state, done):
        """ğŸ”§ è®­ç»ƒä¿®å¤ï¼šç¡®ä¿è®­ç»ƒ-æ‰§è¡Œä¸€è‡´æ€§
        
        æ ¸å¿ƒé—®é¢˜ï¼š
        - æ‰§è¡Œæ—¶ï¼šä½¿ç”¨è¦†ç›–åçš„åŠ¨ä½œï¼ˆStage1å‰10ç»´ + RLå8ç»´ï¼‰
        - å¥–åŠ±ï¼šåŸºäºè¦†ç›–ååŠ¨ä½œçš„æ‰§è¡Œç»“æœ
        - è®­ç»ƒï¼šåº”è¯¥ç”¨è¦†ç›–åçš„åŠ¨ä½œï¼Œè®©ç½‘ç»œå­¦ä¹ æ­£ç¡®çš„å› æœå…³ç³»
        
        è§£å†³æ–¹æ¡ˆï¼š
        - ä½¿ç”¨self._last_covered_actionï¼ˆè¦†ç›–åçš„åŠ¨ä½œï¼‰è¿›è¡Œè®­ç»ƒ
        - ç½‘ç»œä¼šå­¦ä¹ ï¼šè¾“å‡ºæ¥è¿‘è¦†ç›–ååŠ¨ä½œçš„å€¼
        - è™½ç„¶å‰10ç»´ä¼šè¢«å†æ¬¡è¦†ç›–ï¼Œä½†å8ç»´èƒ½æ­£ç¡®å­¦ä¹ 
        - ç½‘ç»œä¼šè‡ªç„¶åœ°å‘ç°ï¼šå‰10ç»´çš„å˜åŒ–ä¸å½±å“å¥–åŠ±
        """
        # ä½¿ç”¨è¦†ç›–åçš„åŠ¨ä½œè¿›è¡Œè®­ç»ƒï¼ˆä¸å®é™…æ‰§è¡Œä¸€è‡´ï¼‰
        training_action = self._last_covered_action if self._last_covered_action is not None else action
        return self.base.train_step(state, training_action, reward, next_state, done)

    def store_experience(self, **kwargs):
        # PPO special path; forward to base env
        if hasattr(self.base, 'store_experience'):
            return self.base.store_experience(**kwargs)
        return {}

    def update(self, *args, **kwargs):
        if hasattr(self.base, 'update'):
            return self.base.update(*args, **kwargs)
        return {}

    # ---- State and reward methods proxy to base env ----
    def get_state_vector(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """è·å–çŠ¶æ€å‘é‡ - ä»£ç†åˆ°åŸºç¡€ç¯å¢ƒ"""
        return self.base.get_state_vector(node_states, system_metrics)

    def calculate_reward(self, system_metrics: Dict, 
                        cache_metrics: Optional[Dict] = None,
                        migration_metrics: Optional[Dict] = None) -> float:
        """è®¡ç®—å¥–åŠ± - ä»£ç†åˆ°åŸºç¡€ç¯å¢ƒ"""
        return self.base.calculate_reward(system_metrics, cache_metrics, migration_metrics)

    def decompose_action(self, action: np.ndarray) -> Dict[str, np.ndarray]:
        """åˆ†è§£åŠ¨ä½œ - ä»£ç†åˆ°åŸºç¡€ç¯å¢ƒ"""
        if hasattr(self.base, 'decompose_action'):
            return self.base.decompose_action(action)
        # Fallback: simple decomposition
        num_rsus = len(getattr(self.simulator, 'rsus', []))
        num_uavs = len(getattr(self.simulator, 'uavs', []))
        rsu_start = 3
        rsu_end = rsu_start + num_rsus
        uav_end = rsu_end + num_uavs
        return {
            'vehicle_agent': action,
            'rsu_agent': action[rsu_start:rsu_end] if len(action) > rsu_start else np.array([]),
            'uav_agent': action[rsu_end:uav_end] if len(action) > rsu_end else np.array([])
        }

    # ---- Model save/load proxy to base env ----
    def save_models(self, filepath: str):
        """ä¿å­˜æ¨¡å‹ - ä»£ç†åˆ°åŸºç¡€ç¯å¢ƒ"""
        if hasattr(self.base, 'save_models'):
            return self.base.save_models(filepath)

    def load_models(self, filepath: str):
        """åŠ è½½æ¨¡å‹ - ä»£ç†åˆ°åŸºç¡€ç¯å¢ƒ"""
        if hasattr(self.base, 'load_models'):
            return self.base.load_models(filepath)
