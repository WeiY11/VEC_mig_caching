"""
SAC (Soft Actor-Critic) å•æ™ºèƒ½ä½“ç®—æ³•å®ç°
ä¸“é—¨é€‚é…MATD3-MIGç³»ç»Ÿçš„VECç¯å¢ƒ

ä¸»è¦ç‰¹ç‚¹:
1. æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ æ¡†æ¶
2. è‡ªåŠ¨æ¸©åº¦å‚æ•°è°ƒèŠ‚
3. åŒQç½‘ç»œå‡å°‘è¿‡ä¼°è®¡
4. é«˜æ ·æœ¬æ•ˆç‡

å¯¹åº”è®ºæ–‡: Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor
"""
# æ€§èƒ½ä¼˜åŒ– - å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰
try:
    from tools.performance_optimization import OPTIMIZED_BATCH_SIZES
except ImportError:
    OPTIMIZED_BATCH_SIZES = {'SAC': 256}  # é»˜è®¤å€¼

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
from torch.distributions import Normal
from collections import deque
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass

from config import config


@dataclass
class SACConfig:
    """SACç®—æ³•é…ç½®"""
    # ç½‘ç»œç»“æ„
    hidden_dim: int = 256
    actor_lr: float = 3e-4
    critic_lr: float = 3e-4
    alpha_lr: float = 3e-4
    
    # SACå‚æ•°
    initial_temperature: float = 0.2
    target_entropy_ratio: float = -1.0  # ç›®æ ‡ç†µæ¯”ä¾‹
    tau: float = 0.005  # è½¯æ›´æ–°ç³»æ•°
    gamma: float = 0.99  # æŠ˜æ‰£å› å­
    
    # è®­ç»ƒå‚æ•°
    batch_size: int = 256
    buffer_size: int = 100000
    update_freq: int = 1
    target_update_freq: int = 1
    warmup_steps: int = 1000
    
    # å…¶ä»–å‚æ•°
    auto_entropy_tuning: bool = True


class SACActor(nn.Module):
    """SAC Actorç½‘ç»œ - éšæœºç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(SACActor, self).__init__()
        
        self.action_dim = action_dim
        
        # å…±äº«ç‰¹å¾å±‚
        self.feature_layers = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # å‡å€¼å’Œå¯¹æ•°æ ‡å‡†å·®è¾“å‡º
        self.mean_layer = nn.Linear(hidden_dim, action_dim)
        self.log_std_layer = nn.Linear(hidden_dim, action_dim)
        
        # å¯¹æ•°æ ‡å‡†å·®çš„èŒƒå›´é™åˆ¶
        self.log_std_min = -20
        self.log_std_max = 2
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.feature_layers:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.0)
        
        nn.init.xavier_uniform_(self.mean_layer.weight)
        nn.init.constant_(self.mean_layer.bias, 0.0)
        nn.init.xavier_uniform_(self.log_std_layer.weight)
        nn.init.constant_(self.log_std_layer.bias, 0.0)
    
    def forward(self, state: torch.Tensor):
        """å‰å‘ä¼ æ’­"""
        features = self.feature_layers(state)
        
        mean = self.mean_layer(features)
        log_std = self.log_std_layer(features)
        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        
        return mean, log_std
    
    def sample(self, state: torch.Tensor, reparam: bool = True):
        """é‡‡æ ·åŠ¨ä½œ"""
        mean, log_std = self.forward(state)
        std = log_std.exp()
        
        normal = Normal(mean, std)
        
        if reparam:
            # é‡å‚æ•°åŒ–æŠ€å·§
            x_t = normal.rsample()
        else:
            x_t = normal.sample()
        
        # tanhå˜æ¢ç¡®ä¿åŠ¨ä½œåœ¨[-1, 1]èŒƒå›´å†…
        action = torch.tanh(x_t)
        
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡ï¼Œéœ€è¦è€ƒè™‘tanhå˜æ¢çš„é›…å¯æ¯”è¡Œåˆ—å¼
        log_prob = normal.log_prob(x_t)
        log_prob -= torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        
        mean = torch.tanh(mean)
        
        return action, log_prob, mean


class SACCritic(nn.Module):
    """SAC Criticç½‘ç»œ - åŒQå‡½æ•°ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(SACCritic, self).__init__()
        
        # Q1ç½‘ç»œ
        self.q1_network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Q2ç½‘ç»œ
        self.q2_network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for network in [self.q1_network, self.q2_network]:
            for layer in network:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    nn.init.constant_(layer.bias, 0.0)
    
    def forward(self, state: torch.Tensor, action: torch.Tensor):
        """å‰å‘ä¼ æ’­"""
        sa = torch.cat([state, action], dim=1)
        
        q1 = self.q1_network(sa)
        q2 = self.q2_network(sa)
        
        return q1, q2


class SACReplayBuffer:
    """SACä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int, state_dim: int, action_dim: int, alpha: float = 0.6):
        self.capacity = capacity
        self.ptr = 0
        self.size = 0
        self.alpha = alpha
        
        # é¢„åˆ†é…å†…å­˜
        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)
        self.rewards = np.zeros(capacity, dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros(capacity, dtype=np.float32)
        
        # ä¼˜å…ˆçº§æ•°ç»„
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.max_priority = 1.0
    
    def push(self, state: np.ndarray, action: np.ndarray, reward: float, 
             next_state: np.ndarray, done: bool):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.next_states[self.ptr] = next_state
        self.dones[self.ptr] = float(done)
        
        # æ–°ç»éªŒä½¿ç”¨æœ€å¤§ä¼˜å…ˆçº§
        self.priorities[self.ptr] = self.max_priority
        
        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    
    def sample(self, batch_size: int, beta: float = 0.4) -> Tuple[torch.Tensor, ...]:
        """ä¼˜å…ˆçº§é‡‡æ ·ç»éªŒæ‰¹æ¬¡"""
        if self.size == 0:
            raise ValueError("Buffer is empty")
        
        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        priorities = self.priorities[:self.size] ** self.alpha
        probabilities = priorities / priorities.sum()
        
        # é‡‡æ ·ç´¢å¼•
        indices = np.random.choice(self.size, batch_size, p=probabilities)
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        weights = (self.size * probabilities[indices]) ** (-beta)
        weights = weights / weights.max()  # å½’ä¸€åŒ–
        
        batch_states = torch.FloatTensor(self.states[indices])
        batch_actions = torch.FloatTensor(self.actions[indices])
        batch_rewards = torch.FloatTensor(self.rewards[indices]).unsqueeze(1)
        batch_next_states = torch.FloatTensor(self.next_states[indices])
        batch_dones = torch.FloatTensor(self.dones[indices]).unsqueeze(1)
        weights = torch.FloatTensor(weights).unsqueeze(1)
        
        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, indices, weights
    
    def update_priorities(self, indices: np.ndarray, td_errors: np.ndarray):
        """æ›´æ–°ä¼˜å…ˆçº§"""
        for idx, td_error in zip(indices, td_errors):
            self.priorities[idx] = abs(td_error) + 1e-6
            self.max_priority = max(self.max_priority, self.priorities[idx])
    
    def __len__(self):
        return self.size


class SACAgent:
    """SACæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, config: SACConfig):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config
        
        # æ€§èƒ½ä¼˜åŒ– - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        self.optimized_batch_size = OPTIMIZED_BATCH_SIZES.get('SAC', config.batch_size)
        self.config.batch_size = self.optimized_batch_size
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºç½‘ç»œ
        self.actor = SACActor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.critic = SACCritic(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.target_critic = SACCritic(state_dim, action_dim, config.hidden_dim).to(self.device)
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
        self.hard_update(self.target_critic, self.critic)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - æé«˜æ”¶æ•›ç¨³å®šæ€§
        self.actor_scheduler = optim.lr_scheduler.StepLR(self.actor_optimizer, step_size=10000, gamma=0.9)
        self.critic_scheduler = optim.lr_scheduler.StepLR(self.critic_optimizer, step_size=10000, gamma=0.9)
        
        # æ¸©åº¦å‚æ•° (è‡ªåŠ¨è°ƒèŠ‚ç†µ)
        if config.auto_entropy_tuning:
            self.target_entropy = config.target_entropy_ratio * action_dim
            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)
            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=config.alpha_lr)
            self.alpha_scheduler = optim.lr_scheduler.StepLR(self.alpha_optimizer, step_size=10000, gamma=0.9)
        else:
            self.log_alpha = torch.log(torch.FloatTensor([config.initial_temperature])).to(self.device)
            self.alpha_scheduler = None
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº - æ”¯æŒä¼˜å…ˆçº§ç»éªŒå›æ”¾
        self.replay_buffer = SACReplayBuffer(config.buffer_size, state_dim, action_dim, alpha=0.6)
        
        # PER betaå‚æ•°
        self.beta = 0.4
        self.beta_increment = (1.0 - 0.4) / max(1, 100000)  # 100kæ­¥å†…ä»0.4å¢åŠ åˆ°1.0
        
        # è®­ç»ƒç»Ÿè®¡
        self.actor_losses = []
        self.critic_losses = []
        self.alpha_losses = []
        self.step_count = 0
    
    @property
    def alpha(self):
        """è·å–å½“å‰æ¸©åº¦å‚æ•°"""
        return self.log_alpha.exp()
    
    def select_action(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        if training:
            with torch.no_grad():
                action, _, _ = self.actor.sample(state_tensor)
        else:
            # è¯„ä¼°æ¨¡å¼ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            with torch.no_grad():
                _, _, action = self.actor.sample(state_tensor)
        
        return action.cpu().numpy()[0]
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, reward: float,
                        next_state: np.ndarray, done: bool):
        """å­˜å‚¨ç»éªŒ"""
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def update(self) -> Dict[str, float]:
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        if len(self.replay_buffer) < self.config.batch_size:
            return {}
        
        self.step_count += 1
        
        # é¢„çƒ­æœŸä¸æ›´æ–°
        if self.step_count < self.config.warmup_steps:
            return {}
        
        # é‡‡æ ·ç»éªŒæ‰¹æ¬¡ - æ”¯æŒä¼˜å…ˆçº§ç»éªŒå›æ”¾
        batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, indices, weights = \
            self.replay_buffer.sample(self.config.batch_size, self.beta)
        
        # æ›´æ–°beta
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        batch_states = batch_states.to(self.device)
        batch_actions = batch_actions.to(self.device)
        batch_rewards = batch_rewards.to(self.device)
        batch_next_states = batch_next_states.to(self.device)
        batch_dones = batch_dones.to(self.device)
        weights = weights.to(self.device)
        
        # æ›´æ–°Criticå¹¶è·å–TDè¯¯å·®
        critic_loss, td_errors = self._update_critic(batch_states, batch_actions, batch_rewards, 
                                        batch_next_states, batch_dones, weights)
        
        # æ›´æ–°ä¼˜å…ˆçº§
        self.replay_buffer.update_priorities(indices, td_errors.detach().cpu().numpy())
        
        # æ›´æ–°Actorå’Œæ¸©åº¦å‚æ•°
        actor_loss, alpha_loss = self._update_actor_and_alpha(batch_states)
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.step_count % self.config.target_update_freq == 0:
            self.soft_update(self.target_critic, self.critic, self.config.tau)
        
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.step_count % 100 == 0:  # æ¯100æ­¥æ›´æ–°ä¸€æ¬¡
            self.actor_scheduler.step()
            self.critic_scheduler.step()
            if self.alpha_scheduler is not None:
                self.alpha_scheduler.step()
        
        return {
            'actor_loss': actor_loss,
            'critic_loss': critic_loss,
            'alpha_loss': alpha_loss,
            'alpha': self.alpha.item(),
            'actor_lr': self.actor_optimizer.param_groups[0]['lr'],
            'critic_lr': self.critic_optimizer.param_groups[0]['lr']
        }
    
    def _update_critic(self, states: torch.Tensor, actions: torch.Tensor, 
                      rewards: torch.Tensor, next_states: torch.Tensor, 
                      dones: torch.Tensor, weights: torch.Tensor) -> Tuple[float, torch.Tensor]:
        """æ›´æ–°Criticç½‘ç»œå¹¶è¿”å›TDè¯¯å·®"""
        with torch.no_grad():
            next_actions, next_log_probs, _ = self.actor.sample(next_states)
            target_q1, target_q2 = self.target_critic(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs
            target_q = rewards + (1 - dones) * self.config.gamma * target_q
        
        current_q1, current_q2 = self.critic(states, actions)
        
        # è®¡ç®—TDè¯¯å·®
        td_errors1 = torch.abs(current_q1 - target_q)
        td_errors2 = torch.abs(current_q2 - target_q)
        td_errors = torch.max(td_errors1, td_errors2).squeeze()
        
        # ä½¿ç”¨é‡è¦æ€§é‡‡æ ·æƒé‡
        critic_loss1 = (weights * F.mse_loss(current_q1, target_q, reduction='none')).mean()
        critic_loss2 = (weights * F.mse_loss(current_q2, target_q, reduction='none')).mean()
        critic_loss = critic_loss1 + critic_loss2
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        
        # æ·»åŠ æ¢¯åº¦è£å‰ªæé«˜ç¨³å®šæ€§
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)
        
        self.critic_optimizer.step()
        
        self.critic_losses.append(critic_loss.item())
        return critic_loss.item(), td_errors
    
    def _update_actor_and_alpha(self, states: torch.Tensor) -> Tuple[float, float]:
        """æ›´æ–°Actorç½‘ç»œå’Œæ¸©åº¦å‚æ•°"""
        # è®¡ç®—ç­–ç•¥æŸå¤±
        actions, log_probs, _ = self.actor.sample(states)
        q1, q2 = self.critic(states, actions)
        q = torch.min(q1, q2)
        
        actor_loss = (self.alpha * log_probs - q).mean()
        
        # æ›´æ–°Actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        
        # æ·»åŠ æ¢¯åº¦è£å‰ªæé«˜ç¨³å®šæ€§
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)
        
        self.actor_optimizer.step()
        
        self.actor_losses.append(actor_loss.item())
        
        # æ›´æ–°æ¸©åº¦å‚æ•°
        alpha_loss = 0.0
        if self.config.auto_entropy_tuning:
            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
            
            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            
            # æ¸©åº¦å‚æ•°ä¹Ÿéœ€è¦æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_([self.log_alpha], max_norm=1.0)
            
            self.alpha_optimizer.step()
            
            self.alpha_losses.append(alpha_loss.item())
            alpha_loss = alpha_loss.item()
        
        return actor_loss.item(), alpha_loss
    
    def soft_update(self, target: nn.Module, source: nn.Module, tau: float):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
    
    def hard_update(self, target: nn.Module, source: nn.Module):
        """ç¡¬æ›´æ–°ç½‘ç»œå‚æ•°"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(param.data)
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'target_critic_state_dict': self.target_critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'log_alpha': self.log_alpha,
            'alpha_optimizer_state_dict': self.alpha_optimizer.state_dict() if self.config.auto_entropy_tuning else None,
            'step_count': self.step_count
        }, f"{filepath}_sac.pth")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(f"{filepath}_sac.pth", map_location=self.device)
        
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.target_critic.load_state_dict(checkpoint['target_critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.log_alpha = checkpoint['log_alpha']
        self.step_count = checkpoint['step_count']
        
        if self.config.auto_entropy_tuning and checkpoint['alpha_optimizer_state_dict'] is not None:
            self.alpha_optimizer.load_state_dict(checkpoint['alpha_optimizer_state_dict'])


class SACEnvironment:
    """SACè®­ç»ƒç¯å¢ƒ"""
    
    def __init__(self):
        self.config = SACConfig()
        
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—çŠ¶æ€ç»´åº¦ï¼Œä¸TD3ä¿æŒä¸€è‡´
        self.state_dim = 130  # è½¦è¾†60 + RSU54 + UAV16 = 130ç»´
        self.action_dim = 30  # æ•´åˆæ‰€æœ‰èŠ‚ç‚¹åŠ¨ä½œ
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agent = SACAgent(self.state_dim, self.action_dim, self.config)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_count = 0
        self.step_count = 0
        
        print(f"âœ“ SACç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ çŠ¶æ€ç»´åº¦: {self.state_dim}")
        print(f"âœ“ åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"âœ“ è‡ªåŠ¨ç†µè°ƒèŠ‚: {self.config.auto_entropy_tuning}")
        print(f"âœ“ ç›®æ ‡ç†µ: {self.config.target_entropy_ratio * self.action_dim}")
    
    def get_state_vector(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """æ„å»ºå…¨å±€çŠ¶æ€å‘é‡"""
        # åŸºç¡€ç³»ç»ŸçŠ¶æ€
        base_state = np.array([
            system_metrics.get('avg_task_delay', 0.0) / 1.0,
            system_metrics.get('total_energy_consumption', 0.0) / 1000.0,
            system_metrics.get('data_loss_rate', 0.0),
            system_metrics.get('cache_hit_rate', 0.0),
            system_metrics.get('migration_success_rate', 0.0),
        ])
        
        # èŠ‚ç‚¹ç‰¹å®šçŠ¶æ€ (ç®€åŒ–å®ç°)
        node_states_flat = np.random.randn(self.state_dim - len(base_state))
        
        return np.concatenate([base_state, node_states_flat])
    
    def decompose_action(self, action: np.ndarray) -> Dict[str, np.ndarray]:
        """å°†å…¨å±€åŠ¨ä½œåˆ†è§£ä¸ºå„èŠ‚ç‚¹åŠ¨ä½œ"""
        actions = {}
        start_idx = 0
        
        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“ç±»å‹åˆ†é…åŠ¨ä½œ
        for agent_type in ['vehicle_agent', 'rsu_agent', 'uav_agent']:
            end_idx = start_idx + 10  # æ¯ä¸ªæ™ºèƒ½ä½“10ä¸ªåŠ¨ä½œç»´åº¦
            actions[agent_type] = action[start_idx:end_idx]
            start_idx = end_idx
        
        return actions
    
    def get_actions(self, state: np.ndarray, training: bool = True) -> Dict[str, np.ndarray]:
        """è·å–åŠ¨ä½œ"""
        global_action = self.agent.select_action(state, training)
        return self.decompose_action(global_action)
    
    def calculate_reward(self, system_metrics: Dict) -> float:
        """è®¡ç®—å¥–åŠ± - ä½¿ç”¨æ ‡å‡†åŒ–å¥–åŠ±å‡½æ•°"""
        from utils.standardized_reward import calculate_standardized_reward
        return calculate_standardized_reward(system_metrics, agent_type='single_agent')
    
    def train_step(self, state: np.ndarray, action: Union[np.ndarray, int], reward: float,
                   next_state: np.ndarray, done: bool) -> Dict:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        # SACéœ€è¦numpyæ•°ç»„ï¼Œå¦‚æœæ˜¯æ•´æ•°åˆ™è½¬æ¢
        if isinstance(action, int):
            action = np.array([action], dtype=np.float32)
        elif not isinstance(action, np.ndarray):
            action = np.array(action, dtype=np.float32)
        
        # å­˜å‚¨ç»éªŒ
        self.agent.store_experience(state, action, reward, next_state, done)
        
        # æ›´æ–°ç½‘ç»œ
        training_info = self.agent.update()
        
        self.step_count += 1
        
        return training_info
    
    def save_models(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(filepath, exist_ok=True)
        self.agent.save_model(filepath)
        print(f"âœ“ SACæ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_models(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        self.agent.load_model(filepath)
        print(f"âœ“ SACæ¨¡å‹å·²åŠ è½½: {filepath}")
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, reward: float,
                        next_state: np.ndarray, done: bool, log_prob: float = 0.0, value: float = 0.0):
        """å­˜å‚¨ç»éªŒåˆ°ç¼“å†²åŒº - æ”¯æŒPPOå…¼å®¹æ€§"""
        # SACåªä½¿ç”¨å‰5ä¸ªå‚æ•°ï¼Œlog_probå’Œvalueè¢«å¿½ç•¥
        self.agent.store_experience(state, action, reward, next_state, done)
        self.step_count += 1
    
    def update(self, last_value: float = 0.0) -> Dict:
        """æ›´æ–°ç½‘ç»œå‚æ•° - æ”¯æŒPPOå…¼å®¹æ€§"""
        # SACä¸ä½¿ç”¨last_valueå‚æ•°
        return self.agent.update()
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return {
            'actor_loss_avg': float(np.mean(self.agent.actor_losses[-100:])) if self.agent.actor_losses else 0.0,
            'critic_loss_avg': float(np.mean(self.agent.critic_losses[-100:])) if self.agent.critic_losses else 0.0,
            'alpha_loss_avg': float(np.mean(self.agent.alpha_losses[-100:])) if self.agent.alpha_losses else 0.0,
            'alpha': self.agent.alpha.item(),
            'buffer_size': len(self.agent.replay_buffer),
            'step_count': self.step_count,
            'auto_entropy_tuning': self.config.auto_entropy_tuning
        }