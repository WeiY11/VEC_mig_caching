"""
å¢å¼ºå‹TD3æ™ºèƒ½ä½“ - é›†æˆ5é¡¹é«˜çº§ä¼˜åŒ–

æ•´åˆäº†ä»¥ä¸‹ä¼˜åŒ–æŠ€æœ¯ï¼š
1. é˜Ÿåˆ—çº¦æŸçš„åˆ†å¸ƒå¼Criticï¼ˆQR-DQNé£æ ¼ï¼‰
2. å¸¦ç†µæ­£åˆ™çš„SACç‰¹æ€§ï¼ˆè‡ªé€‚åº”æ¸©åº¦ï¼‰
3. æ¨¡å‹åŒ–é˜Ÿåˆ—é¢„æµ‹ï¼ˆDreamer/MBPOé£æ ¼ï¼‰
4. é˜Ÿåˆ—æ„ŸçŸ¥çš„ä¼˜å…ˆç»éªŒå›æ”¾
5. GNNè·¯ç”±å™¨çš„èšåˆç‰¹æ€§ï¼ˆGATé£æ ¼ï¼‰

æ‰€æœ‰ä¼˜åŒ–å‡å¯é€šè¿‡é…ç½®å‚æ•°ç‹¬ç«‹å¯ç”¨/ç¦ç”¨ã€‚

ä½œè€…ï¼šVEC_mig_caching Team
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, Tuple, Optional, Any, List
import os

# å¯¼å…¥åŸºç¡€TD3ç»„ä»¶
from .td3 import TD3Actor, TD3Critic, GraphFeatureExtractor

# å¯¼å…¥å¢å¼ºç»„ä»¶
from .enhanced_td3_config import EnhancedTD3Config
from .quantile_critic import DistributionalCritic
from .queue_aware_replay import QueueAwareReplayBuffer
from .queue_dynamics_model import QueueDynamicsModel, ModelBasedRollout, ModelTrainer
from .gat_router import GATRouterActor


class EnhancedTD3Agent:
    """
    å¢å¼ºå‹TD3æ™ºèƒ½ä½“
    
    ç›¸æ¯”æ ‡å‡†TD3ï¼Œå¢åŠ äº†5é¡¹å¯é€‰ä¼˜åŒ–ï¼š
    1. åˆ†å¸ƒå¼Critic - æŠ‘åˆ¶å°¾éƒ¨æ—¶å»¶
    2. ç†µæ­£åˆ™åŒ– - ç»´æŒæ¢ç´¢
    3. æ¨¡å‹åŒ–é˜Ÿåˆ—é¢„æµ‹ - åŠ é€Ÿæ”¶æ•›
    4. é˜Ÿåˆ—æ„ŸçŸ¥å›æ”¾ - æ™ºèƒ½é‡‡æ ·
    5. GATè·¯ç”±å™¨ - ååŒç¼“å­˜
    """
    
    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        config: EnhancedTD3Config,
        num_vehicles: Optional[int] = None,
        num_rsus: Optional[int] = None,
        num_uavs: Optional[int] = None,
        global_dim: int = 8,
        central_state_dim: Optional[int] = None,
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config
        self.device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
        
        # æ‹“æ‰‘ä¿¡æ¯
        self.num_vehicles = num_vehicles or 12
        self.num_rsus = num_rsus or 4
        self.num_uavs = num_uavs or 2
        self.global_dim = global_dim
        self.central_state_dim = central_state_dim or 0
        
        # ========== æ„å»ºActorç½‘ç»œ ==========
        if config.use_gat_router:
            # ä½¿ç”¨GATè·¯ç”±å™¨
            print("[EnhancedTD3] ä½¿ç”¨GATè·¯ç”±å™¨æ„å»ºActor")
            self.graph_encoder = GATRouterActor(
                num_vehicles=self.num_vehicles,
                num_rsus=self.num_rsus,
                num_uavs=self.num_uavs,
                global_feature_dim=self.global_dim,
                hidden_dim=config.gat_hidden_dim,
                num_heads=config.num_attention_heads,
                edge_feature_dim=config.edge_feature_dim,
                central_state_dim=self.central_state_dim,  # æ·»åŠ ä¸­å¤®çŠ¶æ€ç»´åº¦
            ).to(self.device)
            actor_input_dim = config.gat_hidden_dim
        else:
            # ä½¿ç”¨æ ‡å‡†å›¾ç¼–ç å™¨ï¼Œä¼ å…¥central_dimå‚æ•°
            # ğŸ¯ ä¿®å¤: è®©GraphFeatureExtractorå¤„ç†ä¸­å¤®èµ„æºçŠ¶æ€
            self.graph_encoder = GraphFeatureExtractor(
                num_vehicles=self.num_vehicles,
                num_rsus=self.num_rsus,
                num_uavs=self.num_uavs,
                embed_dim=config.graph_embed_dim,
                central_dim=self.central_state_dim,  # æ·»åŠ ä¸­å¤®èµ„æºç»´åº¦
            ).to(self.device)
            # GraphFeatureExtractorè¾“å‡ºå·²åŒ…å«ä¸­å¤®èµ„æºç¼–ç 
            actor_input_dim = self.graph_encoder.output_dim
        
        # Actorä¸»ç½‘ç»œï¼ˆä¸å†éœ€è¦æ‰‹åŠ¨æ·»åŠ central_state_dimï¼‰
        # ğŸ¯ ä¿®å¤: ç›´æ¥ä½¿ç”¨graph_encoderçš„è¾“å‡ºç»´åº¦
        self.actor = nn.Sequential(
            nn.Linear(actor_input_dim, config.hidden_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_dim, action_dim),
            nn.Tanh(),
        ).to(self.device)
        
        # Target Actor
        self.target_graph_encoder = self._clone_network(self.graph_encoder)
        self.target_actor = self._clone_network(self.actor)
        
        # ========== æ„å»ºCriticç½‘ç»œ ==========
        if config.use_distributional_critic:
            # ä½¿ç”¨åˆ†å¸ƒå¼Critic
            print(f"[EnhancedTD3] ä½¿ç”¨åˆ†å¸ƒå¼Critic (n_quantiles={config.n_quantiles})")
            self.critic = DistributionalCritic(
                state_dim=state_dim,
                action_dim=action_dim,
                hidden_dim=config.hidden_dim,
                n_quantiles=config.n_quantiles,
                quantile_embedding_dim=config.quantile_embedding_dim,
                kappa=config.quantile_kappa,
            ).to(self.device)
            self.target_critic = self._clone_network(self.critic)
        else:
            # ä½¿ç”¨æ ‡å‡†Twin Critic
            self.critic = TD3Critic(
                state_dim=state_dim,
                action_dim=action_dim,
                hidden_dim=config.hidden_dim,
            ).to(self.device)
            self.target_critic = self._clone_network(self.critic)
        
        # ========== ä¼˜åŒ–å™¨ ==========
        self.actor_optimizer = optim.Adam(
            list(self.graph_encoder.parameters()) + list(self.actor.parameters()),
            lr=config.actor_lr
        )
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)
        
        # ========== ç†µæ­£åˆ™åŒ– ==========
        if config.use_entropy_reg:
            print(f"[EnhancedTD3] å¯ç”¨ç†µæ­£åˆ™åŒ– (initial_alpha={config.initial_alpha})")
            self.use_entropy_reg = True
            self.log_alpha = torch.tensor(
                np.log(config.initial_alpha), requires_grad=True, device=self.device
            )
            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=config.alpha_lr)
            self.target_entropy = -config.target_entropy_ratio * action_dim
            self.auto_tune_alpha = config.auto_tune_alpha
        else:
            self.use_entropy_reg = False
            self.log_alpha = None
        
        # ========== ç»éªŒå›æ”¾ç¼“å†²åŒº ==========
        if config.use_queue_aware_replay:
            print(f"[EnhancedTD3] ä½¿ç”¨é˜Ÿåˆ—æ„ŸçŸ¥å›æ”¾ (queue_priority_weight={config.queue_priority_weight})")
            self.replay_buffer = QueueAwareReplayBuffer(
                capacity=config.buffer_size,
                state_dim=state_dim,
                action_dim=action_dim,
                alpha=config.alpha,
                queue_priority_weight=config.queue_priority_weight,
                queue_metrics_ema_decay=config.queue_metrics_ema_decay,
            )
        else:
            # ä½¿ç”¨æ ‡å‡†ä¼˜å…ˆå›æ”¾ï¼ˆä»td3.pyå¯¼å…¥ï¼‰
            from .td3 import TD3ReplayBuffer
            self.replay_buffer = TD3ReplayBuffer(
                capacity=config.buffer_size,
                state_dim=state_dim,
                action_dim=action_dim,
                alpha=config.alpha,
            )
        
        # ========== æ¨¡å‹åŒ–é˜Ÿåˆ—é¢„æµ‹ ==========
        if config.use_model_based_rollout:
            print(f"[EnhancedTD3] å¯ç”¨æ¨¡å‹åŒ–é˜Ÿåˆ—é¢„æµ‹ (rollout_horizon={config.rollout_horizon})")
            self.use_model_based = True
            self.dynamics_model = QueueDynamicsModel(
                state_dim=state_dim,
                action_dim=action_dim,
                hidden_dims=config.model_hidden_dims,
            ).to(self.device)
            self.model_rollout = ModelBasedRollout(
                dynamics_model=self.dynamics_model,
                rollout_horizon=config.rollout_horizon,
                imagined_reward_weight=config.imagined_reward_weight,
                overflow_penalty=config.overflow_penalty,
                device=self.device,
            )
            self.model_trainer = ModelTrainer(
                model=self.dynamics_model,
                learning_rate=config.model_lr,
                batch_size=config.batch_size,
                train_iterations=config.model_train_iterations,
                device=self.device,
            )
            self.model_train_freq = config.model_train_freq
            self.model_step_count = 0
        else:
            self.use_model_based = False
            self.dynamics_model = None
        
        # ========== PERå‚æ•° ==========
        self.beta = config.beta_start
        self.beta_increment = config.beta_increment
        
        # ========== æ¢ç´¢å™ªå£° ==========
        self.exploration_noise = config.exploration_noise
        self.step_count = 0
        self.update_count = 0
        
        # ========== è®­ç»ƒç»Ÿè®¡ ==========
        self.actor_losses = []
        self.critic_losses = []
        self.entropy_values = []
        self.alpha_values = []
        
        print(f"[EnhancedTD3] åˆå§‹åŒ–å®Œæˆ")
        print(f"  - åˆ†å¸ƒå¼Critic: {config.use_distributional_critic}")
        print(f"  - ç†µæ­£åˆ™åŒ–: {config.use_entropy_reg}")
        print(f"  - æ¨¡å‹åŒ–é¢„æµ‹: {config.use_model_based_rollout}")
        print(f"  - é˜Ÿåˆ—æ„ŸçŸ¥å›æ”¾: {config.use_queue_aware_replay}")
        print(f"  - GATè·¯ç”±å™¨: {config.use_gat_router}")
    
    def _clone_network(self, network: nn.Module) -> nn.Module:
        """å…‹éš†ç½‘ç»œç”¨äºåˆ›å»ºtargetç½‘ç»œ"""
        import copy
        clone = copy.deepcopy(network)
        clone.to(self.device)
        return clone
    
    @property
    def alpha(self) -> float:
        """è·å–å½“å‰ç†µæ¸©åº¦å‚æ•°"""
        if self.use_entropy_reg:
            return self.log_alpha.exp().item()
        return 0.0
    
    def select_action(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ
        
        ğŸ¯ ä¿®å¤: çŠ¶æ€å‘é‡å·²ç»åŒ…å«ä¸­å¤®èµ„æºçŠ¶æ€ï¼ˆæ¥è‡ªEnhancedTD3Wrapperï¼‰
        ä¸å†éœ€è¦æ‰‹åŠ¨æ·»åŠ å…¨é›¶ä¸­å¤®çŠ¶æ€
        
        Args:
            state: çŠ¶æ€å‘é‡ï¼ˆå·²åŒ…å«ä¸­å¤®èµ„æºçŠ¶æ€ï¼‰
            training: æ˜¯å¦è®­ç»ƒæ¨¡å¼
        
        Returns:
            action: åŠ¨ä½œå‘é‡
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            # ç›´æ¥ä½¿ç”¨çŠ¶æ€ï¼Œä¸éœ€è¦æ‰‹åŠ¨æ·»åŠ ä¸­å¤®çŠ¶æ€
            encoded_state = self.graph_encoder(state_tensor)
            
            # ç”ŸæˆåŠ¨ä½œ
            action_tensor = self.actor(encoded_state)
        
        action = action_tensor.cpu().numpy()[0]
        
        # æ·»åŠ æ¢ç´¢å™ªå£°
        if training:
            noise = np.random.normal(0, self.exploration_noise, size=action.shape)
            action = np.clip(action + noise, -1.0, 1.0)
        
        return action
    
    def store_experience(
        self,
        state: np.ndarray,
        action: np.ndarray,
        reward: float,
        next_state: np.ndarray,
        done: bool,
        queue_metrics: Optional[Dict[str, float]] = None,
    ):
        """å­˜å‚¨ç»éªŒ"""
        if self.config.use_queue_aware_replay:
            # é˜Ÿåˆ—æ„ŸçŸ¥å›æ”¾éœ€è¦é˜Ÿåˆ—æŒ‡æ ‡
            self.replay_buffer.push(state, action, reward, next_state, done, queue_metrics)
        else:
            # æ ‡å‡†å›æ”¾
            self.replay_buffer.push(state, action, reward, next_state, done)
    
    def update(self) -> Dict[str, float]:
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        if len(self.replay_buffer) < self.config.batch_size:
            return {}
        
        self.step_count += 1
        
        # é¢„çƒ­æœŸä¸æ›´æ–°
        if self.step_count < self.config.warmup_steps:
            return {}
        
        self.update_count += 1
        
        # é‡‡æ ·ç»éªŒæ‰¹æ¬¡
        batch = self.replay_buffer.sample(self.config.batch_size, self.beta)
        states, actions, rewards, next_states, dones, indices, weights = batch
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        states = states.to(self.device)
        actions = actions.to(self.device)
        rewards = rewards.to(self.device)
        next_states = next_states.to(self.device)
        dones = dones.to(self.device)
        weights = weights.to(self.device)
        
        # æ›´æ–°beta
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        # ========== æ›´æ–°Critic ==========
        critic_loss, td_errors = self._update_critic(
            states, actions, rewards, next_states, dones, weights
        )
        
        # æ›´æ–°ä¼˜å…ˆçº§
        self.replay_buffer.update_priorities(indices, td_errors.detach().cpu().numpy())
        
        training_info = {'critic_loss': critic_loss}
        
        # ========== å»¶è¿Ÿç­–ç•¥æ›´æ–° ==========
        if self.update_count % self.config.policy_delay == 0:
            actor_loss, entropy_info = self._update_actor(states)
            training_info['actor_loss'] = actor_loss
            training_info.update(entropy_info)
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self._soft_update(self.target_graph_encoder, self.graph_encoder, self.config.tau)
            self._soft_update(self.target_actor, self.actor, self.config.tau)
            self._soft_update(self.target_critic, self.critic, self.config.tau)
        
        # ========== æ¨¡å‹åŒ–é˜Ÿåˆ—é¢„æµ‹ ==========
        if self.use_model_based:
            self.model_step_count += 1
            
            # å®šæœŸè®­ç»ƒåŠ¨æ€æ¨¡å‹
            if self.model_step_count % self.model_train_freq == 0:
                model_stats = self.model_trainer.train(
                    self.replay_buffer,
                    min_buffer_size=self.config.min_model_buffer_size
                )
                training_info.update({f'model_{k}': v for k, v in model_stats.items()})
                
                # ç”Ÿæˆåˆæˆtransitions
                if len(self.replay_buffer) >= self.config.min_model_buffer_size:
                    self._generate_synthetic_data()
        
        # è¡°å‡å™ªå£°
        self.exploration_noise = max(
            self.config.min_noise,
            self.exploration_noise * self.config.noise_decay
        )
        training_info['exploration_noise'] = self.exploration_noise
        
        return training_info
    
    def _update_critic(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        rewards: torch.Tensor,
        next_states: torch.Tensor,
        dones: torch.Tensor,
        weights: torch.Tensor,
    ) -> Tuple[float, torch.Tensor]:
        """æ›´æ–°Criticç½‘ç»œ"""
        if self.config.use_distributional_critic:
            return self._update_distributional_critic(
                states, actions, rewards, next_states, dones, weights
            )
        else:
            return self._update_standard_critic(
                states, actions, rewards, next_states, dones, weights
            )
    
    def _update_distributional_critic(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        rewards: torch.Tensor,
        next_states: torch.Tensor,
        dones: torch.Tensor,
        weights: torch.Tensor,
    ) -> Tuple[float, torch.Tensor]:
        """æ›´æ–°åˆ†å¸ƒå¼Critic"""
        with torch.no_grad():
            # ç”Ÿæˆç›®æ ‡åŠ¨ä½œ
            next_encoded = self.target_graph_encoder(next_states)
            next_actions = self.target_actor(next_encoded)
            
            # æ·»åŠ ç›®æ ‡å™ªå£°
            noise = torch.randn_like(next_actions) * self.config.target_noise
            noise = torch.clamp(noise, -self.config.noise_clip, self.config.noise_clip)
            next_actions = torch.clamp(next_actions + noise, -1.0, 1.0)
            
            # è·å–ç›®æ ‡åˆ†ä½æ•°Qå€¼
            target_q1_quantiles, target_q2_quantiles = self.target_critic(next_states, next_actions)
            target_q_quantiles = torch.min(target_q1_quantiles, target_q2_quantiles)
            
            # Bootstrap
            target_quantiles = rewards + (1 - dones) * self.config.gamma * target_q_quantiles
        
        #  è®¡ç®—æŸå¤±
        loss, td_errors = self.critic.compute_loss(
            states, actions, target_quantiles, weights
        )
        
        # åå‘ä¼ æ’­
        self.critic_optimizer.zero_grad()
        loss.backward()
        if self.config.use_gradient_clip:
            torch.nn.utils.clip_grad_norm_(
                self.critic.parameters(), self.config.gradient_clip_norm
            )
        self.critic_optimizer.step()
        
        self.critic_losses.append(loss.item())
        return loss.item(), td_errors
    
    def _update_standard_critic(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        rewards: torch.Tensor,
        next_states: torch.Tensor,
        dones: torch.Tensor,
        weights: torch.Tensor,
    ) -> Tuple[float, torch.Tensor]:
        """æ›´æ–°æ ‡å‡†Twin Critic"""
        with torch.no_grad():
            next_encoded = self.target_graph_encoder(next_states)
            next_actions = self.target_actor(next_encoded)
            
            noise = torch.randn_like(next_actions) * self.config.target_noise
            noise = torch.clamp(noise, -self.config.noise_clip, self.config.noise_clip)
            next_actions = torch.clamp(next_actions + noise, -1.0, 1.0)
            
            target_q1, target_q2 = self.target_critic(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2)
            target_q = rewards + (1 - dones) * self.config.gamma * target_q
        
        current_q1, current_q2 = self.critic(states, actions)
        td_errors_q1 = current_q1 - target_q
        td_errors_q2 = current_q2 - target_q
        
        critic_loss = (weights * td_errors_q1.pow(2)).mean() + (weights * td_errors_q2.pow(2)).mean()
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        if self.config.use_gradient_clip:
            torch.nn.utils.clip_grad_norm_(
                self.critic.parameters(), self.config.gradient_clip_norm
            )
        self.critic_optimizer.step()
        
        self.critic_losses.append(critic_loss.item())
        td_errors = td_errors_q1.detach().abs().squeeze()
        return critic_loss.item(), td_errors
    
    def _update_actor(self, states: torch.Tensor) -> Tuple[float, Dict[str, float]]:
        """æ›´æ–°Actorç½‘ç»œ"""
        # ç›´æ¥ç¼–ç çŠ¶æ€ï¼Œä¸éœ€è¦æ‰‹åŠ¨æ·»åŠ ä¸­å¤®çŠ¶æ€
        # ğŸ¯ ä¿®å¤: çŠ¶æ€å‘é‡å·²ç»åŒ…å«ä¸­å¤®èµ„æºçŠ¶æ€ï¼ˆæ¥è‡ªEnhancedTD3Wrapperï¼‰
        encoded_states = self.graph_encoder(states)
        
        # ç”ŸæˆåŠ¨ä½œ
        actions = self.actor(encoded_states)
        
        # è®¡ç®—Qå€¼
        if self.config.use_distributional_critic:
            q_values = self.critic.q1(states, actions)
        else:
            q_values, _ = self.critic(states, actions)
            q_values = q_values[:, :1]  # åªç”¨Q1
        
        actor_loss = -q_values.mean()
        
        entropy_info = {}
        
        # ========== ç†µæ­£åˆ™åŒ– ==========
        if self.use_entropy_reg:
            # ç®€å•ä¼°è®¡ï¼šåŸºäºåŠ¨ä½œæ–¹å·®
            action_std = actions.std(dim=0).mean()
            entropy = torch.log(action_std + 1e-6)
            
            self.entropy_values.append(entropy.item())
            entropy_info['entropy'] = entropy.item()
            
            # æ·»åŠ ç†µbonus
            actor_loss = actor_loss - self.alpha * entropy
            
            # è‡ªåŠ¨è°ƒèŠ‚æ¸©åº¦
            if self.auto_tune_alpha:
                alpha_loss = -(self.log_alpha * (entropy - self.target_entropy).detach()).mean()
                
                self.alpha_optimizer.zero_grad()
                alpha_loss.backward()
                self.alpha_optimizer.step()
                
                self.alpha_values.append(self.alpha)
                entropy_info['alpha'] = self.alpha
                entropy_info['alpha_loss'] = alpha_loss.item()
        
        # æ›´æ–°Actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        if self.config.use_gradient_clip:
            torch.nn.utils.clip_grad_norm_(
                list(self.graph_encoder.parameters()) + list(self.actor.parameters()),
                self.config.gradient_clip_norm
            )
        self.actor_optimizer.step()
        
        self.actor_losses.append(actor_loss.item())
        return actor_loss.item(), entropy_info
    
    def _soft_update(self, target: nn.Module, source: nn.Module, tau: float):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
    
    def _generate_synthetic_data(self):
        """ä½¿ç”¨æ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®"""
        # ä»replay bufferé‡‡æ ·çœŸå®çŠ¶æ€
        batch_size = min(self.config.rollout_batch_size, len(self.replay_buffer))
        indices = np.random.choice(len(self.replay_buffer), batch_size, replace=False)
        real_states = torch.FloatTensor(self.replay_buffer.states[indices])
        
        # æ‰§è¡Œrollout
        synthetic_transitions = self.model_rollout.generate_synthetic_transitions(
            real_states,
            self.actor,
            num_rollouts_per_state=self.config.num_rollouts_per_state,
        )
        
        # å°†åˆæˆtransitionsåŠ å…¥replay buffer
        for s, a, r, s_next, done in synthetic_transitions:
            # æ³¨æ„ï¼šåˆæˆæ•°æ®å¯èƒ½éœ€è¦é™ä½ä¼˜å…ˆçº§æˆ–æ ‡è®°
            self.store_experience(s, a, r, s_next, done, queue_metrics=None)
    
    def save_model(self, filepath: str) -> str:
        """ä¿å­˜æ¨¡å‹"""
        save_dict = {
            'graph_encoder_state_dict': self.graph_encoder.state_dict(),
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'target_graph_encoder_state_dict': self.target_graph_encoder.state_dict(),
            'target_actor_state_dict': self.target_actor.state_dict(),
            'target_critic_state_dict': self.target_critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'exploration_noise': self.exploration_noise,
            'step_count': self.step_count,
            'update_count': self.update_count,
        }
        
        if self.use_entropy_reg:
            save_dict['log_alpha'] = self.log_alpha
            save_dict['alpha_optimizer_state_dict'] = self.alpha_optimizer.state_dict()
        
        if self.use_model_based:
            save_dict['dynamics_model_state_dict'] = self.dynamics_model.state_dict()
        
        os.makedirs(os.path.dirname(filepath) or '.', exist_ok=True)
        torch.save(save_dict, filepath)
        print(f"[EnhancedTD3] æ¨¡å‹å·²ä¿å­˜: {filepath}")
        return filepath
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.graph_encoder.load_state_dict(checkpoint['graph_encoder_state_dict'])
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.target_graph_encoder.load_state_dict(checkpoint['target_graph_encoder_state_dict'])
        self.target_actor.load_state_dict(checkpoint['target_actor_state_dict'])
        self.target_critic.load_state_dict(checkpoint['target_critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        
        if 'log_alpha' in checkpoint and self.use_entropy_reg:
            self.log_alpha = checkpoint['log_alpha']
            self.alpha_optimizer.load_state_dict(checkpoint['alpha_optimizer_state_dict'])
        
        if 'dynamics_model_state_dict' in checkpoint and self.use_model_based:
            self.dynamics_model.load_state_dict(checkpoint['dynamics_model_state_dict'])
        
        self.exploration_noise = checkpoint['exploration_noise']
        self.step_count = checkpoint['step_count']
        self.update_count = checkpoint['update_count']
        
        print(f"[EnhancedTD3] æ¨¡å‹å·²åŠ è½½: {filepath}")
