#!/usr/bin/env python3
"""
TD3å¢žå¼ºæŽ¢ç´¢ç‰ˆæœ¬ - å€Ÿé‰´SACçš„æŽ¢ç´¢ä¼˜åŠ¿

æ ¸å¿ƒæ”¹è¿›ï¼š
1. è‡ªé€‚åº”æŽ¢ç´¢å™ªå£°ï¼ˆåŸºäºŽçŠ¶æ€ï¼‰
2. é’ˆå¯¹ç¼“å­˜åŠ¨ä½œç»´åº¦çš„é¢å¤–æŽ¢ç´¢
3. æ—©æœŸé«˜æŽ¢ç´¢ï¼ŒåŽæœŸé€æ¸é™ä½Ž

ä½¿ç”¨ï¼š
    export TD3_ENHANCED_EXPLORATION=1
    python train_single_agent.py --algorithm TD3 --episodes 800
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Optional
from single_agent.td3 import TD3Agent, TD3Config


class EnhancedExplorationTD3Agent(TD3Agent):
    """å¢žå¼ºæŽ¢ç´¢ç‰ˆTD3 - å€Ÿé‰´SACçš„æŽ¢ç´¢ç­–ç•¥"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # ðŸ”¥ å¢žå¼ºæŽ¢ç´¢å‚æ•°
        self.base_exploration_noise = 0.3  # æé«˜åŸºç¡€å™ªå£°ï¼ˆä»Ž0.2ï¼‰
        self.cache_exploration_bonus = 0.15  # ç¼“å­˜ç»´åº¦é¢å¤–æŽ¢ç´¢
        self.exploration_noise = self.base_exploration_noise
        
        # è‡ªé€‚åº”æŽ¢ç´¢ï¼ˆåŸºäºŽæ€§èƒ½ï¼‰
        self.recent_cache_hits = []
        self.adaptive_exploration = True
        
        print("ðŸ”¥ TD3å¢žå¼ºæŽ¢ç´¢ç‰ˆæœ¬å·²å¯ç”¨")
        print(f"   åŸºç¡€å™ªå£°: {self.base_exploration_noise}")
        print(f"   ç¼“å­˜æŽ¢ç´¢: +{self.cache_exploration_bonus}")
    
    def select_action(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ - å¢žå¼ºæŽ¢ç´¢ç‰ˆ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.actor(state_tensor).cpu().numpy()[0]
        
        if training:
            # ðŸ”¥ è‡ªé€‚åº”å™ªå£°ï¼šæ—©æœŸé«˜ï¼ŒåŽæœŸä½Ž
            # åŠ¨æ€è°ƒæ•´åŸºäºŽè®­ç»ƒè¿›åº¦
            progress = min(1.0, self.step_count / 100000.0)
            adaptive_noise = self.base_exploration_noise * (1.0 - 0.7 * progress)
            
            # åŸºç¡€æŽ¢ç´¢å™ªå£°
            noise = np.random.normal(0, adaptive_noise, size=action.shape)
            
            # ðŸŽ¯ ç¼“å­˜/è¿ç§»ç»´åº¦é¢å¤–æŽ¢ç´¢ï¼ˆåŽ8ç»´ï¼‰
            # è¿™éƒ¨åˆ†å¯¹å»¶è¿Ÿå½±å“å¤§ï¼Œéœ€è¦å……åˆ†æŽ¢ç´¢
            cache_start = len(action) - 8
            if cache_start > 0:
                cache_noise = np.random.normal(
                    0, 
                    self.cache_exploration_bonus, 
                    size=8
                )
                noise[cache_start:] += cache_noise
            
            action = np.clip(action + noise, -1.0, 1.0)
        
        return action
    
    def update_exploration(self, cache_hit_rate: float):
        """åŸºäºŽç¼“å­˜å‘½ä¸­çŽ‡åŠ¨æ€è°ƒæ•´æŽ¢ç´¢"""
        if not self.adaptive_exploration:
            return
        
        self.recent_cache_hits.append(cache_hit_rate)
        if len(self.recent_cache_hits) > 50:
            self.recent_cache_hits.pop(0)
        
        # å¦‚æžœç¼“å­˜å‘½ä¸­çŽ‡æŒç»­ä½Žï¼Œå¢žåŠ æŽ¢ç´¢
        if len(self.recent_cache_hits) >= 20:
            avg_hit_rate = np.mean(self.recent_cache_hits[-20:])
            if avg_hit_rate < 0.45:  # ä½ŽäºŽ45%
                self.cache_exploration_bonus = min(0.25, self.cache_exploration_bonus * 1.05)
                print(f"ðŸ” ä½Žç¼“å­˜å‘½ä¸­çŽ‡({avg_hit_rate:.1%})ï¼Œå¢žåŠ æŽ¢ç´¢: {self.cache_exploration_bonus:.3f}")


# ä¾¿æ·åˆ›å»ºå‡½æ•°
def create_enhanced_td3_env(num_vehicles=12, num_rsus=4, num_uavs=2):
    """åˆ›å»ºå¢žå¼ºæŽ¢ç´¢ç‰ˆTD3çŽ¯å¢ƒ"""
    from single_agent.td3 import TD3Environment
    from single_agent.common_state_action import UnifiedStateActionSpace
    
    # åˆ›å»ºæ ‡å‡†TD3çŽ¯å¢ƒ
    env = TD3Environment(num_vehicles, num_rsus, num_uavs)
    
    # æ›¿æ¢agentä¸ºå¢žå¼ºç‰ˆ
    state_dim = env.state_dim
    action_dim = env.action_dim
    config = env.config
    
    enhanced_agent = EnhancedExplorationTD3Agent(
        state_dim, action_dim, config,
        num_vehicles, num_rsus, num_uavs
    )
    
    env.agent = enhanced_agent
    
    return env

