#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„TD3ç®—æ³• - é’ˆå¯¹VECç³»ç»Ÿä¼˜åŒ–

æ ¸å¿ƒæ”¹è¿›ï¼ˆç›¸æ¯”CAMTD3ï¼‰ï¼š
1. ç§»é™¤å¯å‘å¼èåˆæœºåˆ¶ï¼ˆå®éªŒè¯æ˜å¹²æ‰°å­¦ä¹ ï¼‰
2. ä¼˜åŒ–æ¢ç´¢ç­–ç•¥ï¼ˆadaptive noiseï¼‰
3. å¢å¼ºå¥–åŠ±å¡‘å½¢ï¼ˆprogress-based reward shapingï¼‰
4. æ”¹è¿›ç½‘ç»œç»“æ„ï¼ˆæ›´é€‚åˆVECä»»åŠ¡ï¼‰

æ€§èƒ½ç›®æ ‡ï¼š
- æ—¶å»¶ < 0.20sï¼ˆæ¯”Randomçš„0.56sæå‡60%+ï¼‰
- èƒ½è€— < 6500Jï¼ˆæ¯”Randomçš„6763Jé™ä½5%+ï¼‰
- å®Œæˆç‡ > 98%ï¼ˆæ¯”Randomçš„99.1%æŒå¹³æˆ–ç•¥é«˜ï¼‰
"""

from __future__ import annotations

import numpy as np
from typing import Dict, Optional
from single_agent.td3 import TD3Environment, TD3Config


class ImprovedTD3Config(TD3Config):
    """æ”¹è¿›çš„TD3é…ç½®"""
    
    def __init__(self):
        super().__init__()
        # ä¼˜åŒ–å­¦ä¹ ç‡ï¼ˆæ›´æ¿€è¿›çš„å­¦ä¹ ï¼‰
        self.actor_lr = 2e-4  # æé«˜è‡³2e-4
        self.critic_lr = 3e-4  # æé«˜è‡³3e-4
        
        # ä¼˜åŒ–æ¢ç´¢å‚æ•°ï¼ˆè‡ªé€‚åº”æ¢ç´¢ï¼‰
        self.exploration_noise = 0.2  # åˆå§‹æ¢ç´¢å™ªå£°
        self.noise_decay = 0.9995  # ç¼“æ…¢è¡°å‡
        self.min_noise = 0.08  # ä¿æŒé€‚åº¦æ¢ç´¢
        
        # ä¼˜åŒ–TD3å‚æ•°
        self.policy_delay = 2  # æ ‡å‡†å»¶è¿Ÿ
        self.tau = 0.005  # æ ‡å‡†è½¯æ›´æ–°
        
        # ä¼˜åŒ–è®­ç»ƒå‚æ•°
        self.batch_size = 256
        self.warmup_steps = 2000  # å‡å°‘é¢„çƒ­æ—¶é—´
        
        # å¯ç”¨è¿›åº¦å¥–åŠ±å¡‘å½¢
        self.use_progress_shaping = True
        self.progress_alpha = 0.1  # è¿›åº¦å¥–åŠ±æƒé‡


class ImprovedTD3Environment(TD3Environment):
    """æ”¹è¿›çš„TD3ç¯å¢ƒ - ç§»é™¤å¯å‘å¼èåˆï¼Œä¸“æ³¨ä¼˜åŒ–å­¦ä¹ """
    
    def __init__(self, num_vehicles: int = 12, num_rsus: int = 4, num_uavs: int = 2):
        super().__init__(num_vehicles, num_rsus, num_uavs)
        self.algorithm_label = "Improved-TD3"
        
        # ä½¿ç”¨æ”¹è¿›çš„é…ç½®
        self.config = ImprovedTD3Config()
        
        # è¿›åº¦è¿½è¸ªï¼ˆç”¨äºå¥–åŠ±å¡‘å½¢ï¼‰
        self.best_delay = float('inf')
        self.best_energy = float('inf')
        self.episode_count = 0
        
        print(f"\nğŸš€ Improved TD3 å·²å¯ç”¨")
        print(f"   æ ¸å¿ƒæ”¹è¿›:")
        print(f"   âœ“ ç§»é™¤å¯å‘å¼èåˆï¼ˆé¿å…å¹²æ‰°å­¦ä¹ ï¼‰")
        print(f"   âœ“ è‡ªé€‚åº”æ¢ç´¢å™ªå£°ï¼ˆæ›´å¥½çš„æ¢ç´¢-åˆ©ç”¨å¹³è¡¡ï¼‰")
        print(f"   âœ“ è¿›åº¦å¥–åŠ±å¡‘å½¢ï¼ˆåŠ é€Ÿæ”¶æ•›ï¼‰")
        print(f"   âœ“ ä¼˜åŒ–è¶…å‚æ•°ï¼ˆæ›´å¿«å­¦ä¹ ï¼‰\n")
    
    def calculate_reward(self, system_metrics: Dict, 
                        cache_metrics: Optional[Dict] = None,
                        migration_metrics: Optional[Dict] = None) -> float:
        """
        æ”¹è¿›çš„å¥–åŠ±å‡½æ•° - æ·»åŠ è¿›åº¦å¥–åŠ±å¡‘å½¢
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        1. åŸºç¡€å¥–åŠ±ï¼šç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
        2. è¿›åº¦å¥–åŠ±ï¼šé¼“åŠ±æŒç»­æ”¹è¿›ï¼ˆé¿å…æ—©æœŸæŒ¯è¡ï¼‰
        """
        from utils.unified_reward_calculator import calculate_unified_reward
        
        # ========== 1. åŸºç¡€å¥–åŠ± ==========
        base_reward = calculate_unified_reward(
            system_metrics, 
            cache_metrics, 
            migration_metrics, 
            algorithm="general"
        )
        
        # ========== 2. è¿›åº¦å¥–åŠ±å¡‘å½¢ï¼ˆå¯é€‰ï¼‰==========
        if not self.config.use_progress_shaping:
            return base_reward
        
        # æå–å½“å‰æ€§èƒ½
        current_delay = max(0.0, float(system_metrics.get('avg_task_delay', 0)))
        current_energy = max(0.0, float(system_metrics.get('total_energy_consumption', 0)))
        
        # è®¡ç®—è¿›åº¦å¥–åŠ±ï¼ˆç›¸å¯¹äºå†å²æœ€ä½³ï¼‰
        progress_reward = 0.0
        
        if current_delay < self.best_delay:
            # æ—¶å»¶æ”¹è¿›
            improvement = (self.best_delay - current_delay) / max(self.best_delay, 0.1)
            progress_reward += improvement * 5.0  # æ—¶å»¶æ”¹è¿›å¥–åŠ±
            self.best_delay = current_delay
        
        if current_energy < self.best_energy:
            # èƒ½è€—æ”¹è¿›
            improvement = (self.best_energy - current_energy) / max(self.best_energy, 1000.0)
            progress_reward += improvement * 3.0  # èƒ½è€—æ”¹è¿›å¥–åŠ±
            self.best_energy = current_energy
        
        # ========== 3. æœ€ç»ˆå¥–åŠ± ==========
        final_reward = base_reward + self.config.progress_alpha * progress_reward
        
        # è£å‰ªåˆ°åˆç†èŒƒå›´
        final_reward = np.clip(final_reward, -30.0, 5.0)
        
        return final_reward
    
    def reset_episode(self):
        """Episodeé‡ç½®æ—¶è°ƒç”¨"""
        self.episode_count += 1
        
        # æ¯50ä¸ªepisodeé‡ç½®æœ€ä½³è®°å½•ï¼ˆå…è®¸é‡æ–°æ¢ç´¢ï¼‰
        if self.episode_count % 50 == 0:
            self.best_delay = float('inf')
            self.best_energy = float('inf')
            print(f"   [Episode {self.episode_count}] é‡ç½®æœ€ä½³è®°å½•ï¼Œé¼“åŠ±æ–°æ¢ç´¢")

