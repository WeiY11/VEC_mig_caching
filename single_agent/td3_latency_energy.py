"""
TD3å»¶æ—¶-èƒ½è€—ååŒä¼˜åŒ–å˜ä½“

åœ¨æ ‡å‡†TD3åŸºç¡€ä¸Šï¼Œå¼•å…¥è‡ªé€‚åº”å¥–åŠ±å¡‘å½¢å™¨ï¼Œä»¥åŠ¨æ€åŠ æƒæ–¹å¼æ›´å¼ºåœ°åŽ‹åˆ¶æ—¶å»¶ä¸Žèƒ½è€—ï¼Œ
åŒæ—¶ä¿æŒä»»åŠ¡å®ŒæˆçŽ‡çº¦æŸã€‚è¯¥çŽ¯å¢ƒä¸ŽåŽŸTD3å…±äº«ç›¸åŒçš„ç½‘ç»œç»“æž„ä¸Žè®­ç»ƒæµç¨‹ï¼Œä»…è°ƒæ•´å¥–åŠ±è®¾è®¡ã€‚
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, Optional

import numpy as np

from config import config
from .td3 import TD3Environment


def _safe_float(value, default: float = 0.0) -> float:
    """é˜²å¾¡å¼è½¬æ¢ä¸ºfloatã€‚"""
    if value is None:
        return default
    try:
        return float(value)
    except (TypeError, ValueError):
        return default


def _safe_int(value, default: int = 0) -> int:
    """é˜²å¾¡å¼è½¬æ¢ä¸ºintã€‚"""
    if value is None:
        return default
    try:
        return int(value)
    except (TypeError, ValueError):
        return default


@dataclass
class LatencyEnergyRewardShaper:
    """
    å»¶æ—¶-èƒ½è€—è‡ªé€‚åº”å¥–åŠ±å¡‘å½¢å™¨

    é€šè¿‡æŒ‡æ•°æ»‘åŠ¨å¹³å‡è¿½è¸ªæŒ‡æ ‡ï¼Œå¹¶æ ¹æ®åç¦»ç¨‹åº¦åŠ¨æ€è°ƒæ•´æƒé‡ã€‚åŒæ—¶ç»“åˆå®ŒæˆçŽ‡çº¦æŸä¸Žçªå¢žæƒ©ç½šï¼Œ
    è¾¾åˆ°â€œæ—¶å»¶ä¸Žèƒ½è€—ååŒæœ€å°åŒ–â€çš„ç›®æ ‡ã€‚
    """

    # ðŸ”§ P0ä¿®å¤ï¼šç»Ÿä¸€ç›®æ ‡å€¼ï¼Œä¸Žsystem_configä¸€è‡´
    delay_target: float = field(default_factory=lambda: getattr(config.rl, "latency_target", 0.40))  # æ”¹ä¸º0.40s
    energy_target: float = field(default_factory=lambda: getattr(config.rl, "energy_target", 1200.0))  # æ”¹ä¸º1200J
    delay_tolerance: float = field(default_factory=lambda: getattr(config.rl, "latency_upper_tolerance", 0.80))  # æ”¹ä¸º0.80s
    energy_tolerance: float = field(default_factory=lambda: getattr(config.rl, "energy_upper_tolerance", 1800.0))  # æ”¹ä¸º1800J

    # ðŸ”§ P0ä¿®å¤ï¼šé™ä½Žæƒ©ç½šç³»æ•°ï¼Œé¿å…æƒ©ç½šç´¯åŠ 
    base_delay_weight: float = 1.5  # é™ä½Ž 2.4 â†’ 1.5
    base_energy_weight: float = 1.0  # é™ä½Ž 1.6 â†’ 1.0
    adaptive_gain: float = 0.8  # é™ä½Ž 1.8 â†’ 0.8ï¼Œé™åˆ¶æ”¾å¤§å€æ•°
    balance_gain: float = 0.0  # ç¦ç”¨å¹³è¡¡æƒ©ç½šï¼Œå‡å°‘æƒ©ç½šé¡¹
    balance_tolerance: float = 0.12
    completion_target: float = 0.95
    completion_penalty_gain: float = 3.0  # é™ä½Ž 12.0 â†’ 3.0
    dropped_penalty_gain: float = 0.02  # é™ä½Ž 0.05 â†’ 0.02
    spike_penalty_gain: float = 0.0  # ç¦ç”¨çªå¢žæƒ©ç½šï¼Œå‡å°‘æƒ©ç½šé¡¹
    ema_alpha: float = 0.12

    # ðŸ”§ P0ä¿®å¤ï¼šæ‰©å¤§å¥–åŠ±è£å‰ªèŒƒå›´ï¼Œå…è®¸æ›´å¤§çš„æ­£è´Ÿåé¦ˆ
    reward_clip_low: float = -15.0  # æ‰©å¤§ -40.0 â†’ -15.0
    reward_clip_high: float = 5.0  # å…è®¸è¾ƒå°çš„æ­£å¥–åŠ± -1e-3 â†’ +5.0

    _delay_ema: Optional[float] = None
    _energy_ema: Optional[float] = None
    _last_breakdown: Dict[str, float] = field(default_factory=dict, init=False)

    def _update_ema(self, current: float, ema_value: Optional[float]) -> float:
        if ema_value is None:
            return current
        return self.ema_alpha * current + (1.0 - self.ema_alpha) * ema_value

    def calculate_reward(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict] = None,
        migration_metrics: Optional[Dict] = None,
    ) -> float:
        """æ ¹æ®ç³»ç»ŸæŒ‡æ ‡è®¡ç®—å¥–åŠ±ã€‚"""
        avg_delay = max(0.0, _safe_float(system_metrics.get("avg_task_delay"), 0.0))
        total_energy = max(0.0, _safe_float(system_metrics.get("total_energy_consumption"), 0.0))
        completion_rate = np.clip(_safe_float(system_metrics.get("task_completion_rate"), 0.0), 0.0, 1.0)
        dropped_tasks = max(0, _safe_int(system_metrics.get("dropped_tasks"), 0))

        # æ›´æ–°æŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼Œæ•æ‰è¶‹åŠ¿ç”¨äºŽçªå¢žæƒ©ç½š
        self._delay_ema = self._update_ema(avg_delay, self._delay_ema)
        self._energy_ema = self._update_ema(total_energy, self._energy_ema)

        delay_ratio = avg_delay / max(self.delay_target, 1e-6)
        energy_ratio = total_energy / max(self.energy_target, 1e-6)

        adaptive_delay_weight = self.base_delay_weight * (1.0 + self.adaptive_gain * max(0.0, delay_ratio - 1.0))
        adaptive_energy_weight = self.base_energy_weight * (1.0 + self.adaptive_gain * max(0.0, energy_ratio - 1.0))

        base_cost = adaptive_delay_weight * delay_ratio + adaptive_energy_weight * energy_ratio

        # ðŸ”§ P0ä¿®å¤ï¼šä»…ä¿ç•™3ä¸ªæ ¸å¿ƒæƒ©ç½šé¡¹ï¼Œé¿å…ç´¯åŠ è¿‡åº¦
        # çº¦æŸé¡¹1ï¼šå®ŒæˆçŽ‡æƒ©ç½šï¼ˆé™ä½Žç³»æ•°ï¼‰
        completion_penalty = self.completion_penalty_gain * max(0.0, self.completion_target - completion_rate)
        
        # çº¦æŸé¡¹2ï¼šä¸¢åŒ…æƒ©ç½šï¼ˆé™ä½Žç³»æ•°ï¼‰
        dropped_penalty = self.dropped_penalty_gain * dropped_tasks
        
        # çº¦æŸé¡¹3ï¼šé˜ˆå€¼æƒ©ç½šï¼ˆä»…åœ¨ä¸¥é‡è¶…æ ‡æ—¶è§¦å‘ï¼‰
        delay_threshold_penalty = 0.0
        if avg_delay > self.delay_tolerance:
            delay_threshold_penalty = (avg_delay - self.delay_tolerance) / max(self.delay_target, 1e-6) * 0.5

        energy_threshold_penalty = 0.0
        if total_energy > self.energy_tolerance:
            energy_threshold_penalty = (total_energy - self.energy_tolerance) / max(self.energy_target, 1e-6) * 0.3

        # ðŸ”§ P0ä¿®å¤ï¼šç§»é™¤å¹³è¡¡æƒ©ç½šå’Œçªå¢žæƒ©ç½šï¼Œå‡å°‘æƒ©ç½šé¡¹
        # balance_penalty = 0.0  # å·²é€šè¿‡ balance_gain=0.0 ç¦ç”¨
        # spike_penalty = 0.0  # å·²é€šè¿‡ spike_penalty_gain=0.0 ç¦ç”¨

        total_cost = (
            base_cost
            + completion_penalty
            + dropped_penalty
            + delay_threshold_penalty
            + energy_threshold_penalty
        )

        # ðŸ”§ P0ä¿®å¤ï¼šä½¿ç”¨è´Ÿæˆæœ¬ä½œä¸ºå¥–åŠ±ï¼ˆæ€§èƒ½è¶Šå¥½æˆæœ¬è¶Šä½Žå¥–åŠ±è¶Šé«˜ï¼‰
        reward = -float(total_cost)
        clipped_reward = float(np.clip(reward, self.reward_clip_low, self.reward_clip_high))

        self._last_breakdown = {
            "reward": clipped_reward,
            "total_cost": total_cost,
            "base_cost": base_cost,
            "adaptive_delay_weight": adaptive_delay_weight,
            "adaptive_energy_weight": adaptive_energy_weight,
            "delay_ratio": delay_ratio,
            "energy_ratio": energy_ratio,
            "completion_penalty": completion_penalty,
            "dropped_penalty": dropped_penalty,
            "balance_penalty": 0.0,  # å·²ç¦ç”¨
            "spike_penalty": 0.0,  # å·²ç¦ç”¨
            "delay_threshold_penalty": delay_threshold_penalty,
            "energy_threshold_penalty": energy_threshold_penalty,
            "completion_rate": completion_rate,
            "avg_delay": avg_delay,
            "total_energy": total_energy,
        }

        return clipped_reward

    def get_last_breakdown(self) -> Dict[str, float]:
        """è¿”å›žä¸Šä¸€è½®è®¡ç®—çš„å¥–åŠ±åˆ†è§£ã€‚"""
        return dict(self._last_breakdown)

    def format_breakdown(self, system_metrics: Dict) -> str:
        """æ ¼å¼åŒ–å¯è¯»çš„å¥–åŠ±åˆ†è§£æŠ¥å‘Šã€‚"""
        breakdown = self.get_last_breakdown()
        if not breakdown:
            # ç¡®ä¿å…ˆè®¡ç®—
            self.calculate_reward(system_metrics)
            breakdown = self.get_last_breakdown()

        lines = [
            "TD3-LE å¥–åŠ±åˆ†è§£ (P0ä¿®å¤ç‰ˆ):",
            f"  Total Reward: {breakdown.get('reward', 0.0):.4f} (= -total_cost)",
            f"  Total Cost: {breakdown.get('total_cost', 0.0):.4f}",
            f"  Base Cost: {breakdown.get('base_cost', 0.0):.4f}",
            f"  Delay Ratio: {breakdown.get('delay_ratio', 0.0):.3f}  (Weight={breakdown.get('adaptive_delay_weight', 0.0):.2f})",
            f"  Energy Ratio: {breakdown.get('energy_ratio', 0.0):.3f} (Weight={breakdown.get('adaptive_energy_weight', 0.0):.2f})",
            f"  Completion Penalty: {breakdown.get('completion_penalty', 0.0):.3f}",
            f"  Dropped Penalty: {breakdown.get('dropped_penalty', 0.0):.3f}",
            f"  Delay Threshold Penalty: {breakdown.get('delay_threshold_penalty', 0.0):.3f}",
            f"  Energy Threshold Penalty: {breakdown.get('energy_threshold_penalty', 0.0):.3f}",
            f"  Completion Rate: {breakdown.get('completion_rate', 0.0):.3f}",
            f"  Avg Delay: {breakdown.get('avg_delay', 0.0):.3f}s (Target={0.40}s)",
            f"  Total Energy: {breakdown.get('total_energy', 0.0):.1f}J (Target={1200.0}J)",
        ]
        return "\n".join(lines)


class TD3LatencyEnergyEnvironment(TD3Environment):
    """
    TD3å»¶æ—¶-èƒ½è€—ä¼˜åŒ–çŽ¯å¢ƒã€‚

    ç»§æ‰¿è‡ªæ ‡å‡†TD3çŽ¯å¢ƒï¼Œä»…é‡å†™å¥–åŠ±è®¡ç®—é€»è¾‘ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´å¼ºåœ°é©±åŠ¨å»¶æ—¶å’Œèƒ½è€—åŒæ­¥ä¸‹é™ã€‚
    """

    def __init__(self, num_vehicles: int, num_rsus: int, num_uavs: int):
        super().__init__(num_vehicles, num_rsus, num_uavs)
        self._reward_shaper = LatencyEnergyRewardShaper()
        self.last_reward_breakdown: Dict[str, float] = {}
        print("âš¡ TD3-LE å¥–åŠ±å¡‘å½¢å™¨å·²å¯ç”¨ï¼šé‡ç‚¹æœ€å°åŒ–æ—¶å»¶ä¸Žèƒ½è€—")

    def calculate_reward(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict] = None,
        migration_metrics: Optional[Dict] = None,
    ) -> float:
        reward = self._reward_shaper.calculate_reward(system_metrics, cache_metrics, migration_metrics)
        self.last_reward_breakdown = self._reward_shaper.get_last_breakdown()
        return reward

    def get_reward_breakdown(self, system_metrics: Dict) -> str:
        return self._reward_shaper.format_breakdown(system_metrics)
