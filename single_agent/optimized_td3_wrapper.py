"""
ç²¾ç®€ä¼˜åŒ–TD3 - ä»…åŒ…å«æœ€æœ‰æ•ˆçš„ä¸¤ä¸ªä¼˜åŒ–
Queue-aware Replay + GNN Attention

ä¸“ä¸ºVECåœºæ™¯ä¼˜åŒ–ï¼š
- é˜Ÿåˆ—æ„ŸçŸ¥å›æ”¾ï¼šå¿«é€Ÿå­¦ä¹ é«˜è´Ÿè½½åœºæ™¯
- GNNæ³¨æ„åŠ›ï¼šå¤§å¹…æå‡ç¼“å­˜å‘½ä¸­ç‡ï¼ˆ0.2%â†’24%ï¼‰

ä½œè€…ï¼šVEC_mig_caching Team
"""

from typing import Optional, Dict, Union, Any
import numpy as np
from scipy.special import softmax

from .enhanced_td3_agent import EnhancedTD3Agent
from .enhanced_td3_config import EnhancedTD3Config


def create_optimized_config() -> EnhancedTD3Config:
    """åˆ›å»ºç²¾ç®€ä¼˜åŒ–é…ç½® - âœ¨ ä½¿ç”¨æœ€æ–°GATä¼˜åŒ–"""
    return EnhancedTD3Config(
        # âœ… æ ¸å¿ƒä¼˜åŒ–1ï¼šé˜Ÿåˆ—æ„ŸçŸ¥å›æ”¾
        use_queue_aware_replay=True,
        queue_priority_weight=0.2,  # ä¿æŒé™ä½çš„æƒé‡
        queue_occ_coef=0.5,
        packet_loss_coef=0.3,
        migration_cong_coef=0.2,
        queue_metrics_ema_decay=0.8,
        
        # âœ… æ ¸å¿ƒä¼˜åŒ–2ï¼šGNNæ³¨æ„åŠ›
        use_gat_router=True,
        num_attention_heads=6,  # ğŸ”§ 4 -> 6 (æ¢å¤é€‚ä¸­å¤æ‚åº¦)
        gat_hidden_dim=192,
        gat_dropout=0.15,

        # âŒ ç¦ç”¨å…¶ä»–ä¼˜åŒ–
        use_distributional_critic=False,
        use_entropy_reg=False,
        use_model_based_rollout=False,

        # ğŸ”§ åŸºç¡€å‚æ•°ä¼˜åŒ–
        hidden_dim=512,
        batch_size=768,  # ğŸ”§ 1024 -> 768 (æŠ˜ä¸­æ–¹æ¡ˆï¼šå…¼é¡¾ç¨³å®šä¸æ›´æ–°é¢‘ç‡)
        buffer_size=100000,
        warmup_steps=2000,  # ğŸ”§ æ˜¾å¼å¢åŠ é¢„çƒ­æ­¥æ•° (çº¦20 episodes)

        # ğŸ”§ å­¦ä¹ ç‡ä¼˜åŒ– (è½»å¾®å›è°ƒï¼Œé…åˆé«˜æ¢ç´¢)
        actor_lr=3e-5,    # ğŸ”§ 5e-5 -> 3e-5 (æ›´ç²¾ç»†çš„æ›´æ–°)
        critic_lr=8e-5,   # ä¿æŒ 8e-5

        # ğŸ”§ æ¢ç´¢ç­–ç•¥ä¼˜åŒ– (æ¢å¤æ ‡å‡†æ¢ç´¢ä»¥æå‡ç¨³å®šæ€§)
        exploration_noise=0.15,  # ğŸ”§ 0.30 -> 0.15 (é™ä½æ¢ç´¢å™ªå£°ï¼Œå‡å°‘éœ‡è¡)
        noise_decay=0.998,       # ğŸ”§ 0.9998 -> 0.998 (åŠ å¿«å™ªå£°è¡°å‡ï¼Œè®©åæœŸæ›´ç¨³å®š)
        min_noise=0.02,          # ğŸ”§ 0.05 -> 0.02 (é™ä½åº•å™ª)
        target_noise=0.02,       # ğŸ”§ 0.04 -> 0.02 (é™ä½ç›®æ ‡å™ªå£°)
        noise_clip=0.05,         # ğŸ”§ 0.12 -> 0.05 (æ”¶ç´§è£å‰ªèŒƒå›´)

        # å¥–åŠ±å½’ä¸€åŒ–
        reward_norm_beta=0.997,
        reward_norm_clip=5.0,
    )


class OptimizedTD3Wrapper:
    """
    ç²¾ç®€ä¼˜åŒ–TD3åŒ…è£…å™¨
    
    åªåŒ…å«æœ€æœ‰æ•ˆçš„ä¸¤ä¸ªä¼˜åŒ–ï¼š
    1. Queue-aware Replay - æå‡è®­ç»ƒæ•ˆç‡5å€
    2. GNN Attention - ç¼“å­˜å‘½ä¸­ç‡æå‡120å€
    """
    
    def __init__(
        self,
        num_vehicles: int = 12,
        num_rsus: int = 4,
        num_uavs: int = 2,
        use_central_resource: bool = True,
        simulation_only: bool = False,
    ):
        self.num_vehicles = num_vehicles
        self.num_rsus = num_rsus
        self.num_uavs = num_uavs
        self.use_central_resource = use_central_resource
        self.simulation_only = simulation_only
        
        # åˆ›å»ºä¼˜åŒ–é…ç½®
        config = create_optimized_config()
        
        # è®¡ç®—ç»´åº¦
        vehicle_state_dim = num_vehicles * 5  # è½¦è¾†ä¿æŒ5ç»´
        rsu_state_dim = num_rsus * 5  # ğŸ”§ ä¿®å¤2ï¼šRSUç»Ÿä¸€ä¸º5ç»´ï¼ˆä¸å®é™…çŠ¶æ€æ„å»ºä¸€è‡´ï¼‰
        uav_state_dim = num_uavs * 5  # ğŸ”§ ä¿®å¤2ï¼šUAVç»Ÿä¸€ä¸º5ç»´ï¼ˆä¸å®é™…çŠ¶æ€æ„å»ºä¸€è‡´ï¼‰
        global_state_dim = 8
        base_state_dim = vehicle_state_dim + rsu_state_dim + uav_state_dim + global_state_dim
        
        if use_central_resource:
            self.central_state_dim = 16
            # ğŸ”§ P0ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—state_dimï¼ŒåŠ ä¸Šcentral_state_dim
            self.state_dim = base_state_dim + self.central_state_dim
        else:
            self.central_state_dim = 0
            self.state_dim = base_state_dim
        
        self.base_action_dim = 3 + num_rsus + num_uavs + 10
        
        if use_central_resource:
            self.central_resource_action_dim = num_vehicles + num_vehicles + num_rsus + num_uavs
            self.action_dim = self.base_action_dim + self.central_resource_action_dim
        else:
            self.central_resource_action_dim = 0
            self.action_dim = self.base_action_dim
        
        # å¦‚æœåªæ˜¯ä»¿çœŸè¿›ç¨‹ï¼Œè·³è¿‡åŠ è½½æ²‰é‡çš„ç¥ç»ç½‘ç»œ
        if simulation_only:
            self.agent = None
            print("[OptimizedTD3] Simulation-only mode initialized (No Agent Loaded)")
            return

        # åˆ›å»ºä¼˜åŒ–TD3æ™ºèƒ½ä½“
        self.agent = EnhancedTD3Agent(
            state_dim=self.state_dim,
            action_dim=self.action_dim,
            config=config,
            num_vehicles=num_vehicles,
            num_rsus=num_rsus,
            num_uavs=num_uavs,
            global_dim=global_state_dim,
            central_state_dim=self.central_state_dim,
        )
        
        print("[OptimizedTD3] init done")
        print(f"  topology: vehicles={num_vehicles}, rsus={num_rsus}, uavs={num_uavs}")
        print(f"  state_dim: {self.state_dim}")
        print(f"  action_dim: {self.action_dim}")
        print("  optimizations: Queue-aware Replay + GNN Attention")
        
        # ???????/??????????????
        self._last_queue_metrics = {
            'queue_occupancy': 0.0,
            'packet_loss': 0.0,
            'migration_congestion': 0.0,
        }
        self._queue_pressure_ema: Optional[float] = None
    
    def select_action(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        if self.agent is None:
            # Simulation-only mode: return random action or zeros
            # This shouldn't be called in worker process usually, as actions come from main process
            return np.zeros(self.action_dim, dtype=np.float32)
        return self.agent.select_action(state, training=training)
    
    def store_experience(
        self,
        state: np.ndarray,
        action: np.ndarray,
        reward: float,
        next_state: np.ndarray,
        done: bool,
        queue_metrics: Optional[dict] = None,
    ):
        self.agent.store_experience(state, action, reward, next_state, done, queue_metrics)
    
    def update(self) -> dict:
        return self.agent.update()
    
    def save_model(self, filepath: str) -> str:
        return self.agent.save_model(filepath)
    
    def save_models(self, filepath: str) -> str:
        return self.save_model(filepath)
    
    def load_model(self, filepath: str):
        self.agent.load_model(filepath)
    
    def load_models(self, filepath: str):
        self.load_model(filepath)
    
    def _extract_central_state(self, resource_state: Dict):
        """ä»resource_stateæå–ä¸­å¤®èµ„æºçŠ¶æ€"""
        central_state = []
        
        try:
            # å¸¦å®½åˆ†é…ç»Ÿè®¡
            bandwidth_alloc = resource_state.get('bandwidth_allocation', [])
            if isinstance(bandwidth_alloc, (list, np.ndarray)) and len(bandwidth_alloc) > 0:
                bw_array = np.array(bandwidth_alloc, dtype=np.float32)
                bw_array = np.nan_to_num(bw_array, nan=0.0)
                central_state.extend([
                    float(np.mean(bw_array)),
                    float(np.max(bw_array)),
                    float(np.min(bw_array)),
                    float(np.std(bw_array))
                ])
            else:
                central_state.extend([1.0/self.num_vehicles] * 4)
            
            # è½¦è¾†è®¡ç®—èµ„æº
            vehicle_compute = resource_state.get('vehicle_compute_allocation', [])
            if isinstance(vehicle_compute, (list, np.ndarray)) and len(vehicle_compute) > 0:
                vc_array = np.array(vehicle_compute, dtype=np.float32)
                vc_array = np.nan_to_num(vc_array, nan=0.0)
                central_state.extend([
                    float(np.mean(vc_array)),
                    float(np.max(vc_array)),
                    float(np.min(vc_array)),
                    float(np.std(vc_array))
                ])
            else:
                central_state.extend([1.0/self.num_vehicles] * 4)
            
            # RSUè®¡ç®—èµ„æº
            rsu_compute = resource_state.get('rsu_compute_allocation', [])
            if isinstance(rsu_compute, (list, np.ndarray)) and len(rsu_compute) >= self.num_rsus:
                rc_array = np.array(rsu_compute[:self.num_rsus], dtype=np.float32)
                rc_array = np.nan_to_num(rc_array, nan=1.0/self.num_rsus)
                central_state.extend([float(v) for v in rc_array])
            else:
                central_state.extend([1.0/self.num_rsus] * self.num_rsus)
            
            # UAVè®¡ç®—èµ„æº
            uav_compute = resource_state.get('uav_compute_allocation', [])
            if isinstance(uav_compute, (list, np.ndarray)) and len(uav_compute) >= self.num_uavs:
                uc_array = np.array(uav_compute[:self.num_uavs], dtype=np.float32)
                uc_array = np.nan_to_num(uc_array, nan=1.0/self.num_uavs)
                central_state.extend([float(v) for v in uc_array])
            else:
                central_state.extend([1.0/self.num_uavs] * self.num_uavs)
            
            while len(central_state) < 16:
                central_state.append(0.0)
            
            central_state = central_state[:16]
            
        except Exception as e:
            print(f"âš ï¸ ä¸­å¤®èµ„æºçŠ¶æ€æå–å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            central_state = [
                1.0/self.num_vehicles, 1.0/self.num_vehicles, 1.0/self.num_vehicles, 0.0,
                1.0/self.num_vehicles, 1.0/self.num_vehicles, 1.0/self.num_vehicles, 0.0,
                1.0/self.num_rsus, 1.0/self.num_rsus, 1.0/self.num_rsus, 1.0/self.num_rsus,
                1.0/self.num_uavs, 1.0/self.num_uavs, 0.0, 0.0
            ]
        
        central_state = [float(v) if np.isfinite(v) else 0.0 for v in central_state]
        return central_state
    
    def get_state_vector(
        self,
        node_states: Dict,
        system_metrics: Dict,
        resource_state: Optional[Dict] = None,
    ) -> np.ndarray:
        """æ„å»ºçŠ¶æ€å‘é‡"""
        state_components = []
        
        # èŠ‚ç‚¹çŠ¶æ€
        for i in range(self.num_vehicles):
            vehicle_key = f'vehicle_{i}'
            if vehicle_key in node_states:
                vehicle_state = node_states[vehicle_key][:5]  # è½¦è¾†ä¿æŒ5ç»´
                valid_state = [float(v) if np.isfinite(v) else 0.5 for v in vehicle_state]
                state_components.extend(valid_state)
            else:
                state_components.extend([0.5, 0.5, 0.0, 0.0, 0.0])
        
        for i in range(self.num_rsus):
            rsu_key = f'rsu_{i}'
            if rsu_key in node_states:
                rsu_state = node_states[rsu_key][:5]  # ğŸ”§ ä¿®å¤2ï¼šRSUç»Ÿä¸€ä¸º5ç»´
                valid_state = [float(v) if np.isfinite(v) else 0.5 for v in rsu_state]
                state_components.extend(valid_state)
            else:
                state_components.extend([0.5, 0.5, 0.0, 0.0, 0.0])  # é»˜è®¤5ç»´
        
        for i in range(self.num_uavs):
            uav_key = f'uav_{i}'
            if uav_key in node_states:
                uav_state = node_states[uav_key][:5]  # ğŸ”§ ä¿®å¤2ï¼šUAVç»Ÿä¸€ä¸º5ç»´
                valid_state = [float(v) if np.isfinite(v) else 0.5 for v in uav_state]
                state_components.extend(valid_state)
            else:
                state_components.extend([0.5, 0.5, 0.5, 0.0, 0.0])  # é»˜è®¤5ç»´ï¼ˆé«˜åº¦ç»´åº¦å·²åŒ…å«ï¼‰
        
        # å…¨å±€çŠ¶æ€
        # ğŸ”§ P0ä¿®å¤ï¼šå½’ä¸€åŒ–å› å­å¿…é¡»ä¸UnifiedRewardCalculatorä¸¥æ ¼å¯¹é½
        # ä»é…ç½®è¯»å–ç›®æ ‡å€¼ï¼Œç¡®ä¿çŠ¶æ€å½’ä¸€åŒ–ä¸å¥–åŠ±è®¡ç®—ä½¿ç”¨ç›¸åŒåŸºå‡†
        from config import config
        latency_target = float(getattr(config.rl, 'latency_target', 0.4))  # ä¸config.rlé»˜è®¤å€¼å¯¹é½
        energy_target = float(getattr(config.rl, 'energy_target', 3500.0))  # ä¸config.rlé»˜è®¤å€¼å¯¹é½
        
        global_state = [
            float(system_metrics.get('avg_task_delay', 0.0) / max(latency_target, 1e-6)),
            float(system_metrics.get('total_energy_consumption', 0.0) / max(energy_target, 1e-6)),
            float(system_metrics.get('task_completion_rate', 0.95)),
            float(system_metrics.get('cache_hit_rate', 0.85)),
            float(system_metrics.get('queue_overload_flag', 0.0)),
            float(system_metrics.get('rsu_offload_ratio', 0.5)),
            float(system_metrics.get('uav_offload_ratio', 0.2)),
            float(system_metrics.get('local_offload_ratio', 0.3)),
        ]
        global_state = [float(v) if np.isfinite(v) else 0.0 for v in global_state]
        state_components.extend(global_state)
        
        # ä¸­å¤®èµ„æºçŠ¶æ€
        if self.central_state_dim > 0 and resource_state is not None:
            central_state_vector = self._extract_central_state(resource_state)
            state_components.extend(central_state_vector)
        
        state_vector = np.array(state_components, dtype=np.float32)
        
        if np.any(np.isnan(state_vector)) or np.any(np.isinf(state_vector)):
            state_vector = np.nan_to_num(state_vector, nan=0.5, posinf=1.0, neginf=0.0)
        
        if state_vector.size < self.state_dim:
            padding_needed = self.state_dim - state_vector.size
            state_vector = np.pad(state_vector, (0, padding_needed), mode='constant', constant_values=0.5)
        elif state_vector.size > self.state_dim:
            state_vector = state_vector[:self.state_dim]
            
        return state_vector
    
    def calculate_reward(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict] = None,
        migration_metrics: Optional[Dict] = None
    ) -> tuple[float, Dict[str, float]]:
        """
        è®¡ç®—å¥–åŠ±å¹¶è¿”å›ç»„ä»¶å­—å…¸
        
        ä½¿ç”¨ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ç¡®ä¿ä¸€è‡´æ€§ã€‚
        å½’ä¸€åŒ–åŸºå‡†è‡ªåŠ¨ä¸çŠ¶æ€å½’ä¸€åŒ–å¯¹é½ï¼ˆé€šè¿‡config.rl.latency_targetå’Œenergy_targetï¼‰ã€‚
        
        Returns:
            tuple: (reward, reward_components)
        """
        from utils.unified_reward_calculator import _general_reward_calculator
        return _general_reward_calculator.calculate_reward(system_metrics, cache_metrics, migration_metrics)
    
    def get_actions(self, state: np.ndarray, training: bool = True) -> Dict:
        """è·å–åŠ¨ä½œ"""
        global_action = self.agent.select_action(state, training)
        actions = self.decompose_action(global_action)
        return actions
    
    def decompose_action(self, action: np.ndarray) -> Dict:
        """åˆ†è§£åŠ¨ä½œ"""
        actions = {}
        idx = 0
        
        # 1. åŸºç¡€åŠ¨ä½œ (Offload + RSU/UAV Selection + Control Params)
        base_segment = action[:self.base_action_dim]
        
        offload_preference = base_segment[:3]
        idx = 3
        
        rsu_selection = base_segment[idx:idx + self.num_rsus]
        idx += self.num_rsus
        
        uav_selection = base_segment[idx:idx + self.num_uavs]
        idx += self.num_uavs
        
        control_params = base_segment[idx:idx + 10]
        
        actions['vehicle_agent'] = action.copy() # ä¿ç•™åŸå§‹å®Œæ•´åŠ¨ä½œä¾›å‚è€ƒ
        actions['offload_preference'] = {
            'local': float(offload_preference[0]),
            'rsu': float(offload_preference[1]),
            'uav': float(offload_preference[2])
        }
        actions['rsu_agent'] = rsu_selection
        actions['uav_agent'] = uav_selection
        actions['control_params'] = control_params
        
        # 2. ğŸ”§ ä¿®å¤ï¼šæå–ä¸­å¤®èµ„æºåŠ¨ä½œ (Central Resource Allocation)
        if self.use_central_resource:
            # actionçš„ååŠéƒ¨åˆ†æ˜¯ä¸­å¤®èµ„æºåŠ¨ä½œ
            central_segment = action[self.base_action_dim:]
            
            # ç¡®ä¿é•¿åº¦åŒ¹é…
            expected_len = self.central_resource_action_dim
            if len(central_segment) >= expected_len:
                c_idx = 0
                
                # è½¦è¾†å¸¦å®½åˆ†é…æƒé‡ (num_vehicles)
                bw_alloc = central_segment[c_idx:c_idx + self.num_vehicles]
                c_idx += self.num_vehicles
                
                # è½¦è¾†è®¡ç®—èµ„æºåˆ†é…æƒé‡ (num_vehicles)
                comp_alloc = central_segment[c_idx:c_idx + self.num_vehicles]
                c_idx += self.num_vehicles
                
                # RSUè®¡ç®—èµ„æºé¢„ç•™ (num_rsus)
                rsu_alloc = central_segment[c_idx:c_idx + self.num_rsus]
                c_idx += self.num_rsus
                
                # UAVè®¡ç®—èµ„æºé¢„ç•™ (num_uavs)
                uav_alloc = central_segment[c_idx:c_idx + self.num_uavs]
                
                actions['central_resource'] = {
                    'bandwidth_weights': softmax(bw_alloc),
                    'compute_weights': softmax(comp_alloc),
                    'rsu_reservation': softmax(rsu_alloc),
                    'uav_reservation': softmax(uav_alloc)
                }
            else:
                # ç»´åº¦ä¸åŒ¹é…æ—¶çš„å›é€€
                print(f"âš ï¸ åŠ¨ä½œç»´åº¦è­¦å‘Š: Central segment len {len(central_segment)} < expected {expected_len}")
                actions['central_resource'] = None
        
        return actions

    # ================== è®­ç»ƒæ¥å£ & é˜Ÿåˆ—ä¿¡å· ================== #
    def update_queue_metrics(self, step_stats: Dict[str, Any]) -> None:
        """ä»stepç»Ÿè®¡ä¸­æå–é˜Ÿåˆ—/ä¸¢åŒ…ä¿¡å·ï¼Œé©±åŠ¨Queue-aware Replayã€‚"""
        try:
            # ğŸ”§ P1ä¿®å¤ï¼šæ”¹è¿›é˜Ÿåˆ—æŒ‡æ ‡æå–ï¼Œåˆ†èŠ‚ç‚¹ç±»å‹æå–
            # 1. è½¦è¾†çº§åˆ«é˜Ÿåˆ—å‹åŠ›
            vehicle_queue_pressure = []
            queue_rho_by_node = step_stats.get('queue_rho_by_node', {})
            if isinstance(queue_rho_by_node, dict):
                for node_key, rho_value in queue_rho_by_node.items():
                    if node_key.startswith('vehicle_'):
                        try:
                            vehicle_queue_pressure.append(float(rho_value))
                        except (TypeError, ValueError):
                            pass
            
            # 2. ç»¼åˆé˜Ÿåˆ—å‹åŠ›æŒ‡æ ‡
            queue_rho_max = float(step_stats.get('queue_rho_max', 0.0) or 0.0)
            queue_overload_flag = 1.0 if step_stats.get('queue_overload_flag', False) else 0.0
            
            # 3. è®¡ç®—å¹³å‡è½¦è¾†é˜Ÿåˆ—å‹åŠ›
            avg_vehicle_pressure = float(np.mean(vehicle_queue_pressure)) if vehicle_queue_pressure else 0.0
            
            # 4. ç»¼åˆé˜Ÿåˆ—å‹åŠ›ï¼šæœ€å¤§å€¼ + è½¦è¾†å¹³å‡ + è¿‡è½½æ ‡å¿—
            queue_occ = float(max(
                queue_rho_max,
                avg_vehicle_pressure,
                queue_overload_flag
            ))
            
            # 5. ä¸¢åŒ…ç‡æŒ‡æ ‡
            packet_loss = float(
                step_stats.get('data_loss_ratio_bytes', step_stats.get('packet_loss', 0.0)) or 0.0
            )
            
            # 6. è¿ç§»æ‹¥å¡æŒ‡æ ‡
            migration_cong = float(
                max(
                    step_stats.get('cache_eviction_rate', 0.0) or 0.0,
                    step_stats.get('migration_queue_pressure', 0.0) or 0.0,
                )
            )
        except Exception:
            queue_occ, packet_loss, migration_cong = 0.0, 0.0, 0.0
        
        queue_occ = float(np.clip(queue_occ, 0.0, 1.0))
        packet_loss = float(np.clip(packet_loss, 0.0, 1.0))
        migration_cong = float(np.clip(migration_cong, 0.0, 1.0))
        
        # å¹³æ»‘é˜Ÿåˆ—å‹åŠ›ï¼Œé¿å…æŠ–åŠ¨
        if self._queue_pressure_ema is None:
            self._queue_pressure_ema = queue_occ
        else:
            self._queue_pressure_ema = 0.8 * self._queue_pressure_ema + 0.2 * queue_occ
        queue_occ = float(np.clip(self._queue_pressure_ema, 0.0, 1.0))
        
        self._last_queue_metrics = {
            'queue_occupancy': queue_occ,
            'packet_loss': packet_loss,
            'migration_congestion': migration_cong,
        }

    def update_priority_signal(self, queue_pressure: Union[float, int]) -> None:
        """å…¼å®¹ä¸Šå±‚çš„é˜Ÿåˆ—å‹åŠ›æ¥å£ï¼Œç›´æ¥è½¬æˆé˜Ÿåˆ—å ç”¨ç‡ä¿¡å·ã€‚"""
        try:
            qp = float(queue_pressure)
        except Exception:
            qp = 0.0
        qp = float(np.clip(qp, 0.0, 1.0))
        self.update_queue_metrics({'queue_rho_max': qp})

    def train_step(
        self,
        state: np.ndarray,
        action: Union[np.ndarray, float, int],
        reward: float,
        next_state: np.ndarray,
        done: bool,
    ) -> Dict[str, Any]:
        """å•æ­¥è®­ç»ƒï¼šå†™å…¥ç»éªŒ + æ›´æ–°ç½‘ç»œ"""
        action_arr = np.asarray(action, dtype=np.float32)
        if action_arr.ndim > 1:
            action_arr = action_arr.flatten()
        
        # ä½¿ç”¨æœ€æ–°çš„é˜Ÿåˆ—æŒ‡æ ‡é©±åŠ¨ä¼˜å…ˆçº§é‡‡æ ·
        self.store_experience(state, action_arr, reward, next_state, done, self._last_queue_metrics)
        training_info = self.update()
        return training_info


# åˆ«å
OptimizedTD3Environment = OptimizedTD3Wrapper
