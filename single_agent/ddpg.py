"""
DDPG (Deep Deterministic Policy Gradient) å•æ™ºèƒ½ä½“ç®—æ³•å®ç°
ä¸“é—¨é€‚é…MATD3-MIGç³»ç»Ÿçš„VECç¯å¢ƒ

ä¸»è¦ç‰¹ç‚¹:
1. Actor-Criticæ¶æ„å¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´
2. ç»éªŒå›æ”¾æœºåˆ¶æé«˜æ ·æœ¬æ•ˆç‡
3. ç›®æ ‡ç½‘ç»œç¨³å®šè®­ç»ƒè¿‡ç¨‹
4. å™ªå£°æ¢ç´¢ç­–ç•¥

å¯¹åº”è®ºæ–‡: Continuous Control with Deep Reinforcement Learning
"""
# æ€§èƒ½ä¼˜åŒ– - å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰
try:
    from tools.performance_optimization import OPTIMIZED_BATCH_SIZES
except ImportError:
    OPTIMIZED_BATCH_SIZES = {'DDPG': 128}  # é»˜è®¤å€¼

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
from collections import deque
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass

from config import config


@dataclass
class DDPGConfig:
    """DDPGç®—æ³•é…ç½® - ä¼˜åŒ–æ”¶æ•›æ€§ï¼ˆæ ¹æ®è¯Šæ–­ç»“æœè°ƒæ•´ï¼‰"""
    # ç½‘ç»œç»“æ„ - å¢åŠ å®¹é‡æé«˜è¡¨ç°åŠ›
    hidden_dim: int = 256      # å¢åŠ ç½‘ç»œå®¹é‡ï¼ˆä»128åˆ°256ï¼‰
    actor_lr: float = 1e-4     # é™ä½actorå­¦ä¹ ç‡æé«˜ç¨³å®šæ€§
    critic_lr: float = 3e-4    # æé«˜criticå­¦ä¹ ç‡åŠ å¿«å­¦ä¹ 
    
    # è®­ç»ƒå‚æ•° - ä¼˜åŒ–æ”¶æ•›æ€§
    batch_size: int = 128      # å¢åŠ æ‰¹æ¬¡å¤§å°ï¼ˆä»64åˆ°128ï¼‰
    buffer_size: int = 100000  # å¢åŠ ç¼“å†²åŒºå¤§å°
    tau: float = 0.005         # å‡å°è½¯æ›´æ–°ç³»æ•°ï¼ˆä»0.01åˆ°0.005ï¼‰
    gamma: float = 0.99        # æé«˜æŠ˜æ‰£å› å­ï¼ˆä»0.95åˆ°0.99ï¼‰
    
    # æ¢ç´¢å‚æ•° - åŠ å¼ºæ¢ç´¢
    noise_scale: float = 0.3   # å¢åŠ åˆå§‹æ¢ç´¢ï¼ˆä»0.2åˆ°0.3ï¼‰
    noise_decay: float = 0.9999 # æ›´æ…¢çš„å™ªå£°è¡°å‡ï¼ˆä»0.995åˆ°0.9999ï¼‰
    min_noise: float = 0.1     # æé«˜æœ€å°æ¢ç´¢ï¼ˆä»0.05åˆ°0.1ï¼‰
    
    # è®­ç»ƒé¢‘ç‡
    update_freq: int = 1
    warmup_steps: int = 1000   # å¢åŠ é¢„çƒ­æ­¥æ•°ï¼ˆä»500åˆ°1000ï¼‰


class DDPGActor(nn.Module):
    """DDPG Actorç½‘ç»œ - ç¡®å®šæ€§ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256, max_action: float = 1.0):
        super(DDPGActor, self).__init__()
        
        self.max_action = max_action
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.0)
        
        # æœ€åä¸€å±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
        nn.init.uniform_(self.network[-2].weight, -3e-3, 3e-3)
        nn.init.uniform_(self.network[-2].bias, -3e-3, 3e-3)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        return self.max_action * self.network(state)


class DDPGCritic(nn.Module):
    """DDPG Criticç½‘ç»œ - Qå‡½æ•°ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(DDPGCritic, self).__init__()
        
        # çŠ¶æ€ç¼–ç å™¨
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU()
        )
        
        # çŠ¶æ€-åŠ¨ä½œèåˆç½‘ç»œ
        self.fusion_network = nn.Sequential(
            nn.Linear(hidden_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in [self.state_encoder, self.fusion_network]:
            for layer in module:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    nn.init.constant_(layer.bias, 0.0)
        
        # æœ€åä¸€å±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
        nn.init.uniform_(self.fusion_network[-1].weight, -3e-3, 3e-3)
        nn.init.uniform_(self.fusion_network[-1].bias, -3e-3, 3e-3)
    
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        state_features = self.state_encoder(state)
        fusion_input = torch.cat([state_features, action], dim=1)
        return self.fusion_network(fusion_input)


class DDPGReplayBuffer:
    """DDPGç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int, state_dim: int, action_dim: int):
        self.capacity = capacity
        self.ptr = 0
        self.size = 0
        
        # é¢„åˆ†é…å†…å­˜
        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)
        self.rewards = np.zeros(capacity, dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros(capacity, dtype=np.float32)
    
    def push(self, state: np.ndarray, action: np.ndarray, reward: float, 
             next_state: np.ndarray, done: bool):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.next_states[self.ptr] = next_state
        self.dones[self.ptr] = float(done)
        
        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    
    def sample(self, batch_size: int) -> Tuple[torch.Tensor, ...]:
        """é‡‡æ ·ç»éªŒæ‰¹æ¬¡"""
        indices = np.random.choice(self.size, batch_size, replace=False)
        
        batch_states = torch.FloatTensor(self.states[indices])
        batch_actions = torch.FloatTensor(self.actions[indices])
        batch_rewards = torch.FloatTensor(self.rewards[indices]).unsqueeze(1)
        batch_next_states = torch.FloatTensor(self.next_states[indices])
        batch_dones = torch.FloatTensor(self.dones[indices]).unsqueeze(1)
        
        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones
    
    def __len__(self):
        return self.size


class DDPGAgent:
    """DDPGæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, config: DDPGConfig):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config
        
        # æ€§èƒ½ä¼˜åŒ– - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        self.optimized_batch_size = OPTIMIZED_BATCH_SIZES.get('DDPG', config.batch_size)
        self.config.batch_size = self.optimized_batch_size
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºç½‘ç»œ
        self.actor = DDPGActor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.critic = DDPGCritic(state_dim, action_dim, config.hidden_dim).to(self.device)
        
        # ç›®æ ‡ç½‘ç»œ
        self.target_actor = DDPGActor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.target_critic = DDPGCritic(state_dim, action_dim, config.hidden_dim).to(self.device)
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
        self.hard_update(self.target_actor, self.actor)
        self.hard_update(self.target_critic, self.critic)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = DDPGReplayBuffer(config.buffer_size, state_dim, action_dim)
        
        # æ¢ç´¢å™ªå£°
        self.noise_scale = config.noise_scale
        self.step_count = 0
        
        # è®­ç»ƒç»Ÿè®¡
        self.actor_losses = []
        self.critic_losses = []
    
    def select_action(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.actor(state_tensor).cpu().numpy()[0]
        
        # æ·»åŠ æ¢ç´¢å™ªå£°
        if training:
            noise = np.random.normal(0, self.noise_scale, size=action.shape)
            action = np.clip(action + noise, -1.0, 1.0)
        
        return action
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, reward: float,
                        next_state: np.ndarray, done: bool):
        """å­˜å‚¨ç»éªŒ"""
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def update(self) -> Dict[str, float]:
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        if len(self.replay_buffer) < self.config.batch_size:
            return {}
        
        self.step_count += 1
        
        # é¢„çƒ­æœŸä¸æ›´æ–°
        if self.step_count < self.config.warmup_steps:
            return {}
        
        # é‡‡æ ·ç»éªŒæ‰¹æ¬¡
        batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = \
            self.replay_buffer.sample(self.config.batch_size)
        
        batch_states = batch_states.to(self.device)
        batch_actions = batch_actions.to(self.device)
        batch_rewards = batch_rewards.to(self.device)
        batch_next_states = batch_next_states.to(self.device)
        batch_dones = batch_dones.to(self.device)
        
        # æ›´æ–°Critic
        critic_loss = self._update_critic(batch_states, batch_actions, batch_rewards, 
                                        batch_next_states, batch_dones)
        
        # æ›´æ–°Actor
        actor_loss = self._update_actor(batch_states)
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.soft_update(self.target_actor, self.actor, self.config.tau)
        self.soft_update(self.target_critic, self.critic, self.config.tau)
        
        # è¡°å‡å™ªå£°
        self.noise_scale = max(self.config.min_noise, 
                              self.noise_scale * self.config.noise_decay)
        
        return {
            'actor_loss': actor_loss,
            'critic_loss': critic_loss,
            'noise_scale': self.noise_scale
        }
    
    def _update_critic(self, states: torch.Tensor, actions: torch.Tensor, 
                      rewards: torch.Tensor, next_states: torch.Tensor, 
                      dones: torch.Tensor) -> float:
        """æ›´æ–°Criticç½‘ç»œ"""
        with torch.no_grad():
            next_actions = self.target_actor(next_states)
            target_q = self.target_critic(next_states, next_actions)
            target_q = rewards + (1 - dones) * self.config.gamma * target_q
        
        current_q = self.critic(states, actions)
        critic_loss = F.mse_loss(current_q, target_q)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)
        self.critic_optimizer.step()
        
        self.critic_losses.append(critic_loss.item())
        return critic_loss.item()
    
    def _update_actor(self, states: torch.Tensor) -> float:
        """æ›´æ–°Actorç½‘ç»œ"""
        actions = self.actor(states)
        actor_loss = -self.critic(states, actions).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
        self.actor_optimizer.step()
        
        self.actor_losses.append(actor_loss.item())
        return actor_loss.item()
    
    def soft_update(self, target: nn.Module, source: nn.Module, tau: float):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
    
    def hard_update(self, target: nn.Module, source: nn.Module):
        """ç¡¬æ›´æ–°ç½‘ç»œå‚æ•°"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(param.data)
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'target_actor_state_dict': self.target_actor.state_dict(),
            'target_critic_state_dict': self.target_critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'noise_scale': self.noise_scale,
            'step_count': self.step_count
        }, f"{filepath}_ddpg.pth")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(f"{filepath}_ddpg.pth", map_location=self.device)
        
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.target_actor.load_state_dict(checkpoint['target_actor_state_dict'])
        self.target_critic.load_state_dict(checkpoint['target_critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.noise_scale = checkpoint['noise_scale']
        self.step_count = checkpoint['step_count']


class DDPGEnvironment:
    """DDPGè®­ç»ƒç¯å¢ƒ"""
    
    def __init__(self):
        self.config = DDPGConfig()
        
        # ç¯å¢ƒé…ç½® - æ•´åˆVECç³»ç»ŸçŠ¶æ€
        self.state_dim = 60  # æ•´åˆæ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€
        self.action_dim = 30  # æ•´åˆæ‰€æœ‰èŠ‚ç‚¹åŠ¨ä½œ
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agent = DDPGAgent(self.state_dim, self.action_dim, self.config)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_count = 0
        self.step_count = 0
        
        print(f"âœ“ DDPGç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ çŠ¶æ€ç»´åº¦: {self.state_dim}")
        print(f"âœ“ åŠ¨ä½œç»´åº¦: {self.action_dim}")
    
    def get_state_vector(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """æ„å»ºå…¨å±€çŠ¶æ€å‘é‡ - ä¿®å¤çŠ¶æ€è¡¨ç¤ºé—®é¢˜"""
        state_components = []
        
        # 1. åŸºç¡€ç³»ç»ŸçŠ¶æ€ (8ç»´) - å¢åŠ æ›´å¤šåŠ¨æ€ç‰¹å¾
        base_state = [
            system_metrics.get('avg_task_delay', 0.0) / 1.0,
            system_metrics.get('total_energy_consumption', 0.0) / 1000.0,
            system_metrics.get('data_loss_rate', 0.0),
            system_metrics.get('cache_hit_rate', 0.0),
            system_metrics.get('migration_success_rate', 0.0),
            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ å˜åŒ–æ€§æ›´å¼ºçš„ç³»ç»Ÿç‰¹å¾
            system_metrics.get('task_completion_rate', 0.0),  # ä»»åŠ¡å®Œæˆç‡
            min(1.0, system_metrics.get('avg_task_delay', 0.15) / 0.5),  # å»¶è¿Ÿè´Ÿè½½æŒ‡æ ‡
            min(1.0, system_metrics.get('total_energy_consumption', 600.0) / 1500.0),  # èƒ½è€—è´Ÿè½½æŒ‡æ ‡
        ]
        state_components.extend(base_state)
        
        # 2. è½¦è¾†çŠ¶æ€ (12è½¦è¾† Ã— 4ç»´ = 48ç»´) - ä½¿ç”¨çœŸå®çŠ¶æ€è€Œééšæœºæ•°
        vehicle_count = 0
        for i in range(12):  # æ”¯æŒæœ€å¤š12ä¸ªè½¦è¾†
            vehicle_key = f'vehicle_{i}'
            if vehicle_key in node_states:
                vehicle_state = node_states[vehicle_key]
                # æå–è½¦è¾†çš„å…³é”®çŠ¶æ€ç‰¹å¾
                if len(vehicle_state) >= 5:
                    vehicle_features = [
                        float(vehicle_state[0]),  # ä½ç½®x (å·²å½’ä¸€åŒ–)
                        float(vehicle_state[1]),  # ä½ç½®y (å·²å½’ä¸€åŒ–)  
                        float(vehicle_state[2]),  # é€Ÿåº¦ (å·²å½’ä¸€åŒ–)
                        float(vehicle_state[3]),  # ä»»åŠ¡æ•° (å·²å½’ä¸€åŒ–)
                    ]
                else:
                    # å¦‚æœçŠ¶æ€ç»´åº¦ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    vehicle_features = [0.5, 0.5, 0.5, 0.0]
                vehicle_count += 1
            else:
                # è½¦è¾†ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤çŠ¶æ€
                vehicle_features = [0.0, 0.0, 0.0, 0.0]
            
            state_components.extend(vehicle_features)
        
        # 3. RSUçŠ¶æ€ (6ä¸ªRSU Ã— 3ç»´ = 18ç»´)  
        for i in range(6):  # æ”¯æŒæœ€å¤š6ä¸ªRSU
            rsu_key = f'rsu_{i}'
            if rsu_key in node_states:
                rsu_state = node_states[rsu_key]
                if len(rsu_state) >= 5:
                    rsu_features = [
                        float(rsu_state[2]),  # ç¼“å­˜åˆ©ç”¨ç‡
                        float(rsu_state[3]),  # é˜Ÿåˆ—é•¿åº¦ (å·²å½’ä¸€åŒ–)
                        float(rsu_state[4]),  # èƒ½è€— (å·²å½’ä¸€åŒ–)
                    ]
                else:
                    rsu_features = [0.5, 0.5, 0.5]
            else:
                rsu_features = [0.0, 0.0, 0.0]
            
            state_components.extend(rsu_features)
        
        # 4. UAVçŠ¶æ€ (2ä¸ªUAV Ã— 4ç»´ = 8ç»´)
        for i in range(2):  # æ”¯æŒæœ€å¤š2ä¸ªUAV
            uav_key = f'uav_{i}'
            if uav_key in node_states:
                uav_state = node_states[uav_key]
                if len(uav_state) >= 5:
                    uav_features = [
                        float(uav_state[2]),  # é«˜åº¦ (å·²å½’ä¸€åŒ–)
                        float(uav_state[3]),  # ç¼“å­˜åˆ©ç”¨ç‡
                        float(uav_state[4]),  # èƒ½è€— (å·²å½’ä¸€åŒ–)
                        1.0,  # ç”µæ± ç”µé‡ (ç®€åŒ–ä¸ºå›ºå®šå€¼)
                    ]
                else:
                    uav_features = [0.8, 0.5, 0.5, 1.0]
            else:
                uav_features = [0.0, 0.0, 0.0, 0.5]
            
            state_components.extend(uav_features)
        
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿çŠ¶æ€å‘é‡é•¿åº¦ä¸º60ç»´ï¼Œç”¨æœ‰æ„ä¹‰çš„ç‰¹å¾å¡«å……
        current_length = len(state_components)
        if current_length < self.state_dim:
            padding_size = self.state_dim - current_length
            
            # æ·»åŠ æœ‰æ„ä¹‰çš„æ´¾ç”Ÿç‰¹å¾è€Œéå‘¨æœŸæ€§å¡«å……
            for i in range(padding_size):
                if i < 4:  # ç³»ç»Ÿè´Ÿè½½åˆ†å¸ƒç‰¹å¾
                    load_factor = system_metrics.get('total_energy_consumption', 600.0) / 1000.0
                    feature_val = 0.3 + 0.4 * np.sin(load_factor * np.pi + i)
                elif i < 8:  # å»¶è¿Ÿåˆ†å¸ƒç‰¹å¾
                    delay_factor = system_metrics.get('avg_task_delay', 0.15)
                    feature_val = 0.4 + 0.3 * np.cos(delay_factor * 10 + i)
                elif i < 12:  # å®Œæˆç‡ç›¸å…³ç‰¹å¾
                    completion_factor = system_metrics.get('task_completion_rate', 0.9)
                    feature_val = completion_factor * (0.5 + 0.3 * np.sin(i * 0.5))
                else:  # ç¼“å­˜æ•ˆç‡ç‰¹å¾
                    cache_factor = system_metrics.get('cache_hit_rate', 0.3)
                    feature_val = cache_factor * (0.6 + 0.2 * np.cos(i * 0.7))
                
                # ç¡®ä¿ç‰¹å¾å€¼åœ¨åˆç†èŒƒå›´å†…
                feature_val = np.clip(feature_val, 0.0, 1.0)
                state_components.append(float(feature_val))
        elif current_length > self.state_dim:
            # å¦‚æœç»´åº¦è¿‡å¤šï¼Œæˆªæ–­
            state_components = state_components[:self.state_dim]
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è¿›è¡Œæ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        state_vector = np.array(state_components, dtype=np.float32)
        
        # æ£€æŸ¥å¹¶å¤„ç†NaN/Infå€¼
        if np.any(np.isnan(state_vector)) or np.any(np.isinf(state_vector)):
            print(f"âš ï¸ è­¦å‘Š: çŠ¶æ€å‘é‡åŒ…å«æ— æ•ˆå€¼ï¼Œè¿›è¡Œä¿®å¤")
            state_vector = np.nan_to_num(state_vector, nan=0.5, posinf=1.0, neginf=0.0)
        
        # ç¡®ä¿çŠ¶æ€å€¼åœ¨åˆç†èŒƒå›´å†…
        state_vector = np.clip(state_vector, -5.0, 5.0)
        
        return state_vector
    
    def decompose_action(self, action: np.ndarray) -> Dict[str, np.ndarray]:
        """å°†å…¨å±€åŠ¨ä½œåˆ†è§£ä¸ºå„èŠ‚ç‚¹åŠ¨ä½œ"""
        actions = {}
        start_idx = 0
        
        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“ç±»å‹åˆ†é…åŠ¨ä½œ
        for agent_type in ['vehicle_agent', 'rsu_agent', 'uav_agent']:
            end_idx = start_idx + 10  # æ¯ä¸ªæ™ºèƒ½ä½“10ä¸ªåŠ¨ä½œç»´åº¦
            actions[agent_type] = action[start_idx:end_idx]
            start_idx = end_idx
        
        return actions
    
    def get_actions(self, state: np.ndarray, training: bool = True) -> Dict[str, np.ndarray]:
        """è·å–åŠ¨ä½œ"""
        global_action = self.agent.select_action(state, training)
        return self.decompose_action(global_action)
    
    def calculate_reward(self, system_metrics: Dict) -> float:
        """è®¡ç®—å¥–åŠ± - ä¿®å¤ç‰ˆæœ¬ï¼Œè§£å†³ç›¸å…³æ€§å’Œå•è°ƒæ€§é—®é¢˜"""
        # æå–æŒ‡æ ‡å¹¶è¿›è¡Œæ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        delay = float(system_metrics.get('avg_task_delay', 0.15))
        energy = float(system_metrics.get('total_energy_consumption', 600.0)) / 1000.0  # å½’ä¸€åŒ–
        loss_rate = float(system_metrics.get('data_loss_rate', 0.05))
        completion_rate = float(system_metrics.get('task_completion_rate', 0.9))
        cache_hit_rate = float(system_metrics.get('cache_hit_rate', 0.3))
        
        # æ•°å€¼å®‰å…¨æ£€æŸ¥å’Œçº¦æŸ
        delay = np.clip(delay, 0.01, 2.0) if np.isfinite(delay) else 0.15
        energy = np.clip(energy, 0.1, 3.0) if np.isfinite(energy) else 0.6
        loss_rate = np.clip(loss_rate, 0.0, 1.0) if np.isfinite(loss_rate) else 0.05
        completion_rate = np.clip(completion_rate, 0.0, 1.0) if np.isfinite(completion_rate) else 0.9
        cache_hit_rate = np.clip(cache_hit_rate, 0.0, 1.0) if np.isfinite(cache_hit_rate) else 0.3
        
        # ğŸ”§ ä¿®å¤ï¼šå¼ºåŒ–å¥–åŠ±å‡½æ•°ï¼Œç¡®ä¿å¼ºç›¸å…³æ€§å’Œå•è°ƒæ€§
        # 1. å¼ºåŒ–æƒ©ç½šé¡¹ - ç¡®ä¿ä¸ä¼˜åŒ–ç›®æ ‡å¼ºè´Ÿç›¸å…³
        delay_penalty = -15.0 * delay        # å¼ºåŒ–å»¶è¿Ÿæƒ©ç½šï¼Œç¡®ä¿è´Ÿç›¸å…³
        energy_penalty = -8.0 * energy       # å¼ºåŒ–èƒ½è€—æƒ©ç½š
        loss_penalty = -25.0 * loss_rate     # å¼ºåŒ–ä¸¢å¤±ç‡æƒ©ç½š
        
        # 2. å¼ºåŒ–å¥–åŠ±é¡¹ - ç¡®ä¿ä¸æ€§èƒ½æŒ‡æ ‡å¼ºæ­£ç›¸å…³
        completion_reward = 20.0 * completion_rate  # å¼ºåŒ–å®Œæˆç‡å¥–åŠ±ï¼Œè§£å†³ç›¸å…³æ€§é—®é¢˜
        cache_reward = 10.0 * cache_hit_rate        # å¼ºåŒ–ç¼“å­˜å‘½ä¸­ç‡å¥–åŠ±
        
        # 3. çº¿æ€§ç»„åˆç¡®ä¿å•è°ƒæ€§ï¼ˆå»é™¤éçº¿æ€§å‡½æ•°é¿å…éå•è°ƒæ€§ï¼‰
        base_reward = delay_penalty + energy_penalty + loss_penalty + completion_reward + cache_reward
        
        # 4. å¤§å¹…æ”¾å¤§ä¿¡å·å¼ºåº¦ï¼ˆè§£å†³ä¿¡å·è¿‡å¼±é—®é¢˜ï¼‰
        amplified_reward = base_reward * 3.0  # 3å€æ”¾å¤§ï¼Œå¢å¼ºå­¦ä¹ ä¿¡å·
        
        # 5. é€‚å½“çš„å¥–åŠ±èŒƒå›´ï¼ˆä¿æŒä¿¡å·å¼ºåº¦çš„åŒæ—¶é¿å…æ•°å€¼é—®é¢˜ï¼‰
        final_reward = np.clip(amplified_reward, -80.0, 50.0)
        
        return float(final_reward)
    
    def train_step(self, state: np.ndarray, action: Union[np.ndarray, int], reward: float,
                   next_state: np.ndarray, done: bool) -> Dict:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        # DDPGéœ€è¦numpyæ•°ç»„ï¼Œå¦‚æœæ˜¯æ•´æ•°åˆ™è½¬æ¢
        if isinstance(action, int):
            action = np.array([action], dtype=np.float32)
        elif not isinstance(action, np.ndarray):
            action = np.array(action, dtype=np.float32)
        
        # å­˜å‚¨ç»éªŒ
        self.agent.store_experience(state, action, reward, next_state, done)
        
        # æ›´æ–°ç½‘ç»œ
        training_info = self.agent.update()
        
        self.step_count += 1
        
        return training_info
    
    def save_models(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(filepath, exist_ok=True)
        self.agent.save_model(filepath)
        print(f"âœ“ DDPGæ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_models(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        self.agent.load_model(filepath)
        print(f"âœ“ DDPGæ¨¡å‹å·²åŠ è½½: {filepath}")
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, reward: float,
                        next_state: np.ndarray, done: bool, log_prob: float = 0.0, value: float = 0.0):
        """å­˜å‚¨ç»éªŒåˆ°ç¼“å†²åŒº - æ”¯æŒPPOå…¼å®¹æ€§"""
        # DDPGåªä½¿ç”¨å‰5ä¸ªå‚æ•°ï¼Œlog_probå’Œvalueè¢«å¿½ç•¥
        self.agent.store_experience(state, action, reward, next_state, done)
        self.step_count += 1
    
    def update(self, last_value: float = 0.0) -> Dict:
        """æ›´æ–°ç½‘ç»œå‚æ•° - æ”¯æŒPPOå…¼å®¹æ€§"""
        # DDPGä¸ä½¿ç”¨last_valueå‚æ•°
        return self.agent.update()
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return {
            'actor_loss_avg': float(np.mean(self.agent.actor_losses[-100:])) if self.agent.actor_losses else 0.0,
            'critic_loss_avg': float(np.mean(self.agent.critic_losses[-100:])) if self.agent.critic_losses else 0.0,
            'noise_scale': self.agent.noise_scale,
            'buffer_size': len(self.agent.replay_buffer),
            'step_count': self.step_count
        }