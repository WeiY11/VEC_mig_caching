"""
DDPG (Deep Deterministic Policy Gradient) å•æ™ºèƒ½ä½“ç®—æ³•å®ç°
ä¸“é—¨é€‚é…MATD3-MIGç³»ç»Ÿçš„VECç¯å¢ƒ

ä¸»è¦ç‰¹ç‚¹:
1. Actor-Criticæ¶æ„å¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´
2. ç»éªŒå›æ”¾æœºåˆ¶æé«˜æ ·æœ¬æ•ˆç‡
3. ç›®æ ‡ç½‘ç»œç¨³å®šè®­ç»ƒè¿‡ç¨‹
4. å™ªå£°æ¢ç´¢ç­–ç•¥

å¯¹åº”è®ºæ–‡: Continuous Control with Deep Reinforcement Learning
"""
# æ€§èƒ½ä¼˜åŒ– - å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰
try:
    from tools.performance_optimization import OPTIMIZED_BATCH_SIZES
except ImportError:
    OPTIMIZED_BATCH_SIZES = {'DDPG': 128}  # é»˜è®¤å€¼

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
from collections import deque
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass

from config import config


@dataclass
class DDPGConfig:
    """ğŸ”§ DDPGç®—æ³•é…ç½® - ä¿®å¤å…³é”®é—®é¢˜ï¼Œå¯¹æ ‡TD3ç¨³å®šæ€§"""
    # ç½‘ç»œç»“æ„ - å¯¹æ ‡TD3å®¹é‡
    hidden_dim: int = 400      # ğŸ”§ æå‡åˆ°400ï¼Œä¸TD3ä¸€è‡´
    actor_lr: float = 5e-5     # ğŸ”§ ä¸TD3ä¸€è‡´çš„å­¦ä¹ ç‡
    critic_lr: float = 1e-4    # ä¿æŒcriticå­¦ä¹ ç‡
    
    # è®­ç»ƒå‚æ•°
    batch_size: int = 256
    buffer_size: int = 100000  # ğŸ”§ ä¸TD3ä¸€è‡´
    tau: float = 0.003         # è½¯æ›´æ–°ç³»æ•°
    gamma: float = 0.99
    
    # æ¢ç´¢å‚æ•°
    noise_scale: float = 0.2   # ğŸ”§ ä¸TD3çš„exploration_noiseä¸€è‡´
    noise_decay: float = 0.9998 # ğŸ”§ ä¸TD3ä¸€è‡´çš„è¡°å‡ç‡
    min_noise: float = 0.05
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¼•å…¥ç­–ç•¥å»¶è¿Ÿæ›´æ–°ï¼ˆå€Ÿé‰´TD3ï¼‰
    policy_delay: int = 4      # ğŸ”§ æ¯4æ¬¡æ›´æ–°ä¸€æ¬¡Actorï¼Œä¸TD3ä¸€è‡´
    update_freq: int = 1       # æ”¹å›æ¯æ­¥éƒ½é‡‡æ ·
    warmup_steps: int = 1000   # ğŸ”§ ä¸TD3ä¸€è‡´
    
    # ğŸ”§ æ–°å¢ï¼šç›®æ ‡ç­–ç•¥å¹³æ»‘åŒ–ï¼ˆå€Ÿé‰´TD3ï¼‰
    target_noise: float = 0.05  # ç›®æ ‡åŠ¨ä½œå™ªå£°
    noise_clip: float = 0.2     # å™ªå£°è£å‰ªèŒƒå›´
    
    # PERå‚æ•°
    use_per: bool = True
    per_alpha: float = 0.6
    per_beta_start: float = 0.4
    per_beta_frames: int = 500000
    
    # æ¢¯åº¦å¤„ç†
    gradient_clip: float = 1.0  # ğŸ”§ æ”¾å®½åˆ°1.0ï¼Œä¸TD3ä¸€è‡´
    reward_scale: float = 1.0
    reward_normalize: bool = False


class DDPGActor(nn.Module):
    """DDPG Actorç½‘ç»œ - ç¡®å®šæ€§ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256, max_action: float = 1.0):
        super(DDPGActor, self).__init__()
        
        self.max_action = max_action
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.0)
        
        # æœ€åä¸€å±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
        nn.init.uniform_(self.network[-2].weight, -3e-3, 3e-3)
        nn.init.uniform_(self.network[-2].bias, -3e-3, 3e-3)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        return self.max_action * self.network(state)


class DDPGCritic(nn.Module):
    """DDPG Criticç½‘ç»œ - Qå‡½æ•°ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(DDPGCritic, self).__init__()
        
        # çŠ¶æ€ç¼–ç å™¨
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU()
        )
        
        # çŠ¶æ€-åŠ¨ä½œèåˆç½‘ç»œ
        self.fusion_network = nn.Sequential(
            nn.Linear(hidden_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in [self.state_encoder, self.fusion_network]:
            for layer in module:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    nn.init.constant_(layer.bias, 0.0)
        
        # æœ€åä¸€å±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
        nn.init.uniform_(self.fusion_network[-1].weight, -3e-3, 3e-3)
        nn.init.uniform_(self.fusion_network[-1].bias, -3e-3, 3e-3)
    
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        state_features = self.state_encoder(state)
        fusion_input = torch.cat([state_features, action], dim=1)
        return self.fusion_network(fusion_input)


class DDPGReplayBuffer:
    """ğŸ”§ DDPGä¼˜å…ˆç»éªŒå›æ”¾ç¼“å†²åŒº (æ”¯æŒPER)"""
    
    def __init__(self, capacity: int, state_dim: int, action_dim: int, alpha: float = 0.6, use_per: bool = True):
        self.capacity = capacity
        self.ptr = 0
        self.size = 0
        self.use_per = use_per
        self.alpha = alpha  # PERä¼˜å…ˆçº§æŒ‡æ•°
        
        # é¢„åˆ†é…å†…å­˜
        self.states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)
        self.rewards = np.zeros(capacity, dtype=np.float32)
        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)
        self.dones = np.zeros(capacity, dtype=np.float32)
        
        # ğŸ”§ PERä¼˜å…ˆçº§æ•°ç»„
        if self.use_per:
            self.priorities = np.zeros(capacity, dtype=np.float32)
    
    def push(self, state: np.ndarray, action: np.ndarray, reward: float, 
             next_state: np.ndarray, done: bool):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        # ğŸ”§ æ–°æ ·æœ¬ä½¿ç”¨æœ€å¤§ä¼˜å…ˆçº§
        if self.use_per:
            max_prio = self.priorities[:self.size].max() if self.size > 0 else 1.0
            self.priorities[self.ptr] = max_prio
        
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.next_states[self.ptr] = next_state
        self.dones[self.ptr] = float(done)
        
        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    
    def sample(self, batch_size: int, beta: float = 0.4) -> Tuple[torch.Tensor, ...]:
        """ğŸ”§ é‡‡æ ·ç»éªŒæ‰¹æ¬¡ï¼ˆæ”¯æŒPERï¼‰"""
        if self.use_per:
            # PERé‡‡æ ·
            prios = self.priorities[:self.size]
            probs = prios ** self.alpha
            probs /= probs.sum()
            
            indices = np.random.choice(self.size, batch_size, p=probs, replace=False)
            
            # è®¡ç®—é‡è¦æ€§æƒé‡
            weights = (self.size * probs[indices]) ** (-beta)
            weights /= weights.max()
            weights = torch.FloatTensor(weights).unsqueeze(1)
        else:
            # å‡åŒ€é‡‡æ ·
            indices = np.random.choice(self.size, batch_size, replace=False)
            weights = torch.ones(batch_size, 1)
        
        batch_states = torch.FloatTensor(self.states[indices])
        batch_actions = torch.FloatTensor(self.actions[indices])
        batch_rewards = torch.FloatTensor(self.rewards[indices]).unsqueeze(1)
        batch_next_states = torch.FloatTensor(self.next_states[indices])
        batch_dones = torch.FloatTensor(self.dones[indices]).unsqueeze(1)
        
        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, indices, weights
    
    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """ğŸ”§ æ›´æ–°æ ·æœ¬ä¼˜å…ˆçº§"""
        if self.use_per:
            self.priorities[indices] = priorities
    
    def __len__(self):
        return self.size


class DDPGAgent:
    """DDPGæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, config: DDPGConfig):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config
        
        # æ€§èƒ½ä¼˜åŒ– - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        self.optimized_batch_size = OPTIMIZED_BATCH_SIZES.get('DDPG', config.batch_size)
        self.config.batch_size = self.optimized_batch_size
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºç½‘ç»œ
        self.actor = DDPGActor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.critic = DDPGCritic(state_dim, action_dim, config.hidden_dim).to(self.device)
        
        # ç›®æ ‡ç½‘ç»œ
        self.target_actor = DDPGActor(state_dim, action_dim, config.hidden_dim).to(self.device)
        self.target_critic = DDPGCritic(state_dim, action_dim, config.hidden_dim).to(self.device)
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
        self.hard_update(self.target_actor, self.actor)
        self.hard_update(self.target_critic, self.critic)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)
        
        # ğŸ”§ ä¼˜åŒ–çš„ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆæ”¯æŒPERï¼‰
        self.replay_buffer = DDPGReplayBuffer(
            config.buffer_size, state_dim, action_dim, 
            alpha=config.per_alpha if hasattr(config, 'per_alpha') else 0.6,
            use_per=config.use_per if hasattr(config, 'use_per') else False
        )
        
        # ğŸ”§ PER betaå‚æ•°
        self.beta = config.per_beta_start if hasattr(config, 'per_beta_start') else 0.4
        self.beta_increment = (1.0 - self.beta) / (config.per_beta_frames if hasattr(config, 'per_beta_frames') else 500000)
        
        # æ¢ç´¢å™ªå£°
        self.noise_scale = config.noise_scale
        self.step_count = 0
        self.update_count = 0  # ğŸ”§ æ·»åŠ æ›´æ–°è®¡æ•°
        
        # ğŸ”§ å¥–åŠ±ç»Ÿè®¡ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
        self.reward_stats = {'mean': 0.0, 'std': 1.0}
        self.reward_history = deque(maxlen=10000)
        
        # è®­ç»ƒç»Ÿè®¡
        self.actor_losses = []
        self.critic_losses = []
    
    def select_action(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action = self.actor(state_tensor).cpu().numpy()[0]
        
        # æ·»åŠ æ¢ç´¢å™ªå£°
        if training:
            noise = np.random.normal(0, self.noise_scale, size=action.shape)
            action = np.clip(action + noise, -1.0, 1.0)
        
        return action
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, reward: float,
                        next_state: np.ndarray, done: bool):
        """å­˜å‚¨ç»éªŒ"""
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def update(self) -> Dict[str, float]:
        """ğŸ”§ ä¼˜åŒ–çš„æ›´æ–°ç½‘ç»œå‚æ•° - å¼•å…¥ç­–ç•¥å»¶è¿Ÿæ›´æ–°ï¼ˆå€Ÿé‰´TD3ï¼‰"""
        if len(self.replay_buffer) < self.config.batch_size:
            return {}
        
        self.step_count += 1
        
        # é¢„çƒ­æœŸä¸æ›´æ–°
        if self.step_count < self.config.warmup_steps:
            return {}
        
        # ğŸ”§ æ›´æ–°é¢‘ç‡æ§åˆ¶
        if self.step_count % self.config.update_freq != 0:
            return {}
        
        self.update_count += 1
        
        # ğŸ”§ é‡‡æ ·ç»éªŒæ‰¹æ¬¡ï¼ˆæ”¯æŒPERï¼‰
        sample_result = self.replay_buffer.sample(self.config.batch_size, self.beta)
        if len(sample_result) == 7:  # PERæ¨¡å¼
            batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, indices, weights = sample_result
        else:  # æ™®é€šæ¨¡å¼ï¼ˆå…¼å®¹ï¼‰
            batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = sample_result[:5]
            indices = np.arange(len(batch_states))
            weights = torch.ones(len(batch_states), 1)
        
        # ğŸ”§ æ›´æ–°beta
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        batch_states = batch_states.to(self.device)
        batch_actions = batch_actions.to(self.device)
        batch_rewards = batch_rewards.to(self.device)
        batch_next_states = batch_next_states.to(self.device)
        batch_dones = batch_dones.to(self.device)
        weights = weights.to(self.device)
        
        # ğŸ”§ å¥–åŠ±å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
        if self.config.reward_normalize if hasattr(self.config, 'reward_normalize') else False:
            self.reward_history.extend(batch_rewards.cpu().numpy().flatten())
            if len(self.reward_history) > 100:
                self.reward_stats['mean'] = np.mean(self.reward_history)
                self.reward_stats['std'] = np.std(self.reward_history) + 1e-8
                batch_rewards = (batch_rewards - self.reward_stats['mean']) / self.reward_stats['std']
        
        # ğŸ”§ æ›´æ–°Criticå¹¶è·å–TDè¯¯å·®
        critic_loss, td_errors = self._update_critic(
            batch_states, batch_actions, batch_rewards, 
            batch_next_states, batch_dones, weights
        )
        
        # ğŸ”§ æ›´æ–°PERä¼˜å…ˆçº§
        if self.config.use_per if hasattr(self.config, 'use_per') else False:
            priorities = td_errors.detach().cpu().numpy() + 1e-6
            self.replay_buffer.update_priorities(indices, priorities)
        
        training_info = {'critic_loss': critic_loss}
        
        # ğŸ”§ ç­–ç•¥å»¶è¿Ÿæ›´æ–°ï¼ˆå€Ÿé‰´TD3ï¼‰- æ¯policy_delayæ¬¡æ›´æ–°ä¸€æ¬¡Actor
        if self.update_count % self.config.policy_delay == 0:
            actor_loss = self._update_actor(batch_states)
            training_info['actor_loss'] = actor_loss
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆåªåœ¨æ›´æ–°Actoræ—¶æ›´æ–°ï¼‰
            self.soft_update(self.target_actor, self.actor, self.config.tau)
            self.soft_update(self.target_critic, self.critic, self.config.tau)
        
        # ğŸ”§ ä¼˜åŒ–çš„å™ªå£°è¡°å‡
        self.noise_scale = max(self.config.min_noise, self.noise_scale * self.config.noise_decay)
        training_info['noise_scale'] = self.noise_scale
        training_info['buffer_size'] = len(self.replay_buffer)
        training_info['beta'] = self.beta
        training_info['update_count'] = self.update_count
        
        return training_info
    
    def _update_critic(self, states: torch.Tensor, actions: torch.Tensor, 
                      rewards: torch.Tensor, next_states: torch.Tensor, 
                      dones: torch.Tensor, weights: torch.Tensor) -> Tuple[float, torch.Tensor]:
        """ğŸ”§ ä¼˜åŒ–çš„Criticæ›´æ–° - æ·»åŠ ç›®æ ‡ç­–ç•¥å¹³æ»‘åŒ–ï¼ˆå€Ÿé‰´TD3ï¼‰"""
        with torch.no_grad():
            next_actions = self.target_actor(next_states)
            
            # ğŸ”§ ç›®æ ‡ç­–ç•¥å¹³æ»‘åŒ–ï¼šæ·»åŠ è£å‰ªå™ªå£°ï¼ˆå€Ÿé‰´TD3ï¼‰
            noise = torch.randn_like(next_actions) * self.config.target_noise
            noise = torch.clamp(noise, -self.config.noise_clip, self.config.noise_clip)
            next_actions = torch.clamp(next_actions + noise, -1.0, 1.0)
            
            target_q = self.target_critic(next_states, next_actions)
            target_q = rewards + (1 - dones) * self.config.gamma * target_q
        
        current_q = self.critic(states, actions)
        
        # ğŸ”§ è®¡ç®—TDè¯¯å·®ï¼ˆç”¨äºPERï¼‰
        td_errors = torch.abs(current_q - target_q)
        
        # ğŸ”§ åŠ æƒMSEæŸå¤±
        critic_loss = (weights * F.mse_loss(current_q, target_q, reduction='none')).mean()
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        
        # ğŸ”§ ä½¿ç”¨é…ç½®çš„æ¢¯åº¦è£å‰ª
        grad_clip = self.config.gradient_clip if hasattr(self.config, 'gradient_clip') else 1.0
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), grad_clip)
        
        self.critic_optimizer.step()
        
        self.critic_losses.append(critic_loss.item())
        return critic_loss.item(), td_errors.squeeze()
    
    def _update_actor(self, states: torch.Tensor) -> float:
        """ğŸ”§ ä¼˜åŒ–çš„Actoræ›´æ–°"""
        actions = self.actor(states)
        actor_loss = -self.critic(states, actions).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        
        # ğŸ”§ ä½¿ç”¨é…ç½®çš„æ¢¯åº¦è£å‰ª
        grad_clip = self.config.gradient_clip if hasattr(self.config, 'gradient_clip') else 1.0
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), grad_clip)
        
        self.actor_optimizer.step()
        
        self.actor_losses.append(actor_loss.item())
        return actor_loss.item()
    
    def soft_update(self, target: nn.Module, source: nn.Module, tau: float):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
    
    def hard_update(self, target: nn.Module, source: nn.Module):
        """ç¡¬æ›´æ–°ç½‘ç»œå‚æ•°"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(param.data)
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'target_actor_state_dict': self.target_actor.state_dict(),
            'target_critic_state_dict': self.target_critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'noise_scale': self.noise_scale,
            'step_count': self.step_count
        }, f"{filepath}_ddpg.pth")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(f"{filepath}_ddpg.pth", map_location=self.device)
        
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.target_actor.load_state_dict(checkpoint['target_actor_state_dict'])
        self.target_critic.load_state_dict(checkpoint['target_critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.noise_scale = checkpoint['noise_scale']
        self.step_count = checkpoint['step_count']


class DDPGEnvironment:
    """DDPGè®­ç»ƒç¯å¢ƒ"""
    
    def __init__(self):
        self.config = DDPGConfig()
        
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—çŠ¶æ€ç»´åº¦ï¼Œä¸TD3ä¿æŒä¸€è‡´
        self.state_dim = 130  # è½¦è¾†60 + RSU54 + UAV16 = 130ç»´
        self.action_dim = 18  # æ”¯æŒè‡ªé€‚åº”ç¼“å­˜è¿ç§»æ§åˆ¶ï¼Œä¸TD3ä¿æŒä¸€è‡´
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agent = DDPGAgent(self.state_dim, self.action_dim, self.config)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_count = 0
        self.step_count = 0
        
        print(f"âœ“ DDPGç¯å¢ƒåˆå§‹åŒ–å®Œæˆ (å·²ä¼˜åŒ–)")
        print(f"âœ“ çŠ¶æ€ç»´åº¦: {self.state_dim}")
        print(f"âœ“ åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"âœ“ ç­–ç•¥å»¶è¿Ÿæ›´æ–°: {self.config.policy_delay} (å€Ÿé‰´TD3)")
        print(f"âœ“ ç›®æ ‡ç­–ç•¥å¹³æ»‘åŒ–: å·²å¯ç”¨ (target_noise={self.config.target_noise})")
        print(f"âœ“ ç½‘ç»œå®¹é‡: hidden_dim={self.config.hidden_dim}")
    
    def get_state_vector(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """
        ğŸ”§ ä¿®å¤ï¼šæ„å»ºå‡†ç¡®çš„130ç»´çŠ¶æ€å‘é‡ï¼Œä¸TD3å®Œå…¨ä¸€è‡´
        çŠ¶æ€ç»„æˆ: è½¦è¾†60ç»´ + RSU54ç»´ + UAV16ç»´ = 130ç»´
        """
        state_components = []
        
        # 1. è½¦è¾†çŠ¶æ€ (12Ã—5=60ç»´) - ä¸TD3ä¸€è‡´
        for i in range(12):
            vehicle_key = f'vehicle_{i}'
            if vehicle_key in node_states:
                vehicle_state = node_states[vehicle_key]
                # ç¡®ä¿æ•°å€¼æœ‰æ•ˆæ€§
                valid_state = []
                for val in vehicle_state[:5]:
                    if np.isfinite(val):
                        valid_state.append(float(val))
                    else:
                        valid_state.append(0.5)
                state_components.extend(valid_state)
                
                # è¡¥é½åˆ°5ç»´
                while len(state_components) % 5 != 0:
                    state_components.append(0.0)
            else:
                # é»˜è®¤è½¦è¾†çŠ¶æ€: [ä½ç½®x, ä½ç½®y, é€Ÿåº¦, é˜Ÿåˆ—, èƒ½è€—]
                state_components.extend([0.5, 0.5, 0.0, 0.0, 0.0])
        
        # 2. RSUçŠ¶æ€ (6Ã—9=54ç»´) - ä¸TD3ä¸€è‡´
        for i in range(6):
            rsu_key = f'rsu_{i}'
            if rsu_key in node_states:
                rsu_state = node_states[rsu_key]
                # ç¡®ä¿æ•°å€¼æœ‰æ•ˆæ€§å’Œç»´åº¦æ­£ç¡®
                valid_rsu_state = []
                for j, val in enumerate(rsu_state[:9]):
                    if np.isfinite(val):
                        # å¯¹ç¼“å­˜åˆ©ç”¨ç‡(ç¬¬2ç»´)è¿›è¡Œç‰¹æ®Šæ£€æŸ¥
                        if j == 2:  # ç¼“å­˜åˆ©ç”¨ç‡ç»´åº¦
                            valid_rsu_state.append(min(1.0, max(0.0, float(val))))
                        else:
                            valid_rsu_state.append(float(val))
                    else:
                        valid_rsu_state.append(0.5 if j < 2 else 0.0)
                
                # è¡¥é½åˆ°9ç»´
                while len(valid_rsu_state) < 9:
                    valid_rsu_state.append(0.0)
                
                state_components.extend(valid_rsu_state)
            else:
                # é»˜è®¤RSUçŠ¶æ€: [ä½ç½®x, ä½ç½®y, ç¼“å­˜åˆ©ç”¨ç‡, é˜Ÿåˆ—, èƒ½è€—, ç¼“å­˜å‚æ•°4ç»´]
                state_components.extend([0.5, 0.5, 0.0, 0.0, 0.0, 0.7, 0.35, 0.05, 0.3])
        
        # 3. UAVçŠ¶æ€ (2Ã—8=16ç»´) - ä¸TD3ä¸€è‡´
        for i in range(2):
            uav_key = f'uav_{i}'
            if uav_key in node_states:
                uav_state = node_states[uav_key]
                # ç¡®ä¿æ•°å€¼æœ‰æ•ˆæ€§
                valid_uav_state = []
                for j, val in enumerate(uav_state[:8]):
                    if np.isfinite(val):
                        # å¯¹ç¼“å­˜åˆ©ç”¨ç‡(ç¬¬3ç»´)è¿›è¡Œç‰¹æ®Šå¤„ç†
                        if j == 3:  # ç¼“å­˜åˆ©ç”¨ç‡ç»´åº¦
                            valid_uav_state.append(min(1.0, max(0.0, float(val))))
                        else:
                            valid_uav_state.append(float(val))
                    else:
                        valid_uav_state.append(0.5 if j < 3 else 0.0)
                
                # è¡¥é½åˆ°8ç»´
                while len(valid_uav_state) < 8:
                    valid_uav_state.append(0.0)
                
                state_components.extend(valid_uav_state)
            else:
                # é»˜è®¤UAVçŠ¶æ€: [ä½ç½®x, ä½ç½®y, ä½ç½®z, ç¼“å­˜åˆ©ç”¨ç‡, èƒ½è€—, è¿ç§»å‚æ•°3ç»´]
                state_components.extend([0.5, 0.5, 0.5, 0.0, 0.0, 0.75, 1.0, 0.3])
        
        # ç¡®ä¿çŠ¶æ€å‘é‡æ­£å¥½æ˜¯130ç»´
        state_vector = np.array(state_components[:130], dtype=np.float32)
        
        # å¦‚æœç»´åº¦ä¸è¶³130ï¼Œè¡¥é½
        if len(state_vector) < 130:
            padding_needed = 130 - len(state_vector)
            state_vector = np.pad(state_vector, (0, padding_needed), mode='constant', constant_values=0.5)
        
        # æ•°å€¼å®‰å…¨æ£€æŸ¥
        state_vector = np.nan_to_num(state_vector, nan=0.5, posinf=1.0, neginf=0.0)
        
        return state_vector
    
    def decompose_action(self, action: np.ndarray) -> Dict[str, np.ndarray]:
        """
        å°†å…¨å±€åŠ¨ä½œåˆ†è§£ä¸ºå„èŠ‚ç‚¹åŠ¨ä½œ
        ğŸ¤– æ›´æ–°æ”¯æŒ18ç»´åŠ¨ä½œç©ºé—´ï¼Œä¸TD3ä¿æŒä¸€è‡´ï¼š
        - vehicle_agent: 18ç»´ (11ç»´åŸæœ‰ + 7ç»´ç¼“å­˜è¿ç§»æ§åˆ¶)
        """
        actions = {}
        
        # ç¡®ä¿actioné•¿åº¦è¶³å¤Ÿ
        if len(action) < 18:
            action = np.pad(action, (0, 18-len(action)), mode='constant')
        
        # ğŸ¤– vehicle_agent è·å¾—æ‰€æœ‰18ç»´åŠ¨ä½œ
        # å‰11ç»´ï¼šä»»åŠ¡åˆ†é…(3) + RSUé€‰æ‹©(6) + UAVé€‰æ‹©(2)
        # å7ç»´ï¼šç¼“å­˜æ§åˆ¶(4) + è¿ç§»æ§åˆ¶(3)
        actions['vehicle_agent'] = action[:18]
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä»vehicle_agentä¸­æå–RSUå’ŒUAVé€‰æ‹©
        # è®­ç»ƒæ¡†æ¶éœ€è¦ä»rsu_agentå’Œuav_agentè·å–é€‰æ‹©æ¦‚ç‡
        actions['rsu_agent'] = action[3:9]   # RSUé€‰æ‹©ï¼ˆ6ç»´ï¼‰
        actions['uav_agent'] = action[9:11]  # UAVé€‰æ‹©ï¼ˆ2ç»´ï¼‰
        
        return actions
    
    def get_actions(self, state: np.ndarray, training: bool = True) -> Dict[str, np.ndarray]:
        """è·å–åŠ¨ä½œ"""
        global_action = self.agent.select_action(state, training)
        return self.decompose_action(global_action)
    
    def calculate_reward(self, system_metrics: Dict, 
                       cache_metrics: Optional[Dict] = None,
                       migration_metrics: Optional[Dict] = None) -> float:
        """
        ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å¢å¼ºå¥–åŠ±è®¡ç®—å™¨ï¼Œä¸TD3ä¿æŒä¸€è‡´
        æ”¯æŒç¼“å­˜å’Œè¿ç§»å­ç³»ç»Ÿçš„ç»¼åˆå¥–åŠ±è®¡ç®—
        """
        try:
            from utils.enhanced_reward_calculator import calculate_enhanced_reward
            return calculate_enhanced_reward(system_metrics, cache_metrics, migration_metrics)
        except ImportError:
            # å›é€€åˆ°ç®€å•å¥–åŠ±
            from utils.simple_reward_calculator import calculate_simple_reward
            return calculate_simple_reward(system_metrics)
    
    def train_step(self, state: np.ndarray, action: Union[np.ndarray, int], reward: float,
                   next_state: np.ndarray, done: bool) -> Dict:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        # DDPGéœ€è¦numpyæ•°ç»„ï¼Œå¦‚æœæ˜¯æ•´æ•°åˆ™è½¬æ¢
        if isinstance(action, int):
            action = np.array([action], dtype=np.float32)
        elif not isinstance(action, np.ndarray):
            action = np.array(action, dtype=np.float32)
        
        # å­˜å‚¨ç»éªŒ
        self.agent.store_experience(state, action, reward, next_state, done)
        
        # æ›´æ–°ç½‘ç»œ
        training_info = self.agent.update()
        
        self.step_count += 1
        
        return training_info
    
    def save_models(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(filepath, exist_ok=True)
        self.agent.save_model(filepath)
        print(f"âœ“ DDPGæ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_models(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        self.agent.load_model(filepath)
        print(f"âœ“ DDPGæ¨¡å‹å·²åŠ è½½: {filepath}")
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, reward: float,
                        next_state: np.ndarray, done: bool, log_prob: float = 0.0, value: float = 0.0):
        """å­˜å‚¨ç»éªŒåˆ°ç¼“å†²åŒº - æ”¯æŒPPOå…¼å®¹æ€§"""
        # DDPGåªä½¿ç”¨å‰5ä¸ªå‚æ•°ï¼Œlog_probå’Œvalueè¢«å¿½ç•¥
        self.agent.store_experience(state, action, reward, next_state, done)
        self.step_count += 1
    
    def update(self, last_value: float = 0.0) -> Dict:
        """æ›´æ–°ç½‘ç»œå‚æ•° - æ”¯æŒPPOå…¼å®¹æ€§"""
        # DDPGä¸ä½¿ç”¨last_valueå‚æ•°
        return self.agent.update()
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return {
            'actor_loss_avg': float(np.mean(self.agent.actor_losses[-100:])) if self.agent.actor_losses else 0.0,
            'critic_loss_avg': float(np.mean(self.agent.critic_losses[-100:])) if self.agent.critic_losses else 0.0,
            'noise_scale': self.agent.noise_scale,
            'buffer_size': len(self.agent.replay_buffer),
            'step_count': self.step_count
        }