"""
ä¼˜åŒ–çš„TD3ç®—æ³•å®ç° - ä¿®å¤å…³é”®é—®é¢˜
åŸºäºåˆ†ææŠ¥å‘Šçš„æ”¹è¿›ç‰ˆæœ¬

ä¸»è¦æ”¹è¿›ï¼š
1. é‡æ„çŠ¶æ€ç©ºé—´è®¾è®¡
2. é‡æ–°è®¾è®¡åŠ¨ä½œç©ºé—´
3. ä¼˜åŒ–å¥–åŠ±å‡½æ•°
4. ä¿®å¤ç¯å¢ƒäº¤äº’
5. è°ƒæ•´è¶…å‚æ•°é…ç½®
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from collections import deque
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass

from config import config


@dataclass
class OptimizedTD3Config:
    """ä¼˜åŒ–çš„TD3é…ç½® - ä¿®å¤ç‰ˆæœ¬"""
    # ç½‘ç»œç»“æ„
    hidden_dim: int = 256        # é€‚å½“å¢åŠ ç½‘ç»œå®¹é‡ä»¥å­¦ä¹ å¤æ‚ç­–ç•¥
    actor_lr: float = 3e-4       # æé«˜å­¦ä¹ ç‡ï¼ŒåŠ é€Ÿæ”¶æ•›
    critic_lr: float = 3e-4      # æé«˜å­¦ä¹ ç‡ï¼ŒåŠ é€Ÿæ”¶æ•›
    
    # è®­ç»ƒå‚æ•°
    batch_size: int = 256        # é€‚ä¸­æ‰¹æ¬¡å¤§å°
    buffer_size: int = 500000    # é€‚ä¸­ç¼“å†²åŒº
    tau: float = 0.005           # æ ‡å‡†è½¯æ›´æ–°é€Ÿç‡
    gamma: float = 0.99          # æ ‡å‡†æŠ˜æ‰£å› å­
    
    # TD3ç‰¹æœ‰å‚æ•°
    policy_delay: int = 2        # æ ‡å‡†ç­–ç•¥å»¶è¿Ÿ
    target_noise: float = 0.2    # æ ‡å‡†ç›®æ ‡å™ªå£°
    noise_clip: float = 0.5      # æ ‡å‡†å™ªå£°è£å‰ª
    
    # æ¢ç´¢å‚æ•°
    exploration_noise: float = 0.25   # é€‚ä¸­çš„åˆå§‹æ¢ç´¢
    noise_decay: float = 0.9998       # æ›´ç¼“æ…¢çš„è¡°å‡é€Ÿåº¦
    min_noise: float = 0.08           # ä¿ç•™æ›´å¤šæ¢ç´¢
    
    # è®­ç»ƒæ§åˆ¶
    warmup_steps: int = 10000        # å‡å°‘é¢„çƒ­æ­¥æ•°ï¼Œçº¦50ä¸ªepisode
    update_freq: int = 1             # æ¯æ­¥éƒ½æ›´æ–°
    
    # æ­£åˆ™åŒ–å‚æ•°
    weight_decay: float = 1e-5       # æ›´å°çš„L2æ­£åˆ™åŒ–
    grad_clip: float = 1.0           # é€‚åº¦æ¢¯åº¦è£å‰ª


class VECActionSpace:
    """VECç³»ç»ŸåŠ¨ä½œç©ºé—´å®šä¹‰"""
    
    def __init__(self):
        # åŠ¨ä½œç»´åº¦å®šä¹‰
        self.vehicle_actions = 5    # æœ¬åœ°å¤„ç†æ¯”ä¾‹ã€å¸è½½ç›®æ ‡é€‰æ‹©ç­‰
        self.rsu_actions = 8        # è®¡ç®—èµ„æºåˆ†é…ã€ç¼“å­˜ç­–ç•¥ã€è¿ç§»å†³ç­–ç­‰
        self.uav_actions = 6        # è®¡ç®—èµ„æºåˆ†é…ã€ç§»åŠ¨ç­–ç•¥ç­‰
        
        self.num_vehicles = config.network.num_vehicles  # 12
        self.num_rsus = config.network.num_rsus          # 6  
        self.num_uavs = config.network.num_uavs          # 2
        
        self.total_dim = (
            self.num_vehicles * self.vehicle_actions +  # 12 * 5 = 60
            self.num_rsus * self.rsu_actions +          # 6 * 8 = 48
            self.num_uavs * self.uav_actions            # 2 * 6 = 12
        )  # æ€»è®¡ï¼š120ç»´
    
    def decompose_action(self, action: np.ndarray) -> Dict:
        """å°†å…¨å±€åŠ¨ä½œåˆ†è§£ä¸ºå…·ä½“å†³ç­–"""
        actions = {}
        idx = 0
        
        # è½¦è¾†åŠ¨ä½œ
        for i in range(self.num_vehicles):
            vehicle_action = action[idx:idx+self.vehicle_actions]
            actions[f'vehicle_{i}'] = {
                'local_processing_ratio': np.clip(vehicle_action[0], 0, 1),
                'offload_target_rsu': np.argmax(vehicle_action[1:4]) if len(vehicle_action) > 3 else 0,
                'offload_target_uav': int(vehicle_action[4] > 0) if len(vehicle_action) > 4 else 0,
            }
            idx += self.vehicle_actions
        
        # RSUåŠ¨ä½œ
        for i in range(self.num_rsus):
            rsu_action = action[idx:idx+self.rsu_actions]
            actions[f'rsu_{i}'] = {
                'cpu_allocation': np.clip(rsu_action[0], 0.5, 1.0),
                'cache_policy': np.argmax(rsu_action[1:4]),  # LRU/LFU/FIFO
                'migration_threshold': np.clip(rsu_action[4], 0.5, 0.9),
                'bandwidth_allocation': np.clip(rsu_action[5:8], 0.1, 1.0),
            }
            idx += self.rsu_actions
        
        # UAVåŠ¨ä½œ
        for i in range(self.num_uavs):
            uav_action = action[idx:idx+self.uav_actions]
            actions[f'uav_{i}'] = {
                'cpu_allocation': np.clip(uav_action[0], 0.3, 1.0),
                'power_management': np.clip(uav_action[1], 0.5, 1.0),
                'service_priority': np.clip(uav_action[2:6], 0, 1),
            }
            idx += self.uav_actions
        
        return actions


class VECStateSpace:
    """VECç³»ç»ŸçŠ¶æ€ç©ºé—´å®šä¹‰"""
    
    def __init__(self, system_config=None):
        # å¦‚æœæä¾›äº†é…ç½®ï¼Œä½¿ç”¨æä¾›çš„é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        if system_config is not None:
            self.num_vehicles = system_config.network.num_vehicles
            self.num_rsus = system_config.network.num_rsus
            self.num_uavs = system_config.network.num_uavs
        else:
            self.num_vehicles = config.network.num_vehicles
            self.num_rsus = config.network.num_rsus
            self.num_uavs = config.network.num_uavs
        
        # çŠ¶æ€ç»´åº¦è®¡ç®—
        self.vehicle_state_dim = 7  # ä½ç½®x,y + é€Ÿåº¦x,y + é˜Ÿåˆ—åˆ©ç”¨ç‡ + ğŸ”§CPUå®¹é‡ + ğŸ”§å½“å‰ä»»åŠ¡è´Ÿè½½
        self.rsu_state_dim = 7      # CPUåˆ©ç”¨ç‡ + é˜Ÿåˆ—åˆ©ç”¨ç‡ + ç¼“å­˜åˆ©ç”¨ç‡ + èƒ½è€— + ğŸ”§èµ„æºå®¹é‡ + ğŸ”§å¹³å‡è·ç¦» + ğŸ”§ç¼“å­˜å‘½ä¸­ç‡
        self.uav_state_dim = 6      # CPUåˆ©ç”¨ç‡ + é˜Ÿåˆ—åˆ©ç”¨ç‡ + ç”µæ± ç”µé‡ + èƒ½è€— + ğŸ”§èµ„æºå®¹é‡ + ğŸ”§å¹³å‡è·ç¦»
        self.global_state_dim = 16  # å…¨å±€ç³»ç»ŸæŒ‡æ ‡ï¼ˆåŸºç¡€8ç»´ + ä»»åŠ¡ç±»å‹8ç»´ï¼‰
        
        self.total_dim = (
            self.num_vehicles * self.vehicle_state_dim +  # 12 * 7 = 84
            self.num_rsus * self.rsu_state_dim +          # 4 * 7 = 28
            self.num_uavs * self.uav_state_dim +          # 2 * 6 = 12
            self.global_state_dim                         # 16
        )  # æ€»è®¡ï¼š140ç»´ (ğŸ”§ä¼˜åŒ–å+34ç»´)
    
    def encode_state(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """æ„å»ºç¬¦åˆè®ºæ–‡çš„VECç³»ç»ŸçŠ¶æ€å‘é‡"""
        state_components = []
        
        # 1. è½¦è¾†çŠ¶æ€ (12è½¦è¾† Ã— 7ç»´ = 84ç»´)
        for i in range(self.num_vehicles):
            vehicle_id = f'vehicle_{i}'
            if vehicle_id in node_states:
                vehicle = node_states[vehicle_id]
                # ğŸ”§ æ–°å¢ï¼šè½¦è¾†CPUå®¹é‡å’Œå½“å‰ä»»åŠ¡è´Ÿè½½ï¼Œè®©æ™ºèƒ½ä½“çŸ¥é“æœ¬åœ°è®¡ç®—èƒ½åŠ›
                # ğŸ ä¿®å¤ï¼šnode_statesæ˜¯numpyæ•°ç»„ï¼Œä¸æ˜¯å¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨ç´¢å¼•è®¿é—®
                if isinstance(vehicle, np.ndarray):
                    # ä» train_single_agent.py: [pos_x, pos_y, velocity, queue_len, energy]
                    # éœ€è¦æ·»åŠ  cpu_capacity å’Œ task_load
                    vehicle_state = [
                        vehicle[0],  # position_x (normalized)
                        vehicle[1],  # position_y (normalized)
                        0.0,         # velocity_x (TODO: ä» velocityè®¡ç®—)
                        0.0,         # velocity_y 
                        vehicle[3],  # queue_utilization
                        1.5e9 / 20e9,  # ğŸ”§ CPUå®¹é‡ (1.5GHz/20GHz = 0.075)
                        vehicle[3],  # ğŸ”§ ä»»åŠ¡è´Ÿè½½ï¼ˆä½¿ç”¨queue_utilizationï¼‰
                    ]
                else:
                    # å¦‚æœæ˜¯å¯¹è±¡ç±»å‹ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
                    cpu_freq = getattr(vehicle, 'cpu_frequency', 1.5e9)
                    queue_len = getattr(vehicle, 'queue_length', 0)
                    vehicle_state = [
                        getattr(vehicle.position, 'x', 0.0) / 2000.0,
                        getattr(vehicle.position, 'y', 0.0) / 2000.0,
                        getattr(vehicle, 'velocity_x', 0.0) / 30.0,
                        getattr(vehicle, 'velocity_y', 0.0) / 30.0,
                        getattr(vehicle, 'queue_utilization', 0.5),
                        cpu_freq / 20e9,
                        min(queue_len / 20.0, 1.0),
                    ]
            else:
                # é»˜è®¤çŠ¶æ€
                vehicle_state = [0.5, 0.5, 0.0, 0.0, 0.5, 0.075, 0.5]
            state_components.extend(vehicle_state)
        
        # 2. RSUçŠ¶æ€ (æŒ‰é…ç½®æ•°é‡ Ã— 7ç»´)
        for i in range(self.num_rsus):
            rsu_id = f'rsu_{i}'
            if rsu_id in node_states:
                rsu = node_states[rsu_id]
                # ğŸ ä¿®å¤ï¼šnode_statesæ˜¯numpyæ•°ç»„ï¼Œä¸æ˜¯å¯¹è±¡
                if isinstance(rsu, np.ndarray):
                    # ä» train_single_agent.py: [pos_x, pos_y, cache_util, queue_len, energy, cpu_freq_norm]
                    # éœ€è¦æ·»åŠ  avg_distance å’Œ cache_hit_rate
                    rsu_state = [
                        rsu[3],      # queue_utilization (CPUåˆ©ç”¨ç‡ç”¨é˜Ÿåˆ—ä»£æ›¿)
                        rsu[3],      # queue_utilization
                        rsu[2],      # cache_utilization
                        rsu[4],      # energy_consumption (normalized)
                        rsu[5] if len(rsu) > 5 else 0.625,  # ğŸ”§ CPUå®¹é‡ (12.5GHz/20GHz)
                        0.5,         # ğŸ”§ å¹³å‡è·ç¦»ï¼ˆé»˜è®¤ï¼Œæ— æ³•è®¡ç®—ï¼‰
                        0.5,         # ğŸ”§ ç¼“å­˜å‘½ä¸­ç‡ï¼ˆé»˜è®¤ï¼‰
                    ]
                else:
                    # å¦‚æœæ˜¯å¯¹è±¡ç±»å‹ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
                    cpu_freq = getattr(rsu, 'cpu_frequency', 12.5e9)
                    cache_hit_rate = getattr(rsu, 'recent_cache_hit_rate', 0.5)
                    rsu_pos = getattr(rsu, 'position', None)
                    avg_distance = 0.5
                    if rsu_pos:
                        distances = []
                        for j in range(self.num_vehicles):
                            v_id = f'vehicle_{j}'
                            if v_id in node_states:
                                v_pos = getattr(node_states[v_id], 'position', None)
                                if v_pos:
                                    dist = ((rsu_pos.x - v_pos.x)**2 + (rsu_pos.y - v_pos.y)**2)**0.5
                                    distances.append(dist)
                        if distances:
                            avg_distance = min(sum(distances) / len(distances) / 1000.0, 1.0)
                    
                    rsu_state = [
                        getattr(rsu, 'cpu_utilization', 0.5),
                        getattr(rsu, 'queue_utilization', 0.5),
                        getattr(rsu, 'cache_utilization', 0.5),
                        getattr(rsu, 'energy_consumption', 500.0) / 1000.0,
                        cpu_freq / 20e9,
                        avg_distance,
                        cache_hit_rate,
                    ]
            else:
                rsu_state = [0.5, 0.5, 0.5, 0.5, 0.625, 0.5, 0.5]
            state_components.extend(rsu_state)
        
        # 3. UAVçŠ¶æ€ (æŒ‰é…ç½®æ•°é‡ Ã— 6ç»´)
        for i in range(self.num_uavs):
            uav_id = f'uav_{i}'
            if uav_id in node_states:
                uav = node_states[uav_id]
                # ğŸ ä¿®å¤ï¼šnode_statesæ˜¯numpyæ•°ç»„ï¼Œä¸æ˜¯å¯¹è±¡
                if isinstance(uav, np.ndarray):
                    # ä» train_single_agent.py: [pos_x, pos_y, pos_z, cache_util, energy, cpu_freq_norm]
                    # éœ€è¦æ·»åŠ  avg_distance
                    uav_state = [
                        uav[3] if len(uav) > 3 else 0.5,  # queue_utilization (CPUåˆ©ç”¨ç‡ç”¨ç¼“å­˜ä»£æ›¿)
                        uav[3] if len(uav) > 3 else 0.5,  # queue_utilization
                        0.8,         # battery_level (é»˜è®¤)
                        uav[4] if len(uav) > 4 else 0.5,  # energy_consumption (normalized)
                        uav[5] if len(uav) > 5 else 0.25, # ğŸ”§ CPUå®¹é‡ (5GHz/20GHz)
                        0.5,         # ğŸ”§ å¹³å‡è·ç¦»ï¼ˆé»˜è®¤ï¼Œæ— æ³•è®¡ç®—ï¼‰
                    ]
                else:
                    # å¦‚æœæ˜¯å¯¹è±¡ç±»å‹ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
                    cpu_freq = getattr(uav, 'cpu_frequency', 5.0e9)
                    uav_pos = getattr(uav, 'position', None)
                    avg_distance = 0.5
                    if uav_pos:
                        distances = []
                        for j in range(self.num_vehicles):
                            v_id = f'vehicle_{j}'
                            if v_id in node_states:
                                v_pos = getattr(node_states[v_id], 'position', None)
                                if v_pos:
                                    dist = ((uav_pos.x - v_pos.x)**2 + (uav_pos.y - v_pos.y)**2)**0.5
                                    distances.append(dist)
                        if distances:
                            avg_distance = min(sum(distances) / len(distances) / 1000.0, 1.0)
                    
                    uav_state = [
                        getattr(uav, 'cpu_utilization', 0.5),
                        getattr(uav, 'queue_utilization', 0.5),
                        getattr(uav, 'battery_level', 0.8),
                        getattr(uav, 'energy_consumption', 50.0) / 100.0,
                        cpu_freq / 20e9,
                        avg_distance,
                    ]
            else:
                uav_state = [0.5, 0.5, 0.8, 0.5, 0.25, 0.5]
            state_components.extend(uav_state)
        
        # 4. å…¨å±€ç³»ç»ŸçŠ¶æ€ (8ç»´)
        global_state = [
            system_metrics.get('avg_task_delay', 1.0) / 2.0,
            system_metrics.get('total_energy_consumption', 2500.0) / 5000.0,
            system_metrics.get('data_loss_rate', 0.1),
            system_metrics.get('task_completion_rate', 0.8),
            system_metrics.get('cache_hit_rate', 0.6),
            system_metrics.get('migration_success_rate', 0.0),
            system_metrics.get('network_utilization', 0.5),
            system_metrics.get('load_balance_index', 0.5),
        ]
        def _extract_metric(key: str) -> List[float]:
            values = system_metrics.get(key, [])
            if isinstance(values, np.ndarray):
                values = values.tolist()
            elif not isinstance(values, (list, tuple)):
                values = []
            values = [float(np.clip(v, 0.0, 1.0)) for v in values[:4]]
            if len(values) < 4:
                values.extend([0.0] * (4 - len(values)))
            return values
        global_state.extend(_extract_metric('task_type_queue_distribution'))
        global_state.extend(_extract_metric('task_type_deadline_remaining'))
        state_components.extend(global_state)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶æ£€æŸ¥NaNå€¼
        state_vector = np.array(state_components, dtype=np.float32)
        
        # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
        if np.any(np.isnan(state_vector)) or np.any(np.isinf(state_vector)):
            print(f"è­¦å‘Š: çŠ¶æ€å‘é‡åŒ…å«NaNæˆ–Infå€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼æ›¿æ¢")
            state_vector = np.nan_to_num(state_vector, nan=0.5, posinf=1.0, neginf=0.0)
        
        return state_vector


class OptimizedTD3Actor(nn.Module):
    """ä¼˜åŒ–çš„TD3 Actorç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 512, max_action: float = 1.0):
        super(OptimizedTD3Actor, self).__init__()
        
        self.max_action = max_action
        
        # æ›´æ·±çš„ç½‘ç»œç»“æ„
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            
            nn.Linear(hidden_dim // 2, action_dim),
            nn.Tanh()
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.0)
        
        # æœ€åä¸€å±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
        nn.init.uniform_(self.network[-2].weight, -3e-3, 3e-3)
        nn.init.uniform_(self.network[-2].bias, -3e-3, 3e-3)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.max_action * self.network(state)


class OptimizedTD3Critic(nn.Module):
    """ä¼˜åŒ–çš„TD3 Twin Criticç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 512):
        super(OptimizedTD3Critic, self).__init__()
        
        # Q1ç½‘ç»œ
        self.q1_network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # Q2ç½‘ç»œ
        self.q2_network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            
            nn.Linear(hidden_dim // 2, 1)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        for network in [self.q1_network, self.q2_network]:
            for layer in network:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    nn.init.constant_(layer.bias, 0.0)
            
            # æœ€åä¸€å±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
            nn.init.uniform_(network[-1].weight, -3e-3, 3e-3)
            nn.init.uniform_(network[-1].bias, -3e-3, 3e-3)
    
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­ - è¿”å›ä¸¤ä¸ªQå€¼"""
        sa = torch.cat([state, action], dim=1)
        
        q1 = self.q1_network(sa)
        q2 = self.q2_network(sa)
        
        return q1, q2
    
    def q1(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """åªè¿”å›Q1å€¼"""
        sa = torch.cat([state, action], dim=1)
        return self.q1_network(sa)


class OptimizedTD3Environment:
    """ä¼˜åŒ–çš„TD3è®­ç»ƒç¯å¢ƒ - å¸¦å¥–åŠ±ç¨³å®šæœºåˆ¶"""
    
    def __init__(self, system_config=None):
        self.config = OptimizedTD3Config()
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´
        self.state_space = VECStateSpace(system_config)
        self.action_space = VECActionSpace()
        
        # ç¯å¢ƒé…ç½®
        self.state_dim = self.state_space.total_dim    # 106ç»´ (ğŸ”§ä¿®å¤å+6ç»´)
        self.action_dim = self.action_space.total_dim  # 120ç»´
        
        # å¥–åŠ±ç¨³å®šæœºåˆ¶
        self.reward_history = deque(maxlen=100)  # å¥–åŠ±å†å²
        self.reward_mean = 0.0
        self.reward_std = 1.0
        self.reward_smoothing = 0.9  # å¹³æ»‘ç³»æ•°
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        from single_agent.td3_optimized_agent import OptimizedTD3Agent
        self.agent = OptimizedTD3Agent(self.state_dim, self.action_dim, self.config)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_count = 0
        self.step_count = 0
        self.prev_metrics = None
        
        print(f"âœ“ ä¼˜åŒ–TD3ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ çŠ¶æ€ç»´åº¦: {self.state_dim}, åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"âœ“ å¥–åŠ±ç¨³å®šæœºåˆ¶å·²å¯ç”¨")
    
    def get_state_vector(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """æ„å»ºçŠ¶æ€å‘é‡"""
        return self.state_space.encode_state(node_states, system_metrics)
    
    def decompose_action(self, action: np.ndarray) -> Dict[str, np.ndarray]:
        """åˆ†è§£åŠ¨ä½œ"""
        return self.action_space.decompose_action(action)
    
    def get_actions(self, state: np.ndarray, training: bool = True) -> Dict[str, np.ndarray]:
        """è·å–åŠ¨ä½œ"""
        global_action = self.agent.select_action(state, training)
        return self.decompose_action(global_action)
    
    def calculate_reward(self, system_metrics: Dict, prev_metrics: Optional[Dict] = None) -> float:
        """è®¡ç®—å¥–åŠ± - åŸºäºæˆæœ¬çš„è´Ÿå¥–åŠ± + å¸è½½æ¿€åŠ±ï¼ˆå¼•å¯¼RSU/UAVå¸è½½ï¼‰"""
        try:
            # æå–åŸå§‹æŒ‡æ ‡
            delay = max(system_metrics.get('avg_task_delay', 2.0), 0.1)
            energy = max(system_metrics.get('total_energy_consumption', 600.0), 100.0)
            completion = np.clip(system_metrics.get('task_completion_rate', 0.95), 0.0, 1.0)
            cache_hit = np.clip(system_metrics.get('cache_hit_rate', 0.85), 0.0, 1.0)
            data_loss = system_metrics.get('data_loss_rate', 0.0)
            
            # ğŸ”§ æ–°å¢ï¼šæå–å¸è½½æ¯”ä¾‹ï¼ˆå¼•å¯¼RSU/UAVå¸è½½ï¼‰
            rsu_ratio = system_metrics.get('rsu_offload_ratio', 0.0)
            uav_ratio = system_metrics.get('uav_offload_ratio', 0.0)
            local_ratio = system_metrics.get('local_offload_ratio', 1.0)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå½’ä¸€åŒ–ä½¿ç”¨åˆç†çš„åŸºå‡†å€¼
            # å»¶è¿Ÿå½’ä¸€åŒ–: ä»¥2.5sä¸ºåŸºå‡†ï¼ˆ12è½¦è¾†åœºæ™¯çš„åˆç†å»¶è¿Ÿï¼‰
            delay_norm = delay / 2.5
            
            # èƒ½è€—å½’ä¸€åŒ–: ä»¥10000Jä¸ºåŸºå‡†ï¼ˆ12è½¦*800J/è½¦ + ä½™é‡ï¼‰
            energy_norm = energy / 10000.0
            
            # å®Œæˆç‡æƒ©ç½š: ä½äº98%æ—¶é¢å¤–æƒ©ç½š
            completion_penalty = max(0, (0.98 - completion)) * 3.0
            
            # æ•°æ®ä¸¢å¤±æƒ©ç½š: æŒ‰å®é™…æ¯”ä¾‹æƒ©ç½š
            loss_penalty = data_loss * 2.0
            
            # ç¼“å­˜å‘½ä¸­ç‡å¥–åŠ±: é«˜å‘½ä¸­ç‡å‡å°‘æˆæœ¬
            cache_bonus = (cache_hit - 0.5) * 0.15  # è¶…è¿‡50%å¼€å§‹æœ‰å¥–åŠ±
            
            # ğŸ‰ å¸è½½å¥–åŠ±æœºåˆ¶ä¼˜åŒ–ï¼šæ˜ç¡®åŒºåˆ†RSU/UAV,è§£å†³åå‘UAV/æœ¬åœ°çš„é—®é¢˜
            # 
            # ã€æ ¸å¿ƒé—®é¢˜ã€‘åŸè®¾è®¡è™½ç„¶RSUç³»æ•°é«˜(25.0),ä½†æ™ºèƒ½ä½“ä»åå‘UAV/æœ¬åœ°,åŸå› :
            # 1. RSUå¸è½½çš„ç´¯ç§¯å»¶è¿Ÿ(ä¸Šä¼ +é˜Ÿåˆ—+å¤„ç†+ä¸‹è½½)å¯èƒ½è¶…è¿‡æœ¬åœ°/UAV
            # 2. RSUé˜Ÿåˆ—å®¹æ˜“æ»¡è½½å¯¼è‡´æ‹’ç»,æ™ºèƒ½ä½“å­¦åˆ°"RSUä¸å¯é "
            # 3. å¥–åŠ±ä¿¡å·ä¼ é€’æ•ˆç‡ä½,éœ€è¦æ›´å¼ºçš„RSUåå¥½å¼•å¯¼
            # 
            # ã€ä¿®å¤ç­–ç•¥ã€‘
            # - æè‡´å¼ºåŒ–RSUå¥–åŠ±:ä»25.0æå‡åˆ°50.0 (ç¿»å€)
            # - é™ä½UAVå¥–åŠ±:ä»3.0é™ä½åˆ°1.5 (é¿å…ä¸RSUç«äº‰)
            # - å¢å¼ºæœ¬åœ°å¤„ç†æƒ©ç½š:ä»10.0æå‡åˆ°15.0 (å¼ºåˆ¶å¸è½½)
            # - æ·»åŠ RSUä¼˜å…ˆå¥–åŠ±:é¢å¤–ç»™äºˆRSUä½¿ç”¨ç‡è¶…è¿‡50%çš„å¥–åŠ±
            
            # RSUå¸è½½å¥–åŠ±ï¼šæ¯1%è·å¾—0.50å¥–åŠ±(æè‡´å¼ºåŒ–)
            rsu_bonus = rsu_ratio * 50.0  # 50%å æ¯”â†’25.0å¥–åŠ±, 60%â†’30.0å¥–åŠ±
            
            # RSUä¼˜å…ˆé¢å¤–å¥–åŠ±ï¼šå½“RSUå æ¯”>50%æ—¶,æ¯è¶…1%é¢å¤–+0.20å¥–åŠ±
            rsu_priority_bonus = max(0, rsu_ratio - 0.5) * 20.0  # å¼•å¯¼>50%å æ¯”
            
            # UAVå¸è½½å¥–åŠ±ï¼šæ¯1%è·å¾—0.15å¥–åŠ±(é™ä½é¿å…ç«äº‰)
            uav_bonus = uav_ratio * 1.5  # 50%å æ¯”â†’0.75å¥–åŠ±
            
            # æœ¬åœ°å¤„ç†æƒ©ç½šï¼šæ¯1%æ‰£é™¤0.15(å¼ºåŒ–æƒ©ç½š)
            local_penalty = local_ratio * 15.0  # 50%å æ¯”â†’æ‰£7.5
            
            # è®¡ç®—æ€»æˆæœ¬ï¼ˆå½’ä¸€åŒ–åçš„åŠ æƒå’Œï¼‰
            cost = (
                1.2 * delay_norm +           # å»¶è¿Ÿæˆæœ¬
                0.8 * energy_norm +          # èƒ½è€—æˆæœ¬
                completion_penalty +         # å®Œæˆç‡æƒ©ç½š
                loss_penalty                 # æ•°æ®ä¸¢å¤±æƒ©ç½š
            )
            
            # ğŸ¯ å¥–åŠ± = -æˆæœ¬ + å…¨éƒ¨å¥–åŠ±ï¼ˆæè‡´å¼ºåŒ–RSUå¸è½½ä¿¡å·ï¼‰
            # ç›®æ ‡: è®©RSUå¸è½½å¥–åŠ±èƒ½å¤Ÿå‹å€’å»¶è¿Ÿ/èƒ½è€—æˆæœ¬,æ˜ç¡®ä¼˜äºUAV/æœ¬åœ°
            # 
            # ã€æœŸæœ›åˆ†å¸ƒå¯¹æ¯”ã€‘
            # âŒ é”™è¯¯åˆ†å¸ƒ(Local 70%, UAV 20%, RSU 10%):
            #    å¥–åŠ± = -3.0 + 0.15(ç¼“å­˜) + 5.0(RSU) + 0.0(ä¼˜å…ˆ) + 0.3(UAV) - 10.5(æœ¬åœ°) = -8.05
            # 
            # âœ… ç›®æ ‡åˆ†å¸ƒ(RSU 60%, UAV 20%, Local 20%):
            #    å¥–åŠ± = -3.0 + 0.15(ç¼“å­˜) + 30.0(RSU) + 2.0(ä¼˜å…ˆ) + 0.3(UAV) - 3.0(æœ¬åœ°) = +26.45
            # 
            # ğŸ¯ æœ€ä¼˜åˆ†å¸ƒ(RSU 70%, UAV 15%, Local 15%):
            #    å¥–åŠ± = -3.0 + 0.15(ç¼“å­˜) + 35.0(RSU) + 4.0(ä¼˜å…ˆ) + 0.225(UAV) - 2.25(æœ¬åœ°) = +34.125
            # 
            # å·®è·: æœ€ä¼˜(+34.1) vs é”™è¯¯(-8.0) = 42.1å·®è·,ä¿¡å·æå¼º!
            reward = -cost + cache_bonus + rsu_bonus + rsu_priority_bonus + uav_bonus - local_penalty
            
            # è£å‰ªåˆ°åˆç†èŒƒå›´ï¼ˆç°åœ¨å¯ä»¥æ˜¯å¾ˆå¤§çš„æ­£å€¼ï¼‰
            # ğŸ”§ ä¿®å¤:æ‰©å¤§ä¸Šé™åˆ°50.0ä»¥å®¹çº³RSUæè‡´å¥–åŠ± (æœ€ä¼˜åˆ†å¸ƒå¯è¾¾+34)
            reward = np.clip(reward, -10.0, 50.0)  # æ‰©å¤§èŒƒå›´ä»¥å®¹çº³æ›´é«˜å¥–åŠ±
            
            return float(reward)
            
        except Exception as e:
            print(f"âš ï¸ å¥–åŠ±è®¡ç®—é”™è¯¯: {e}")
            return -2.5  # é»˜è®¤æƒ©ç½šå€¼
    
    def train_step(self, state: np.ndarray, action: Union[np.ndarray, int], reward: float,
                   next_state: np.ndarray, done: bool) -> Dict:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        # ç¡®ä¿åŠ¨ä½œæ˜¯numpyæ•°ç»„
        if isinstance(action, int):
            action = np.array([action], dtype=np.float32)
        elif not isinstance(action, np.ndarray):
            action = np.array(action, dtype=np.float32)
        
        # å­˜å‚¨ç»éªŒ
        self.agent.store_experience(state, action, reward, next_state, done)
        
        # æ›´æ–°ç½‘ç»œ
        training_info = self.agent.update()
        
        self.step_count += 1
        
        return training_info
    
    def save_models(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(filepath, exist_ok=True)
        self.agent.save_model(filepath)
        print(f"âœ“ ä¼˜åŒ–TD3æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_models(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        self.agent.load_model(filepath)
        print(f"âœ“ ä¼˜åŒ–TD3æ¨¡å‹å·²åŠ è½½: {filepath}")
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        # å°†dequeè½¬æ¢ä¸ºlistä»¥æ”¯æŒåˆ‡ç‰‡æ“ä½œ
        actor_losses_list = list(self.agent.actor_losses) if self.agent.actor_losses else []
        critic_losses_list = list(self.agent.critic_losses) if self.agent.critic_losses else []
        
        return {
            'actor_loss_avg': float(np.mean(actor_losses_list[-100:])) if actor_losses_list else 0.0,
            'critic_loss_avg': float(np.mean(critic_losses_list[-100:])) if critic_losses_list else 0.0,
            'exploration_noise': self.agent.exploration_noise,
            'buffer_size': len(self.agent.replay_buffer),
            'step_count': self.step_count,
            'update_count': self.agent.update_count,
            'policy_delay': self.config.policy_delay,
            'state_dim': self.state_dim,
            'action_dim': self.action_dim,
        }
