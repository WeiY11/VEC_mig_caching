"""
ä¼˜åŒ–çš„TD3ç®—æ³•å®ç° - ä¿®å¤å…³é”®é—®é¢˜
åŸºäºåˆ†ææŠ¥å‘Šçš„æ”¹è¿›ç‰ˆæœ¬

ä¸»è¦æ”¹è¿›ï¼š
1. é‡æ„çŠ¶æ€ç©ºé—´è®¾è®¡
2. é‡æ–°è®¾è®¡åŠ¨ä½œç©ºé—´
3. ä¼˜åŒ–å¥–åŠ±å‡½æ•°
4. ä¿®å¤ç¯å¢ƒäº¤äº’
5. è°ƒæ•´è¶…å‚æ•°é…ç½®
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from collections import deque
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass

from config import config


@dataclass
class OptimizedTD3Config:
    """ä¼˜åŒ–çš„TD3é…ç½® - ä¿®å¤ç‰ˆæœ¬"""
    # ç½‘ç»œç»“æ„
    hidden_dim: int = 256        # é€‚å½“å¢åŠ ç½‘ç»œå®¹é‡ä»¥å­¦ä¹ å¤æ‚ç­–ç•¥
    actor_lr: float = 1e-4       # é™ä½å­¦ä¹ ç‡ï¼Œé¿å…è¿‡å¿«æ”¶æ•›ï¼ˆä»3e-4åˆ°1e-4ï¼‰
    critic_lr: float = 1e-4      # é™ä½å­¦ä¹ ç‡ï¼Œé¿å…è¿‡å¿«æ”¶æ•›
    
    # è®­ç»ƒå‚æ•°
    batch_size: int = 256        # é€‚ä¸­æ‰¹æ¬¡å¤§å°
    buffer_size: int = 500000    # é€‚ä¸­ç¼“å†²åŒº
    tau: float = 0.005           # æ ‡å‡†è½¯æ›´æ–°é€Ÿç‡
    gamma: float = 0.99          # æ ‡å‡†æŠ˜æ‰£å› å­
    
    # TD3ç‰¹æœ‰å‚æ•°
    policy_delay: int = 2        # æ ‡å‡†ç­–ç•¥å»¶è¿Ÿ
    target_noise: float = 0.2    # æ ‡å‡†ç›®æ ‡å™ªå£°
    noise_clip: float = 0.5      # æ ‡å‡†å™ªå£°è£å‰ª
    
    # æ¢ç´¢å‚æ•°
    exploration_noise: float = 0.3   # å¢åŠ åˆå§‹æ¢ç´¢ï¼ˆä»0.2åˆ°0.3ï¼‰
    noise_decay: float = 0.9990      # åŠ å¿«è¡°å‡é€Ÿåº¦ï¼ˆä»0.9995åˆ°0.9990ï¼‰
    min_noise: float = 0.02          # ä¿ç•™æœ€å°æ¢ç´¢ï¼ˆä»0.05é™åˆ°0.02)
    
    # è®­ç»ƒæ§åˆ¶
    warmup_steps: int = 20000        # å¢åŠ é¢„çƒ­æ­¥æ•°,çº¦100ä¸ªepisode
    update_freq: int = 1             # æ¯æ­¥éƒ½æ›´æ–°
    
    # æ­£åˆ™åŒ–å‚æ•°
    weight_decay: float = 1e-5       # æ›´å°çš„L2æ­£åˆ™åŒ–
    grad_clip: float = 1.0           # é€‚åº¦æ¢¯åº¦è£å‰ª


class VECActionSpace:
    """VECç³»ç»ŸåŠ¨ä½œç©ºé—´å®šä¹‰"""
    
    def __init__(self):
        # åŠ¨ä½œç»´åº¦å®šä¹‰
        self.vehicle_actions = 5    # æœ¬åœ°å¤„ç†æ¯”ä¾‹ã€å¸è½½ç›®æ ‡é€‰æ‹©ç­‰
        self.rsu_actions = 8        # è®¡ç®—èµ„æºåˆ†é…ã€ç¼“å­˜ç­–ç•¥ã€è¿ç§»å†³ç­–ç­‰
        self.uav_actions = 6        # è®¡ç®—èµ„æºåˆ†é…ã€ç§»åŠ¨ç­–ç•¥ç­‰
        
        self.num_vehicles = config.network.num_vehicles  # 12
        self.num_rsus = config.network.num_rsus          # 6  
        self.num_uavs = config.network.num_uavs          # 2
        
        self.total_dim = (
            self.num_vehicles * self.vehicle_actions +  # 12 * 5 = 60
            self.num_rsus * self.rsu_actions +          # 6 * 8 = 48
            self.num_uavs * self.uav_actions            # 2 * 6 = 12
        )  # æ€»è®¡ï¼š120ç»´
    
    def decompose_action(self, action: np.ndarray) -> Dict:
        """å°†å…¨å±€åŠ¨ä½œåˆ†è§£ä¸ºå…·ä½“å†³ç­–"""
        actions = {}
        idx = 0
        
        # è½¦è¾†åŠ¨ä½œ
        for i in range(self.num_vehicles):
            vehicle_action = action[idx:idx+self.vehicle_actions]
            actions[f'vehicle_{i}'] = {
                'local_processing_ratio': np.clip(vehicle_action[0], 0, 1),
                'offload_target_rsu': np.argmax(vehicle_action[1:4]) if len(vehicle_action) > 3 else 0,
                'offload_target_uav': int(vehicle_action[4] > 0) if len(vehicle_action) > 4 else 0,
            }
            idx += self.vehicle_actions
        
        # RSUåŠ¨ä½œ
        for i in range(self.num_rsus):
            rsu_action = action[idx:idx+self.rsu_actions]
            actions[f'rsu_{i}'] = {
                'cpu_allocation': np.clip(rsu_action[0], 0.5, 1.0),
                'cache_policy': np.argmax(rsu_action[1:4]),  # LRU/LFU/FIFO
                'migration_threshold': np.clip(rsu_action[4], 0.5, 0.9),
                'bandwidth_allocation': np.clip(rsu_action[5:8], 0.1, 1.0),
            }
            idx += self.rsu_actions
        
        # UAVåŠ¨ä½œ
        for i in range(self.num_uavs):
            uav_action = action[idx:idx+self.uav_actions]
            actions[f'uav_{i}'] = {
                'cpu_allocation': np.clip(uav_action[0], 0.3, 1.0),
                'power_management': np.clip(uav_action[1], 0.5, 1.0),
                'service_priority': np.clip(uav_action[2:6], 0, 1),
            }
            idx += self.uav_actions
        
        return actions


class VECStateSpace:
    """VECç³»ç»ŸçŠ¶æ€ç©ºé—´å®šä¹‰"""
    
    def __init__(self, system_config=None):
        # å¦‚æœæä¾›äº†é…ç½®ï¼Œä½¿ç”¨æä¾›çš„é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        if system_config is not None:
            self.num_vehicles = system_config.network.num_vehicles
            self.num_rsus = system_config.network.num_rsus
            self.num_uavs = system_config.network.num_uavs
        else:
            self.num_vehicles = config.network.num_vehicles
            self.num_rsus = config.network.num_rsus
            self.num_uavs = config.network.num_uavs
        
        # çŠ¶æ€ç»´åº¦è®¡ç®—
        self.vehicle_state_dim = 5  # ä½ç½®x,y + é€Ÿåº¦x,y + é˜Ÿåˆ—åˆ©ç”¨ç‡
        self.rsu_state_dim = 5      # CPUåˆ©ç”¨ç‡ + é˜Ÿåˆ—åˆ©ç”¨ç‡ + ç¼“å­˜åˆ©ç”¨ç‡ + èƒ½è€— + ğŸ”§èµ„æºå®¹é‡
        self.uav_state_dim = 5      # CPUåˆ©ç”¨ç‡ + é˜Ÿåˆ—åˆ©ç”¨ç‡ + ç”µæ± ç”µé‡ + èƒ½è€— + ğŸ”§èµ„æºå®¹é‡
        self.global_state_dim = 16  # å…¨å±€ç³»ç»ŸæŒ‡æ ‡ï¼ˆåŸºç¡€8ç»´ + ä»»åŠ¡ç±»å‹8ç»´ï¼‰
        
        self.total_dim = (
            self.num_vehicles * self.vehicle_state_dim +  # 12 * 5 = 60
            self.num_rsus * self.rsu_state_dim +          # 4 * 5 = 20
            self.num_uavs * self.uav_state_dim +          # 2 * 5 = 10
            self.global_state_dim                         # 16
        )  # æ€»è®¡ï¼š106ç»´ (ğŸ”§ä¿®å¤å‰100ç»´)
    
    def encode_state(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """æ„å»ºç¬¦åˆè®ºæ–‡çš„VECç³»ç»ŸçŠ¶æ€å‘é‡"""
        state_components = []
        
        # 1. è½¦è¾†çŠ¶æ€ (12è½¦è¾† Ã— 5ç»´ = 60ç»´)
        for i in range(self.num_vehicles):
            vehicle_id = f'vehicle_{i}'
            if vehicle_id in node_states:
                vehicle = node_states[vehicle_id]
                vehicle_state = [
                    getattr(vehicle.position, 'x', 0.0) / 2000.0,  # å½’ä¸€åŒ–ä½ç½®
                    getattr(vehicle.position, 'y', 0.0) / 2000.0,
                    getattr(vehicle, 'velocity_x', 0.0) / 30.0,    # å½’ä¸€åŒ–é€Ÿåº¦
                    getattr(vehicle, 'velocity_y', 0.0) / 30.0,
                    getattr(vehicle, 'queue_utilization', 0.5),    # é˜Ÿåˆ—åˆ©ç”¨ç‡
                ]
            else:
                # é»˜è®¤çŠ¶æ€
                vehicle_state = [0.5, 0.5, 0.0, 0.0, 0.5]
            state_components.extend(vehicle_state)
        
        # 2. RSUçŠ¶æ€ (æŒ‰é…ç½®æ•°é‡ Ã— 5ç»´)
        for i in range(self.num_rsus):
            rsu_id = f'rsu_{i}'
            if rsu_id in node_states:
                rsu = node_states[rsu_id]
                # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ cpu_frequencyè®©æ™ºèƒ½ä½“çŸ¥é“RSUå®¹é‡ä¼˜åŠ¿
                cpu_freq = getattr(rsu, 'cpu_frequency', 12.5e9)  # é»˜è®¤12.5 GHz
                rsu_state = [
                    getattr(rsu, 'cpu_utilization', 0.5),         # CPUåˆ©ç”¨ç‡
                    getattr(rsu, 'queue_utilization', 0.5),       # é˜Ÿåˆ—åˆ©ç”¨ç‡
                    getattr(rsu, 'cache_utilization', 0.5),       # ç¼“å­˜åˆ©ç”¨ç‡
                    getattr(rsu, 'energy_consumption', 500.0) / 1000.0,  # å½’ä¸€åŒ–èƒ½è€—
                    cpu_freq / 20e9,  # ğŸ”§ æ–°å¢ï¼šå½’ä¸€åŒ–CPUå®¹é‡ï¼ˆ20GHzæœ€å¤§å€¼ï¼‰
                ]
            else:
                rsu_state = [0.5, 0.5, 0.5, 0.5, 0.625]  # é»˜è®¤12.5/20=0.625
            state_components.extend(rsu_state)
        
        # 3. UAVçŠ¶æ€ (æŒ‰é…ç½®æ•°é‡ Ã— 5ç»´)
        for i in range(self.num_uavs):
            uav_id = f'uav_{i}'
            if uav_id in node_states:
                uav = node_states[uav_id]
                # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ cpu_frequencyè®©æ™ºèƒ½ä½“çŸ¥é“UAVå®¹é‡è¾ƒå¼±
                cpu_freq = getattr(uav, 'cpu_frequency', 5.0e9)  # é»˜è®¤5.0 GHz
                uav_state = [
                    getattr(uav, 'cpu_utilization', 0.5),
                    getattr(uav, 'queue_utilization', 0.5),
                    getattr(uav, 'battery_level', 0.8),           # ç”µæ± ç”µé‡
                    getattr(uav, 'energy_consumption', 50.0) / 100.0,
                    cpu_freq / 20e9,  # ğŸ”§ æ–°å¢ï¼šå½’ä¸€åŒ–CPUå®¹é‡ï¼ˆ20GHzæœ€å¤§å€¼ï¼‰
                ]
            else:
                uav_state = [0.5, 0.5, 0.8, 0.5, 0.25]  # é»˜è®¤5.0/20=0.25
            state_components.extend(uav_state)
        
        # 4. å…¨å±€ç³»ç»ŸçŠ¶æ€ (8ç»´)
        global_state = [
            system_metrics.get('avg_task_delay', 1.0) / 2.0,
            system_metrics.get('total_energy_consumption', 2500.0) / 5000.0,
            system_metrics.get('data_loss_rate', 0.1),
            system_metrics.get('task_completion_rate', 0.8),
            system_metrics.get('cache_hit_rate', 0.6),
            system_metrics.get('migration_success_rate', 0.0),
            system_metrics.get('network_utilization', 0.5),
            system_metrics.get('load_balance_index', 0.5),
        ]
        def _extract_metric(key: str) -> List[float]:
            values = system_metrics.get(key, [])
            if isinstance(values, np.ndarray):
                values = values.tolist()
            elif not isinstance(values, (list, tuple)):
                values = []
            values = [float(np.clip(v, 0.0, 1.0)) for v in values[:4]]
            if len(values) < 4:
                values.extend([0.0] * (4 - len(values)))
            return values
        global_state.extend(_extract_metric('task_type_queue_distribution'))
        global_state.extend(_extract_metric('task_type_deadline_remaining'))
        state_components.extend(global_state)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶æ£€æŸ¥NaNå€¼
        state_vector = np.array(state_components, dtype=np.float32)
        
        # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
        if np.any(np.isnan(state_vector)) or np.any(np.isinf(state_vector)):
            print(f"è­¦å‘Š: çŠ¶æ€å‘é‡åŒ…å«NaNæˆ–Infå€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼æ›¿æ¢")
            state_vector = np.nan_to_num(state_vector, nan=0.5, posinf=1.0, neginf=0.0)
        
        return state_vector


class OptimizedTD3Actor(nn.Module):
    """ä¼˜åŒ–çš„TD3 Actorç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 512, max_action: float = 1.0):
        super(OptimizedTD3Actor, self).__init__()
        
        self.max_action = max_action
        
        # æ›´æ·±çš„ç½‘ç»œç»“æ„
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            
            nn.Linear(hidden_dim // 2, action_dim),
            nn.Tanh()
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.0)
        
        # æœ€åä¸€å±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
        nn.init.uniform_(self.network[-2].weight, -3e-3, 3e-3)
        nn.init.uniform_(self.network[-2].bias, -3e-3, 3e-3)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.max_action * self.network(state)


class OptimizedTD3Critic(nn.Module):
    """ä¼˜åŒ–çš„TD3 Twin Criticç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 512):
        super(OptimizedTD3Critic, self).__init__()
        
        # Q1ç½‘ç»œ
        self.q1_network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # Q2ç½‘ç»œ
        self.q2_network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            
            nn.Linear(hidden_dim // 2, 1)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        for network in [self.q1_network, self.q2_network]:
            for layer in network:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    nn.init.constant_(layer.bias, 0.0)
            
            # æœ€åä¸€å±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
            nn.init.uniform_(network[-1].weight, -3e-3, 3e-3)
            nn.init.uniform_(network[-1].bias, -3e-3, 3e-3)
    
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­ - è¿”å›ä¸¤ä¸ªQå€¼"""
        sa = torch.cat([state, action], dim=1)
        
        q1 = self.q1_network(sa)
        q2 = self.q2_network(sa)
        
        return q1, q2
    
    def q1(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """åªè¿”å›Q1å€¼"""
        sa = torch.cat([state, action], dim=1)
        return self.q1_network(sa)


class OptimizedTD3Environment:
    """ä¼˜åŒ–çš„TD3è®­ç»ƒç¯å¢ƒ - å¸¦å¥–åŠ±ç¨³å®šæœºåˆ¶"""
    
    def __init__(self, system_config=None):
        self.config = OptimizedTD3Config()
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´
        self.state_space = VECStateSpace(system_config)
        self.action_space = VECActionSpace()
        
        # ç¯å¢ƒé…ç½®
        self.state_dim = self.state_space.total_dim    # 106ç»´ (ğŸ”§ä¿®å¤å+6ç»´)
        self.action_dim = self.action_space.total_dim  # 120ç»´
        
        # å¥–åŠ±ç¨³å®šæœºåˆ¶
        self.reward_history = deque(maxlen=100)  # å¥–åŠ±å†å²
        self.reward_mean = 0.0
        self.reward_std = 1.0
        self.reward_smoothing = 0.9  # å¹³æ»‘ç³»æ•°
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        from single_agent.td3_optimized_agent import OptimizedTD3Agent
        self.agent = OptimizedTD3Agent(self.state_dim, self.action_dim, self.config)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_count = 0
        self.step_count = 0
        self.prev_metrics = None
        
        print(f"âœ“ ä¼˜åŒ–TD3ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ çŠ¶æ€ç»´åº¦: {self.state_dim}, åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"âœ“ å¥–åŠ±ç¨³å®šæœºåˆ¶å·²å¯ç”¨")
    
    def get_state_vector(self, node_states: Dict, system_metrics: Dict) -> np.ndarray:
        """æ„å»ºçŠ¶æ€å‘é‡"""
        return self.state_space.encode_state(node_states, system_metrics)
    
    def decompose_action(self, action: np.ndarray) -> Dict[str, np.ndarray]:
        """åˆ†è§£åŠ¨ä½œ"""
        return self.action_space.decompose_action(action)
    
    def get_actions(self, state: np.ndarray, training: bool = True) -> Dict[str, np.ndarray]:
        """è·å–åŠ¨ä½œ"""
        global_action = self.agent.select_action(state, training)
        return self.decompose_action(global_action)
    
    def calculate_reward(self, system_metrics: Dict, prev_metrics: Optional[Dict] = None) -> float:
        """è®¡ç®—å¥–åŠ± - ä¿®å¤ç‰ˆæœ¬(åŸºäºæˆæœ¬çš„è´Ÿå¥–åŠ±)"""
        try:
            # æå–åŸå§‹æŒ‡æ ‡
            delay = max(system_metrics.get('avg_task_delay', 2.0), 0.1)
            energy = max(system_metrics.get('total_energy_consumption', 600.0), 100.0)
            completion = np.clip(system_metrics.get('task_completion_rate', 0.95), 0.0, 1.0)
            cache_hit = np.clip(system_metrics.get('cache_hit_rate', 0.85), 0.0, 1.0)
            data_loss = system_metrics.get('data_loss_rate', 0.0)
            
            # ä½¿ç”¨å®é™…å€¼èŒƒå›´è¿›è¡Œå½’ä¸€åŒ–(åŸºäºè§‚å¯Ÿåˆ°çš„è®­ç»ƒæ•°æ®)
            # å»¶è¿Ÿ: 0.5-4.0s -> å½’ä¸€åŒ–åˆ°[0, 1]
            delay_norm = np.clip((delay - 0.5) / 3.5, 0.0, 1.0)
            
            # èƒ½è€—: 500-3000J -> å½’ä¸€åŒ–åˆ°[0, 1] (ä¿®æ­£ä¸ºå®é™…èŒƒå›´)
            energy_norm = np.clip((energy - 500.0) / 2500.0, 0.0, 1.0)
            
            # æ•°æ®æŸå¤±ç‡: 0-0.6 -> å½’ä¸€åŒ–åˆ°[0, 1]
            loss_norm = np.clip(data_loss / 0.6, 0.0, 1.0)
            
            # å®Œæˆç‡æƒ©ç½š: ä½äº95%æ—¶ç»™äºˆé¢å¤–æƒ©ç½š
            completion_penalty = 0.0
            if completion < 0.95:
                completion_penalty = (0.95 - completion) * 5.0  # æ¯é™ä½1%æƒ©ç½š0.05
            
            # ç¼“å­˜å‘½ä¸­ç‡: 0.0-0.8 -> å½’ä¸€åŒ–åˆ°[0, 1]
            cache_norm = np.clip(cache_hit / 0.8, 0.0, 1.0)
            cache_bonus = (cache_norm - 0.5) * 0.2  # è¶…è¿‡40%æ‰æœ‰å¥–åŠ±,å¦åˆ™æƒ©ç½š
            
            # è®¡ç®—æ€»æˆæœ¬(å…¨æ˜¯æƒ©ç½šé¡¹)
            cost = (
                1.5 * delay_norm +           # å»¶è¿Ÿæˆæœ¬
                1.0 * energy_norm +          # èƒ½è€—æˆæœ¬
                2.0 * loss_norm +            # æ•°æ®æŸå¤±æˆæœ¬(æœ€é‡è¦)
                completion_penalty           # å®Œæˆç‡æƒ©ç½š
            )
            
            # å¥–åŠ± = -æˆæœ¬ + å°é¢ç¼“å­˜å¥–åŠ±
            reward = -cost + cache_bonus
            
            # è£å‰ªåˆ°åˆç†èŒƒå›´(æ°¸è¿œæ˜¯è´Ÿæ•°æˆ–æ¥è¿‘0)
            reward = np.clip(reward, -5.0, 0.1)
            
            # é™¤ä»¥æ¯ä¸ªepisodeæ­¥æ•°,å¾—åˆ°per-stepå¥–åŠ±
            reward = reward / 200.0  # max_steps_per_episode=200
            
            return float(reward)
            
        except Exception as e:
            print(f"âš ï¸ å¥–åŠ±è®¡ç®—é”™è¯¯: {e}")
            return -0.025  # é»˜è®¤æƒ©ç½šå€¼
    
    def train_step(self, state: np.ndarray, action: Union[np.ndarray, int], reward: float,
                   next_state: np.ndarray, done: bool) -> Dict:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        # ç¡®ä¿åŠ¨ä½œæ˜¯numpyæ•°ç»„
        if isinstance(action, int):
            action = np.array([action], dtype=np.float32)
        elif not isinstance(action, np.ndarray):
            action = np.array(action, dtype=np.float32)
        
        # å­˜å‚¨ç»éªŒ
        self.agent.store_experience(state, action, reward, next_state, done)
        
        # æ›´æ–°ç½‘ç»œ
        training_info = self.agent.update()
        
        self.step_count += 1
        
        return training_info
    
    def save_models(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(filepath, exist_ok=True)
        self.agent.save_model(filepath)
        print(f"âœ“ ä¼˜åŒ–TD3æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_models(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        self.agent.load_model(filepath)
        print(f"âœ“ ä¼˜åŒ–TD3æ¨¡å‹å·²åŠ è½½: {filepath}")
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        # å°†dequeè½¬æ¢ä¸ºlistä»¥æ”¯æŒåˆ‡ç‰‡æ“ä½œ
        actor_losses_list = list(self.agent.actor_losses) if self.agent.actor_losses else []
        critic_losses_list = list(self.agent.critic_losses) if self.agent.critic_losses else []
        
        return {
            'actor_loss_avg': float(np.mean(actor_losses_list[-100:])) if actor_losses_list else 0.0,
            'critic_loss_avg': float(np.mean(critic_losses_list[-100:])) if critic_losses_list else 0.0,
            'exploration_noise': self.agent.exploration_noise,
            'buffer_size': len(self.agent.replay_buffer),
            'step_count': self.step_count,
            'update_count': self.agent.update_count,
            'policy_delay': self.config.policy_delay,
            'state_dim': self.state_dim,
            'action_dim': self.action_dim,
        }
