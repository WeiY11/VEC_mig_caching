#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®éªŒæ•°æ®éªŒè¯æ¨¡å—
ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„åˆç†æ€§ï¼Œæ£€æµ‹å¼‚å¸¸å€¼å’Œé”™è¯¯æ•°æ®
"""

import numpy as np
import json
import logging
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from enum import Enum


class ValidationLevel(Enum):
    """éªŒè¯çº§åˆ«"""
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"


@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœ"""
    level: ValidationLevel
    metric_name: str
    value: float
    expected_range: Tuple[float, float]
    message: str
    suggestion: str = ""


class SystemMetricsValidator:
    """ç³»ç»ŸæŒ‡æ ‡éªŒè¯å™¨"""
    
    def __init__(self):
        # å®šä¹‰å„é¡¹æŒ‡æ ‡çš„åˆç†èŒƒå›´
        self.metric_ranges = {
            # å»¶è¿Ÿç›¸å…³ (ç§’)
            'avg_task_delay': (0.001, 10.0),
            'max_task_delay': (0.001, 30.0),
            'transmission_delay': (0.0001, 5.0),
            'processing_delay': (0.001, 20.0),
            'waiting_delay': (0.0, 15.0),
            
            # èƒ½è€—ç›¸å…³ (ç„¦è€³)
            'total_energy_consumption': (0.0, 5000.0),
            'avg_energy_per_task': (0.1, 500.0),
            'vehicle_energy': (0.0, 1000.0),
            'rsu_energy': (0.0, 2000.0),
            'uav_energy': (0.0, 800.0),
            
            # ç‡ç±»æŒ‡æ ‡ (0-1)
            'task_completion_rate': (0.0, 1.0),
            'cache_hit_rate': (0.0, 1.0),
            'data_loss_rate': (0.0, 1.0),
            'migration_success_rate': (0.0, 1.0),
            'delay_violation_rate': (0.0, 1.0),
            
            # åˆ©ç”¨ç‡æŒ‡æ ‡ (0-1)
            'cpu_utilization': (0.0, 1.0),
            'bandwidth_utilization': (0.0, 1.0),
            'cache_utilization': (0.0, 1.0),
            'queue_utilization': (0.0, 1.0),
            
            # ç”µæ± ç›¸å…³ (0-1)
            'avg_uav_battery': (0.0, 1.0),
            'min_uav_battery': (0.0, 1.0),
            
            # è´Ÿè½½ç›¸å…³
            'load_factor': (0.0, 0.99),
            'queue_length': (0, 1000),
            'system_load_ratio': (0.0, 2.0),
        }
        
        # å®šä¹‰æŒ‡æ ‡é—´çš„é€»è¾‘å…³ç³»
        self.logical_constraints = [
            ('task_completion_rate', 'data_loss_rate', 'completion_loss_consistency'),
            ('cache_hit_rate', 'avg_task_delay', 'cache_delay_correlation'),
            ('cpu_utilization', 'total_energy_consumption', 'utilization_energy_correlation'),
        ]
        
        # å†å²æ•°æ®ç”¨äºè¶‹åŠ¿åˆ†æ
        self.history_buffer = []
        self.max_history_size = 100
        
        # å¼‚å¸¸æ£€æµ‹å‚æ•°
        self.outlier_threshold = 3.0  # æ ‡å‡†å·®å€æ•°
        
    def validate_single_metric(self, metric_name: str, value: Any) -> List[ValidationResult]:
        """
        éªŒè¯å•ä¸ªæŒ‡æ ‡
        
        Args:
            metric_name: æŒ‡æ ‡åç§°
            value: æŒ‡æ ‡å€¼
            
        Returns:
            éªŒè¯ç»“æœåˆ—è¡¨
        """
        results = []
        
        # åŸºæœ¬ç±»å‹æ£€æŸ¥
        if not isinstance(value, (int, float)):
            results.append(ValidationResult(
                level=ValidationLevel.ERROR,
                metric_name=metric_name,
                value=0.0,
                expected_range=(0.0, 0.0),
                message=f"æŒ‡æ ‡ {metric_name} çš„å€¼ç±»å‹é”™è¯¯: {type(value)}",
                suggestion="æ£€æŸ¥æ•°æ®ç”Ÿæˆé€»è¾‘ï¼Œç¡®ä¿è¿”å›æ•°å€¼ç±»å‹"
            ))
            return results
        
        # æ•°å€¼æœ‰æ•ˆæ€§æ£€æŸ¥
        if not np.isfinite(value):
            level = ValidationLevel.CRITICAL if np.isnan(value) else ValidationLevel.ERROR
            results.append(ValidationResult(
                level=level,
                metric_name=metric_name,
                value=float(value),
                expected_range=(0.0, 0.0),
                message=f"æŒ‡æ ‡ {metric_name} åŒ…å«æ— æ•ˆå€¼: {value}",
                suggestion="æ£€æŸ¥è®¡ç®—é€»è¾‘ä¸­çš„é™¤é›¶é”™è¯¯æˆ–æ•°å€¼æº¢å‡º"
            ))
            return results
        
        # èŒƒå›´æ£€æŸ¥
        if metric_name in self.metric_ranges:
            min_val, max_val = self.metric_ranges[metric_name]
            if not (min_val <= value <= max_val):
                level = ValidationLevel.ERROR if value < 0 else ValidationLevel.WARNING
                results.append(ValidationResult(
                    level=level,
                    metric_name=metric_name,
                    value=float(value),
                    expected_range=(min_val, max_val),
                    message=f"æŒ‡æ ‡ {metric_name} è¶…å‡ºåˆç†èŒƒå›´: {value} (æœŸæœ›: {min_val}-{max_val})",
                    suggestion=self._get_range_suggestion(metric_name, value, min_val, max_val)
                ))
        
        return results
    
    def validate_system_metrics(self, metrics: Dict) -> List[ValidationResult]:
        """
        éªŒè¯å®Œæ•´çš„ç³»ç»ŸæŒ‡æ ‡
        
        Args:
            metrics: ç³»ç»ŸæŒ‡æ ‡å­—å…¸
            
        Returns:
            éªŒè¯ç»“æœåˆ—è¡¨
        """
        results = []
        
        # éªŒè¯æ¯ä¸ªæŒ‡æ ‡
        for metric_name, value in metrics.items():
            results.extend(self.validate_single_metric(metric_name, value))
        
        # éªŒè¯æŒ‡æ ‡é—´çš„é€»è¾‘å…³ç³»
        results.extend(self._validate_logical_constraints(metrics))
        
        # å¼‚å¸¸å€¼æ£€æµ‹
        results.extend(self._detect_outliers(metrics))
        
        # æ›´æ–°å†å²è®°å½•
        self._update_history(metrics)
        
        return results
    
    def _validate_logical_constraints(self, metrics: Dict) -> List[ValidationResult]:
        """éªŒè¯æŒ‡æ ‡é—´çš„é€»è¾‘çº¦æŸ"""
        results = []
        
        # å®Œæˆç‡ä¸ä¸¢å¤±ç‡çš„ä¸€è‡´æ€§æ£€æŸ¥
        completion_rate = metrics.get('task_completion_rate', 0.0)
        loss_rate = metrics.get('data_loss_rate', 0.0)
        if completion_rate + loss_rate > 1.1:  # å…è®¸å°è¯¯å·®
            results.append(ValidationResult(
                level=ValidationLevel.WARNING,
                metric_name='completion_loss_consistency',
                value=completion_rate + loss_rate,
                expected_range=(0.0, 1.0),
                message=f"å®Œæˆç‡({completion_rate:.3f})ä¸ä¸¢å¤±ç‡({loss_rate:.3f})ä¹‹å’Œè¶…è¿‡1",
                suggestion="æ£€æŸ¥ä»»åŠ¡ç»Ÿè®¡é€»è¾‘ï¼Œç¡®ä¿åˆ†ç±»äº’æ–¥ä¸”å®Œæ•´"
            ))
        
        # ç¼“å­˜å‘½ä¸­ç‡ä¸å»¶è¿Ÿçš„ç›¸å…³æ€§æ£€æŸ¥
        cache_hit_rate = metrics.get('cache_hit_rate', 0.0)
        avg_delay = metrics.get('avg_task_delay', 0.0)
        if cache_hit_rate > 0.8 and avg_delay > 2.0:
            results.append(ValidationResult(
                level=ValidationLevel.INFO,
                metric_name='cache_delay_correlation',
                value=avg_delay,
                expected_range=(0.0, 1.0),
                message=f"é«˜ç¼“å­˜å‘½ä¸­ç‡({cache_hit_rate:.3f})ä½†å»¶è¿Ÿè¾ƒé«˜({avg_delay:.3f}s)",
                suggestion="æ£€æŸ¥ç¼“å­˜ç­–ç•¥å®ç°æˆ–å»¶è¿Ÿè®¡ç®—é€»è¾‘"
            ))
        
        return results
    
    def _detect_outliers(self, metrics: Dict) -> List[ValidationResult]:
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        results = []
        
        if len(self.history_buffer) < 10:
            return results  # å†å²æ•°æ®ä¸è¶³ï¼Œè·³è¿‡å¼‚å¸¸æ£€æµ‹
        
        for metric_name, current_value in metrics.items():
            if not isinstance(current_value, (int, float)) or not np.isfinite(current_value):
                continue
            
            # è·å–å†å²æ•°æ®
            historical_values = [h.get(metric_name, current_value) for h in self.history_buffer[-20:]]
            historical_values = [v for v in historical_values if isinstance(v, (int, float)) and np.isfinite(v)]
            
            if len(historical_values) < 5:
                continue
            
            # è®¡ç®—ç»Ÿè®¡é‡
            mean_val = np.mean(historical_values)
            std_val = np.std(historical_values)
            
            if std_val > 1e-6:  # é¿å…é™¤é›¶
                z_score = abs(current_value - mean_val) / std_val
                if z_score > self.outlier_threshold:
                    results.append(ValidationResult(
                        level=ValidationLevel.WARNING,
                        metric_name=metric_name,
                        value=float(current_value),
                        expected_range=(mean_val - 2*std_val, mean_val + 2*std_val),
                        message=f"æŒ‡æ ‡ {metric_name} å¯èƒ½æ˜¯å¼‚å¸¸å€¼: {current_value:.4f} (Z-score: {z_score:.2f})",
                        suggestion="æ£€æŸ¥è¯¥æ—¶æ­¥çš„ç‰¹æ®Šæƒ…å†µæˆ–æ•°æ®è®°å½•é”™è¯¯"
                    ))
        
        return results
    
    def _update_history(self, metrics: Dict):
        """æ›´æ–°å†å²è®°å½•"""
        self.history_buffer.append(metrics.copy())
        if len(self.history_buffer) > self.max_history_size:
            self.history_buffer.pop(0)
    
    def _get_range_suggestion(self, metric_name: str, value: float, 
                            min_val: float, max_val: float) -> str:
        """è·å–èŒƒå›´é”™è¯¯çš„å»ºè®®"""
        if value < min_val:
            if 'rate' in metric_name or 'ratio' in metric_name:
                return "æ£€æŸ¥åˆ†æ¯æ˜¯å¦ä¸ºé›¶æˆ–è®¡ç®—é€»è¾‘é”™è¯¯"
            elif 'delay' in metric_name:
                return "æ£€æŸ¥æ—¶é—´è®¡ç®—æ˜¯å¦æ­£ç¡®ï¼Œå¯èƒ½å­˜åœ¨è´Ÿå€¼"
            elif 'energy' in metric_name:
                return "æ£€æŸ¥èƒ½è€—è®¡ç®—é€»è¾‘ï¼Œè´Ÿèƒ½è€—ä¸åˆç†"
            else:
                return "æ£€æŸ¥è®¡ç®—é€»è¾‘ï¼Œç¡®ä¿ç»“æœéè´Ÿ"
        else:
            if 'rate' in metric_name or 'ratio' in metric_name:
                return "æ£€æŸ¥åˆ†å­æ˜¯å¦è¶…è¿‡åˆ†æ¯ï¼Œæˆ–è®¡ç®—å…¬å¼é”™è¯¯"
            elif 'delay' in metric_name:
                return "æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä»»åŠ¡å †ç§¯æˆ–å¤„ç†èƒ½åŠ›ä¸è¶³"
            elif 'energy' in metric_name:
                return "æ£€æŸ¥èƒ½è€—æ¨¡å‹å‚æ•°ï¼Œå¯èƒ½å­˜åœ¨æ•°å€¼è¿‡å¤§"
            else:
                return "æ£€æŸ¥æ•°å€¼è®¡ç®—æ˜¯å¦åˆç†ï¼Œå¯èƒ½å­˜åœ¨ç´¯ç§¯é”™è¯¯"
    
    def generate_validation_report(self, results: List[ValidationResult]) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        if not results:
            return "âœ… æ‰€æœ‰æŒ‡æ ‡éªŒè¯é€šè¿‡ï¼Œæ•°æ®è´¨é‡è‰¯å¥½"
        
        # æŒ‰çº§åˆ«åˆ†ç»„
        by_level = {}
        for result in results:
            level = result.level.value
            if level not in by_level:
                by_level[level] = []
            by_level[level].append(result)
        
        report = "ğŸ“Š æ•°æ®éªŒè¯æŠ¥å‘Š\n"
        report += "=" * 50 + "\n"
        
        for level in ['CRITICAL', 'ERROR', 'WARNING', 'INFO']:
            if level in by_level:
                icon = {'CRITICAL': 'ğŸ”´', 'ERROR': 'âŒ', 'WARNING': 'âš ï¸', 'INFO': 'â„¹ï¸'}[level]
                report += f"\n{icon} {level} ({len(by_level[level])} é¡¹):\n"
                
                for result in by_level[level]:
                    report += f"  â€¢ {result.metric_name}: {result.message}\n"
                    if result.suggestion:
                        report += f"    å»ºè®®: {result.suggestion}\n"
        
        return report


class ExperimentDataValidator:
    """å®éªŒæ•°æ®éªŒè¯å™¨"""
    
    def __init__(self):
        self.metrics_validator = SystemMetricsValidator()
        self.training_validator = TrainingDataValidator()
        
    def validate_experiment_results(self, results_file: str) -> Dict:
        """
        éªŒè¯å®éªŒç»“æœæ–‡ä»¶
        
        Args:
            results_file: ç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            return {
                'status': 'ERROR',
                'message': f"æ— æ³•è¯»å–ç»“æœæ–‡ä»¶: {e}",
                'validation_results': []
            }
        
        validation_results = []
        
        # éªŒè¯è®­ç»ƒç»“æœ
        if 'episode_metrics' in data:
            for episode, metrics in enumerate(data['episode_metrics']):
                if isinstance(metrics, dict):
                    results = self.metrics_validator.validate_system_metrics(metrics)
                    validation_results.extend(results)
        
        # éªŒè¯æœ€ç»ˆæ€§èƒ½
        if 'final_performance' in data:
            results = self.metrics_validator.validate_system_metrics(data['final_performance'])
            validation_results.extend(results)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.metrics_validator.generate_validation_report(validation_results)
        
        return {
            'status': 'PASS' if not any(r.level in [ValidationLevel.ERROR, ValidationLevel.CRITICAL] 
                                      for r in validation_results) else 'FAIL',
            'message': 'æ•°æ®éªŒè¯å®Œæˆ',
            'validation_results': validation_results,
            'report': report,
            'total_issues': len(validation_results),
            'critical_issues': len([r for r in validation_results if r.level == ValidationLevel.CRITICAL]),
            'error_issues': len([r for r in validation_results if r.level == ValidationLevel.ERROR])
        }


class TrainingDataValidator:
    """è®­ç»ƒæ•°æ®éªŒè¯å™¨"""
    
    def __init__(self):
        self.reward_range = (-20.0, 20.0)
        self.convergence_threshold = 0.1
        
    def validate_training_convergence(self, rewards: List[float]) -> ValidationResult:
        """éªŒè¯è®­ç»ƒæ”¶æ•›æ€§"""
        if len(rewards) < 10:
            return ValidationResult(
                level=ValidationLevel.WARNING,
                metric_name='training_convergence',
                value=0.0,
                expected_range=(0.0, 1.0),
                message="è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ¤æ–­æ”¶æ•›æ€§",
                suggestion="å¢åŠ è®­ç»ƒè½®æ¬¡æˆ–æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹"
            )
        
        # è®¡ç®—æœ€å20%æ•°æ®çš„æ–¹å·®
        recent_rewards = rewards[-max(10, len(rewards)//5):]
        variance = np.var(recent_rewards)
        
        if variance > self.convergence_threshold:
            return ValidationResult(
                level=ValidationLevel.WARNING,
                metric_name='training_convergence',
                value=variance,
                expected_range=(0.0, self.convergence_threshold),
                message=f"è®­ç»ƒå¯èƒ½æœªæ”¶æ•›ï¼Œæœ€è¿‘å¥–åŠ±æ–¹å·®: {variance:.4f}",
                suggestion="è€ƒè™‘å¢åŠ è®­ç»ƒè½®æ¬¡æˆ–è°ƒæ•´å­¦ä¹ ç‡"
            )
        
        return ValidationResult(
            level=ValidationLevel.INFO,
            metric_name='training_convergence',
            value=variance,
            expected_range=(0.0, self.convergence_threshold),
            message=f"è®­ç»ƒæ”¶æ•›è‰¯å¥½ï¼Œå¥–åŠ±æ–¹å·®: {variance:.4f}",
            suggestion=""
        )


# å…¨å±€éªŒè¯å™¨å®ä¾‹
experiment_validator = ExperimentDataValidator()


def validate_system_metrics(metrics: Dict) -> List[ValidationResult]:
    """
    éªŒè¯ç³»ç»ŸæŒ‡æ ‡çš„ä¾¿æ·æ¥å£
    
    Args:
        metrics: ç³»ç»ŸæŒ‡æ ‡å­—å…¸
        
    Returns:
        éªŒè¯ç»“æœåˆ—è¡¨
    """
    validator = SystemMetricsValidator()
    return validator.validate_system_metrics(metrics)


def quick_validate(metrics: Dict) -> str:
    """
    å¿«é€ŸéªŒè¯å¹¶è¿”å›ç®€è¦æŠ¥å‘Š
    
    Args:
        metrics: ç³»ç»ŸæŒ‡æ ‡å­—å…¸
        
    Returns:
        ç®€è¦éªŒè¯æŠ¥å‘Š
    """
    results = validate_system_metrics(metrics)
    if not results:
        return "âœ… æ•°æ®éªŒè¯é€šè¿‡"
    
    critical_count = len([r for r in results if r.level == ValidationLevel.CRITICAL])
    error_count = len([r for r in results if r.level == ValidationLevel.ERROR])
    warning_count = len([r for r in results if r.level == ValidationLevel.WARNING])
    
    if critical_count > 0:
        return f"ğŸ”´ ä¸¥é‡é—®é¢˜: {critical_count} é¡¹"
    elif error_count > 0:
        return f"âŒ é”™è¯¯: {error_count} é¡¹"
    elif warning_count > 0:
        return f"âš ï¸ è­¦å‘Š: {warning_count} é¡¹"
    else:
        return "â„¹ï¸ ä¿¡æ¯æç¤º"