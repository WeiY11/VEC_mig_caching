#!/usr/bin/env python3
"""
ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ï¼Œä¾›æ‰€æœ‰å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•ä½¿ç”¨ã€‚

æ ¸å¿ƒç†å¿µæ˜¯æˆæœ¬æœ€å°åŒ–ï¼šæ›´ä½çš„å»¶è¿Ÿå’Œæ›´ä½çš„èƒ½è€—ä¼šå¸¦æ¥æ›´é«˜ï¼ˆæ›´å°‘è´Ÿå€¼ï¼‰çš„å¥–åŠ±ã€‚
æŸäº›ç®—æ³•ï¼ˆå¦‚SACï¼‰æœŸæœ›æ­£å‘å¥–åŠ±ï¼Œå› æ­¤æˆ‘ä»¬ä¸ºè¿™ç§æƒ…å†µä¿ç•™äº†ä¸€ä¸ªå°çš„å¯é€‰å¥–åŠ±ã€‚

Unified reward calculator used by all single-agent RL algorithms.
The philosophy is cost-minimisation: lower latency and lower energy
lead to higher (less negative) rewards. Some algorithms (SAC) expect
positive rewards, so we keep a small optional bonus for that case.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, Optional, List

import numpy as np

from config import config


class RunningMeanStd:
    """Tracks the running mean and variance of a stream of data."""
    def __init__(self, epsilon=1e-4, shape=()):
        self.mean = np.zeros(shape, 'float64')
        self.var = np.ones(shape, 'float64')
        self.count = epsilon

    def update(self, x):
        if np.isscalar(x) or (isinstance(x, np.ndarray) and x.ndim == 0):
            batch_mean = float(x)
            batch_var = 0.0
            batch_count = 1
        else:
            batch_mean = np.mean(x, axis=0)
            batch_var = np.var(x, axis=0)
            batch_count = x.shape[0]
        self.update_from_moments(batch_mean, batch_var, batch_count)

    def update_from_moments(self, batch_mean, batch_var, batch_count):
        self.mean, self.var, self.count = self.update_mean_var_count_from_moments(
            self.mean, self.var, self.count, batch_mean, batch_var, batch_count
        )

    def update_mean_var_count_from_moments(self, mean, var, count, batch_mean, batch_var, batch_count):
        delta = batch_mean - mean
        tot_count = count + batch_count

        new_mean = mean + delta * batch_count / tot_count
        m_a = var * count
        m_b = batch_var * batch_count
        M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count
        new_var = M2 / tot_count
        new_count = tot_count

        return new_mean, new_var, new_count


@dataclass
class RewardMetrics:
    """æå–åçš„åŸå§‹æŒ‡æ ‡ï¼Œä¾¿äºåç»­ç»Ÿä¸€è®¡ç®—ã€‚"""
    avg_delay: float = 0.0
    total_energy: float = 0.0
    dropped_tasks: int = 0
    completion_rate: float = 0.0
    data_loss_ratio: float = 0.0
    cache_utilization: float = 0.0
    queue_overload_events: float = 0.0
    remote_rejection_rate: float = 0.0
    rsu_offload_ratio: float = 0.0
    uav_offload_ratio: float = 0.0
    local_offload_ratio: float = 0.0  # æœ¬åœ°å¤„ç†å æ¯”
    cache_hit_rate: float = 0.0
    cache_miss_rate: float = 0.0
    migration_cost: float = 0.0
    migration_effectiveness: float = 0.0
    prefetch_events: float = 0.0
    total_cache_requests: float = 1.0
    prefetch_lead: float = 0.0
    migration_backoff: float = 0.0


@dataclass
class RewardComponents:
    """åˆ†è§£åçš„æˆæœ¬å’Œå¥–åŠ±ç»„æˆï¼Œä¾¿äºè°ƒè¯•ä¸æ‰©å±•ã€‚"""
    norm_delay: float
    norm_energy: float
    core_cost: float
    drop_penalty: float = 0.0
    completion_gap_penalty: float = 0.0
    data_loss_penalty: float = 0.0
    cache_pressure_penalty: float = 0.0
    queue_penalty: float = 0.0
    remote_reject_penalty: float = 0.0
    offload_bonus: float = 0.0
    local_penalty: float = 0.0  # æœ¬åœ°å¤„ç†é¢å¤–æƒ©ç½š
    cache_penalty: float = 0.0
    cache_bonus: float = 0.0
    migration_penalty: float = 0.0
    joint_coupling_penalty: float = 0.0
    joint_bonus: float = 0.0
    total_cost: float = 0.0
    reward_pre_clip: float = 0.0
    reward: float = 0.0


class UnifiedRewardCalculator:
    """
    å¯å¤ç”¨çš„å¥–åŠ±è®¡ç®—å™¨ï¼Œç”¨äºå•æ™ºèƒ½ä½“è®­ç»ƒå™¨ã€‚
    
    è¯¥ç±»å®ç°äº†ç»Ÿä¸€çš„å¥–åŠ±è®¡ç®—é€»è¾‘ï¼Œæ”¯æŒä¸åŒç®—æ³•ï¼ˆå¦‚SACã€TD3ç­‰ï¼‰çš„ç‰¹å®šéœ€æ±‚ã€‚
    é‡‡ç”¨æˆæœ¬æœ€å°åŒ–æ–¹æ³•ï¼šå»¶è¿Ÿè¶Šä½ã€èƒ½è€—è¶Šä½ã€ä»»åŠ¡ä¸¢å¼ƒè¶Šå°‘ï¼Œå¥–åŠ±è¶Šé«˜ã€‚
    
    Reusable reward calculator for the single-agent trainers.
    """

    def __init__(self, algorithm: str = "general") -> None:
        """
        åˆå§‹åŒ–ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ã€‚
        
        Args:
            algorithm: ç®—æ³•åç§°ï¼Œç”¨äºç‰¹å®šç®—æ³•çš„è°ƒæ•´ï¼ˆå¦‚"SAC"ã€"TD3"ç­‰ï¼‰
                      ä¸åŒç®—æ³•å¯èƒ½æœ‰ä¸åŒçš„å½’ä¸€åŒ–å› å­å’Œå¥–åŠ±èŒƒå›´
        """
        self.algorithm = algorithm.upper()

        # ä»é…ç½®ä¸­è·å–æ ¸å¿ƒæƒé‡å‚æ•°
        # Core weights taken from configuration.
        self.weight_delay = float(config.rl.reward_weight_delay)  # å»¶è¿Ÿæƒé‡
        self.weight_energy = float(config.rl.reward_weight_energy)  # èƒ½è€—æƒé‡
        self.penalty_dropped = float(config.rl.reward_penalty_dropped)  # ä»»åŠ¡ä¸¢å¼ƒæƒ©ç½š
        self.weight_cache = float(getattr(config.rl, "reward_weight_cache", 0.0))
        self.weight_cache_bonus = float(getattr(config.rl, "reward_weight_cache_bonus", 0.0))
        self.weight_migration = float(getattr(config.rl, "reward_weight_migration", 0.0))
        # ğŸ”§ P0ä¿®å¤ï¼šå°†fallbacké»˜è®¤å€¼ä»0.05æ”¹ä¸º0.0ï¼Œé˜²æ­¢bonusæŠµæ¶ˆcore_cost
        self.weight_joint = float(getattr(config.rl, "reward_weight_joint", 0.0))
        # è¾¹ç¼˜è®¡ç®—å¸è½½å¥–åŠ±ï¼šé€‚åº¦æ¿€åŠ±è¿œç¨‹å¤„ç†ï¼ˆé»˜è®¤0.0ï¼Œé¿å…å¹²æ‰°æ ¸å¿ƒä¼˜åŒ–ï¼‰
        # ğŸ”§ P0ä¿®å¤ï¼šå°†fallbacké»˜è®¤å€¼ä»0.5æ”¹ä¸º0.0ï¼Œé˜²æ­¢bonusæŠµæ¶ˆcore_cost
        self.weight_offload_bonus = float(getattr(config.rl, "reward_weight_offload_bonus", 0.0))
        # æœ¬åœ°å¤„ç†æƒ©ç½šï¼šç§»é™¤é¢å¤–æƒ©ç½šï¼ˆé»˜è®¤0.0ï¼‰
        self.weight_local_penalty = float(getattr(config.rl, "reward_weight_local_penalty", 0.0))
        self.completion_target = float(getattr(config.rl, "completion_target", 0.95))
        self.weight_completion_gap = float(getattr(config.rl, "reward_weight_completion_gap", 0.0))
        self.weight_loss_ratio = float(getattr(config.rl, "reward_weight_loss_ratio", 0.0))
        self.cache_pressure_threshold = float(getattr(config.rl, "cache_pressure_threshold", 0.85))
        self.weight_cache_pressure = float(getattr(config.rl, "reward_weight_cache_pressure", 0.0))
        self.weight_queue_overload = float(getattr(config.rl, "reward_weight_queue_overload", 0.0))
        self.weight_remote_reject = float(getattr(config.rl, "reward_weight_remote_reject", 0.0))
        self.latency_target = float(getattr(config.rl, "latency_target", 1.5))
        self.energy_target = float(getattr(config.rl, "energy_target", 1000.0))  # ğŸ”§ 9000 â†’ 1000 (å¯¹é½å®é™…èƒ½è€—)
        self.latency_tolerance = float(getattr(config.rl, "latency_upper_tolerance", self.latency_target * 2.0))
        self.energy_tolerance = float(getattr(config.rl, "energy_upper_tolerance", self.energy_target * 1.5))
        # åˆ†æ®µå®¹é”™/é’³ä½
        # ğŸ”§ v12ä¿®å¤ï¼šæ‰©å¤§è£å‰ªèŒƒå›´é…åˆæ›´å¤§çš„æƒé‡
        # æ ¸å¿ƒæƒé‡å¢åŠ åˆ°(5.0, 3.0)ï¼Œéœ€è¦æ›´å¤§çš„è£å‰ªèŒƒå›´
        self.total_cost_clip = float(getattr(config.rl, "reward_total_cost_clip", 50.0))  # ğŸ”§ v12: 10 â†’ 50
        self.component_clip = float(getattr(config.rl, "reward_component_clip", 10.0))    # ğŸ”§ v12: 3 â†’ 10
        # å½’ä¸€åŒ–ä»»åŠ¡ä¼˜å…ˆçº§æƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # Normalise priority weights if they exist.
        priority_weights = getattr(config, "task", None)
        priority_weights = getattr(priority_weights, "type_priority_weights", None)
        if isinstance(priority_weights, dict) and priority_weights:
            # è®¡ç®—æƒé‡æ€»å’Œå¹¶å½’ä¸€åŒ–
            total = sum(float(v) for v in priority_weights.values()) or 1.0
            self.task_priority_weights = {
                int(task_type): float(value) / total
                for task_type, value in priority_weights.items()
            }
        else:
            # é»˜è®¤æ‰€æœ‰ä»»åŠ¡ç±»å‹æƒé‡ç›¸ç­‰
            self.task_priority_weights = {1: 0.25, 2: 0.25, 3: 0.25, 4: 0.25}

        # ğŸ”§ P0ä¿®å¤ï¼šå½’ä¸€åŒ–å› å­å¿…é¡»ä¸ä¼˜åŒ–ç›®æ ‡å€¼ä¸¥æ ¼å¯¹é½
        # Normalisation factors MUST align with optimization targets (latency_target and energy_target).
        # ç›®æ ‡å€¼ä»config.rlè¯»å–ï¼šlatency_target=0.4s, energy_target=3500J
        # å½’ä¸€åŒ–åŸºå‡†ç›´æ¥ä½¿ç”¨è¿™äº›ç›®æ ‡å€¼ï¼Œç¡®ä¿çŠ¶æ€å½’ä¸€åŒ–ä¸å¥–åŠ±è®¡ç®—ä¸€è‡´
        # âš ï¸ å…³é”®ï¼šOptimizedTD3Wrapperçš„çŠ¶æ€å½’ä¸€åŒ–ä¹Ÿä½¿ç”¨ç›¸åŒçš„config.rlç›®æ ‡å€¼
        self.delay_normalizer = self.latency_target  # ä¸ç›®æ ‡å€¼å¯¹é½
        self.energy_normalizer = self.energy_target  # ä¸ç›®æ ‡å€¼å¯¹é½
        self.delay_bonus_scale = max(1e-6, self.latency_target)
        self.energy_bonus_scale = max(1e-6, self.energy_target)
        
        # ğŸ†• åŠ¨æ€å½’ä¸€åŒ–é…ç½®
        # âš ï¸ å½“å‰ç¦ç”¨ä»¥æ”¹å–„æ”¶æ•›æ€§ï¼ˆconfig.rl.use_dynamic_reward_normalization=Falseï¼‰
        # å¦‚æœæœªæ¥å¯ç”¨ï¼Œéœ€è¦å……åˆ†æµ‹è¯•åŠ¨æ€å½’ä¸€åŒ–å¯¹è®­ç»ƒç¨³å®šæ€§çš„å½±å“
        self.use_dynamic_normalization = getattr(config.rl, "use_dynamic_reward_normalization", False)
        if self.use_dynamic_normalization:
            self.delay_rms = RunningMeanStd(shape=())
            self.energy_rms = RunningMeanStd(shape=())
            # åˆå§‹åŒ–ä¸ºç›®æ ‡å€¼ï¼Œé¿å…åˆæœŸæ³¢åŠ¨è¿‡å¤§
            self.delay_rms.mean = self.latency_target
            self.energy_rms.mean = self.energy_target
            print(f"   âš ï¸ Dynamic Normalization: ENABLED (Experimental)")
            print(f"      Initial: delay={self.latency_target:.2f}s, energy={self.energy_target:.0f}J")
        else:
            print(f"   Dynamic Normalization: DISABLED (Recommended)")

        # å·²ç§»é™¤SACçš„ç‰¹æ®Šå½’ä¸€åŒ–å‚æ•°ï¼Œæ‰€æœ‰ç®—æ³•ç°åœ¨ä½¿ç”¨ç»Ÿä¸€çš„å½’ä¸€åŒ–é€»è¾‘

        norm_cfg = getattr(config, "normalization", None)
        if norm_cfg is not None:
            self.latency_target = float(getattr(norm_cfg, "delay_reference", self.latency_target))
            self.latency_tolerance = float(getattr(norm_cfg, "delay_upper_reference", self.latency_tolerance))
            self.energy_target = float(getattr(norm_cfg, "energy_reference", self.energy_target))
            self.energy_tolerance = float(getattr(norm_cfg, "energy_upper_reference", self.energy_tolerance))
            self.delay_normalizer = float(
                getattr(norm_cfg, "delay_normalizer_value", self.delay_normalizer)
            )
            self.energy_normalizer = float(
                getattr(norm_cfg, "energy_normalizer_value", self.energy_normalizer)
            )
            self.delay_bonus_scale = max(1e-6, self.latency_target)
            self.energy_bonus_scale = max(1e-6, self.energy_target)

        # ğŸ”§ v12ä¿®å¤ï¼šæ‰©å¤§è£å‰ªèŒƒå›´é…åˆæ›´å¤§çš„æƒé‡
        # ç”¨æ›´å¤§çš„æƒé‡(5.0, 3.0)å’Œæ›´ä½çš„ç›®æ ‡(0.3s, 200J)
        # å¥–åŠ±ä¼šåœ¨æ›´å¤§çš„èŒƒå›´æ³¢åŠ¨ï¼Œéœ€è¦æ‰©å¤§è£å‰ªèŒƒå›´
        self.reward_clip_range = (-50.0, 0.0)  # ğŸ”§ v12: -10 â†’ -50 (æ‰©å¤§5å€)

        print(f"[OK] Unified reward calculator ({self.algorithm})")
        print(
            f"   Core weights: delay={self.weight_delay:.2f}, "
            f"energy={self.weight_energy:.2f}, drop={self.penalty_dropped:.3f}"
        )
        print(
            f"   Normalisation: delay/{self.delay_normalizer:.2f}, "
            f"energy/{self.energy_normalizer:.0f}"
        )

    # ------------------------------------------------------------------ #
    # è¾…åŠ©æ–¹æ³• / Helpers

    @staticmethod
    def _safe_float(value: Optional[float], default: float = 0.0) -> float:
        """
        å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°ã€‚
        
        Args:
            value: å¾…è½¬æ¢çš„å€¼
            default: è½¬æ¢å¤±è´¥æ—¶çš„é»˜è®¤å€¼
            
        Returns:
            è½¬æ¢åçš„æµ®ç‚¹æ•°æˆ–é»˜è®¤å€¼
        """
        if value is None:
            return default
        try:
            val = float(value)
            if not np.isfinite(val):
                return default
            return val
        except (TypeError, ValueError):
            return default

    @staticmethod
    def _safe_int(value: Optional[int], default: int = 0) -> int:
        """
        å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•´æ•°ã€‚
        
        Args:
            value: å¾…è½¬æ¢çš„å€¼
            default: è½¬æ¢å¤±è´¥æ—¶çš„é»˜è®¤å€¼
            
        Returns:
            è½¬æ¢åçš„æ•´æ•°æˆ–é»˜è®¤å€¼
        """
        if value is None:
            return default
        try:
            if isinstance(value, float) and not np.isfinite(value):
                return default
            return int(value)
        except (TypeError, ValueError, OverflowError):
            return default

    @staticmethod
    def _to_float_list(source) -> List[float]:
        """
        å°†è¾“å…¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°åˆ—è¡¨ã€‚
        
        æ”¯æŒnumpyæ•°ç»„ã€åˆ—è¡¨ã€å…ƒç»„ç­‰å¤šç§è¾“å…¥ç±»å‹ã€‚
        å¦‚æœæŸä¸ªå…ƒç´ æ— æ³•è½¬æ¢ï¼Œåˆ™ä½¿ç”¨0.0ä½œä¸ºé»˜è®¤å€¼ã€‚
        
        Args:
            source: å¾…è½¬æ¢çš„æ•°æ®æº
            
        Returns:
            æµ®ç‚¹æ•°åˆ—è¡¨
        """
        if isinstance(source, np.ndarray):
            iterable = source.tolist()
        elif isinstance(source, (list, tuple)):
            iterable = list(source)
        else:
            return []
        result: List[float] = []
        for item in iterable:
            try:
                result.append(float(item))
            except (TypeError, ValueError):
                result.append(0.0)
        return result

    @staticmethod
    def _piecewise_ratio(value: float, target: float, tolerance: float) -> float:
        """
        åˆ†æ®µå®¹é”™çš„å½’ä¸€åŒ–æ¯”ä¾‹ï¼šä½äºç›®æ ‡æ—¶åŠå¹…æƒ©ç½šï¼Œç›®æ ‡-å®¹å·®çº¿æ€§ï¼Œè¶…å®¹å·®è¶…çº¿æ€§ã€‚
        """
        v = max(0.0, float(value))
        t = max(1e-6, float(target))
        tol = max(t, float(tolerance))
        if v <= t:
            return 0.5 * (v / t)
        if v <= tol:
            return 1.0 + (v - t) / max(tol - t, 1e-6)
        return 2.0 + (v - tol) / max(t, 1e-6)

    # ------------------------------------------------------------------ #
    # ------------------------------------------------------------------ #
    # ??API / Public API

    def _extract_metrics(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict],
        migration_metrics: Optional[Dict],
    ) -> RewardMetrics:
        """?????????????????????"""
        metrics = RewardMetrics()
        metrics.avg_delay = max(0.0, self._safe_float(system_metrics.get("avg_task_delay")))
        metrics.total_energy = max(0.0, self._safe_float(system_metrics.get("total_energy_consumption")))
        metrics.dropped_tasks = max(0, self._safe_int(system_metrics.get("dropped_tasks")))
        metrics.completion_rate = max(0.0, self._safe_float(system_metrics.get("task_completion_rate")))
        metrics.data_loss_ratio = max(0.0, self._safe_float(system_metrics.get("data_loss_ratio_bytes")))
        metrics.cache_utilization = max(0.0, self._safe_float(system_metrics.get("cache_utilization")))
        metrics.queue_overload_events = max(0.0, self._safe_float(system_metrics.get("queue_overload_events")))
        metrics.remote_rejection_rate = max(0.0, self._safe_float(system_metrics.get("remote_rejection_rate")))
        metrics.rsu_offload_ratio = max(0.0, self._safe_float(system_metrics.get("rsu_offload_ratio")))
        metrics.uav_offload_ratio = max(0.0, self._safe_float(system_metrics.get("uav_offload_ratio")))
        metrics.local_offload_ratio = max(0.0, self._safe_float(system_metrics.get("local_offload_ratio", 0.0)))

        if cache_metrics:
            metrics.cache_hit_rate = float(max(0.0, min(1.0, self._safe_float(cache_metrics.get("hit_rate"), 0.0))))
            metrics.cache_miss_rate = float(max(0.0, min(1.0, self._safe_float(cache_metrics.get("miss_rate"), 0.0))))
            metrics.prefetch_events = max(0.0, self._safe_float(cache_metrics.get("prefetch_events"), 0.0))
            metrics.total_cache_requests = max(1.0, self._safe_float(cache_metrics.get("total_requests"), 1.0))
            cache_joint = cache_metrics.get("joint_params", {}) if isinstance(cache_metrics, dict) else {}
            metrics.prefetch_lead = self._safe_float(cache_joint.get("prefetch_lead_time"), 0.0)
        if migration_metrics:
            metrics.migration_cost = max(0.0, self._safe_float(migration_metrics.get("migration_cost"), 0.0))
            metrics.migration_effectiveness = float(max(0.0, min(1.0, self._safe_float(migration_metrics.get("effectiveness"), 0.0))))
            migration_joint = migration_metrics.get("joint_params", {}) if isinstance(migration_metrics, dict) else {}
            metrics.migration_backoff = float(max(0.0, min(1.0, self._safe_float(migration_joint.get("migration_backoff"), 0.0))))
        return metrics

    def _compute_components(self, m: RewardMetrics) -> RewardComponents:
        """
        ğŸ”§ v13ä¼˜åŒ–ï¼šæç®€æˆæœ¬è®¡ç®—
        
        æ ¸å¿ƒç­–ç•¥ï¼šåªä¿ç•™delayå’Œenergyçš„æ ¸å¿ƒæˆæœ¬ï¼Œç§»é™¤æ‰€æœ‰bonusé¡¹
        é—®é¢˜è¯Šæ–­ï¼šä¹‹å‰bonuså’Œpenaltyç›¸äº’æŠµæ¶ˆï¼Œå¯¼è‡´æ€»å·®å¼‚åªæœ‰3-5%
        è§£å†³æ–¹æ¡ˆï¼šæç®€åŒ–å¥–åŠ±å‡½æ•°ï¼Œè®©æ™ºèƒ½ä½“èƒ½æ¸…æ™°çœ‹åˆ°ä¼˜åŒ–æ–¹å‘
        """
        import os

        # ğŸ”§ v13æç®€ç‰ˆï¼šåªä¿ç•™æ ¸å¿ƒæˆæœ¬ï¼Œç§»é™¤æ‰€æœ‰bonusé¡¹
        # ç›®æ ‡ï¼šè®©å¥–åŠ±å·®å¼‚æœ€å¤§åŒ–ï¼Œè®©æ™ºèƒ½ä½“èƒ½æ¸…æ™°çœ‹åˆ°ä¼˜åŒ–æ–¹å‘
        
        # --- æ ¸å¿ƒæˆæœ¬ï¼šçº¿æ€§å½’ä¸€åŒ– ---
        norm_delay = m.avg_delay / max(self.delay_normalizer, 1e-6)
        norm_energy = m.total_energy / max(self.energy_normalizer, 1e-6)
        delay_penalty = self.weight_delay * norm_delay
        energy_penalty = self.weight_energy * norm_energy
        core_cost = delay_penalty + energy_penalty

        # --- ğŸ”§ v13: ç®€åŒ–ä¸¢å¼ƒæƒ©ç½š ---
        drop_penalty = self.penalty_dropped * m.dropped_tasks
        
        # --- ğŸ”§ v13: ç§»é™¤æ‰€æœ‰å¤æ‚çš„bonuså’Œpenaltyé¡¹ ---
        # è¿™äº›é¡¹ä¹‹å‰ç›¸äº’æŠµæ¶ˆï¼Œå¯¼è‡´å¥–åŠ±å·®å¼‚åªæœ‰3-5%
        completion_gap = max(0.0, self.completion_target - m.completion_rate)
        completion_gap_penalty = self.weight_completion_gap * completion_gap
        data_loss_penalty = 0.0       # ç¦ç”¨
        cache_pressure_penalty = 0.0  # ç¦ç”¨
        queue_penalty = 0.0           # ç¦ç”¨
        remote_reject_penalty = 0.0   # ç¦ç”¨
        local_penalty = 0.0           # ç¦ç”¨
        cache_penalty = 0.0           # ç¦ç”¨
        migration_penalty = 0.0       # ç¦ç”¨
        offload_bonus = 0.0           # ç¦ç”¨
        cache_bonus = 0.0             # ç¦ç”¨
        joint_bonus = 0.0             # ç¦ç”¨
        joint_coupling_penalty = 0.0  # ç¦ç”¨

        # --- ğŸ”§ v13: æç®€æ€»æˆæœ¬ = core_cost + drop_penalty ---
        total_cost = core_cost + drop_penalty + completion_gap_penalty
        total_cost = float(np.clip(total_cost, 0.0, self.total_cost_clip))

        return RewardComponents(
            norm_delay=norm_delay,
            norm_energy=norm_energy,
            core_cost=core_cost,
            drop_penalty=drop_penalty,
            completion_gap_penalty=completion_gap_penalty,
            data_loss_penalty=data_loss_penalty,
            cache_pressure_penalty=cache_pressure_penalty,
            queue_penalty=queue_penalty,
            remote_reject_penalty=remote_reject_penalty,
            offload_bonus=offload_bonus,
            local_penalty=local_penalty,
            cache_penalty=cache_penalty,
            cache_bonus=cache_bonus,
            migration_penalty=migration_penalty,
            joint_coupling_penalty=joint_coupling_penalty,
            joint_bonus=joint_bonus,
            total_cost=total_cost,
            reward_pre_clip=-total_cost,
            reward=-total_cost,
        )

    def _compose_reward(self, components: RewardComponents, completion_rate: float) -> RewardComponents:
        """ç»„è£…æœ€ç»ˆå¥–åŠ±ï¼Œä½¿ç”¨é…ç½®çš„è£å‰ªèŒƒå›´
        
        æ‰€æœ‰ç®—æ³•ç»Ÿä¸€ä½¿ç”¨æˆæœ¬æœ€å°åŒ–å¥–åŠ±ï¼šreward = -total_cost
        å¥–åŠ±èŒƒå›´: [-10.0, 0.0]ï¼Œè¶Šæ¥è¿‘0è¡¨ç¤ºæ€§èƒ½è¶Šå¥½
        
        ğŸ”§ 2024-12-02 v4ä¿®å¤ï¼šæ·»åŠ Reward Scalingæ”¾å¤§å¥–åŠ±å·®å¼‚
        é—®é¢˜ï¼šå¥–åŠ±ä¿¡å·å¤ªå¼±(~0.01æ–¹å·®)ï¼ŒTD3æ¢¯åº¦ä¸æ˜æ˜¾
        è§£å†³ï¼šä½¿ç”¨reward_scaleæ”¾å¤§å·®å¼‚ï¼Œè®©ç­–ç•¥æ”¹è¿›æ›´æ˜æ˜¾
        """
        # ğŸ”§ Reward Scalingï¼šæ”¾å¤§å¥–åŠ±ä¿¡å·
        # ğŸ”§ v7ä¿®å¤ï¼š5.0 â†’ 1.0 (ç§»é™¤è¿‡åº¦æ”¾å¤§ï¼Œé™ä½å¥–åŠ±æ–¹å·®)
        # é—®é¢˜ï¼šreward_scale=5å¯¼è‡´å¥–åŠ±åœ¨-1200~-1700æ³¢åŠ¨ï¼Œå·®å¼‚è¢«æ”¾å¤§åæ›´éš¾å­¦ä¹ 
        reward_scale = float(getattr(config.rl, 'reward_scale', 1.0))
        
        # æˆæœ¬æœ€å°åŒ–ï¼šå¥–åŠ± = -æˆæœ¬ * scale
        reward_raw = -abs(components.total_cost) * reward_scale
        reward_clipped = float(np.clip(reward_raw, self.reward_clip_range[0] * reward_scale, self.reward_clip_range[1]))
        components.reward_pre_clip = reward_raw
        components.reward = reward_clipped if np.isfinite(reward_clipped) else 0.0
        return components

    def calculate_reward(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict] = None,
        migration_metrics: Optional[Dict] = None,
    ) -> tuple[float, Dict[str, float]]:
        """è®¡ç®—å¥–åŠ±å¹¶è¿”å›æ ‡é‡å€¼å’Œç»„ä»¶å­—å…¸
        
        Returns:
            tuple: (reward, reward_components)
                - reward: æ€»å¥–åŠ±æ ‡é‡å€¼
                - reward_components: åŒ…å«å„åˆ†é‡çš„å­—å…¸
        """
        # Debug print for first few calls to verify input range
        if not hasattr(self, '_debug_count'):
            self._debug_count = 0
        if self._debug_count < 10:
            print(f"[RewardDebug] Metrics: delay={system_metrics.get('avg_task_delay', 0):.4f}, energy={system_metrics.get('total_energy_consumption', 0):.1f}, completion={system_metrics.get('task_completion_rate', 0):.2f}")
            self._debug_count += 1

        metrics = self._extract_metrics(system_metrics, cache_metrics, migration_metrics)
        components = self._compute_components(metrics)
        components = self._compose_reward(components, metrics.completion_rate)
        
        # ğŸ” ä¸´æ—¶è¯Šæ–­ï¼šæ‰“å°å¥–åŠ±åˆ†è§£ï¼Œæ‰¾å‡ºè¢«å‹ç¼©çš„åŸå› 
        if self._debug_count <= 5:
            print(f"[RewardBreakdown] Episode {self._debug_count}:")
            print(f"  norm_delay={components.norm_delay:.4f}, norm_energy={components.norm_energy:.4f}")
            print(f"  core_cost={components.core_cost:.4f}")
            print(f"  offload_bonus={components.offload_bonus:.4f}")
            print(f"  total_cost={components.total_cost:.4f}")
            print(f"  reward_pre_clip={components.reward_pre_clip:.4f}")
            print(f"  reward_final={components.reward:.4f}")
        
        # æ„é€ å¥–åŠ±ç»„ä»¶å­—å…¸ä¾›è°ƒè¯•ä½¿ç”¨
        reward_components = {
            'delay': -components.norm_delay * self.weight_delay,
            'energy': -components.norm_energy * self.weight_energy,
            'cache': -components.cache_penalty + components.cache_bonus,
            'penalty': -(components.drop_penalty + components.completion_gap_penalty + 
                        components.queue_penalty + components.remote_reject_penalty),
            'core_cost': -components.core_cost,
            'total': components.reward
        }
        
        final_reward = components.reward if np.isfinite(components.reward) else 0.0
        return final_reward, reward_components

    def update_targets(
        self,
        latency_target: Optional[float] = None,
        energy_target: Optional[float] = None,
    ) -> None:
        """åŠ¨æ€æ›´æ–°ç›®æ ‡å€¼ï¼Œä½¿å¥–åŠ±å‡½æ•°å¯ä»¥åœ¨è®­ç»ƒä¸­è‡ªé€‚åº”æ‹“æ‰‘å˜åŒ–ã€‚
        
        ğŸ”§ P0ä¿®å¤ï¼šåŒæ­¥æ›´æ–°å½’ä¸€åŒ–å› å­ï¼Œç¡®ä¿å¥–åŠ±è®¡ç®—ä¸ç›®æ ‡å€¼ä¸€è‡´
        """
        if latency_target is not None:
            self.latency_target = float(latency_target)
            self.latency_tolerance = float(
                getattr(config.rl, "latency_upper_tolerance", self.latency_target * 2.0)
            )
            # ğŸ”§ P0ä¿®å¤ï¼šåŒæ­¥å½’ä¸€åŒ–å› å­
            self.delay_normalizer = self.latency_target
            self.delay_bonus_scale = max(1e-6, self.latency_target)
        if energy_target is not None:
            self.energy_target = float(energy_target)
            self.energy_tolerance = float(
                getattr(config.rl, "energy_upper_tolerance", self.energy_target * 1.5)
            )
            # ğŸ”§ P0ä¿®å¤ï¼šåŒæ­¥å½’ä¸€åŒ–å› å­
            self.energy_normalizer = self.energy_target
            self.energy_bonus_scale = max(1e-6, self.energy_target)

    def get_reward_breakdown(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict] = None,
        migration_metrics: Optional[Dict] = None,
    ) -> str:
        """ç”Ÿæˆå¥–åŠ±åˆ†è§£ï¼Œä¾¿äºå¿«é€Ÿè¯Šæ–­å„æˆæœ¬æ¥æºã€‚"""
        metrics = self._extract_metrics(system_metrics, cache_metrics, migration_metrics)
        components = self._compute_components(metrics)
        components = self._compose_reward(components, metrics.completion_rate)

        lines = [
            f"Reward report ({self.algorithm}):",
            f"  Total Reward        : {components.reward:.4f} (pre-clip {components.reward_pre_clip:.4f})",
            f"  Core Cost (D+E)     : {components.core_cost:.4f}",
            f"    - Delay (norm/w)  : {components.norm_delay:.4f} / w={self.weight_delay}",
            f"    - Energy (norm/w) : {components.norm_energy:.4f} / w={self.weight_energy}",
            f"  Drop Penalty        : {components.drop_penalty:.4f}",
            f"  Completion Gap      : {components.completion_gap_penalty:.4f}",
            f"  Data Loss Penalty   : {components.data_loss_penalty:.4f}",
            f"  Cache Pressure      : {components.cache_pressure_penalty:.4f}",
            f"  Queue Penalty       : {components.queue_penalty:.4f}",
            f"  Remote Reject       : {components.remote_reject_penalty:.4f}",
            f"  Cache Penalty/Bonus : {components.cache_penalty:.4f} / -{components.cache_bonus:.4f}",
            f"  Migration Penalty   : {components.migration_penalty:.4f}",
            f"  Joint Penalty/Bonus : {components.joint_coupling_penalty:.4f} / -{components.joint_bonus:.4f}",
            f"  Offload Bonus       : -{components.offload_bonus:.4f}",
            f"  ----------------------------------------",
            f"  Total Cost          : {components.total_cost:.4f}",
        ]
        return "\n".join(lines)


# ---------------------------------------------------------------------- #
# ä¾¿æ·çš„å•ä¾‹å¯¹è±¡ï¼Œåœ¨æ•´ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨
# Convenience singletons used across the project.

_general_reward_calculator = UnifiedRewardCalculator(algorithm="general")
_sac_reward_calculator = UnifiedRewardCalculator(algorithm="sac")


def calculate_unified_reward(
    system_metrics: Dict,
    cache_metrics: Optional[Dict] = None,
    migration_metrics: Optional[Dict] = None,
    algorithm: str = "general",
) -> float:
    """
    ç»Ÿä¸€å¥–åŠ±è®¡ç®—çš„ä¾¿æ·å‡½æ•°ã€‚
    
    æ ¹æ®æŒ‡å®šç®—æ³•é€‰æ‹©ç›¸åº”çš„å¥–åŠ±è®¡ç®—å™¨ï¼Œè®¡ç®—å¹¶è¿”å›å¥–åŠ±å€¼ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        cache_metrics: å¯é€‰çš„ç¼“å­˜æŒ‡æ ‡
        migration_metrics: å¯é€‰çš„è¿ç§»æŒ‡æ ‡
        algorithm: ç®—æ³•åç§°ï¼ˆ"SAC"æˆ–"general"ï¼‰
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    calculator = _sac_reward_calculator if algorithm.upper() == "SAC" else _general_reward_calculator
    reward, _ = calculator.calculate_reward(system_metrics, cache_metrics, migration_metrics)
    return reward


def get_reward_breakdown(system_metrics: Dict, algorithm: str = "general") -> str:
    """
    è·å–å¥–åŠ±åˆ†è§£æŠ¥å‘Šçš„ä¾¿æ·å‡½æ•°ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        algorithm: ç®—æ³•åç§°ï¼ˆ"SAC"æˆ–"general"ï¼‰
        
    Returns:
        æ ¼å¼åŒ–çš„å¥–åŠ±åˆ†è§£æŠ¥å‘Šå­—ç¬¦ä¸²
    """
    calculator = _sac_reward_calculator if algorithm.upper() == "SAC" else _general_reward_calculator
    return calculator.get_reward_breakdown(system_metrics)


def update_reward_targets(
    latency_target: Optional[float] = None,
    energy_target: Optional[float] = None,
) -> None:
    """
    åŠ¨æ€æ›´æ–°å…¨å±€å¥–åŠ±ç›®æ ‡ï¼Œç¡®ä¿å•ä¾‹è®¡ç®—å™¨ä¸å…¨å±€configä¿æŒåŒæ­¥ã€‚
    """
    if latency_target is not None:
        config.rl.latency_target = float(latency_target)
    if energy_target is not None:
        config.rl.energy_target = float(energy_target)
    _general_reward_calculator.update_targets(latency_target, energy_target)
    _sac_reward_calculator.update_targets(latency_target, energy_target)


# ---------------------------------------------------------------------- #
# å‘åå…¼å®¹çš„è¾…åŠ©å‡½æ•°åç§°
# Backwards-compatible helper names.

def calculate_enhanced_reward(
    system_metrics: Dict,
    cache_metrics: Optional[Dict] = None,
    migration_metrics: Optional[Dict] = None,
) -> float:
    """
    å¢å¼ºå¥–åŠ±è®¡ç®—ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    
    è¿™æ˜¯calculate_unified_rewardçš„åˆ«åï¼Œä½¿ç”¨"general"ç®—æ³•ã€‚
    ä¿ç•™æ­¤å‡½æ•°ä»¥ç¡®ä¿ä¸æ—§ä»£ç çš„å…¼å®¹æ€§ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        cache_metrics: å¯é€‰çš„ç¼“å­˜æŒ‡æ ‡
        migration_metrics: å¯é€‰çš„è¿ç§»æŒ‡æ ‡
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    return calculate_unified_reward(system_metrics, cache_metrics, migration_metrics, "general")


def calculate_sac_reward(system_metrics: Dict) -> float:
    """
    SACç®—æ³•ä¸“ç”¨å¥–åŠ±è®¡ç®—ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    
    ä¸ºSACç®—æ³•æä¾›æ­£å‘å¥–åŠ±ç©ºé—´çš„ä¾¿æ·å‡½æ•°ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼ï¼ˆå¯èƒ½ä¸ºæ­£å€¼ï¼‰
    """
    return calculate_unified_reward(system_metrics, algorithm="sac")


def calculate_simple_reward(system_metrics: Dict) -> float:
    """
    ç®€å•å¥–åŠ±è®¡ç®—ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    
    è¿™æ˜¯calculate_unified_rewardçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨"general"ç®—æ³•ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    return calculate_unified_reward(system_metrics, algorithm="general")
