#!/usr/bin/env python3
"""
ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ï¼Œä¾›æ‰€æœ‰å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•ä½¿ç”¨ã€‚

æ ¸å¿ƒç†å¿µæ˜¯æˆæœ¬æœ€å°åŒ–ï¼šæ›´ä½çš„å»¶è¿Ÿå’Œæ›´ä½çš„èƒ½è€—ä¼šå¸¦æ¥æ›´é«˜ï¼ˆæ›´å°‘è´Ÿå€¼ï¼‰çš„å¥–åŠ±ã€‚
æŸäº›ç®—æ³•ï¼ˆå¦‚SACï¼‰æœŸæœ›æ­£å‘å¥–åŠ±ï¼Œå› æ­¤æˆ‘ä»¬ä¸ºè¿™ç§æƒ…å†µä¿ç•™äº†ä¸€ä¸ªå°çš„å¯é€‰å¥–åŠ±ã€‚

Unified reward calculator used by all single-agent RL algorithms.
The philosophy is cost-minimisation: lower latency and lower energy
lead to higher (less negative) rewards. Some algorithms (SAC) expect
positive rewards, so we keep a small optional bonus for that case.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, Optional, List

import numpy as np

from config import config


class RunningMeanStd:
    """Tracks the running mean and variance of a stream of data."""
    def __init__(self, epsilon=1e-4, shape=()):
        self.mean = np.zeros(shape, 'float64')
        self.var = np.ones(shape, 'float64')
        self.count = epsilon

    def update(self, x):
        if np.isscalar(x) or (isinstance(x, np.ndarray) and x.ndim == 0):
            batch_mean = float(x)
            batch_var = 0.0
            batch_count = 1
        else:
            batch_mean = np.mean(x, axis=0)
            batch_var = np.var(x, axis=0)
            batch_count = x.shape[0]
        self.update_from_moments(batch_mean, batch_var, batch_count)

    def update_from_moments(self, batch_mean, batch_var, batch_count):
        self.mean, self.var, self.count = self.update_mean_var_count_from_moments(
            self.mean, self.var, self.count, batch_mean, batch_var, batch_count
        )

    def update_mean_var_count_from_moments(self, mean, var, count, batch_mean, batch_var, batch_count):
        delta = batch_mean - mean
        tot_count = count + batch_count

        new_mean = mean + delta * batch_count / tot_count
        m_a = var * count
        m_b = batch_var * batch_count
        M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count
        new_var = M2 / tot_count
        new_count = tot_count

        return new_mean, new_var, new_count


@dataclass
class RewardMetrics:
    """æå–åçš„åŸå§‹æŒ‡æ ‡ï¼Œä¾¿äºåç»­ç»Ÿä¸€è®¡ç®—ã€‚"""
    avg_delay: float = 0.0
    total_energy: float = 0.0
    dropped_tasks: int = 0
    completion_rate: float = 0.0
    data_loss_ratio: float = 0.0
    cache_utilization: float = 0.0
    queue_overload_events: float = 0.0
    remote_rejection_rate: float = 0.0
    rsu_offload_ratio: float = 0.0
    uav_offload_ratio: float = 0.0
    local_offload_ratio: float = 0.0  # æœ¬åœ°å¤„ç†å æ¯”
    cache_hit_rate: float = 0.0
    cache_miss_rate: float = 0.0
    migration_cost: float = 0.0
    migration_effectiveness: float = 0.0
    prefetch_events: float = 0.0
    total_cache_requests: float = 1.0
    prefetch_lead: float = 0.0
    migration_backoff: float = 0.0


@dataclass
class RewardComponents:
    """åˆ†è§£åçš„æˆæœ¬å’Œå¥–åŠ±ç»„æˆï¼Œä¾¿äºè°ƒè¯•ä¸æ‰©å±•ã€‚"""
    norm_delay: float
    norm_energy: float
    core_cost: float
    drop_penalty: float = 0.0
    completion_gap_penalty: float = 0.0
    data_loss_penalty: float = 0.0
    cache_pressure_penalty: float = 0.0
    queue_penalty: float = 0.0
    remote_reject_penalty: float = 0.0
    offload_bonus: float = 0.0
    local_penalty: float = 0.0  # æœ¬åœ°å¤„ç†é¢å¤–æƒ©ç½š
    cache_penalty: float = 0.0
    cache_bonus: float = 0.0
    migration_penalty: float = 0.0
    joint_coupling_penalty: float = 0.0
    joint_bonus: float = 0.0
    total_cost: float = 0.0
    reward_pre_clip: float = 0.0
    reward: float = 0.0


class UnifiedRewardCalculator:
    """
    å¯å¤ç”¨çš„å¥–åŠ±è®¡ç®—å™¨ï¼Œç”¨äºå•æ™ºèƒ½ä½“è®­ç»ƒå™¨ã€‚
    
    è¯¥ç±»å®ç°äº†ç»Ÿä¸€çš„å¥–åŠ±è®¡ç®—é€»è¾‘ï¼Œæ”¯æŒä¸åŒç®—æ³•ï¼ˆå¦‚SACã€TD3ç­‰ï¼‰çš„ç‰¹å®šéœ€æ±‚ã€‚
    é‡‡ç”¨æˆæœ¬æœ€å°åŒ–æ–¹æ³•ï¼šå»¶è¿Ÿè¶Šä½ã€èƒ½è€—è¶Šä½ã€ä»»åŠ¡ä¸¢å¼ƒè¶Šå°‘ï¼Œå¥–åŠ±è¶Šé«˜ã€‚
    
    Reusable reward calculator for the single-agent trainers.
    """

    def __init__(self, algorithm: str = "general") -> None:
        """
        åˆå§‹åŒ–ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ã€‚
        
        Args:
            algorithm: ç®—æ³•åç§°ï¼Œç”¨äºç‰¹å®šç®—æ³•çš„è°ƒæ•´ï¼ˆå¦‚"SAC"ã€"TD3"ç­‰ï¼‰
                      ä¸åŒç®—æ³•å¯èƒ½æœ‰ä¸åŒçš„å½’ä¸€åŒ–å› å­å’Œå¥–åŠ±èŒƒå›´
        """
        self.algorithm = algorithm.upper()

        # ä»é…ç½®ä¸­è·å–æ ¸å¿ƒæƒé‡å‚æ•°
        # Core weights taken from configuration.
        self.weight_delay = float(config.rl.reward_weight_delay)  # å»¶è¿Ÿæƒé‡
        self.weight_energy = float(config.rl.reward_weight_energy)  # èƒ½è€—æƒé‡
        self.penalty_dropped = float(config.rl.reward_penalty_dropped)  # ä»»åŠ¡ä¸¢å¼ƒæƒ©ç½š
        self.weight_cache = float(getattr(config.rl, "reward_weight_cache", 0.0))
        self.weight_cache_bonus = float(getattr(config.rl, "reward_weight_cache_bonus", 0.0))
        self.weight_migration = float(getattr(config.rl, "reward_weight_migration", 0.0))
        # ğŸ”§ P0ä¿®å¤ï¼šå°†fallbacké»˜è®¤å€¼ä»0.05æ”¹ä¸º0.0ï¼Œé˜²æ­¢bonusæŠµæ¶ˆcore_cost
        self.weight_joint = float(getattr(config.rl, "reward_weight_joint", 0.0))
        # è¾¹ç¼˜è®¡ç®—å¸è½½å¥–åŠ±ï¼šé€‚åº¦æ¿€åŠ±è¿œç¨‹å¤„ç†ï¼ˆé»˜è®¤0.0ï¼Œé¿å…å¹²æ‰°æ ¸å¿ƒä¼˜åŒ–ï¼‰
        # ğŸ”§ P0ä¿®å¤ï¼šå°†fallbacké»˜è®¤å€¼ä»0.5æ”¹ä¸º0.0ï¼Œé˜²æ­¢bonusæŠµæ¶ˆcore_cost
        self.weight_offload_bonus = float(getattr(config.rl, "reward_weight_offload_bonus", 0.0))
        # æœ¬åœ°å¤„ç†æƒ©ç½šï¼šç§»é™¤é¢å¤–æƒ©ç½šï¼ˆé»˜è®¤0.0ï¼‰
        self.weight_local_penalty = float(getattr(config.rl, "reward_weight_local_penalty", 0.0))
        self.completion_target = float(getattr(config.rl, "completion_target", 0.95))
        self.weight_completion_gap = float(getattr(config.rl, "reward_weight_completion_gap", 0.0))
        self.weight_loss_ratio = float(getattr(config.rl, "reward_weight_loss_ratio", 0.0))
        self.cache_pressure_threshold = float(getattr(config.rl, "cache_pressure_threshold", 0.85))
        self.weight_cache_pressure = float(getattr(config.rl, "reward_weight_cache_pressure", 0.0))
        self.weight_queue_overload = float(getattr(config.rl, "reward_weight_queue_overload", 0.0))
        self.weight_remote_reject = float(getattr(config.rl, "reward_weight_remote_reject", 0.0))
        self.latency_target = float(getattr(config.rl, "latency_target", 0.1))  # ğŸ”§ v16: 0.1s
        self.energy_target = float(getattr(config.rl, "energy_target", 10000.0))  # ğŸ”§ v16: 10000J
        self.latency_tolerance = float(getattr(config.rl, "latency_upper_tolerance", self.latency_target * 2.0))
        self.energy_tolerance = float(getattr(config.rl, "energy_upper_tolerance", self.energy_target * 1.5))
        # åˆ†æ®µå®¹é”™/é’³ä½
        # ğŸ”§ v12ä¿®å¤ï¼šæ‰©å¤§è£å‰ªèŒƒå›´é…åˆæ›´å¤§çš„æƒé‡
        # æ ¸å¿ƒæƒé‡å¢åŠ åˆ°(5.0, 3.0)ï¼Œéœ€è¦æ›´å¤§çš„è£å‰ªèŒƒå›´
        self.total_cost_clip = float(getattr(config.rl, "reward_total_cost_clip", 50.0))  # ğŸ”§ v12: 10 â†’ 50
        self.component_clip = float(getattr(config.rl, "reward_component_clip", 10.0))    # ğŸ”§ v12: 3 â†’ 10
        # å½’ä¸€åŒ–ä»»åŠ¡ä¼˜å…ˆçº§æƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # Normalise priority weights if they exist.
        priority_weights = getattr(config, "task", None)
        priority_weights = getattr(priority_weights, "type_priority_weights", None)
        if isinstance(priority_weights, dict) and priority_weights:
            # è®¡ç®—æƒé‡æ€»å’Œå¹¶å½’ä¸€åŒ–
            total = sum(float(v) for v in priority_weights.values()) or 1.0
            self.task_priority_weights = {
                int(task_type): float(value) / total
                for task_type, value in priority_weights.items()
            }
        else:
            # é»˜è®¤æ‰€æœ‰ä»»åŠ¡ç±»å‹æƒé‡ç›¸ç­‰
            self.task_priority_weights = {1: 0.25, 2: 0.25, 3: 0.25, 4: 0.25}

        # ğŸ”§ v27ä¼˜åŒ–ï¼šä½¿ç”¨min-maxå½’ä¸€åŒ–ï¼Œç¡®ä¿å»¶è¿Ÿå’Œèƒ½è€—åœ¨ç›¸åŒé‡çº§[0,1]
        # é—®é¢˜ï¼šç®€å•é™¤ä»¥targetä¼šå¯¼è‡´èŒƒå›´å·®å¼‚å·¨å¤§
        # è§£å†³ï¼šä½¿ç”¨(value - min) / (max - min)å½’ä¸€åŒ–åˆ°[0,1]
        # è¿™æ ·å»¶è¿Ÿå’Œèƒ½è€—éƒ½æœ‰ç›¸åŒçš„ä¿¡å·å¼ºåº¦
        self.delay_min = float(getattr(config.rl, "latency_min", 0.05))  # æœ€ä¼˜å»¶è¿Ÿ(RSUå¤„ç†ç®€å•ä»»åŠ¡)
        self.delay_max = float(getattr(config.rl, "latency_upper_tolerance", 2.0))  # æœ€å·®å»¶è¿Ÿ(æœ¬åœ°å¤„ç†å¤æ‚ä»»åŠ¡)
        self.energy_min = float(getattr(config.rl, "energy_min", 1000.0))  # æœ€ä¼˜èƒ½è€—(episode)
        self.energy_max = float(getattr(config.rl, "energy_upper_tolerance", 25000.0))  # æœ€å·®èƒ½è€—(episode)
        
        # å½’ä¸€åŒ–èŒƒå›´
        self.delay_range = max(self.delay_max - self.delay_min, 1e-6)
        self.energy_range = max(self.energy_max - self.energy_min, 1e-6)
        
        # ä¿ç•™æ—§å‚æ•°å…¼å®¹æ€§
        self.delay_normalizer = self.latency_target
        self.energy_normalizer = self.energy_target
        self.delay_bonus_scale = max(1e-6, self.latency_target)
        self.energy_bonus_scale = max(1e-6, self.energy_target)
        
        # ğŸ†• åŠ¨æ€å½’ä¸€åŒ–é…ç½®
        # âš ï¸ å½“å‰ç¦ç”¨ä»¥æ”¹å–„æ”¶æ•›æ€§ï¼ˆconfig.rl.use_dynamic_reward_normalization=Falseï¼‰
        # å¦‚æœæœªæ¥å¯ç”¨ï¼Œéœ€è¦å……åˆ†æµ‹è¯•åŠ¨æ€å½’ä¸€åŒ–å¯¹è®­ç»ƒç¨³å®šæ€§çš„å½±å“
        self.use_dynamic_normalization = getattr(config.rl, "use_dynamic_reward_normalization", False)
        if self.use_dynamic_normalization:
            self.delay_rms = RunningMeanStd(shape=())
            self.energy_rms = RunningMeanStd(shape=())
            # åˆå§‹åŒ–ä¸ºç›®æ ‡å€¼ï¼Œé¿å…åˆæœŸæ³¢åŠ¨è¿‡å¤§
            self.delay_rms.mean = self.latency_target
            self.energy_rms.mean = self.energy_target
            print(f"   âš ï¸ Dynamic Normalization: ENABLED (Experimental)")
            print(f"      Initial: delay={self.latency_target:.2f}s, energy={self.energy_target:.0f}J")
        else:
            print(f"   Dynamic Normalization: DISABLED (Recommended)")

        # å·²ç§»é™¤SACçš„ç‰¹æ®Šå½’ä¸€åŒ–å‚æ•°ï¼Œæ‰€æœ‰ç®—æ³•ç°åœ¨ä½¿ç”¨ç»Ÿä¸€çš„å½’ä¸€åŒ–é€»è¾‘

        norm_cfg = getattr(config, "normalization", None)
        if norm_cfg is not None:
            self.latency_target = float(getattr(norm_cfg, "delay_reference", self.latency_target))
            self.latency_tolerance = float(getattr(norm_cfg, "delay_upper_reference", self.latency_tolerance))
            self.energy_target = float(getattr(norm_cfg, "energy_reference", self.energy_target))
            self.energy_tolerance = float(getattr(norm_cfg, "energy_upper_reference", self.energy_tolerance))
            self.delay_normalizer = float(
                getattr(norm_cfg, "delay_normalizer_value", self.delay_normalizer)
            )
            self.energy_normalizer = float(
                getattr(norm_cfg, "energy_normalizer_value", self.energy_normalizer)
            )
            self.delay_bonus_scale = max(1e-6, self.latency_target)
            self.energy_bonus_scale = max(1e-6, self.energy_target)

        # ğŸ”§ v29ä¿®å¤ï¼šæ”¶ç´§è£å‰ªèŒƒå›´ï¼Œé¿å…å¥–åŠ±ä¿¡å·è¿‡äºåˆ†æ•£
        # åŸæ¥-50åˆ°0èŒƒå›´å¤ªå®½ï¼Œä¸åŒç­–ç•¥çš„å·®å¼‚è¢«ç¨€é‡Š
        # æ”¶ç´§åˆ°-10åˆ°0ï¼Œæ”¾å¤§ç­–ç•¥æ”¹è¿›çš„åé¦ˆä¿¡å·
        self.reward_clip_range = (-10.0, 0.0)  # ğŸ”§ v29: -50 â†’ -10 (æ”¶ç´§5å€)

        print(f"[OK] Unified reward calculator ({self.algorithm})")
        print(
            f"   Core weights: delay={self.weight_delay:.2f}, "
            f"energy={self.weight_energy:.2f}, drop={self.penalty_dropped:.3f}"
        )
        print(
            f"   Normalisation: delay/{self.delay_normalizer:.2f}, "
            f"energy/{self.energy_normalizer:.0f}"
        )

    # ------------------------------------------------------------------ #
    # è¾…åŠ©æ–¹æ³• / Helpers

    @staticmethod
    def _safe_float(value: Optional[float], default: float = 0.0) -> float:
        """
        å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°ã€‚
        
        Args:
            value: å¾…è½¬æ¢çš„å€¼
            default: è½¬æ¢å¤±è´¥æ—¶çš„é»˜è®¤å€¼
            
        Returns:
            è½¬æ¢åçš„æµ®ç‚¹æ•°æˆ–é»˜è®¤å€¼
        """
        if value is None:
            return default
        try:
            val = float(value)
            if not np.isfinite(val):
                return default
            return val
        except (TypeError, ValueError):
            return default

    @staticmethod
    def _safe_int(value: Optional[int], default: int = 0) -> int:
        """
        å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•´æ•°ã€‚
        
        Args:
            value: å¾…è½¬æ¢çš„å€¼
            default: è½¬æ¢å¤±è´¥æ—¶çš„é»˜è®¤å€¼
            
        Returns:
            è½¬æ¢åçš„æ•´æ•°æˆ–é»˜è®¤å€¼
        """
        if value is None:
            return default
        try:
            if isinstance(value, float) and not np.isfinite(value):
                return default
            return int(value)
        except (TypeError, ValueError, OverflowError):
            return default

    @staticmethod
    def _to_float_list(source) -> List[float]:
        """
        å°†è¾“å…¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°åˆ—è¡¨ã€‚
        
        æ”¯æŒnumpyæ•°ç»„ã€åˆ—è¡¨ã€å…ƒç»„ç­‰å¤šç§è¾“å…¥ç±»å‹ã€‚
        å¦‚æœæŸä¸ªå…ƒç´ æ— æ³•è½¬æ¢ï¼Œåˆ™ä½¿ç”¨0.0ä½œä¸ºé»˜è®¤å€¼ã€‚
        
        Args:
            source: å¾…è½¬æ¢çš„æ•°æ®æº
            
        Returns:
            æµ®ç‚¹æ•°åˆ—è¡¨
        """
        if isinstance(source, np.ndarray):
            iterable = source.tolist()
        elif isinstance(source, (list, tuple)):
            iterable = list(source)
        else:
            return []
        result: List[float] = []
        for item in iterable:
            try:
                result.append(float(item))
            except (TypeError, ValueError):
                result.append(0.0)
        return result

    @staticmethod
    def _piecewise_ratio(value: float, target: float, tolerance: float) -> float:
        """
        åˆ†æ®µå®¹é”™çš„å½’ä¸€åŒ–æ¯”ä¾‹ï¼šä½äºç›®æ ‡æ—¶åŠå¹…æƒ©ç½šï¼Œç›®æ ‡-å®¹å·®çº¿æ€§ï¼Œè¶…å®¹å·®è¶…çº¿æ€§ã€‚
        """
        v = max(0.0, float(value))
        t = max(1e-6, float(target))
        tol = max(t, float(tolerance))
        if v <= t:
            return 0.5 * (v / t)
        if v <= tol:
            return 1.0 + (v - t) / max(tol - t, 1e-6)
        return 2.0 + (v - tol) / max(t, 1e-6)

    # ------------------------------------------------------------------ #
    # ------------------------------------------------------------------ #
    # ??API / Public API

    def _extract_metrics(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict],
        migration_metrics: Optional[Dict],
    ) -> RewardMetrics:
        """?????????????????????"""
        metrics = RewardMetrics()
        metrics.avg_delay = max(0.0, self._safe_float(system_metrics.get("avg_task_delay")))
        metrics.total_energy = max(0.0, self._safe_float(system_metrics.get("total_energy_consumption")))
        metrics.dropped_tasks = max(0, self._safe_int(system_metrics.get("dropped_tasks")))
        metrics.completion_rate = max(0.0, self._safe_float(system_metrics.get("task_completion_rate")))
        metrics.data_loss_ratio = max(0.0, self._safe_float(system_metrics.get("data_loss_ratio_bytes")))
        metrics.cache_utilization = max(0.0, self._safe_float(system_metrics.get("cache_utilization")))
        metrics.queue_overload_events = max(0.0, self._safe_float(system_metrics.get("queue_overload_events")))
        metrics.remote_rejection_rate = max(0.0, self._safe_float(system_metrics.get("remote_rejection_rate")))
        metrics.rsu_offload_ratio = max(0.0, self._safe_float(system_metrics.get("rsu_offload_ratio")))
        metrics.uav_offload_ratio = max(0.0, self._safe_float(system_metrics.get("uav_offload_ratio")))
        metrics.local_offload_ratio = max(0.0, self._safe_float(system_metrics.get("local_offload_ratio", 0.0)))

        if cache_metrics:
            metrics.cache_hit_rate = float(max(0.0, min(1.0, self._safe_float(cache_metrics.get("hit_rate"), 0.0))))
            metrics.cache_miss_rate = float(max(0.0, min(1.0, self._safe_float(cache_metrics.get("miss_rate"), 0.0))))
            metrics.prefetch_events = max(0.0, self._safe_float(cache_metrics.get("prefetch_events"), 0.0))
            metrics.total_cache_requests = max(1.0, self._safe_float(cache_metrics.get("total_requests"), 1.0))
            cache_joint = cache_metrics.get("joint_params", {}) if isinstance(cache_metrics, dict) else {}
            metrics.prefetch_lead = self._safe_float(cache_joint.get("prefetch_lead_time"), 0.0)
        if migration_metrics:
            metrics.migration_cost = max(0.0, self._safe_float(migration_metrics.get("migration_cost"), 0.0))
            metrics.migration_effectiveness = float(max(0.0, min(1.0, self._safe_float(migration_metrics.get("effectiveness"), 0.0))))
            migration_joint = migration_metrics.get("joint_params", {}) if isinstance(migration_metrics, dict) else {}
            metrics.migration_backoff = float(max(0.0, min(1.0, self._safe_float(migration_joint.get("migration_backoff"), 0.0))))
        return metrics

    def _compute_components(self, m: RewardMetrics) -> RewardComponents:
        """
        ğŸ”§ v27ä¼˜åŒ–ï¼šä½¿ç”¨min-maxå½’ä¸€åŒ–ï¼Œå»¶è¿Ÿå’Œèƒ½è€—éƒ½åœ¨[0,1]èŒƒå›´
        
        è®¾è®¡ç†å¿µï¼š
        1. norm = (value - min) / (max - min) â†’ [0, 1]
        2. å½“value=minæ—¶ï¼Œnorm=0ï¼ˆæœ€ä¼˜ï¼Œæ— æƒ©ç½šï¼‰
        3. å½“value=maxæ—¶ï¼Œnorm=1ï¼ˆæœ€å·®ï¼Œæœ€å¤§æƒ©ç½šï¼‰
        4. è¶…å‡ºèŒƒå›´æ—¶çº¿æ€§å¤–æ¨ï¼ˆå…è®¸norm>1ï¼‰
        
        è¿™æ ·æ™ºèƒ½ä½“ç›´æ¥ä¼˜åŒ–ï¼šminimize(0.5*norm_delay + 0.5*norm_energy)
        """
        # --- æ ¸å¿ƒæˆæœ¬ï¼šmin-maxå½’ä¸€åŒ–åˆ°[0,1] ---
        # å»¶è¿Ÿå½’ä¸€åŒ–
        delay_val = max(0.0, m.avg_delay)
        norm_delay = (delay_val - self.delay_min) / self.delay_range
        norm_delay = max(0.0, norm_delay)  # ä¸å…è®¸è´Ÿå€¼
        
        # èƒ½è€—å½’ä¸€åŒ–
        energy_val = max(0.0, m.total_energy)
        norm_energy = (energy_val - self.energy_min) / self.energy_range
        norm_energy = max(0.0, norm_energy)  # ä¸å…è®¸è´Ÿå€¼
        
        # ğŸ”§ v29ä¼˜åŒ–ï¼šçº¯æˆæœ¬æœ€å°åŒ–ï¼Œä¸æ··å…¥bonus
        # åŠ æƒå’Œï¼ˆä½¿ç”¨é…ç½®çš„æƒé‡ï¼‰
        delay_penalty = self.weight_delay * norm_delay
        energy_penalty = self.weight_energy * norm_energy
        core_cost = delay_penalty + energy_penalty

        # --- ğŸ”§ v29ä¼˜åŒ–ï¼šç®€åŒ–æƒ©ç½šé¡¹ï¼Œä¿æŒæ¸…æ™°è¯­ä¹‰ ---
        # 1. ä¸¢å¼ƒæƒ©ç½š - æ¯ä¸¢ä¸€ä¸ªä»»åŠ¡æ‰£åˆ†
        drop_penalty = self.penalty_dropped * m.dropped_tasks
        
        # 2. å®Œæˆç‡æƒ©ç½š - åªåœ¨ä½äºç›®æ ‡æ—¶æ‰æƒ©ç½šï¼Œä½¿ç”¨å¹³æ–¹é¡¹æ”¾å¤§å·®è·
        # ğŸ”§ v29: ä½¿ç”¨softæƒ©ç½šï¼Œé¿å…è¿‡åº¦ä¿å®ˆ
        completion_gap = max(0.0, self.completion_target - m.completion_rate)
        completion_gap_penalty = self.weight_completion_gap * (completion_gap ** 1.5)  # å¹³æ–¹æ ¹æ”¾å¤§
        
        # 3. æ•°æ®ä¸¢å¤±æƒ©ç½š - ç›´æ¥ä½¿ç”¨ä¸¢å¤±æ¯”ä¾‹
        data_loss_penalty = self.weight_loss_ratio * m.data_loss_ratio
        
        # ğŸ”§ v29ï¼šç§»é™¤offload_bonusï¼Œé¿å…bonus/costè¯­ä¹‰æ··æ·†
        # åŸæ¥çš„offload_bonusä¼šè®©æ™ºèƒ½ä½“å›°æƒ‘ï¼šåˆ°åº•æ˜¯æœ€å°åŒ–costè¿˜æ˜¯æœ€å¤§åŒ–bonusï¼Ÿ
        # ç°åœ¨çº¯ç²¹åšæˆæœ¬æœ€å°åŒ–ï¼Œè¯­ä¹‰æ›´æ¸…æ™°
        offload_bonus = 0.0
        delay_improvement_bonus = 0.0
        
        # å…¶ä»–æƒ©ç½šé¡¹ï¼ˆä¿æŒä¸º0ï¼Œéœ€è¦æ—¶å†å¯ç”¨ï¼‰
        cache_pressure_penalty = 0.0
        queue_penalty = 0.0
        remote_reject_penalty = 0.0
        local_penalty = 0.0
        cache_penalty = 0.0
        migration_penalty = 0.0
        cache_bonus = 0.0
        joint_bonus = 0.0
        joint_coupling_penalty = 0.0

        # ğŸ”§ v29ï¼šç®€åŒ–æ€»æˆæœ¬è®¡ç®—
        total_cost = core_cost + drop_penalty + completion_gap_penalty + data_loss_penalty
        total_cost = float(np.clip(total_cost, 0.0, self.total_cost_clip))

        return RewardComponents(
            norm_delay=norm_delay,
            norm_energy=norm_energy,
            core_cost=core_cost,
            drop_penalty=drop_penalty,
            completion_gap_penalty=completion_gap_penalty,
            data_loss_penalty=data_loss_penalty,
            cache_pressure_penalty=cache_pressure_penalty,
            queue_penalty=queue_penalty,
            remote_reject_penalty=remote_reject_penalty,
            offload_bonus=offload_bonus,
            local_penalty=local_penalty,
            cache_penalty=cache_penalty,
            cache_bonus=cache_bonus,
            migration_penalty=migration_penalty,
            joint_coupling_penalty=joint_coupling_penalty,
            joint_bonus=joint_bonus,
            total_cost=total_cost,
            reward_pre_clip=-total_cost,
            reward=-total_cost,
        )

    def _compose_reward(self, components: RewardComponents, completion_rate: float) -> RewardComponents:
        """ç»„è£…æœ€ç»ˆå¥–åŠ±
        
        ğŸ”§ MDPä¼˜åŒ–v2.0: è½¬ä¸ºæ­£å‘å¥–åŠ±ç©ºé—´ [0, 10]
        
        ä¼˜åŒ–ç‚¹:
        1. ä½¿ç”¨æ­£å‘å¥–åŠ±ï¼ˆåŸºçº¿å¥–åŠ± - æˆæœ¬ï¼‰æ›¿ä»£è´Ÿå‘å¥–åŠ±
        2. å¢åŠ ä»»åŠ¡å®Œæˆç‡bonusï¼Œæä¾›å³æ—¶åé¦ˆ
        3. æ”¾å¤§å¥–åŠ±å·®å¼‚ï¼Œè®©ç­–ç•¥æ”¹è¿›æ›´æ˜æ˜¾
        
        å¥–åŠ±èŒƒå›´: [0, 10]ï¼Œè¶Šæ¥è¿‘10è¡¨ç¤ºæ€§èƒ½è¶Šå¥½
        - 0: æœ€å·®æƒ…å†µï¼ˆé«˜å»¶è¿Ÿã€é«˜èƒ½è€—ã€ä½å®Œæˆç‡ï¼‰
        - 10: ç†æƒ³æƒ…å†µï¼ˆä½å»¶è¿Ÿã€ä½èƒ½è€—ã€é«˜å®Œæˆç‡ï¼‰
        """
        import os
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ­£å‘å¥–åŠ±æ¨¡å¼
        # ğŸ”§ ä¿®å¤ï¼šé»˜è®¤ç¦ç”¨æ­£å‘å¥–åŠ±ï¼Œå›å½’ä¼ ç»Ÿçš„è´Ÿå€¼æƒ©ç½šæ¨¡å¼
        # ç”¨æˆ·åé¦ˆæ­£å‘å¥–åŠ±å¯¼è‡´æ›²çº¿å¹³å¦ï¼Œæ— æ³•è§‚å¯Ÿåˆ°æ”¶æ•›è¶‹åŠ¿
        use_positive_reward = os.environ.get('MDP_POSITIVE_REWARD', '0') == '1'
        
        if use_positive_reward:
            # === æ­£å‘å¥–åŠ±æ¨¡å¼ ===
            # åŸºçº¿å¥–åŠ±ï¼šè¡¨ç¤ºç†æƒ³æƒ…å†µçš„æœ€é«˜åˆ†
            baseline_reward = 10.0
            
            # æ ¸å¿ƒæˆæœ¬å½’ä¸€åŒ–åˆ°[0, baseline]èŒƒå›´
            # total_cost é€šå¸¸åœ¨ [0, total_cost_clip] èŒƒå›´
            normalized_cost = min(components.total_cost / max(self.total_cost_clip, 1.0), 1.0)
            
            # åŸºç¡€å¥–åŠ± = åŸºçº¿ - æˆæœ¬
            base_reward = baseline_reward * (1.0 - normalized_cost * 0.8)  # æˆæœ¬æœ€å¤šæ‰£é™¤80%
            
            # ä»»åŠ¡å®Œæˆç‡bonusï¼šæä¾›å³æ—¶æ­£å‘åé¦ˆ
            completion_bonus = completion_rate * 2.0  # æœ€é«˜+2åˆ†
            
            # ä½å»¶è¿Ÿå¥–åŠ±ï¼šé¼“åŠ±å¿«é€Ÿå¤„ç†
            if components.norm_delay < 0.5:  # å»¶è¿Ÿä½äºç›®æ ‡çš„50%
                delay_bonus = (0.5 - components.norm_delay) * 2.0  # æœ€é«˜+1åˆ†
            else:
                delay_bonus = 0.0
            
            # ç»„åˆå¥–åŠ±
            reward_raw = base_reward + completion_bonus + delay_bonus
            
            # è£å‰ªåˆ°[0, 12]èŒƒå›´
            reward_clipped = float(np.clip(reward_raw, 0.0, 12.0))
        else:
            # === åŸå§‹è´Ÿå‘å¥–åŠ±æ¨¡å¼ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰ ===
            reward_scale = float(getattr(config.rl, 'reward_scale', 1.0))
            reward_raw = -abs(components.total_cost) * reward_scale
            reward_clipped = float(np.clip(reward_raw, 
                                           self.reward_clip_range[0] * reward_scale, 
                                           self.reward_clip_range[1]))
        
        components.reward_pre_clip = reward_raw
        components.reward = reward_clipped if np.isfinite(reward_clipped) else 0.0
        return components

    def calculate_reward(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict] = None,
        migration_metrics: Optional[Dict] = None,
    ) -> tuple[float, Dict[str, float]]:
        """è®¡ç®—å¥–åŠ±å¹¶è¿”å›æ ‡é‡å€¼å’Œç»„ä»¶å­—å…¸
        
        Returns:
            tuple: (reward, reward_components)
                - reward: æ€»å¥–åŠ±æ ‡é‡å€¼
                - reward_components: åŒ…å«å„åˆ†é‡çš„å­—å…¸
        """
        # ğŸ”§ ä¿®å¤: åªåœ¨ä¸»è¿›ç¨‹æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œé¿å…å¹¶è¡Œç¯å¢ƒæ—¥å¿—æ··ä¹±
        import os
        is_main_process = os.environ.get('WORKER_ID', '0') == '0'
        
        if not hasattr(self, '_debug_count'):
            self._debug_count = 0

        metrics = self._extract_metrics(system_metrics, cache_metrics, migration_metrics)
        components = self._compute_components(metrics)
        components = self._compose_reward(components, metrics.completion_rate)
        
        # ä»…ä¸»è¿›ç¨‹æ‰“å°å‰2æ¬¡è°ƒè¯•ä¿¡æ¯
        if is_main_process and self._debug_count < 2:
            print(f"[RewardDebug] delay={system_metrics.get('avg_task_delay', 0):.4f}, energy={system_metrics.get('total_energy_consumption', 0):.1f}")
            print(f"  reward={components.reward:.4f}, core_cost={components.core_cost:.4f}")
            self._debug_count += 1
        
        # æ„é€ å¥–åŠ±ç»„ä»¶å­—å…¸ä¾›è°ƒè¯•ä½¿ç”¨
        reward_components = {
            'delay': -components.norm_delay * self.weight_delay,
            'energy': -components.norm_energy * self.weight_energy,
            'cache': -components.cache_penalty + components.cache_bonus,
            'penalty': -(components.drop_penalty + components.completion_gap_penalty + 
                        components.queue_penalty + components.remote_reject_penalty),
            'core_cost': -components.core_cost,
            'total': components.reward
        }
        
        final_reward = components.reward if np.isfinite(components.reward) else 0.0
        return final_reward, reward_components
    
    def calculate_instant_reward(
        self,
        step_metrics: Dict,
        prev_metrics: Dict = None,
    ) -> tuple[float, Dict[str, float]]:
        """
        ğŸ†• MDPä¼˜åŒ–: è®¡ç®—å³æ—¶å¥–åŠ±ï¼ˆåŸºäºå•æ­¥å¢é‡ï¼‰
        
        ä¼˜åŒ–ç‚¹:
        1. ä½¿ç”¨å•æ­¥å¢é‡è€Œéç´¯ç§¯å€¼
        2. æä¾›å³æ—¶ä»»åŠ¡å®Œæˆå¥–åŠ±
        3. å¥–åŠ±ä¸åŠ¨ä½œç›´æ¥ç›¸å…³ï¼Œå¢å¼ºå› æœå…³ç³»
        
        Args:
            step_metrics: æœ¬æ­¥çš„å¢é‡æŒ‡æ ‡
                - step_completed: æœ¬æ­¥å®Œæˆä»»åŠ¡æ•°
                - step_dropped: æœ¬æ­¥ä¸¢å¼ƒä»»åŠ¡æ•°
                - step_energy: æœ¬æ­¥èƒ½è€—(J)
                - step_delay: æœ¬æ­¥å¹³å‡å»¶è¿Ÿ(s)
                - cache_hits: æœ¬æ­¥ç¼“å­˜å‘½ä¸­æ•°
            prev_metrics: ä¸Šä¸€æ­¥çš„æŒ‡æ ‡ï¼ˆç”¨äºè®¡ç®—æ”¹å–„ï¼‰
        
        Returns:
            (instant_reward, reward_breakdown)
        """
        prev_metrics = prev_metrics or {}
        
        # æå–æœ¬æ­¥æŒ‡æ ‡
        step_completed = int(step_metrics.get('step_completed', 0))
        step_dropped = int(step_metrics.get('step_dropped', 0))
        step_energy = float(step_metrics.get('step_energy', 0.0))
        step_delay = float(step_metrics.get('step_delay', 0.0))
        cache_hits = int(step_metrics.get('cache_hits', 0))
        step_total = step_completed + step_dropped
        
        # === å³æ—¶å¥–åŠ±ç»„ä»¶ ===
        
        # 1. ä»»åŠ¡å®Œæˆå¥–åŠ±ï¼ˆæ¯å®Œæˆä¸€ä¸ªä»»åŠ¡+1åˆ†ï¼‰
        completion_reward = float(step_completed) * 1.0
        
        # 2. ä»»åŠ¡ä¸¢å¼ƒæƒ©ç½šï¼ˆæ¯ä¸¢å¼ƒä¸€ä¸ªä»»åŠ¡-2åˆ†ï¼‰
        drop_penalty = float(step_dropped) * 2.0
        
        # 3. èƒ½è€—æƒ©ç½šï¼ˆå½’ä¸€åŒ–åçš„èƒ½è€—ï¼‰
        energy_penalty = min(1.0, step_energy / 100.0) * 1.0  # æ¯æ­¥100Jæ‰£åˆ†1åˆ†
        
        # 4. å»¶è¿Ÿæƒ©ç½šï¼ˆä»…å½“å»¶è¿Ÿè¶…è¿‡ç›®æ ‡æ—¶ï¼‰
        delay_target = 0.5  # ç›®æ ‡å»¶è¿Ÿ0.5s
        if step_delay > delay_target:
            delay_penalty = (step_delay - delay_target) * 2.0  # æ¯è¶…è¿‡0.1sæ‰£åˆ†0.2åˆ†
        else:
            delay_penalty = 0.0
            # ä½å»¶è¿Ÿå¥–åŠ±
            delay_bonus = (delay_target - step_delay) / delay_target * 0.5
            completion_reward += delay_bonus
        
        # 5. ç¼“å­˜å‘½ä¸­å¥–åŠ±
        cache_reward = float(cache_hits) * 0.5
        
        # 6. æ”¹å–„å¥–åŠ±ï¼ˆå¦‚æœæœ‰ä¸Šä¸€æ­¥æ•°æ®ï¼‰
        improvement_bonus = 0.0
        if prev_metrics:
            prev_delay = float(prev_metrics.get('step_delay', step_delay))
            prev_energy = float(prev_metrics.get('step_energy', step_energy))
            
            # å»¶è¿Ÿæ”¹å–„
            if step_delay < prev_delay:
                improvement_bonus += (prev_delay - step_delay) * 1.0
            
            # èƒ½è€—æ”¹å–„
            if step_energy < prev_energy:
                improvement_bonus += (prev_energy - step_energy) / 100.0 * 0.5
        
        # === ç»„åˆæœ€ç»ˆå¥–åŠ± ===
        # åŸºçº¿å¥–åŠ± + å®Œæˆå¥–åŠ± - æƒ©ç½š
        baseline = 2.0  # æ¯æ­¥åŸºçº¿å¥–åŠ±
        instant_reward = (
            baseline
            + completion_reward
            + cache_reward
            + improvement_bonus
            - drop_penalty
            - energy_penalty
            - delay_penalty
        )
        
        # è£å‰ªåˆ°åˆç†èŒƒå›´
        instant_reward = float(np.clip(instant_reward, -5.0, 10.0))
        
        # æ„é€ å¥–åŠ±åˆ†è§£
        reward_breakdown = {
            'baseline': baseline,
            'completion': completion_reward,
            'cache': cache_reward,
            'improvement': improvement_bonus,
            'drop_penalty': -drop_penalty,
            'energy_penalty': -energy_penalty,
            'delay_penalty': -delay_penalty if step_delay > delay_target else 0.0,
            'total': instant_reward,
        }
        
        return instant_reward, reward_breakdown

    def update_targets(
        self,
        latency_target: Optional[float] = None,
        energy_target: Optional[float] = None,
    ) -> None:
        """åŠ¨æ€æ›´æ–°ç›®æ ‡å€¼ï¼Œä½¿å¥–åŠ±å‡½æ•°å¯ä»¥åœ¨è®­ç»ƒä¸­è‡ªé€‚åº”æ‹“æ‰‘å˜åŒ–ã€‚
        
        ğŸ”§ P0ä¿®å¤ï¼šåŒæ­¥æ›´æ–°å½’ä¸€åŒ–å› å­ï¼Œç¡®ä¿å¥–åŠ±è®¡ç®—ä¸ç›®æ ‡å€¼ä¸€è‡´
        """
        if latency_target is not None:
            self.latency_target = float(latency_target)
            self.latency_tolerance = float(
                getattr(config.rl, "latency_upper_tolerance", self.latency_target * 2.0)
            )
            # ğŸ”§ P0ä¿®å¤ï¼šåŒæ­¥å½’ä¸€åŒ–å› å­
            self.delay_normalizer = self.latency_target
            self.delay_bonus_scale = max(1e-6, self.latency_target)
        if energy_target is not None:
            self.energy_target = float(energy_target)
            self.energy_tolerance = float(
                getattr(config.rl, "energy_upper_tolerance", self.energy_target * 1.5)
            )
            # ğŸ”§ P0ä¿®å¤ï¼šåŒæ­¥å½’ä¸€åŒ–å› å­
            self.energy_normalizer = self.energy_target
            self.energy_bonus_scale = max(1e-6, self.energy_target)

    def get_reward_breakdown(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict] = None,
        migration_metrics: Optional[Dict] = None,
    ) -> str:
        """ç”Ÿæˆå¥–åŠ±åˆ†è§£ï¼Œä¾¿äºå¿«é€Ÿè¯Šæ–­å„æˆæœ¬æ¥æºã€‚"""
        metrics = self._extract_metrics(system_metrics, cache_metrics, migration_metrics)
        components = self._compute_components(metrics)
        components = self._compose_reward(components, metrics.completion_rate)

        lines = [
            f"Reward report ({self.algorithm}):",
            f"  Total Reward        : {components.reward:.4f} (pre-clip {components.reward_pre_clip:.4f})",
            f"  Core Cost (D+E)     : {components.core_cost:.4f}",
            f"    - Delay (norm/w)  : {components.norm_delay:.4f} / w={self.weight_delay}",
            f"    - Energy (norm/w) : {components.norm_energy:.4f} / w={self.weight_energy}",
            f"  Drop Penalty        : {components.drop_penalty:.4f}",
            f"  Completion Gap      : {components.completion_gap_penalty:.4f}",
            f"  Data Loss Penalty   : {components.data_loss_penalty:.4f}",
            f"  Cache Pressure      : {components.cache_pressure_penalty:.4f}",
            f"  Queue Penalty       : {components.queue_penalty:.4f}",
            f"  Remote Reject       : {components.remote_reject_penalty:.4f}",
            f"  Cache Penalty/Bonus : {components.cache_penalty:.4f} / -{components.cache_bonus:.4f}",
            f"  Migration Penalty   : {components.migration_penalty:.4f}",
            f"  Joint Penalty/Bonus : {components.joint_coupling_penalty:.4f} / -{components.joint_bonus:.4f}",
            f"  Offload Bonus       : -{components.offload_bonus:.4f}",
            f"  ----------------------------------------",
            f"  Total Cost          : {components.total_cost:.4f}",
        ]
        return "\n".join(lines)


# ---------------------------------------------------------------------- #
# ä¾¿æ·çš„å•ä¾‹å¯¹è±¡ï¼Œåœ¨æ•´ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨
# Convenience singletons used across the project.

_general_reward_calculator = UnifiedRewardCalculator(algorithm="general")
_sac_reward_calculator = UnifiedRewardCalculator(algorithm="sac")


def calculate_unified_reward(
    system_metrics: Dict,
    cache_metrics: Optional[Dict] = None,
    migration_metrics: Optional[Dict] = None,
    algorithm: str = "general",
) -> float:
    """
    ç»Ÿä¸€å¥–åŠ±è®¡ç®—çš„ä¾¿æ·å‡½æ•°ã€‚
    
    æ ¹æ®æŒ‡å®šç®—æ³•é€‰æ‹©ç›¸åº”çš„å¥–åŠ±è®¡ç®—å™¨ï¼Œè®¡ç®—å¹¶è¿”å›å¥–åŠ±å€¼ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        cache_metrics: å¯é€‰çš„ç¼“å­˜æŒ‡æ ‡
        migration_metrics: å¯é€‰çš„è¿ç§»æŒ‡æ ‡
        algorithm: ç®—æ³•åç§°ï¼ˆ"SAC"æˆ–"general"ï¼‰
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    calculator = _sac_reward_calculator if algorithm.upper() == "SAC" else _general_reward_calculator
    reward, _ = calculator.calculate_reward(system_metrics, cache_metrics, migration_metrics)
    return reward


def get_reward_breakdown(system_metrics: Dict, algorithm: str = "general") -> str:
    """
    è·å–å¥–åŠ±åˆ†è§£æŠ¥å‘Šçš„ä¾¿æ·å‡½æ•°ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        algorithm: ç®—æ³•åç§°ï¼ˆ"SAC"æˆ–"general"ï¼‰
        
    Returns:
        æ ¼å¼åŒ–çš„å¥–åŠ±åˆ†è§£æŠ¥å‘Šå­—ç¬¦ä¸²
    """
    calculator = _sac_reward_calculator if algorithm.upper() == "SAC" else _general_reward_calculator
    return calculator.get_reward_breakdown(system_metrics)


def update_reward_targets(
    latency_target: Optional[float] = None,
    energy_target: Optional[float] = None,
) -> None:
    """
    åŠ¨æ€æ›´æ–°å…¨å±€å¥–åŠ±ç›®æ ‡ï¼Œç¡®ä¿å•ä¾‹è®¡ç®—å™¨ä¸å…¨å±€configä¿æŒåŒæ­¥ã€‚
    """
    if latency_target is not None:
        config.rl.latency_target = float(latency_target)
    if energy_target is not None:
        config.rl.energy_target = float(energy_target)
    _general_reward_calculator.update_targets(latency_target, energy_target)
    _sac_reward_calculator.update_targets(latency_target, energy_target)


# ---------------------------------------------------------------------- #
# å‘åå…¼å®¹çš„è¾…åŠ©å‡½æ•°åç§°
# Backwards-compatible helper names.

def calculate_enhanced_reward(
    system_metrics: Dict,
    cache_metrics: Optional[Dict] = None,
    migration_metrics: Optional[Dict] = None,
) -> float:
    """
    å¢å¼ºå¥–åŠ±è®¡ç®—ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    
    è¿™æ˜¯calculate_unified_rewardçš„åˆ«åï¼Œä½¿ç”¨"general"ç®—æ³•ã€‚
    ä¿ç•™æ­¤å‡½æ•°ä»¥ç¡®ä¿ä¸æ—§ä»£ç çš„å…¼å®¹æ€§ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        cache_metrics: å¯é€‰çš„ç¼“å­˜æŒ‡æ ‡
        migration_metrics: å¯é€‰çš„è¿ç§»æŒ‡æ ‡
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    return calculate_unified_reward(system_metrics, cache_metrics, migration_metrics, "general")


def calculate_sac_reward(system_metrics: Dict) -> float:
    """
    SACç®—æ³•ä¸“ç”¨å¥–åŠ±è®¡ç®—ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    
    ä¸ºSACç®—æ³•æä¾›æ­£å‘å¥–åŠ±ç©ºé—´çš„ä¾¿æ·å‡½æ•°ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼ï¼ˆå¯èƒ½ä¸ºæ­£å€¼ï¼‰
    """
    return calculate_unified_reward(system_metrics, algorithm="sac")


def calculate_simple_reward(system_metrics: Dict) -> float:
    """
    ç®€å•å¥–åŠ±è®¡ç®—ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    
    è¿™æ˜¯calculate_unified_rewardçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨"general"ç®—æ³•ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    return calculate_unified_reward(system_metrics, algorithm="general")
