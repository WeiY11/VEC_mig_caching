#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ (Unified Reward Calculator)
é€‚ç”¨äºæ‰€æœ‰å•æ™ºèƒ½ä½“DRLç®—æ³•ï¼ˆDDPG, TD3, DQN, PPO, SACï¼‰

è®¾è®¡åŸåˆ™ï¼š
1. æ ¸å¿ƒä¼˜åŒ–ç›®æ ‡ï¼šæ—¶å»¶ + èƒ½è€—åŒç›®æ ‡åŠ æƒå’Œ
2. è¾…åŠ©çº¦æŸï¼šé€šè¿‡ä¸¢å¼ƒä»»åŠ¡æƒ©ç½šä¿è¯å®Œæˆç‡
3. æˆæœ¬æœ€å°åŒ–ï¼šå¥–åŠ±ä¸¥æ ¼ä¸ºè´Ÿå€¼ï¼ˆæˆæœ¬ï¼‰
4. ç®—æ³•é€‚é…ï¼šSACä¿ç•™è½»å¾®è°ƒæ•´ä»¥é€‚åº”æœ€å¤§ç†µæ¡†æ¶
"""

import numpy as np
from typing import Dict, Optional
from config import config


class UnifiedRewardCalculator:
    """
    ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ - æ‰€æœ‰ç®—æ³•å…±äº«æ ¸å¿ƒé€»è¾‘
    """

    def __init__(self, algorithm: str = "general"):
        """
        åˆå§‹åŒ–ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨
        
        Args:
            algorithm: ç®—æ³•ç±»å‹ ("general", "sac")
                - "general": é€šç”¨ç‰ˆæœ¬ï¼ˆDDPG, TD3, DQN, PPOï¼‰
                - "sac": SACä¸“ç”¨ç‰ˆæœ¬ï¼ˆè€ƒè™‘æœ€å¤§ç†µç‰¹æ€§ï¼‰
        """
        self.algorithm = algorithm.upper()
        
        # ä»é…ç½®åŠ è½½æ ¸å¿ƒæƒé‡
        self.weight_delay = config.rl.reward_weight_delay      # é»˜è®¤ 2.0
        self.weight_energy = config.rl.reward_weight_energy    # é»˜è®¤ 1.2
        self.penalty_dropped = config.rl.reward_penalty_dropped # é»˜è®¤ 0.02
        
        # ğŸ¯ æ ¸å¿ƒè®¾è®¡ï¼šå½’ä¸€åŒ–å› å­ï¼ˆç¡®ä¿æ—¶å»¶å’Œèƒ½è€—åœ¨ç›¸åŒæ•°é‡çº§ï¼‰
        # ç›®æ ‡ï¼šdelay=0.2s å’Œ energy=600J å½’ä¸€åŒ–åè´¡çŒ®ç›¸å½“
        self.delay_normalizer = 1.0      # 0.2s â†’ 0.2
        self.energy_normalizer = 600.0   # ğŸ”§ è°ƒæ•´ï¼šçªå‡ºèƒ½è€—åé¦ˆ
        
        # ğŸ”§ SACä¸“ç”¨è°ƒæ•´ï¼šæ›´æ¿€è¿›çš„å½’ä¸€åŒ–ä»¥å¹³è¡¡æ¢ç´¢
        if self.algorithm == "SAC":
            self.delay_normalizer = 0.3      # 0.2s â†’ 0.67ï¼ˆæ›´æ•æ„Ÿï¼‰
            self.energy_normalizer = 1500.0  # 1000J â†’ 0.67ï¼ˆæ›´æ•æ„Ÿï¼‰
        
        # å¥–åŠ±èŒƒå›´é™åˆ¶
        if self.algorithm == "SAC":
            # SACå…è®¸å°å¹…æ­£å€¼å¥–åŠ±ï¼ˆæœ€å¤§ç†µéœ€è¦æ˜ç¡®æ¿€åŠ±ï¼‰
            self.reward_clip_range = (-15.0, 3.0)
        else:
            # é€šç”¨ç‰ˆæœ¬ï¼šçº¯æˆæœ¬æœ€å°åŒ–
            self.reward_clip_range = (-25.0, -0.01)
        
        print(f"[OK] ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨åˆå§‹åŒ– ({self.algorithm})")
        print(f"   æ ¸å¿ƒæƒé‡: Delay={self.weight_delay:.1f}, Energy={self.weight_energy:.1f}")
        print(f"   å½’ä¸€åŒ–: Delay/{self.delay_normalizer:.1f}, Energy/{self.energy_normalizer:.0f}")
        print(f"   å¥–åŠ±èŒƒå›´: {self.reward_clip_range}")
        print(f"   ä¼˜åŒ–ç›®æ ‡: æœ€å°åŒ– {self.weight_delay}*Delay + {self.weight_energy}*Energy")

    def calculate_reward(self, 
                        system_metrics: Dict,
                        cache_metrics: Optional[Dict] = None,
                        migration_metrics: Optional[Dict] = None) -> float:
        """
        è®¡ç®—ç»Ÿä¸€å¥–åŠ±ï¼ˆæ”¯æŒç¼“å­˜å’Œè¿ç§»æŒ‡æ ‡ï¼Œä½†ä¸å½±å“æ ¸å¿ƒå¥–åŠ±ï¼‰
        
        Args:
            system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
            cache_metrics: ç¼“å­˜æŒ‡æ ‡ï¼ˆå¯é€‰ï¼Œç”¨äºæœªæ¥æ‰©å±•ï¼‰
            migration_metrics: è¿ç§»æŒ‡æ ‡ï¼ˆå¯é€‰ï¼Œç”¨äºæœªæ¥æ‰©å±•ï¼‰
        
        Returns:
            reward: æ ‡é‡å¥–åŠ±å€¼
        """
        # 1ï¸âƒ£ æå–æ ¸å¿ƒæŒ‡æ ‡ï¼ˆå®‰å…¨å¤„ç†Noneå€¼ï¼‰
        def safe_float(value, default=0.0):
            """å®‰å…¨è½¬æ¢ä¸ºfloatï¼Œå¤„ç†Noneå’Œå¼‚å¸¸å€¼"""
            if value is None:
                return default
            try:
                return max(0.0, float(value))
            except (TypeError, ValueError):
                return default
        
        def safe_int(value, default=0):
            """å®‰å…¨è½¬æ¢ä¸ºintï¼Œå¤„ç†Noneå’Œå¼‚å¸¸å€¼"""
            if value is None:
                return default
            try:
                return max(0, int(value))
            except (TypeError, ValueError):
                return default
        
        avg_delay = safe_float(system_metrics.get('avg_task_delay'), 0.0)
        total_energy = safe_float(system_metrics.get('total_energy_consumption'), 0.0)
        dropped_tasks = safe_int(system_metrics.get('dropped_tasks'), 0)
        
        # 2ï¸âƒ£ å½’ä¸€åŒ–
        norm_delay = avg_delay / self.delay_normalizer
        norm_energy = total_energy / self.energy_normalizer
        
        # 3ï¸âƒ£ è®¡ç®—åŸºç¡€æˆæœ¬ï¼ˆåŒç›®æ ‡åŠ æƒå’Œï¼‰
        base_cost = (self.weight_delay * norm_delay + 
                     self.weight_energy * norm_energy)
        
        # 4ï¸âƒ£ ä¸¢å¼ƒä»»åŠ¡æƒ©ç½šï¼ˆä¿è¯å®Œæˆç‡çº¦æŸï¼‰
        dropped_penalty = self.penalty_dropped * dropped_tasks
        
        # 5ï¸âƒ£ è‡ªé€‚åº”é˜ˆå€¼æƒ©ç½šï¼ˆé˜²æ­¢æç«¯æƒ…å†µï¼‰
        delay_threshold_penalty = 0.0
        energy_threshold_penalty = 0.0
        
        if self.algorithm == "SAC":
            # SACï¼šæ›´æ¿€è¿›çš„é˜ˆå€¼æƒ©ç½š
            if avg_delay > 0.25:
                delay_threshold_penalty = (avg_delay - 0.25) * 8.0
            if total_energy > 2000:
                energy_threshold_penalty = (total_energy - 2000) / 1000.0
        else:
            # é€šç”¨ç®—æ³•ï¼šæ¸©å’Œçš„é˜ˆå€¼æƒ©ç½š
            if avg_delay > 0.30:
                delay_threshold_penalty = (avg_delay - 0.30) * 5.0
            if total_energy > 3000:
                energy_threshold_penalty = (total_energy - 3000) / 1500.0
        
        # 6ï¸âƒ£ æ€»æˆæœ¬
        total_cost = (base_cost + 
                     dropped_penalty + 
                     delay_threshold_penalty + 
                     energy_threshold_penalty)
        
        # 7ï¸âƒ£ SACä¸“ç”¨ï¼šæ­£å‘æ¿€åŠ±æœºåˆ¶ï¼ˆæœ€å¤§ç†µæ¡†æ¶éœ€è¦æ˜ç¡®"å¥½"çš„ä¿¡å·ï¼‰
        bonus = 0.0
        if self.algorithm == "SAC":
            completion_rate = safe_float(system_metrics.get('task_completion_rate'), 0.0)
            
            # å»¶è¿Ÿä¼˜ç§€å¥–åŠ±
            if avg_delay < 0.20:
                bonus += (0.20 - avg_delay) * 3.0
            
            # å®Œæˆç‡ä¼˜ç§€å¥–åŠ±
            if completion_rate > 0.95:
                bonus += (completion_rate - 0.95) * 15.0
        
        # 8ï¸âƒ£ æœ€ç»ˆå¥–åŠ±
        if self.algorithm == "SAC":
            reward = bonus - total_cost  # SAC: bonuså¯èƒ½ä¸ºæ­£
        else:
            reward = -total_cost  # é€šç”¨: çº¯è´Ÿå€¼æˆæœ¬
        
        # 9ï¸âƒ£ è£å‰ªåˆ°åˆç†èŒƒå›´
        clipped_reward = np.clip(reward, *self.reward_clip_range)
        
        return clipped_reward
    
    def get_reward_breakdown(self, system_metrics: Dict) -> str:
        """è·å–å¥–åŠ±åˆ†è§£çš„å¯è¯»æŠ¥å‘Š"""
        def safe_float(value, default=0.0):
            if value is None:
                return default
            try:
                return max(0.0, float(value))
            except (TypeError, ValueError):
                return default
        
        def safe_int(value, default=0):
            if value is None:
                return default
            try:
                return max(0, int(value))
            except (TypeError, ValueError):
                return default
        
        avg_delay = safe_float(system_metrics.get('avg_task_delay'), 0.0)
        total_energy = safe_float(system_metrics.get('total_energy_consumption'), 0.0)
        dropped_tasks = safe_int(system_metrics.get('dropped_tasks'), 0)
        completion_rate = safe_float(system_metrics.get('task_completion_rate'), 0.0)
        
        reward = self.calculate_reward(system_metrics)
        
        breakdown = f"""
å¥–åŠ±åˆ†è§£æŠ¥å‘Š ({self.algorithm}):
â”œâ”€â”€ æ€»å¥–åŠ±: {reward:.3f}
â”œâ”€â”€ æ ¸å¿ƒæŒ‡æ ‡:
â”‚   â”œâ”€â”€ æ—¶å»¶: {avg_delay:.3f}s (å½’ä¸€åŒ–: {avg_delay/self.delay_normalizer:.3f})
â”‚   â”œâ”€â”€ èƒ½è€—: {total_energy:.1f}J (å½’ä¸€åŒ–: {total_energy/self.energy_normalizer:.3f})
â”‚   â””â”€â”€ å®Œæˆç‡: {completion_rate:.1%}
â”œâ”€â”€ æˆæœ¬è´¡çŒ®:
â”‚   â”œâ”€â”€ æ—¶å»¶æˆæœ¬: {self.weight_delay * avg_delay/self.delay_normalizer:.3f}
â”‚   â”œâ”€â”€ èƒ½è€—æˆæœ¬: {self.weight_energy * total_energy/self.energy_normalizer:.3f}
â”‚   â””â”€â”€ ä¸¢å¼ƒæƒ©ç½š: {self.penalty_dropped * dropped_tasks:.3f} ({dropped_tasks}ä¸ªä»»åŠ¡)
â””â”€â”€ ä¼˜åŒ–æ–¹å‘: {'æœ€å¤§åŒ–å¥–åŠ±ï¼ˆå«bonusï¼‰' if self.algorithm == 'SAC' else 'æœ€å°åŒ–æˆæœ¬'}
        """
        
        return breakdown.strip()


# ==================== å…¨å±€å®ä¾‹å’Œä¾¿æ·æ¥å£ ====================

# é€šç”¨ç‰ˆæœ¬ï¼ˆDDPG, TD3, DQN, PPOï¼‰
_general_reward_calculator = UnifiedRewardCalculator(algorithm="general")

# SACä¸“ç”¨ç‰ˆæœ¬
_sac_reward_calculator = UnifiedRewardCalculator(algorithm="sac")


def calculate_unified_reward(system_metrics: Dict,
                             cache_metrics: Optional[Dict] = None,
                             migration_metrics: Optional[Dict] = None,
                             algorithm: str = "general") -> float:
    """
    ç»Ÿä¸€å¥–åŠ±è®¡ç®—æ¥å£ï¼ˆæ‰€æœ‰ç®—æ³•è°ƒç”¨ï¼‰
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        cache_metrics: ç¼“å­˜æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
        migration_metrics: è¿ç§»æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
        algorithm: ç®—æ³•ç±»å‹ ("general" æˆ– "sac")
    
    Returns:
        reward: æ ‡é‡å¥–åŠ±å€¼
    """
    if algorithm.upper() == "SAC":
        calculator = _sac_reward_calculator
    else:
        calculator = _general_reward_calculator
    
    return calculator.calculate_reward(system_metrics, cache_metrics, migration_metrics)


def get_reward_breakdown(system_metrics: Dict, algorithm: str = "general") -> str:
    """è·å–å¥–åŠ±åˆ†è§£æŠ¥å‘Š"""
    if algorithm.upper() == "SAC":
        calculator = _sac_reward_calculator
    else:
        calculator = _general_reward_calculator
    
    return calculator.get_reward_breakdown(system_metrics)


# ==================== å‘åå…¼å®¹æ¥å£ ====================

def calculate_enhanced_reward(system_metrics: Dict,
                             cache_metrics: Optional[Dict] = None,
                             migration_metrics: Optional[Dict] = None) -> float:
    """å‘åå…¼å®¹æ¥å£ï¼ˆä¾›ç°æœ‰ä»£ç è°ƒç”¨ï¼‰"""
    return calculate_unified_reward(system_metrics, cache_metrics, migration_metrics, "general")


def calculate_sac_reward(system_metrics: Dict) -> float:
    """SACä¸“ç”¨æ¥å£ï¼ˆå‘åå…¼å®¹ï¼‰"""
    return calculate_unified_reward(system_metrics, algorithm="sac")


def calculate_simple_reward(system_metrics: Dict) -> float:
    """ç®€åŒ–æ¥å£ï¼ˆå‘åå…¼å®¹ï¼‰"""
    return calculate_unified_reward(system_metrics, algorithm="general")

