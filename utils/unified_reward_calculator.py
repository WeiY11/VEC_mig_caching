#!/usr/bin/env python3
"""
ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ï¼Œä¾›æ‰€æœ‰å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•ä½¿ç”¨ã€‚

æ ¸å¿ƒç†å¿µæ˜¯æˆæœ¬æœ€å°åŒ–ï¼šæ›´ä½çš„å»¶è¿Ÿå’Œæ›´ä½çš„èƒ½è€—ä¼šå¸¦æ¥æ›´é«˜ï¼ˆæ›´å°‘è´Ÿå€¼ï¼‰çš„å¥–åŠ±ã€‚
æŸäº›ç®—æ³•ï¼ˆå¦‚SACï¼‰æœŸæœ›æ­£å‘å¥–åŠ±ï¼Œå› æ­¤æˆ‘ä»¬ä¸ºè¿™ç§æƒ…å†µä¿ç•™äº†ä¸€ä¸ªå°çš„å¯é€‰å¥–åŠ±ã€‚

Unified reward calculator used by all single-agent RL algorithms.
The philosophy is cost-minimisation: lower latency and lower energy
lead to higher (less negative) rewards. Some algorithms (SAC) expect
positive rewards, so we keep a small optional bonus for that case.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, Optional, List

import numpy as np

from config import config


@dataclass
class RewardMetrics:
    """æå–åçš„åŸå§‹æŒ‡æ ‡ï¼Œä¾¿äºåç»­ç»Ÿä¸€è®¡ç®—ã€‚"""
    avg_delay: float = 0.0
    total_energy: float = 0.0
    dropped_tasks: int = 0
    completion_rate: float = 0.0
    data_loss_ratio: float = 0.0
    cache_utilization: float = 0.0
    queue_overload_events: float = 0.0
    remote_rejection_rate: float = 0.0
    rsu_offload_ratio: float = 0.0
    uav_offload_ratio: float = 0.0
    cache_hit_rate: float = 0.0
    cache_miss_rate: float = 0.0
    migration_cost: float = 0.0
    migration_effectiveness: float = 0.0
    prefetch_events: float = 0.0
    total_cache_requests: float = 1.0
    prefetch_lead: float = 0.0
    migration_backoff: float = 0.0


@dataclass
class RewardComponents:
    """åˆ†è§£åçš„æˆæœ¬å’Œå¥–åŠ±ç»„æˆï¼Œä¾¿äºè°ƒè¯•ä¸æ‰©å±•ã€‚"""
    norm_delay: float
    norm_energy: float
    core_cost: float
    drop_penalty: float = 0.0
    completion_gap_penalty: float = 0.0
    data_loss_penalty: float = 0.0
    cache_pressure_penalty: float = 0.0
    queue_penalty: float = 0.0
    remote_reject_penalty: float = 0.0
    offload_bonus: float = 0.0
    cache_penalty: float = 0.0
    cache_bonus: float = 0.0
    migration_penalty: float = 0.0
    joint_coupling_penalty: float = 0.0
    joint_bonus: float = 0.0
    total_cost: float = 0.0
    reward_pre_clip: float = 0.0
    reward: float = 0.0


class UnifiedRewardCalculator:
    """
    å¯å¤ç”¨çš„å¥–åŠ±è®¡ç®—å™¨ï¼Œç”¨äºå•æ™ºèƒ½ä½“è®­ç»ƒå™¨ã€‚
    
    è¯¥ç±»å®ç°äº†ç»Ÿä¸€çš„å¥–åŠ±è®¡ç®—é€»è¾‘ï¼Œæ”¯æŒä¸åŒç®—æ³•ï¼ˆå¦‚SACã€TD3ç­‰ï¼‰çš„ç‰¹å®šéœ€æ±‚ã€‚
    é‡‡ç”¨æˆæœ¬æœ€å°åŒ–æ–¹æ³•ï¼šå»¶è¿Ÿè¶Šä½ã€èƒ½è€—è¶Šä½ã€ä»»åŠ¡ä¸¢å¼ƒè¶Šå°‘ï¼Œå¥–åŠ±è¶Šé«˜ã€‚
    
    Reusable reward calculator for the single-agent trainers.
    """

    def __init__(self, algorithm: str = "general") -> None:
        """
        åˆå§‹åŒ–ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ã€‚
        
        Args:
            algorithm: ç®—æ³•åç§°ï¼Œç”¨äºç‰¹å®šç®—æ³•çš„è°ƒæ•´ï¼ˆå¦‚"SAC"ã€"TD3"ç­‰ï¼‰
                      ä¸åŒç®—æ³•å¯èƒ½æœ‰ä¸åŒçš„å½’ä¸€åŒ–å› å­å’Œå¥–åŠ±èŒƒå›´
        """
        self.algorithm = algorithm.upper()

        # ä»é…ç½®ä¸­è·å–æ ¸å¿ƒæƒé‡å‚æ•°
        # Core weights taken from configuration.
        self.weight_delay = float(config.rl.reward_weight_delay)  # å»¶è¿Ÿæƒé‡
        self.weight_energy = float(config.rl.reward_weight_energy)  # èƒ½è€—æƒé‡
        self.penalty_dropped = float(config.rl.reward_penalty_dropped)  # ä»»åŠ¡ä¸¢å¼ƒæƒ©ç½š
        self.weight_cache = float(getattr(config.rl, "reward_weight_cache", 0.0))  # ç¼“å­˜æƒé‡
        self.weight_cache_bonus = float(getattr(config.rl, "reward_weight_cache_bonus", 0.0))
        self.weight_migration = float(getattr(config.rl, "reward_weight_migration", 0.0))  # è¿ç§»æƒé‡
        self.weight_joint = float(getattr(config.rl, "reward_weight_joint", 0.05))  # ç¼“å­˜-è¿ç§»è”åŠ¨æƒé‡
        # ğŸ”§ æ–°å¢ï¼šè¿œç¨‹å¸è½½æ¿€åŠ±æƒé‡
        self.weight_offload_bonus = float(getattr(config.rl, "reward_weight_offload_bonus", 0.15))  # è¾¹ç¼˜è®¡ç®—åˆ©ç”¨å¥–åŠ±
        self.completion_target = float(getattr(config.rl, "completion_target", 0.95))
        self.weight_completion_gap = float(getattr(config.rl, "reward_weight_completion_gap", 0.0))
        self.weight_loss_ratio = float(getattr(config.rl, "reward_weight_loss_ratio", 0.0))
        self.cache_pressure_threshold = float(getattr(config.rl, "cache_pressure_threshold", 0.85))
        self.weight_cache_pressure = float(getattr(config.rl, "reward_weight_cache_pressure", 0.0))
        self.weight_queue_overload = float(getattr(config.rl, "reward_weight_queue_overload", 0.0))
        self.weight_remote_reject = float(getattr(config.rl, "reward_weight_remote_reject", 0.0))
        self.latency_target = float(getattr(config.rl, "latency_target", 0.4))
        self.energy_target = float(getattr(config.rl, "energy_target", 2200.0))
        self.latency_tolerance = float(getattr(config.rl, "latency_upper_tolerance", self.latency_target * 2.0))
        self.energy_tolerance = float(getattr(config.rl, "energy_upper_tolerance", self.energy_target * 1.5))
        # åˆ†æ®µå®¹é”™/é’³ä½
        self.total_cost_clip = float(getattr(config.rl, "reward_total_cost_clip", 120.0))
        self.component_clip = float(getattr(config.rl, "reward_component_clip", 25.0))

        # å½’ä¸€åŒ–ä»»åŠ¡ä¼˜å…ˆçº§æƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # Normalise priority weights if they exist.
        priority_weights = getattr(config, "task", None)
        priority_weights = getattr(priority_weights, "type_priority_weights", None)
        if isinstance(priority_weights, dict) and priority_weights:
            # è®¡ç®—æƒé‡æ€»å’Œå¹¶å½’ä¸€åŒ–
            total = sum(float(v) for v in priority_weights.values()) or 1.0
            self.task_priority_weights = {
                int(task_type): float(value) / total
                for task_type, value in priority_weights.items()
            }
        else:
            # é»˜è®¤æ‰€æœ‰ä»»åŠ¡ç±»å‹æƒé‡ç›¸ç­‰
            self.task_priority_weights = {1: 0.25, 2: 0.25, 3: 0.25, 4: 0.25}

        # ğŸ”§ ä¿®å¤ï¼šå½’ä¸€åŒ–å› å­å¿…é¡»ä¸ä¼˜åŒ–ç›®æ ‡å€¼å¯¹é½
        # Normalisation factors MUST align with optimization targets (latency_target and energy_target).
        # æ ¹æ®è®­ç»ƒç»“æœï¼ˆEpisode 1000+ï¼‰ï¼šå»¶è¿Ÿç¨³å®šåœ¨~0.05sï¼Œèƒ½è€—ç¨³å®šåœ¨~5000J
        # ç›®æ ‡å€¼è®¾ç½®ä¸ºï¼šlatency_target=0.4s, energy_target=1200J
        # å› æ­¤å½’ä¸€åŒ–åŸºå‡†åº”è¯¥ç›´æ¥ä½¿ç”¨è¿™äº›ç›®æ ‡å€¼ï¼Œè€Œéç¡¬ç¼–ç çš„0.2så’Œ1000J
        self.delay_normalizer = self.latency_target  # ä¸ç›®æ ‡å€¼å¯¹é½
        self.energy_normalizer = self.energy_target  # ä¸ç›®æ ‡å€¼å¯¹é½
        self.delay_bonus_scale = max(1e-6, self.latency_target)
        self.energy_bonus_scale = max(1e-6, self.energy_target)
        
        # SACç®—æ³•ä½¿ç”¨ä¸åŒçš„å½’ä¸€åŒ–å‚æ•°ï¼ˆä½†ä»éœ€ä¸targetå¯¹é½ï¼‰
        if self.algorithm == "SAC":
            self.delay_normalizer = self.latency_target  # å¯¹é½
            self.energy_normalizer = self.energy_target  # å¯¹é½

        norm_cfg = getattr(config, "normalization", None)
        if norm_cfg is not None:
            self.latency_target = float(getattr(norm_cfg, "delay_reference", self.latency_target))
            self.latency_tolerance = float(getattr(norm_cfg, "delay_upper_reference", self.latency_tolerance))
            self.energy_target = float(getattr(norm_cfg, "energy_reference", self.energy_target))
            self.energy_tolerance = float(getattr(norm_cfg, "energy_upper_reference", self.energy_tolerance))
            self.delay_normalizer = float(
                getattr(norm_cfg, "delay_normalizer_value", self.delay_normalizer)
            )
            self.energy_normalizer = float(
                getattr(norm_cfg, "energy_normalizer_value", self.energy_normalizer)
            )
            self.delay_bonus_scale = max(1e-6, self.latency_target)
            self.energy_bonus_scale = max(1e-6, self.energy_target)

        # è®¾ç½®å¥–åŠ±è£å‰ªèŒƒå›´ï¼Œé˜²æ­¢å¥–åŠ±å€¼è¿‡å¤§æˆ–è¿‡å°
        if self.algorithm == "SAC":
            self.reward_clip_range = (-15.0, 3.0)  # SACæœŸæœ›è¾ƒå°çš„å¥–åŠ±èŒƒå›´
        else:
            self.reward_clip_range = (-80.0, -0.005)  # å…¶ä»–ç®—æ³•ä½¿ç”¨è´Ÿå€¼èŒƒå›´

        print(f"[OK] Unified reward calculator ({self.algorithm})")
        print(
            f"   Core weights: delay={self.weight_delay:.2f}, "
            f"energy={self.weight_energy:.2f}, drop={self.penalty_dropped:.3f}"
        )
        print(
            f"   Normalisation: delay/{self.delay_normalizer:.2f}, "
            f"energy/{self.energy_normalizer:.0f}"
        )

    # ------------------------------------------------------------------ #
    # è¾…åŠ©æ–¹æ³• / Helpers

    @staticmethod
    def _safe_float(value: Optional[float], default: float = 0.0) -> float:
        """
        å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°ã€‚
        
        Args:
            value: å¾…è½¬æ¢çš„å€¼
            default: è½¬æ¢å¤±è´¥æ—¶çš„é»˜è®¤å€¼
            
        Returns:
            è½¬æ¢åçš„æµ®ç‚¹æ•°æˆ–é»˜è®¤å€¼
        """
        if value is None:
            return default
        try:
            val = float(value)
            if not np.isfinite(val):
                return default
            return val
        except (TypeError, ValueError):
            return default

    @staticmethod
    def _safe_int(value: Optional[int], default: int = 0) -> int:
        """
        å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•´æ•°ã€‚
        
        Args:
            value: å¾…è½¬æ¢çš„å€¼
            default: è½¬æ¢å¤±è´¥æ—¶çš„é»˜è®¤å€¼
            
        Returns:
            è½¬æ¢åçš„æ•´æ•°æˆ–é»˜è®¤å€¼
        """
        if value is None:
            return default
        try:
            if isinstance(value, float) and not np.isfinite(value):
                return default
            return int(value)
        except (TypeError, ValueError, OverflowError):
            return default

    @staticmethod
    def _to_float_list(source) -> List[float]:
        """
        å°†è¾“å…¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°åˆ—è¡¨ã€‚
        
        æ”¯æŒnumpyæ•°ç»„ã€åˆ—è¡¨ã€å…ƒç»„ç­‰å¤šç§è¾“å…¥ç±»å‹ã€‚
        å¦‚æœæŸä¸ªå…ƒç´ æ— æ³•è½¬æ¢ï¼Œåˆ™ä½¿ç”¨0.0ä½œä¸ºé»˜è®¤å€¼ã€‚
        
        Args:
            source: å¾…è½¬æ¢çš„æ•°æ®æº
            
        Returns:
            æµ®ç‚¹æ•°åˆ—è¡¨
        """
        if isinstance(source, np.ndarray):
            iterable = source.tolist()
        elif isinstance(source, (list, tuple)):
            iterable = list(source)
        else:
            return []
        result: List[float] = []
        for item in iterable:
            try:
                result.append(float(item))
            except (TypeError, ValueError):
                result.append(0.0)
        return result

    @staticmethod
    def _piecewise_ratio(value: float, target: float, tolerance: float) -> float:
        """
        åˆ†æ®µå®¹é”™çš„å½’ä¸€åŒ–æ¯”ä¾‹ï¼šä½äºç›®æ ‡æ—¶åŠå¹…æƒ©ç½šï¼Œç›®æ ‡-å®¹å·®çº¿æ€§ï¼Œè¶…å®¹å·®è¶…çº¿æ€§ã€‚
        """
        v = max(0.0, float(value))
        t = max(1e-6, float(target))
        tol = max(t, float(tolerance))
        if v <= t:
            return 0.5 * (v / t)
        if v <= tol:
            return 1.0 + (v - t) / max(tol - t, 1e-6)
        return 2.0 + (v - tol) / max(t, 1e-6)

    # ------------------------------------------------------------------ #
    # ------------------------------------------------------------------ #
    # ??API / Public API

    def _extract_metrics(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict],
        migration_metrics: Optional[Dict],
    ) -> RewardMetrics:
        """?????????????????????"""
        metrics = RewardMetrics()
        metrics.avg_delay = max(0.0, self._safe_float(system_metrics.get("avg_task_delay")))
        metrics.total_energy = max(0.0, self._safe_float(system_metrics.get("total_energy_consumption")))
        metrics.dropped_tasks = max(0, self._safe_int(system_metrics.get("dropped_tasks")))
        metrics.completion_rate = max(0.0, self._safe_float(system_metrics.get("task_completion_rate")))
        metrics.data_loss_ratio = max(0.0, self._safe_float(system_metrics.get("data_loss_ratio_bytes")))
        metrics.cache_utilization = max(0.0, self._safe_float(system_metrics.get("cache_utilization")))
        metrics.queue_overload_events = max(0.0, self._safe_float(system_metrics.get("queue_overload_events")))
        metrics.remote_rejection_rate = max(0.0, self._safe_float(system_metrics.get("remote_rejection_rate")))
        metrics.rsu_offload_ratio = max(0.0, self._safe_float(system_metrics.get("rsu_offload_ratio")))
        metrics.uav_offload_ratio = max(0.0, self._safe_float(system_metrics.get("uav_offload_ratio")))

        if cache_metrics:
            metrics.cache_hit_rate = float(max(0.0, min(1.0, self._safe_float(cache_metrics.get("hit_rate"), 0.0))))
            metrics.cache_miss_rate = float(max(0.0, min(1.0, self._safe_float(cache_metrics.get("miss_rate"), 0.0))))
            metrics.prefetch_events = max(0.0, self._safe_float(cache_metrics.get("prefetch_events"), 0.0))
            metrics.total_cache_requests = max(1.0, self._safe_float(cache_metrics.get("total_requests"), 1.0))
            cache_joint = cache_metrics.get("joint_params", {}) if isinstance(cache_metrics, dict) else {}
            metrics.prefetch_lead = self._safe_float(cache_joint.get("prefetch_lead_time"), 0.0)
        if migration_metrics:
            metrics.migration_cost = max(0.0, self._safe_float(migration_metrics.get("migration_cost"), 0.0))
            metrics.migration_effectiveness = float(max(0.0, min(1.0, self._safe_float(migration_metrics.get("effectiveness"), 0.0))))
            migration_joint = migration_metrics.get("joint_params", {}) if isinstance(migration_metrics, dict) else {}
            metrics.migration_backoff = float(max(0.0, min(1.0, self._safe_float(migration_joint.get("migration_backoff"), 0.0))))
        return metrics

    def _compute_components(self, m: RewardMetrics) -> RewardComponents:
        """????/????????"""
        norm_delay = self._piecewise_ratio(m.avg_delay, self.latency_target, self.latency_tolerance)
        norm_energy = self._piecewise_ratio(m.total_energy, self.energy_target, self.energy_tolerance)
        core_cost = self.weight_delay * norm_delay + self.weight_energy * norm_energy

        drop_penalty = self.penalty_dropped * m.dropped_tasks
        completion_gap_penalty = self.weight_completion_gap * max(0.0, self.completion_target - m.completion_rate) if self.weight_completion_gap > 0.0 else 0.0
        data_loss_penalty = self.weight_loss_ratio * m.data_loss_ratio if self.weight_loss_ratio > 0.0 else 0.0
        cache_pressure_penalty = 0.0
        if self.weight_cache_pressure > 0.0 and m.cache_utilization > self.cache_pressure_threshold:
            cache_pressure_penalty = self.weight_cache_pressure * (m.cache_utilization - self.cache_pressure_threshold)
        queue_penalty = self.weight_queue_overload * m.queue_overload_events if self.weight_queue_overload > 0.0 else 0.0
        remote_reject_penalty = self.weight_remote_reject * m.remote_rejection_rate if self.weight_remote_reject > 0.0 else 0.0

        offload_bonus = self.weight_offload_bonus * (m.rsu_offload_ratio + m.uav_offload_ratio) if self.weight_offload_bonus > 0.0 else 0.0
        cache_penalty = self.weight_cache * m.cache_miss_rate if self.weight_cache > 0.0 else 0.0
        cache_bonus = self.weight_cache_bonus * m.cache_hit_rate if self.weight_cache_bonus > 0.0 else 0.0
        migration_penalty = self.weight_migration * m.migration_cost if self.weight_migration > 0.0 else 0.0

        joint_bonus = 0.0
        joint_coupling_penalty = 0.0
        if self.weight_joint > 0.0:
            joint_bonus = self.weight_joint * (m.cache_hit_rate * m.migration_effectiveness)
            prefetch_ratio = np.clip(m.prefetch_events / max(1.0, m.total_cache_requests), 0.0, 1.0)
            coupling_penalty = max(0.0, 0.3 - prefetch_ratio) * 0.5
            coupling_penalty += abs(m.prefetch_lead - 1.5) * 0.05
            coupling_penalty += m.migration_backoff * 0.1
            joint_coupling_penalty = self.weight_joint * coupling_penalty

        def _clip(x: float) -> float:
            return float(np.clip(x, -self.component_clip, self.component_clip))

        total_cost = (
            core_cost
            + _clip(drop_penalty)
            + _clip(completion_gap_penalty)
            + _clip(data_loss_penalty)
            + _clip(cache_pressure_penalty)
            + _clip(queue_penalty)
            + _clip(remote_reject_penalty)
            + _clip(cache_penalty)
            + _clip(migration_penalty)
            + _clip(joint_coupling_penalty)
            - _clip(offload_bonus)
            - _clip(cache_bonus)
            - _clip(joint_bonus)
        )
        total_cost = float(np.clip(total_cost, -self.total_cost_clip, self.total_cost_clip))

        return RewardComponents(
            norm_delay=norm_delay,
            norm_energy=norm_energy,
            core_cost=core_cost,
            drop_penalty=drop_penalty,
            completion_gap_penalty=completion_gap_penalty,
            data_loss_penalty=data_loss_penalty,
            cache_pressure_penalty=cache_pressure_penalty,
            queue_penalty=queue_penalty,
            remote_reject_penalty=remote_reject_penalty,
            offload_bonus=offload_bonus,
            cache_penalty=cache_penalty,
            cache_bonus=cache_bonus,
            migration_penalty=migration_penalty,
            joint_coupling_penalty=joint_coupling_penalty,
            joint_bonus=joint_bonus,
            total_cost=total_cost,
            reward_pre_clip=-total_cost,
            reward=-total_cost,
        )

    def _compose_reward(self, components: RewardComponents, completion_rate: float) -> RewardComponents:
        """????????????????"""
        if self.algorithm == "SAC":
            base_reward = 5.0
            completion_bonus = (completion_rate - 0.95) * 10.0 if completion_rate > 0.95 else 0.0
            reward_raw = base_reward + completion_bonus - components.total_cost
            reward_clipped = float(np.clip(reward_raw, -15.0, 10.0))
        else:
            reward_raw = -components.total_cost
            reward_clipped = float(np.clip(reward_raw, -20.0, 0.0))
        components.reward_pre_clip = reward_raw
        components.reward = reward_clipped if np.isfinite(reward_clipped) else 0.0
        return components

    def calculate_reward(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict] = None,
        migration_metrics: Optional[Dict] = None,
    ) -> float:
        """???????????????"""
        metrics = self._extract_metrics(system_metrics, cache_metrics, migration_metrics)
        components = self._compute_components(metrics)
        components = self._compose_reward(components, metrics.completion_rate)
        return components.reward if np.isfinite(components.reward) else 0.0

    def update_targets(
        self,
        latency_target: Optional[float] = None,
        energy_target: Optional[float] = None,
    ) -> None:
        """åŠ¨æ€æ›´æ–°ç›®æ ‡å€¼ï¼Œä½¿å¥–åŠ±å‡½æ•°å¯ä»¥åœ¨è®­ç»ƒä¸­è‡ªé€‚åº”æ‹“æ‰‘å˜åŒ–ã€‚"""
        if latency_target is not None:
            self.latency_target = float(latency_target)
            self.latency_tolerance = float(
                getattr(config.rl, "latency_upper_tolerance", self.latency_target * 2.0)
            )
            self.delay_bonus_scale = max(1e-6, self.latency_target)
        if energy_target is not None:
            self.energy_target = float(energy_target)
            self.energy_tolerance = float(
                getattr(config.rl, "energy_upper_tolerance", self.energy_target * 1.5)
            )
            self.energy_bonus_scale = max(1e-6, self.energy_target)

    def get_reward_breakdown(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict] = None,
        migration_metrics: Optional[Dict] = None,
    ) -> str:
        """ç”Ÿæˆå¥–åŠ±åˆ†è§£ï¼Œä¾¿äºå¿«é€Ÿè¯Šæ–­å„æˆæœ¬æ¥æºã€‚"""
        metrics = self._extract_metrics(system_metrics, cache_metrics, migration_metrics)
        components = self._compute_components(metrics)
        components = self._compose_reward(components, metrics.completion_rate)

        lines = [
            f"Reward report ({self.algorithm}):",
            f"  Total Reward        : {components.reward:.4f} (pre-clip {components.reward_pre_clip:.4f})",
            f"  Core Cost (D+E)     : {components.core_cost:.4f}",
            f"    - Delay (norm/w)  : {components.norm_delay:.4f} / w={self.weight_delay}",
            f"    - Energy (norm/w) : {components.norm_energy:.4f} / w={self.weight_energy}",
            f"  Drop Penalty        : {components.drop_penalty:.4f}",
            f"  Completion Gap      : {components.completion_gap_penalty:.4f}",
            f"  Data Loss Penalty   : {components.data_loss_penalty:.4f}",
            f"  Cache Pressure      : {components.cache_pressure_penalty:.4f}",
            f"  Queue Penalty       : {components.queue_penalty:.4f}",
            f"  Remote Reject       : {components.remote_reject_penalty:.4f}",
            f"  Cache Penalty/Bonus : {components.cache_penalty:.4f} / -{components.cache_bonus:.4f}",
            f"  Migration Penalty   : {components.migration_penalty:.4f}",
            f"  Joint Penalty/Bonus : {components.joint_coupling_penalty:.4f} / -{components.joint_bonus:.4f}",
            f"  Offload Bonus       : -{components.offload_bonus:.4f}",
            f"  ----------------------------------------",
            f"  Total Cost          : {components.total_cost:.4f}",
        ]
        return "\n".join(lines)


# ---------------------------------------------------------------------- #
# ä¾¿æ·çš„å•ä¾‹å¯¹è±¡ï¼Œåœ¨æ•´ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨
# Convenience singletons used across the project.

_general_reward_calculator = UnifiedRewardCalculator(algorithm="general")
_sac_reward_calculator = UnifiedRewardCalculator(algorithm="sac")


def calculate_unified_reward(
    system_metrics: Dict,
    cache_metrics: Optional[Dict] = None,
    migration_metrics: Optional[Dict] = None,
    algorithm: str = "general",
) -> float:
    """
    ç»Ÿä¸€å¥–åŠ±è®¡ç®—çš„ä¾¿æ·å‡½æ•°ã€‚
    
    æ ¹æ®æŒ‡å®šç®—æ³•é€‰æ‹©ç›¸åº”çš„å¥–åŠ±è®¡ç®—å™¨ï¼Œè®¡ç®—å¹¶è¿”å›å¥–åŠ±å€¼ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        cache_metrics: å¯é€‰çš„ç¼“å­˜æŒ‡æ ‡
        migration_metrics: å¯é€‰çš„è¿ç§»æŒ‡æ ‡
        algorithm: ç®—æ³•åç§°ï¼ˆ"SAC"æˆ–"general"ï¼‰
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    calculator = _sac_reward_calculator if algorithm.upper() == "SAC" else _general_reward_calculator
    return calculator.calculate_reward(system_metrics, cache_metrics, migration_metrics)


def get_reward_breakdown(system_metrics: Dict, algorithm: str = "general") -> str:
    """
    è·å–å¥–åŠ±åˆ†è§£æŠ¥å‘Šçš„ä¾¿æ·å‡½æ•°ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        algorithm: ç®—æ³•åç§°ï¼ˆ"SAC"æˆ–"general"ï¼‰
        
    Returns:
        æ ¼å¼åŒ–çš„å¥–åŠ±åˆ†è§£æŠ¥å‘Šå­—ç¬¦ä¸²
    """
    calculator = _sac_reward_calculator if algorithm.upper() == "SAC" else _general_reward_calculator
    return calculator.get_reward_breakdown(system_metrics)


def update_reward_targets(
    latency_target: Optional[float] = None,
    energy_target: Optional[float] = None,
) -> None:
    """
    åŠ¨æ€æ›´æ–°å…¨å±€å¥–åŠ±ç›®æ ‡ï¼Œç¡®ä¿å•ä¾‹è®¡ç®—å™¨ä¸å…¨å±€configä¿æŒåŒæ­¥ã€‚
    """
    if latency_target is not None:
        config.rl.latency_target = float(latency_target)
    if energy_target is not None:
        config.rl.energy_target = float(energy_target)
    _general_reward_calculator.update_targets(latency_target, energy_target)
    _sac_reward_calculator.update_targets(latency_target, energy_target)


# ---------------------------------------------------------------------- #
# å‘åå…¼å®¹çš„è¾…åŠ©å‡½æ•°åç§°
# Backwards-compatible helper names.

def calculate_enhanced_reward(
    system_metrics: Dict,
    cache_metrics: Optional[Dict] = None,
    migration_metrics: Optional[Dict] = None,
) -> float:
    """
    å¢å¼ºå¥–åŠ±è®¡ç®—ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    
    è¿™æ˜¯calculate_unified_rewardçš„åˆ«åï¼Œä½¿ç”¨"general"ç®—æ³•ã€‚
    ä¿ç•™æ­¤å‡½æ•°ä»¥ç¡®ä¿ä¸æ—§ä»£ç çš„å…¼å®¹æ€§ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        cache_metrics: å¯é€‰çš„ç¼“å­˜æŒ‡æ ‡
        migration_metrics: å¯é€‰çš„è¿ç§»æŒ‡æ ‡
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    return calculate_unified_reward(system_metrics, cache_metrics, migration_metrics, "general")


def calculate_sac_reward(system_metrics: Dict) -> float:
    """
    SACç®—æ³•ä¸“ç”¨å¥–åŠ±è®¡ç®—ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    
    ä¸ºSACç®—æ³•æä¾›æ­£å‘å¥–åŠ±ç©ºé—´çš„ä¾¿æ·å‡½æ•°ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼ï¼ˆå¯èƒ½ä¸ºæ­£å€¼ï¼‰
    """
    return calculate_unified_reward(system_metrics, algorithm="sac")


def calculate_simple_reward(system_metrics: Dict) -> float:
    """
    ç®€å•å¥–åŠ±è®¡ç®—ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    
    è¿™æ˜¯calculate_unified_rewardçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨"general"ç®—æ³•ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    return calculate_unified_reward(system_metrics, algorithm="general")

