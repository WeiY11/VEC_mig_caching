#!/usr/bin/env python3
"""
ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ï¼Œä¾›æ‰€æœ‰å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•ä½¿ç”¨ã€‚

æ ¸å¿ƒç†å¿µæ˜¯æˆæœ¬æœ€å°åŒ–ï¼šæ›´ä½çš„å»¶è¿Ÿå’Œæ›´ä½çš„èƒ½è€—ä¼šå¸¦æ¥æ›´é«˜ï¼ˆæ›´å°‘è´Ÿå€¼ï¼‰çš„å¥–åŠ±ã€‚
æŸäº›ç®—æ³•ï¼ˆå¦‚SACï¼‰æœŸæœ›æ­£å‘å¥–åŠ±ï¼Œå› æ­¤æˆ‘ä»¬ä¸ºè¿™ç§æƒ…å†µä¿ç•™äº†ä¸€ä¸ªå°çš„å¯é€‰å¥–åŠ±ã€‚

Unified reward calculator used by all single-agent RL algorithms.
The philosophy is cost-minimisation: lower latency and lower energy
lead to higher (less negative) rewards. Some algorithms (SAC) expect
positive rewards, so we keep a small optional bonus for that case.
"""

from __future__ import annotations

from typing import Dict, Optional, List

import numpy as np

from config import config


class UnifiedRewardCalculator:
    """
    å¯å¤ç”¨çš„å¥–åŠ±è®¡ç®—å™¨ï¼Œç”¨äºå•æ™ºèƒ½ä½“è®­ç»ƒå™¨ã€‚
    
    è¯¥ç±»å®ç°äº†ç»Ÿä¸€çš„å¥–åŠ±è®¡ç®—é€»è¾‘ï¼Œæ”¯æŒä¸åŒç®—æ³•ï¼ˆå¦‚SACã€TD3ç­‰ï¼‰çš„ç‰¹å®šéœ€æ±‚ã€‚
    é‡‡ç”¨æˆæœ¬æœ€å°åŒ–æ–¹æ³•ï¼šå»¶è¿Ÿè¶Šä½ã€èƒ½è€—è¶Šä½ã€ä»»åŠ¡ä¸¢å¼ƒè¶Šå°‘ï¼Œå¥–åŠ±è¶Šé«˜ã€‚
    
    Reusable reward calculator for the single-agent trainers.
    """

    def __init__(self, algorithm: str = "general") -> None:
        """
        åˆå§‹åŒ–ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ã€‚
        
        Args:
            algorithm: ç®—æ³•åç§°ï¼Œç”¨äºç‰¹å®šç®—æ³•çš„è°ƒæ•´ï¼ˆå¦‚"SAC"ã€"TD3"ç­‰ï¼‰
                      ä¸åŒç®—æ³•å¯èƒ½æœ‰ä¸åŒçš„å½’ä¸€åŒ–å› å­å’Œå¥–åŠ±èŒƒå›´
        """
        self.algorithm = algorithm.upper()

        # ä»é…ç½®ä¸­è·å–æ ¸å¿ƒæƒé‡å‚æ•°
        # Core weights taken from configuration.
        self.weight_delay = float(config.rl.reward_weight_delay)  # å»¶è¿Ÿæƒé‡
        self.weight_energy = float(config.rl.reward_weight_energy)  # èƒ½è€—æƒé‡
        self.penalty_dropped = float(config.rl.reward_penalty_dropped)  # ä»»åŠ¡ä¸¢å¼ƒæƒ©ç½š
        self.weight_cache = float(getattr(config.rl, "reward_weight_cache", 0.0))  # ç¼“å­˜æƒé‡
        self.weight_cache_bonus = float(getattr(config.rl, "reward_weight_cache_bonus", 0.0))
        self.weight_migration = float(getattr(config.rl, "reward_weight_migration", 0.0))  # è¿ç§»æƒé‡
        self.weight_joint = float(getattr(config.rl, "reward_weight_joint", 0.05))  # ç¼“å­˜-è¿ç§»è”åŠ¨æƒé‡
        # ğŸ”§ æ–°å¢ï¼šè¿œç¨‹å¸è½½æ¿€åŠ±æƒé‡
        self.weight_offload_bonus = float(getattr(config.rl, "reward_weight_offload_bonus", 0.15))  # è¾¹ç¼˜è®¡ç®—åˆ©ç”¨å¥–åŠ±
        self.completion_target = float(getattr(config.rl, "completion_target", 0.95))
        self.weight_completion_gap = float(getattr(config.rl, "reward_weight_completion_gap", 0.0))
        self.weight_loss_ratio = float(getattr(config.rl, "reward_weight_loss_ratio", 0.0))
        self.cache_pressure_threshold = float(getattr(config.rl, "cache_pressure_threshold", 0.85))
        self.weight_cache_pressure = float(getattr(config.rl, "reward_weight_cache_pressure", 0.0))
        self.weight_queue_overload = float(getattr(config.rl, "reward_weight_queue_overload", 0.0))
        self.weight_remote_reject = float(getattr(config.rl, "reward_weight_remote_reject", 0.0))
        self.latency_target = float(getattr(config.rl, "latency_target", 0.4))
        self.energy_target = float(getattr(config.rl, "energy_target", 1200.0))
        self.latency_tolerance = float(getattr(config.rl, "latency_upper_tolerance", self.latency_target * 2.0))
        self.energy_tolerance = float(getattr(config.rl, "energy_upper_tolerance", self.energy_target * 1.5))

        # å½’ä¸€åŒ–ä»»åŠ¡ä¼˜å…ˆçº§æƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # Normalise priority weights if they exist.
        priority_weights = getattr(config, "task", None)
        priority_weights = getattr(priority_weights, "type_priority_weights", None)
        if isinstance(priority_weights, dict) and priority_weights:
            # è®¡ç®—æƒé‡æ€»å’Œå¹¶å½’ä¸€åŒ–
            total = sum(float(v) for v in priority_weights.values()) or 1.0
            self.task_priority_weights = {
                int(task_type): float(value) / total
                for task_type, value in priority_weights.items()
            }
        else:
            # é»˜è®¤æ‰€æœ‰ä»»åŠ¡ç±»å‹æƒé‡ç›¸ç­‰
            self.task_priority_weights = {1: 0.25, 2: 0.25, 3: 0.25, 4: 0.25}

        # ğŸ”§ ä¿®å¤ï¼šå½’ä¸€åŒ–å› å­å¿…é¡»ä¸ä¼˜åŒ–ç›®æ ‡å€¼å¯¹é½
        # Normalisation factors MUST align with optimization targets (latency_target and energy_target).
        # æ ¹æ®è®­ç»ƒç»“æœï¼ˆEpisode 1000+ï¼‰ï¼šå»¶è¿Ÿç¨³å®šåœ¨~0.05sï¼Œèƒ½è€—ç¨³å®šåœ¨~5000J
        # ç›®æ ‡å€¼è®¾ç½®ä¸ºï¼šlatency_target=0.4s, energy_target=1200J
        # å› æ­¤å½’ä¸€åŒ–åŸºå‡†åº”è¯¥ç›´æ¥ä½¿ç”¨è¿™äº›ç›®æ ‡å€¼ï¼Œè€Œéç¡¬ç¼–ç çš„0.2så’Œ1000J
        self.delay_normalizer = self.latency_target  # ä¸ç›®æ ‡å€¼å¯¹é½
        self.energy_normalizer = self.energy_target  # ä¸ç›®æ ‡å€¼å¯¹é½
        self.delay_bonus_scale = max(1e-6, self.latency_target)
        self.energy_bonus_scale = max(1e-6, self.energy_target)
        
        # SACç®—æ³•ä½¿ç”¨ä¸åŒçš„å½’ä¸€åŒ–å‚æ•°ï¼ˆä½†ä»éœ€ä¸targetå¯¹é½ï¼‰
        if self.algorithm == "SAC":
            self.delay_normalizer = self.latency_target  # å¯¹é½
            self.energy_normalizer = self.energy_target  # å¯¹é½

        norm_cfg = getattr(config, "normalization", None)
        if norm_cfg is not None:
            self.latency_target = float(getattr(norm_cfg, "delay_reference", self.latency_target))
            self.latency_tolerance = float(getattr(norm_cfg, "delay_upper_reference", self.latency_tolerance))
            self.energy_target = float(getattr(norm_cfg, "energy_reference", self.energy_target))
            self.energy_tolerance = float(getattr(norm_cfg, "energy_upper_reference", self.energy_tolerance))
            self.delay_normalizer = float(
                getattr(norm_cfg, "delay_normalizer_value", self.delay_normalizer)
            )
            self.energy_normalizer = float(
                getattr(norm_cfg, "energy_normalizer_value", self.energy_normalizer)
            )
            self.delay_bonus_scale = max(1e-6, self.latency_target)
            self.energy_bonus_scale = max(1e-6, self.energy_target)

        # è®¾ç½®å¥–åŠ±è£å‰ªèŒƒå›´ï¼Œé˜²æ­¢å¥–åŠ±å€¼è¿‡å¤§æˆ–è¿‡å°
        if self.algorithm == "SAC":
            self.reward_clip_range = (-15.0, 3.0)  # SACæœŸæœ›è¾ƒå°çš„å¥–åŠ±èŒƒå›´
        else:
            self.reward_clip_range = (-80.0, -0.005)  # å…¶ä»–ç®—æ³•ä½¿ç”¨è´Ÿå€¼èŒƒå›´

        print(f"[OK] Unified reward calculator ({self.algorithm})")
        print(
            f"   Core weights: delay={self.weight_delay:.2f}, "
            f"energy={self.weight_energy:.2f}, drop={self.penalty_dropped:.3f}"
        )
        print(
            f"   Normalisation: delay/{self.delay_normalizer:.2f}, "
            f"energy/{self.energy_normalizer:.0f}"
        )

    # ------------------------------------------------------------------ #
    # è¾…åŠ©æ–¹æ³• / Helpers

    @staticmethod
    def _safe_float(value: Optional[float], default: float = 0.0) -> float:
        """
        å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°ã€‚
        
        Args:
            value: å¾…è½¬æ¢çš„å€¼
            default: è½¬æ¢å¤±è´¥æ—¶çš„é»˜è®¤å€¼
            
        Returns:
            è½¬æ¢åçš„æµ®ç‚¹æ•°æˆ–é»˜è®¤å€¼
        """
        if value is None:
            return default
        try:
            return float(value)
        except (TypeError, ValueError):
            return default

    @staticmethod
    def _safe_int(value: Optional[int], default: int = 0) -> int:
        """
        å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•´æ•°ã€‚
        
        Args:
            value: å¾…è½¬æ¢çš„å€¼
            default: è½¬æ¢å¤±è´¥æ—¶çš„é»˜è®¤å€¼
            
        Returns:
            è½¬æ¢åçš„æ•´æ•°æˆ–é»˜è®¤å€¼
        """
        if value is None:
            return default
        try:
            return int(value)
        except (TypeError, ValueError):
            return default

    @staticmethod
    def _to_float_list(source) -> List[float]:
        """
        å°†è¾“å…¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°åˆ—è¡¨ã€‚
        
        æ”¯æŒnumpyæ•°ç»„ã€åˆ—è¡¨ã€å…ƒç»„ç­‰å¤šç§è¾“å…¥ç±»å‹ã€‚
        å¦‚æœæŸä¸ªå…ƒç´ æ— æ³•è½¬æ¢ï¼Œåˆ™ä½¿ç”¨0.0ä½œä¸ºé»˜è®¤å€¼ã€‚
        
        Args:
            source: å¾…è½¬æ¢çš„æ•°æ®æº
            
        Returns:
            æµ®ç‚¹æ•°åˆ—è¡¨
        """
        if isinstance(source, np.ndarray):
            iterable = source.tolist()
        elif isinstance(source, (list, tuple)):
            iterable = list(source)
        else:
            return []
        result: List[float] = []
        for item in iterable:
            try:
                result.append(float(item))
            except (TypeError, ValueError):
                result.append(0.0)
        return result

    # ------------------------------------------------------------------ #
    # å…¬å…±API / Public API

    def calculate_reward(
        self,
        system_metrics: Dict,
        cache_metrics: Optional[Dict] = None,
        migration_metrics: Optional[Dict] = None,
    ) -> float:
        """
        ä¸ºå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“è®¡ç®—æ ‡é‡å¥–åŠ±ã€‚
        
        å¥–åŠ±è®¡ç®—åŸºäºç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ï¼Œé‡‡ç”¨æˆæœ¬æœ€å°åŒ–åŸåˆ™ï¼š
        - å»¶è¿Ÿè¶Šä½ï¼Œå¥–åŠ±è¶Šé«˜
        - èƒ½è€—è¶Šä½ï¼Œå¥–åŠ±è¶Šé«˜
        - ä»»åŠ¡ä¸¢å¼ƒè¶Šå°‘ï¼Œå¥–åŠ±è¶Šé«˜
        - å¯é€‰ï¼šç¼“å­˜å‘½ä¸­ç‡è¶Šé«˜ï¼Œå¥–åŠ±è¶Šé«˜
        - å¯é€‰ï¼šè¿ç§»æˆæœ¬è¶Šä½ï¼Œå¥–åŠ±è¶Šé«˜
        
        Args:
            system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡å­—å…¸ï¼ŒåŒ…å«ï¼š
                - avg_task_delay: å¹³å‡ä»»åŠ¡å»¶è¿Ÿï¼ˆç§’ï¼‰
                - total_energy_consumption: æ€»èƒ½è€—ï¼ˆç„¦è€³ï¼‰
                - dropped_tasks: ä¸¢å¼ƒçš„ä»»åŠ¡æ•°é‡
                - task_completion_rate: ä»»åŠ¡å®Œæˆç‡
            cache_metrics: å¯é€‰çš„ç¼“å­˜æŒ‡æ ‡å­—å…¸
                - miss_rate: ç¼“å­˜æœªå‘½ä¸­ç‡
            migration_metrics: å¯é€‰çš„è¿ç§»æŒ‡æ ‡å­—å…¸
                - migration_cost: è¿ç§»æˆæœ¬
                
        Returns:
            æ ‡é‡å¥–åŠ±å€¼ï¼ŒèŒƒå›´ç”±reward_clip_rangeå®šä¹‰
        """

        # ä»ç³»ç»ŸæŒ‡æ ‡ä¸­æå–å…³é”®æ€§èƒ½æ•°æ®ï¼Œç¡®ä¿éè´Ÿå€¼
        avg_delay = max(0.0, self._safe_float(system_metrics.get("avg_task_delay")))
        total_energy = max(
            0.0, self._safe_float(system_metrics.get("total_energy_consumption"))
        )
        dropped_tasks = max(0, self._safe_int(system_metrics.get("dropped_tasks")))
        completion_rate = max(
            0.0, self._safe_float(system_metrics.get("task_completion_rate"))
        )
        data_loss_ratio = max(
            0.0, self._safe_float(system_metrics.get("data_loss_ratio_bytes"))
        )
        cache_utilization = max(
            0.0, self._safe_float(system_metrics.get("cache_utilization"))
        )
        queue_overload_events = max(
            0.0, self._safe_float(system_metrics.get("queue_overload_events"))
        )
        remote_rejection_rate = max(
            0.0, self._safe_float(system_metrics.get("remote_rejection_rate"))
        )
        # ğŸ”§ æ–°å¢ï¼šè¿œç¨‹å¸è½½åˆ©ç”¨ç‡
        rsu_offload_ratio = max(
            0.0, self._safe_float(system_metrics.get("rsu_offload_ratio"))
        )
        uav_offload_ratio = max(
            0.0, self._safe_float(system_metrics.get("uav_offload_ratio"))
        )

        # ğŸ”§ ä¿®å¤é—®é¢˜6ï¼šä½¿ç”¨ delay_normalizer å’Œ energy_normalizer è¿›è¡Œå½’ä¸€åŒ–
        # ========== æ ¸å¿ƒå½’ä¸€åŒ–ï¼šä½¿ç”¨normalizerè¿›è¡Œå°ºåº¦ç»Ÿä¸€ ==========
        # Objective = w_T Ã— (delay/normalizer) + w_E Ã— (energy/normalizer)
        # è¿™æ ·å¯ä»¥ç¡®ä¿ä¸¤ä¸ªæŒ‡æ ‡åœ¨åŒä¸€å°ºåº¦ä¸Šï¼Œæƒé‡æ‰æœ‰æ„ä¹‰
        
        norm_delay = avg_delay / max(self.delay_normalizer, 1e-6)
        norm_energy = total_energy / max(self.energy_normalizer, 1e-6)

        # è®¡ç®—æ ¸å¿ƒæˆæœ¬ï¼šå½’ä¸€åŒ–åçš„åŠ æƒå’Œ
        core_cost = self.weight_delay * norm_delay + self.weight_energy * norm_energy
        
        # ä»»åŠ¡ä¸¢å¼ƒæƒ©ç½šï¼ˆè½»å¾®æƒ©ç½šï¼Œä¸»è¦ç”¨äºä¿è¯å®Œæˆç‡ï¼‰
        drop_penalty = self.penalty_dropped * dropped_tasks
        
        # æ€»æˆæœ¬
        total_cost = core_cost + drop_penalty

        # å®Œæˆç‡å·®è·æƒ©ç½š
        if self.weight_completion_gap > 0.0:
            completion_gap = max(0.0, self.completion_target - completion_rate)
            total_cost += self.weight_completion_gap * completion_gap

        # æ•°æ®ä¸¢å¤±ç‡æƒ©ç½š
        if self.weight_loss_ratio > 0.0:
            total_cost += self.weight_loss_ratio * data_loss_ratio

        # ç¼“å­˜å‹åŠ›æƒ©ç½šï¼ˆè¶…è¿‡é˜ˆå€¼æ‰æƒ©ç½šï¼‰
        if self.weight_cache_pressure > 0.0 and cache_utilization > self.cache_pressure_threshold:
            total_cost += self.weight_cache_pressure * (cache_utilization - self.cache_pressure_threshold)

        # é˜Ÿåˆ—è¿‡è½½äº‹ä»¶æƒ©ç½š
        if self.weight_queue_overload > 0.0 and queue_overload_events > 0.0:
            total_cost += self.weight_queue_overload * queue_overload_events

        if self.weight_remote_reject > 0.0 and remote_rejection_rate > 0.0:
            total_cost += self.weight_remote_reject * remote_rejection_rate

        # ğŸ”§ æ–°å¢ï¼šè¿œç¨‹å¸è½½æ¿€åŠ±ï¼ˆé¼“åŠ±ä½¿ç”¨è¾¹ç¼˜èŠ‚ç‚¹ï¼‰
        if self.weight_offload_bonus > 0.0:
            # è®¡ç®—æ€»è¿œç¨‹å¸è½½ç‡ï¼ˆRSU + UAVï¼‰
            total_offload_ratio = rsu_offload_ratio + uav_offload_ratio
            # å¥–åŠ±ï¼šè¿œç¨‹å¸è½½ç‡è¶Šé«˜ï¼Œæˆæœ¬è¶Šä½
            offload_bonus = self.weight_offload_bonus * total_offload_ratio
            total_cost -= offload_bonus

        # ========== è¾…åŠ©æŒ‡æ ‡ï¼ˆå¯é€‰ï¼Œæƒé‡è¾ƒå°ï¼‰==========
        # æ³¨æ„ï¼šç¼“å­˜å’Œè¿ç§»æ˜¯æ‰‹æ®µï¼Œä¸æ˜¯ä¼˜åŒ–ç›®æ ‡ï¼Œæ‰€ä»¥æƒé‡è®¾ä¸º0
        
        # å¯é€‰çš„ç¼“å­˜æƒ©ç½šï¼ˆé€šå¸¸æƒé‡ä¸º0ï¼‰
        if cache_metrics and self.weight_cache > 0:
            miss_rate = np.clip(self._safe_float(cache_metrics.get("miss_rate"), 0.0), 0.0, 1.0)
            total_cost += self.weight_cache * miss_rate
        if cache_metrics and self.weight_cache_bonus > 0:
            hit_rate = np.clip(self._safe_float(cache_metrics.get("hit_rate"), 0.0), 0.0, 1.0)
            total_cost -= self.weight_cache_bonus * hit_rate

        # å¯é€‰çš„è¿ç§»æƒ©ç½šï¼ˆé€šå¸¸æƒé‡ä¸º0ï¼‰
        if migration_metrics and self.weight_migration > 0:
            migration_cost = self._safe_float(migration_metrics.get("migration_cost"), 0.0)
            total_cost += self.weight_migration * migration_cost

        if cache_metrics and migration_metrics and self.weight_joint > 0:
            hit_rate = np.clip(self._safe_float(cache_metrics.get("hit_rate"), 0.0), 0.0, 1.0)
            effectiveness = np.clip(self._safe_float(migration_metrics.get("effectiveness"), 0.0), 0.0, 1.0)
            joint_bonus = hit_rate * effectiveness

            cache_joint = cache_metrics.get("joint_params", {}) if isinstance(cache_metrics, dict) else {}
            migration_joint = migration_metrics.get("joint_params", {}) if isinstance(migration_metrics, dict) else {}
            prefetch_lead = self._safe_float(cache_joint.get("prefetch_lead_time"), 0.0)
            backoff_factor = np.clip(self._safe_float(migration_joint.get("migration_backoff"), 0.0), 0.0, 1.0)

            total_requests = max(1.0, self._safe_float(cache_metrics.get("total_requests"), 1.0))
            prefetch_events = max(0.0, self._safe_float(cache_metrics.get("prefetch_events"), 0.0))
            prefetch_ratio = np.clip(prefetch_events / total_requests, 0.0, 1.0)

            coupling_penalty = max(0.0, 0.3 - prefetch_ratio) * 0.5
            coupling_penalty += abs(prefetch_lead - 1.5) * 0.05
            coupling_penalty += backoff_factor * 0.1

            total_cost -= self.weight_joint * joint_bonus
            total_cost += self.weight_joint * coupling_penalty

        # ========== è®¡ç®—æœ€ç»ˆå¥–åŠ± ==========
        # å¥–åŠ± = -æˆæœ¬ï¼ˆæˆæœ¬è¶Šä½ï¼Œå¥–åŠ±è¶Šé«˜ï¼‰
        
        if self.algorithm == "SAC":
            # SACéœ€è¦é€‚åº¦æ­£å‘å¥–åŠ±ï¼Œæ·»åŠ åŸºç¡€å¥–åŠ±
            base_reward = 5.0  # åŸºç¡€å¥–åŠ±
            # å®Œæˆç‡é«˜æ—¶é¢å¤–å¥–åŠ±
            completion_bonus = 0.0
            if completion_rate > 0.95:
                completion_bonus = (completion_rate - 0.95) * 10.0
            reward = base_reward + completion_bonus - total_cost
        else:
            # TD3/DDPG/PPOç­‰ç®—æ³•ï¼šç›´æ¥ä½¿ç”¨è´Ÿæˆæœ¬ä½œä¸ºå¥–åŠ±
            reward = -total_cost

        # è£å‰ªå¥–åŠ±å€¼ï¼ˆå½’ä¸€åŒ–åèŒƒå›´æ›´å°ï¼‰
        # é¢„æœŸèŒƒå›´ï¼š-10 åˆ° 0ï¼ˆå¦‚æœä¸€åˆ‡æ­£å¸¸ï¼‰
        if self.algorithm == "SAC":
            reward = float(np.clip(reward, -15.0, 10.0))
        else:
            reward = float(np.clip(reward, -20.0, 0.0))
        
        return reward

    def update_targets(
        self,
        latency_target: Optional[float] = None,
        energy_target: Optional[float] = None,
    ) -> None:
        """åŠ¨æ€æ›´æ–°ç›®æ ‡å€¼ï¼Œä½¿å¥–åŠ±å‡½æ•°å¯ä»¥åœ¨è®­ç»ƒä¸­è‡ªé€‚åº”æ‹“æ‰‘å˜åŒ–ã€‚"""
        if latency_target is not None:
            self.latency_target = float(latency_target)
            self.latency_tolerance = float(
                getattr(config.rl, "latency_upper_tolerance", self.latency_target * 2.0)
            )
            self.delay_bonus_scale = max(1e-6, self.latency_target)
        if energy_target is not None:
            self.energy_target = float(energy_target)
            self.energy_tolerance = float(
                getattr(config.rl, "energy_upper_tolerance", self.energy_target * 1.5)
            )
            self.energy_bonus_scale = max(1e-6, self.energy_target)

    def get_reward_breakdown(self, system_metrics: Dict) -> str:
        """
        è·å–å¥–åŠ±ç»„æˆçš„äººç±»å¯è¯»åˆ†è§£æŠ¥å‘Šã€‚
        
        ç”¨äºè°ƒè¯•å’Œåˆ†æï¼Œæ˜¾ç¤ºå¥–åŠ±çš„å„ä¸ªç»„æˆéƒ¨åˆ†åŠå…¶è´¡çŒ®ã€‚
        
        Args:
            system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡å­—å…¸
            
        Returns:
            æ ¼å¼åŒ–çš„å¤šè¡Œå­—ç¬¦ä¸²ï¼ŒåŒ…å«å¥–åŠ±è¯¦ç»†åˆ†è§£
        """
        avg_delay = max(0.0, self._safe_float(system_metrics.get("avg_task_delay"), 0.0))
        total_energy = max(0.0, self._safe_float(system_metrics.get("total_energy_consumption"), 0.0))
        dropped_tasks = max(0, self._safe_int(system_metrics.get("dropped_tasks"), 0))
        completion_rate = max(0.0, self._safe_float(system_metrics.get("task_completion_rate"), 0.0))

        # è®¡ç®—å½’ä¸€åŒ–å€¼
        norm_delay = avg_delay / max(self.delay_normalizer, 1e-6)
        norm_energy = total_energy / max(self.energy_normalizer, 1e-6)
        
        # è®¡ç®—åŠ æƒæˆæœ¬
        weighted_delay = self.weight_delay * norm_delay
        weighted_energy = self.weight_energy * norm_energy
        core_cost = weighted_delay + weighted_energy
        
        reward = self.calculate_reward(system_metrics)
        lines = [
            f"Reward report ({self.algorithm}):",
            f"  Total Reward        : {reward:.3f}",
            f"  ----------------------------------------",
            f"  Delay               : {avg_delay:.4f}s (target: {self.latency_target}s)",
            f"  Normalized Delay    : {norm_delay:.4f}",
            f"  Weighted Delay      : {weighted_delay:.4f} (w={self.weight_delay})",
            f"  ----------------------------------------",
            f"  Energy              : {total_energy:.2f}J (target: {self.energy_target:.0f}J)",
            f"  Normalized Energy   : {norm_energy:.4f}",
            f"  Weighted Energy     : {weighted_energy:.4f} (w={self.weight_energy})",
            f"  ----------------------------------------",
            f"  Core Cost (D+E)     : {core_cost:.4f}",
            f"  Dropped Tasks       : {dropped_tasks} (penalty: {self.penalty_dropped * dropped_tasks:.4f})",
            f"  Completion Rate     : {completion_rate:.1%}",
        ]
        return "\n".join(lines)


# ---------------------------------------------------------------------- #
# ä¾¿æ·çš„å•ä¾‹å¯¹è±¡ï¼Œåœ¨æ•´ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨
# Convenience singletons used across the project.

_general_reward_calculator = UnifiedRewardCalculator(algorithm="general")
_sac_reward_calculator = UnifiedRewardCalculator(algorithm="sac")


def calculate_unified_reward(
    system_metrics: Dict,
    cache_metrics: Optional[Dict] = None,
    migration_metrics: Optional[Dict] = None,
    algorithm: str = "general",
) -> float:
    """
    ç»Ÿä¸€å¥–åŠ±è®¡ç®—çš„ä¾¿æ·å‡½æ•°ã€‚
    
    æ ¹æ®æŒ‡å®šç®—æ³•é€‰æ‹©ç›¸åº”çš„å¥–åŠ±è®¡ç®—å™¨ï¼Œè®¡ç®—å¹¶è¿”å›å¥–åŠ±å€¼ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        cache_metrics: å¯é€‰çš„ç¼“å­˜æŒ‡æ ‡
        migration_metrics: å¯é€‰çš„è¿ç§»æŒ‡æ ‡
        algorithm: ç®—æ³•åç§°ï¼ˆ"SAC"æˆ–"general"ï¼‰
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    calculator = _sac_reward_calculator if algorithm.upper() == "SAC" else _general_reward_calculator
    return calculator.calculate_reward(system_metrics, cache_metrics, migration_metrics)


def get_reward_breakdown(system_metrics: Dict, algorithm: str = "general") -> str:
    """
    è·å–å¥–åŠ±åˆ†è§£æŠ¥å‘Šçš„ä¾¿æ·å‡½æ•°ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        algorithm: ç®—æ³•åç§°ï¼ˆ"SAC"æˆ–"general"ï¼‰
        
    Returns:
        æ ¼å¼åŒ–çš„å¥–åŠ±åˆ†è§£æŠ¥å‘Šå­—ç¬¦ä¸²
    """
    calculator = _sac_reward_calculator if algorithm.upper() == "SAC" else _general_reward_calculator
    return calculator.get_reward_breakdown(system_metrics)


def update_reward_targets(
    latency_target: Optional[float] = None,
    energy_target: Optional[float] = None,
) -> None:
    """
    åŠ¨æ€æ›´æ–°å…¨å±€å¥–åŠ±ç›®æ ‡ï¼Œç¡®ä¿å•ä¾‹è®¡ç®—å™¨ä¸å…¨å±€configä¿æŒåŒæ­¥ã€‚
    """
    if latency_target is not None:
        config.rl.latency_target = float(latency_target)
    if energy_target is not None:
        config.rl.energy_target = float(energy_target)
    _general_reward_calculator.update_targets(latency_target, energy_target)
    _sac_reward_calculator.update_targets(latency_target, energy_target)


# ---------------------------------------------------------------------- #
# å‘åå…¼å®¹çš„è¾…åŠ©å‡½æ•°åç§°
# Backwards-compatible helper names.

def calculate_enhanced_reward(
    system_metrics: Dict,
    cache_metrics: Optional[Dict] = None,
    migration_metrics: Optional[Dict] = None,
) -> float:
    """
    å¢å¼ºå¥–åŠ±è®¡ç®—ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    
    è¿™æ˜¯calculate_unified_rewardçš„åˆ«åï¼Œä½¿ç”¨"general"ç®—æ³•ã€‚
    ä¿ç•™æ­¤å‡½æ•°ä»¥ç¡®ä¿ä¸æ—§ä»£ç çš„å…¼å®¹æ€§ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        cache_metrics: å¯é€‰çš„ç¼“å­˜æŒ‡æ ‡
        migration_metrics: å¯é€‰çš„è¿ç§»æŒ‡æ ‡
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    return calculate_unified_reward(system_metrics, cache_metrics, migration_metrics, "general")


def calculate_sac_reward(system_metrics: Dict) -> float:
    """
    SACç®—æ³•ä¸“ç”¨å¥–åŠ±è®¡ç®—ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    
    ä¸ºSACç®—æ³•æä¾›æ­£å‘å¥–åŠ±ç©ºé—´çš„ä¾¿æ·å‡½æ•°ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼ï¼ˆå¯èƒ½ä¸ºæ­£å€¼ï¼‰
    """
    return calculate_unified_reward(system_metrics, algorithm="sac")


def calculate_simple_reward(system_metrics: Dict) -> float:
    """
    ç®€å•å¥–åŠ±è®¡ç®—ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    
    è¿™æ˜¯calculate_unified_rewardçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨"general"ç®—æ³•ã€‚
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    return calculate_unified_reward(system_metrics, algorithm="general")

