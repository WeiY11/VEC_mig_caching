#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€å¥–åŠ±å‡½æ•°ä¿®å¤æ¨¡å—
ç¡®ä¿æ‰€æœ‰ç®—æ³•ä½¿ç”¨ç›¸åŒçš„å¥–åŠ±è®¡ç®—é€»è¾‘
"""

import numpy as np
from typing import Dict, Optional
from config import config

class StandardizedRewardFunction:
    """
    æ ‡å‡†åŒ–å¥–åŠ±å‡½æ•° - ä¸¥æ ¼æŒ‰ç…§è®ºæ–‡ç›®æ ‡å‡½æ•°å®ç°
    è®ºæ–‡ç›®æ ‡: min(Ï‰_T * delay + Ï‰_E * energy + Ï‰_D * data_loss)
    å¥–åŠ±å‡½æ•°: reward = -cost
    """
    
    def __init__(self):
        # ä¸¥æ ¼æŒ‰ç…§è®ºæ–‡æƒé‡é…ç½®
        self.weight_delay = config.rl.reward_weight_delay     # Ï‰_T
        self.weight_energy = config.rl.reward_weight_energy   # Ï‰_E  
        self.weight_loss = config.rl.reward_weight_loss       # Ï‰_D
        
        # ğŸ”§ ä¿®å¤ï¼šç­‰å½±å“åŠ›å½’ä¸€åŒ– - ç¡®ä¿ä¸‰ä¸ªç›®æ ‡æƒé‡å¹³è¡¡
        self.delay_normalizer = 1.4        # å»¶è¿Ÿå½’ä¸€åŒ–å› å­ (ç§’) - ç¡®ä¿åˆç†å½±å“åŠ›
        self.energy_normalizer = 122623.0  # èƒ½è€—å½’ä¸€åŒ–å› å­ (J) - é™ä½èƒ½è€—è¿‡åº¦ä¸»å¯¼
        self.loss_normalizer = 0.030       # æ•°æ®ä¸¢å¤±ç‡å½’ä¸€åŒ–å› å­ - æå‡ä¸¢å¤±ç‡å½±å“åŠ›
        
        # å¥–åŠ±èŒƒå›´é™åˆ¶ - ç¡®ä¿æ•°å€¼ç¨³å®š
        self.min_reward = -10.0
        self.max_reward = 5.0
        
        print(f"âœ… æ ‡å‡†åŒ–å¥–åŠ±å‡½æ•°åˆå§‹åŒ–")
        print(f"   æƒé‡é…ç½®: å»¶è¿Ÿ={self.weight_delay}, èƒ½è€—={self.weight_energy}, ä¸¢å¤±={self.weight_loss}")
    
    def calculate_paper_reward(self, system_metrics: Dict) -> float:
        """
        ä¿®å¤ç‰ˆæœ¬ï¼šå› ä¸ºä¹‹å‰çš„å¥–åŠ±ä¿¡å·è¿‡å¼±ï¼Œéœ€è¦å¢å¼ºä¿¡å·å¼ºåº¦
        å¯¹åº”è®ºæ–‡å¼(24): min(Ï‰_T * T + Ï‰_E * E + Ï‰_D * D)
        """
        # æå–ç³»ç»ŸæŒ‡æ ‡
        avg_delay = system_metrics.get('avg_task_delay', 0.0)
        total_energy = system_metrics.get('total_energy_consumption', 0.0)
        data_loss_rate = system_metrics.get('data_loss_rate', 0.0)
        
        # æ•°å€¼æœ‰æ•ˆæ€§æ£€æŸ¥
        avg_delay = max(0.0, float(avg_delay)) if np.isfinite(avg_delay) else 0.0
        total_energy = max(0.0, float(total_energy)) if np.isfinite(total_energy) else 0.0
        data_loss_rate = np.clip(float(data_loss_rate), 0.0, 1.0) if np.isfinite(data_loss_rate) else 0.0
        
        # å½’ä¸€åŒ–æŒ‡æ ‡
        normalized_delay = avg_delay / self.delay_normalizer
        normalized_energy = total_energy / self.energy_normalizer
        normalized_loss = data_loss_rate / self.loss_normalizer
        
        # è®¡ç®—æˆæœ¬å‡½æ•° - ä¸¥æ ¼å¯¹åº”è®ºæ–‡å¼(24)
        cost = (self.weight_delay * normalized_delay + 
                self.weight_energy * normalized_energy + 
                self.weight_loss * normalized_loss)
        
        # è½¬æ¢ä¸ºå¥–åŠ± (æˆæœ¬çš„è´Ÿå€¼)
        base_reward = -cost
        
        # ğŸ”§ ä¿®å¤ï¼šè°ƒæ•´ä¿¡å·å¼ºåº¦å’ŒèŒƒå›´ï¼Œé¿å…è¿‡åº¦é™åˆ¶
        amplified_reward = base_reward * 2.0  # é™ä½æ”¾å¤§å€æ•°ï¼Œé¿å…è¿‡åº¦æ”¾å¤§
        
        # ç¡®ä¿å¥–åŠ±å§‹ç»ˆä¸ºè´Ÿå€¼ï¼ˆå› ä¸ºæ˜¯æˆæœ¬çš„è´Ÿå€¼ï¼‰- æ‰©å¤§èŒƒå›´å…è®¸æ›´å¤šå˜åŒ–
        clipped_reward = np.clip(amplified_reward, -200.0, 0.0)  # ğŸ”§ æ‰©å¤§ä¸‹é™ï¼Œå…è®¸æ›´å¤§å˜åŒ–èŒƒå›´
        
        return clipped_reward
    
    def calculate_with_performance_bonus(self, system_metrics: Dict, 
                                       agent_type: Optional[str] = None) -> float:
        """
        ä¿®å¤ç‰ˆæœ¬ï¼šä¸¥æ ¼éµå¾ªè®ºæ–‡å¥–åŠ±é€»è¾‘ Reward = -Cost
        ä¸æ·»åŠ å¯èƒ½å¯¼è‡´æ­£å€¼çš„performance bonusï¼Œç¡®ä¿å¥–åŠ±å§‹ç»ˆä¸ºè´Ÿå€¼
        """
        # ä¸¥æ ¼ä½¿ç”¨è®ºæ–‡å¥–åŠ±å‡½æ•°ï¼Œä¸æ·»åŠ æ­£å€¼å¥–åŠ±
        paper_reward = self.calculate_paper_reward(system_metrics)
        
        # æ™ºèƒ½ä½“ç‰¹å®šçš„æˆæœ¬è°ƒæ•´ (ä»ç„¶ä¿æŒè´Ÿå€¼é€»è¾‘)
        agent_cost_adjustment = 0.0
        if agent_type:
            if agent_type == 'vehicle_agent':
                # æœ¬åœ°å¤„ç†æ•ˆç‡ä½æ—¶å¢åŠ æˆæœ¬
                local_efficiency = system_metrics.get('local_processing_ratio', 0.0)
                agent_cost_adjustment = -0.5 * (1.0 - local_efficiency)  # ä½æ•ˆç‡æ—¶å¢åŠ æˆæœ¬
            elif agent_type == 'rsu_agent':
                # è´Ÿè½½ä¸å‡è¡¡æ—¶å¢åŠ æˆæœ¬
                avg_utilization = system_metrics.get('avg_rsu_utilization', 0.7)
                load_imbalance = abs(0.7 - avg_utilization)
                agent_cost_adjustment = -0.5 * load_imbalance
            elif agent_type == 'uav_agent':
                # ç”µæ± ç”µé‡ä½æ—¶å¢åŠ æˆæœ¬
                battery_level = system_metrics.get('avg_uav_battery', 1.0)
                agent_cost_adjustment = -0.5 * (1.0 - battery_level)
        
        final_reward = paper_reward + agent_cost_adjustment
        
        # ç¡®ä¿å¥–åŠ±å§‹ç»ˆä¸ºè´Ÿå€¼æˆ–é›¶
        return np.clip(final_reward, -80.0, 0.0)  # ä¸Šé™è®¾ä¸º0ï¼Œç¡®ä¿ä¸ä¼šæœ‰æ­£å€¼


# åˆ›å»ºå…¨å±€æ ‡å‡†åŒ–å¥–åŠ±å‡½æ•°å®ä¾‹
_standardized_reward_function = StandardizedRewardFunction()


def calculate_standardized_reward(system_metrics: Dict, agent_type: Optional[str] = None, 
                                 use_paper_only: bool = False) -> float:
    """
    æ ‡å‡†åŒ–å¥–åŠ±è®¡ç®—æ¥å£ - ä¾›æ‰€æœ‰ç®—æ³•è°ƒç”¨
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡å­—å…¸
        agent_type: æ™ºèƒ½ä½“ç±»å‹ (å¯é€‰)
        use_paper_only: æ˜¯å¦åªä½¿ç”¨è®ºæ–‡å¥–åŠ±å‡½æ•° (é»˜è®¤Falseï¼Œä¼šæ·»åŠ è½»é‡æ¿€åŠ±)
        
    Returns:
        æ ‡å‡†åŒ–è®¡ç®—çš„å¥–åŠ±å€¼
    """
    if use_paper_only:
        return _standardized_reward_function.calculate_paper_reward(system_metrics)
    else:
        return _standardized_reward_function.calculate_with_performance_bonus(system_metrics, agent_type)


def validate_reward_consistency():
    """éªŒè¯å¥–åŠ±å‡½æ•°ä¸€è‡´æ€§"""
    # æµ‹è¯•ç”¨ä¾‹
    test_metrics = {
        'avg_task_delay': 0.1,
        'total_energy_consumption': 500.0,
        'data_loss_rate': 0.05,
        'task_completion_rate': 0.9,
        'cache_hit_rate': 0.8
    }
    
    # è®¡ç®—å¥–åŠ±
    paper_reward = calculate_standardized_reward(test_metrics, use_paper_only=True)
    full_reward = calculate_standardized_reward(test_metrics, use_paper_only=False)
    
    print(f"âœ… å¥–åŠ±å‡½æ•°ä¸€è‡´æ€§éªŒè¯:")
    print(f"   è®ºæ–‡å¥–åŠ±: {paper_reward:.4f}")
    print(f"   å®Œæ•´å¥–åŠ±: {full_reward:.4f}")
    print(f"   æƒé‡éªŒè¯: å»¶è¿Ÿ={config.rl.reward_weight_delay}, èƒ½è€—={config.rl.reward_weight_energy}, ä¸¢å¤±={config.rl.reward_weight_loss}")
    
    return paper_reward, full_reward


if __name__ == "__main__":
    validate_reward_consistency()