#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€å¥–åŠ±å‡½æ•°ä¿®å¤æ¨¡å—
ç¡®ä¿æ‰€æœ‰ç®—æ³•ä½¿ç”¨ç›¸åŒçš„å¥–åŠ±è®¡ç®—é€»è¾‘
"""

import numpy as np
from typing import Dict, Optional
from config import config

class StandardizedRewardFunction:
    """
    æ ‡å‡†åŒ–å¥–åŠ±å‡½æ•° - ä¸¥æ ¼æŒ‰ç…§è®ºæ–‡ç›®æ ‡å‡½æ•°å®ç°
    è®ºæ–‡ç›®æ ‡: min(Ï‰_T * delay + Ï‰_E * energy + Ï‰_D * data_loss)
    å¥–åŠ±å‡½æ•°: reward = -cost
    """
    
    def __init__(self):
        # ä¸¥æ ¼æŒ‰ç…§è®ºæ–‡æƒé‡é…ç½®
        self.weight_delay = config.rl.reward_weight_delay     # Ï‰_T
        self.weight_energy = config.rl.reward_weight_energy   # Ï‰_E  
        self.weight_loss = config.rl.reward_weight_loss       # Ï‰_D
        
        # ç»Ÿä¸€çš„å½’ä¸€åŒ–å‚æ•° - åŸºäºè®ºæ–‡å’Œå®é™…æ•°æ®èŒƒå›´
        self.delay_normalizer = 1.0      # å»¶è¿Ÿå½’ä¸€åŒ–å› å­ (ç§’)
        self.energy_normalizer = 1000.0  # èƒ½è€—å½’ä¸€åŒ–å› å­ (J)
        self.loss_normalizer = 1.0       # æ•°æ®ä¸¢å¤±ç‡å½’ä¸€åŒ–å› å­
        
        # å¥–åŠ±èŒƒå›´é™åˆ¶ - ç¡®ä¿æ•°å€¼ç¨³å®š
        self.min_reward = -10.0
        self.max_reward = 5.0
        
        print(f"âœ… æ ‡å‡†åŒ–å¥–åŠ±å‡½æ•°åˆå§‹åŒ–")
        print(f"   æƒé‡é…ç½®: å»¶è¿Ÿ={self.weight_delay}, èƒ½è€—={self.weight_energy}, ä¸¢å¤±={self.weight_loss}")
    
    def calculate_paper_reward(self, system_metrics: Dict) -> float:
        """
        ä¿®å¤ç‰ˆæœ¬ï¼šå› ä¸ºä¹‹å‰çš„å¥–åŠ±ä¿¡å·è¿‡å¼±ï¼Œéœ€è¦å¢å¼ºä¿¡å·å¼ºåº¦
        å¯¹åº”è®ºæ–‡å¼(24): min(Ï‰_T * T + Ï‰_E * E + Ï‰_D * D)
        """
        # æå–ç³»ç»ŸæŒ‡æ ‡
        avg_delay = system_metrics.get('avg_task_delay', 0.0)
        total_energy = system_metrics.get('total_energy_consumption', 0.0)
        data_loss_rate = system_metrics.get('data_loss_rate', 0.0)
        
        # æ•°å€¼æœ‰æ•ˆæ€§æ£€æŸ¥
        avg_delay = max(0.0, float(avg_delay)) if np.isfinite(avg_delay) else 0.0
        total_energy = max(0.0, float(total_energy)) if np.isfinite(total_energy) else 0.0
        data_loss_rate = np.clip(float(data_loss_rate), 0.0, 1.0) if np.isfinite(data_loss_rate) else 0.0
        
        # å½’ä¸€åŒ–æŒ‡æ ‡
        normalized_delay = avg_delay / self.delay_normalizer
        normalized_energy = total_energy / self.energy_normalizer
        normalized_loss = data_loss_rate / self.loss_normalizer
        
        # è®¡ç®—æˆæœ¬å‡½æ•° - ä¸¥æ ¼å¯¹åº”è®ºæ–‡å¼(24)
        cost = (self.weight_delay * normalized_delay + 
                self.weight_energy * normalized_energy + 
                self.weight_loss * normalized_loss)
        
        # è½¬æ¢ä¸ºå¥–åŠ± (æˆæœ¬çš„è´Ÿå€¼)
        base_reward = -cost
        
        # ğŸ”§ ä¿®å¤ï¼šæ”¾å¤§ä¿¡å·å¼ºåº¦è§£å†³è¯Šæ–­å‘ç°çš„ä¿¡å·è¿‡å¼±é—®é¢˜
        amplified_reward = base_reward * 8.0  # 8å€æ”¾å¤§ï¼Œä½¿å¥–åŠ±å˜åŒ–æ›´æ˜¾è‘—
        
        # åº”ç”¨æ”¾å¤§åçš„å¥–åŠ±èŒƒå›´é™åˆ¶
        clipped_reward = np.clip(amplified_reward, -40.0, 20.0)  # æ‰©å¤§èŒƒå›´ä¿æŒä¿¡å·å¼ºåº¦
        
        return clipped_reward
    
    def calculate_with_performance_bonus(self, system_metrics: Dict, 
                                       agent_type: Optional[str] = None) -> float:
        """
        ä¿®å¤ç‰ˆæœ¬ï¼šåœ¨è®ºæ–‡å¥–åŠ±åŸºç¡€ä¸Šæ·»åŠ æ€§èƒ½æ¿€åŠ±ï¼Œè§£å†³ç›¸å…³æ€§é—®é¢˜
        """
        # åŸºç¡€è®ºæ–‡å¥–åŠ± (å·²æ”¾å¤§)
        base_reward = self.calculate_paper_reward(system_metrics)
        
        # ğŸ”§ ä¿®å¤ï¼šå¼ºåŒ–æ€§èƒ½æ¿€åŠ±è§£å†³ç›¸å…³æ€§é—®é¢˜
        completion_rate = system_metrics.get('task_completion_rate', 0.0)
        cache_hit_rate = system_metrics.get('cache_hit_rate', 0.0)
        
        # æ˜¾è‘—å¢å¼ºæ€§èƒ½å¥–åŠ±ï¼Œç¡®ä¿ä¸æ€§èƒ½æŒ‡æ ‡å¼ºç›¸å…³
        performance_bonus = 5.0 * completion_rate + 3.0 * cache_hit_rate  # æ˜¾è‘—å¢å¼ºç›¸å…³æ€§
        
        # æ™ºèƒ½ä½“ç‰¹å®šå¥–åŠ± (é’ˆå¯¹å¤šæ™ºèƒ½ä½“åœºæ™¯)
        agent_bonus = 0.0
        if agent_type:
            if agent_type == 'vehicle_agent':
                local_efficiency = system_metrics.get('local_processing_ratio', 0.0)
                agent_bonus = 1.0 * local_efficiency  # å¢å¼ºæ™ºèƒ½ä½“å¥–åŠ±
            elif agent_type == 'rsu_agent':
                load_balance = 1.0 - abs(0.7 - system_metrics.get('avg_rsu_utilization', 0.7))
                agent_bonus = 1.0 * load_balance
            elif agent_type == 'uav_agent':
                battery_level = system_metrics.get('avg_uav_battery', 1.0)
                agent_bonus = 1.0 * battery_level
        
        final_reward = base_reward + performance_bonus + agent_bonus
        
        # æ”¾å¤§åçš„èŒƒå›´é™åˆ¶
        return np.clip(final_reward, -80.0, 50.0)


# åˆ›å»ºå…¨å±€æ ‡å‡†åŒ–å¥–åŠ±å‡½æ•°å®ä¾‹
_standardized_reward_function = StandardizedRewardFunction()


def calculate_standardized_reward(system_metrics: Dict, agent_type: Optional[str] = None, 
                                 use_paper_only: bool = False) -> float:
    """
    æ ‡å‡†åŒ–å¥–åŠ±è®¡ç®—æ¥å£ - ä¾›æ‰€æœ‰ç®—æ³•è°ƒç”¨
    
    Args:
        system_metrics: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡å­—å…¸
        agent_type: æ™ºèƒ½ä½“ç±»å‹ (å¯é€‰)
        use_paper_only: æ˜¯å¦åªä½¿ç”¨è®ºæ–‡å¥–åŠ±å‡½æ•° (é»˜è®¤Falseï¼Œä¼šæ·»åŠ è½»é‡æ¿€åŠ±)
        
    Returns:
        æ ‡å‡†åŒ–è®¡ç®—çš„å¥–åŠ±å€¼
    """
    if use_paper_only:
        return _standardized_reward_function.calculate_paper_reward(system_metrics)
    else:
        return _standardized_reward_function.calculate_with_performance_bonus(system_metrics, agent_type)


def validate_reward_consistency():
    """éªŒè¯å¥–åŠ±å‡½æ•°ä¸€è‡´æ€§"""
    # æµ‹è¯•ç”¨ä¾‹
    test_metrics = {
        'avg_task_delay': 0.1,
        'total_energy_consumption': 500.0,
        'data_loss_rate': 0.05,
        'task_completion_rate': 0.9,
        'cache_hit_rate': 0.8
    }
    
    # è®¡ç®—å¥–åŠ±
    paper_reward = calculate_standardized_reward(test_metrics, use_paper_only=True)
    full_reward = calculate_standardized_reward(test_metrics, use_paper_only=False)
    
    print(f"âœ… å¥–åŠ±å‡½æ•°ä¸€è‡´æ€§éªŒè¯:")
    print(f"   è®ºæ–‡å¥–åŠ±: {paper_reward:.4f}")
    print(f"   å®Œæ•´å¥–åŠ±: {full_reward:.4f}")
    print(f"   æƒé‡éªŒè¯: å»¶è¿Ÿ={config.rl.reward_weight_delay}, èƒ½è€—={config.rl.reward_weight_energy}, ä¸¢å¤±={config.rl.reward_weight_loss}")
    
    return paper_reward, full_reward


if __name__ == "__main__":
    validate_reward_consistency()