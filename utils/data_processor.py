#!/usr/bin/env python3
"""
æ•°æ®å¤„ç†å·¥å…·
"""

import numpy as np
import torch
from typing import List, Dict, Tuple, Any, Optional
import json
from pathlib import Path

class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        self.normalization_params = {}
    
    def normalize_data(self, data: np.ndarray, method: str = 'minmax') -> np.ndarray:
        """æ•°æ®å½’ä¸€åŒ–"""
        if method == 'minmax':
            min_val = np.min(data)
            max_val = np.max(data)
            if max_val - min_val == 0:
                return np.zeros_like(data)
            return (data - min_val) / (max_val - min_val)
        
        elif method == 'zscore':
            mean_val = np.mean(data)
            std_val = np.std(data)
            if std_val == 0:
                return np.zeros_like(data)
            return (data - mean_val) / std_val
        
        else:
            raise ValueError(f"Unknown normalization method: {method}")
    
    def denormalize_data(self, normalized_data: np.ndarray, 
                        original_min: float, original_max: float) -> np.ndarray:
        """åå½’ä¸€åŒ–"""
        return normalized_data * (original_max - original_min) + original_min
    
    def smooth_data(self, data: List[float], window_size: int = 10) -> List[float]:
        """æ•°æ®å¹³æ»‘"""
        if len(data) < window_size:
            return data
        
        smoothed = []
        for i in range(len(data)):
            start_idx = max(0, i - window_size // 2)
            end_idx = min(len(data), i + window_size // 2 + 1)
            smoothed.append(np.mean(data[start_idx:end_idx]))
        
        return smoothed
    
    def remove_outliers(self, data: np.ndarray, threshold: float = 3.0) -> np.ndarray:
        """ç§»é™¤å¼‚å¸¸å€¼"""
        mean_val = np.mean(data)
        std_val = np.std(data)
        
        # ä½¿ç”¨Z-scoreæ–¹æ³•
        z_scores = np.abs((data - mean_val) / std_val)
        return data[z_scores < threshold]
    
    def interpolate_missing(self, data: List[Optional[float]]) -> List[float]:
        """æ’å€¼å¡«è¡¥ç¼ºå¤±å€¼"""
        result = []
        last_valid = None
        
        for i, value in enumerate(data):
            if value is not None:
                result.append(value)
                last_valid = value
            else:
                # å¯»æ‰¾ä¸‹ä¸€ä¸ªæœ‰æ•ˆå€¼
                next_valid = None
                for j in range(i + 1, len(data)):
                    if data[j] is not None:
                        next_valid = data[j]
                        break
                
                # æ’å€¼
                if last_valid is not None and next_valid is not None:
                    # çº¿æ€§æ’å€¼
                    interpolated = last_valid + (next_valid - last_valid) * 0.5
                elif last_valid is not None:
                    interpolated = last_valid
                elif next_valid is not None:
                    interpolated = next_valid
                else:
                    interpolated = 0.0
                
                result.append(interpolated)
        
        return result
    
    def batch_data(self, data: List[Any], batch_size: int) -> List[List[Any]]:
        """æ•°æ®åˆ†æ‰¹"""
        batches = []
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            batches.append(batch)
        return batches
    
    def shuffle_data(self, *arrays) -> Tuple:
        """éšæœºæ‰“ä¹±æ•°æ®"""
        if not arrays:
            return ()
        
        indices = np.random.permutation(len(arrays[0]))
        return tuple(array[indices] for array in arrays)
    
    def split_data(self, data: np.ndarray, train_ratio: float = 0.8) -> Tuple[np.ndarray, np.ndarray]:
        """æ•°æ®åˆ†å‰²"""
        split_idx = int(len(data) * train_ratio)
        return data[:split_idx], data[split_idx:]
    
    def convert_to_tensor(self, data: Any, device: str = 'cpu') -> torch.Tensor:
        """è½¬æ¢ä¸ºPyTorchå¼ é‡"""
        if isinstance(data, torch.Tensor):
            return data.to(device)
        elif isinstance(data, np.ndarray):
            return torch.from_numpy(data).float().to(device)
        elif isinstance(data, (list, tuple)):
            return torch.tensor(data, dtype=torch.float32).to(device)
        else:
            raise ValueError(f"Unsupported data type: {type(data)}")
    
    def save_processed_data(self, data: Dict[str, Any], filepath: str):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        json_data = {}
        for key, value in data.items():
            if isinstance(value, np.ndarray):
                json_data[key] = value.tolist()
            elif isinstance(value, torch.Tensor):
                json_data[key] = value.cpu().numpy().tolist()
            else:
                json_data[key] = value
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    def load_processed_data(self, filepath: str) -> Dict[str, Any]:
        """åŠ è½½å¤„ç†åçš„æ•°æ®"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # è½¬æ¢å›numpyæ•°ç»„
        for key, value in data.items():
            if isinstance(value, list):
                data[key] = np.array(value)
        
        return data

def test_data_processor():
    """æµ‹è¯•æ•°æ®å¤„ç†å™¨"""
    print("ğŸ§ª æµ‹è¯•æ•°æ®å¤„ç†å™¨...")
    
    processor = DataProcessor()
    
    # æµ‹è¯•æ•°æ®
    test_data = np.random.randn(100) * 10 + 50
    
    # å½’ä¸€åŒ–æµ‹è¯•
    normalized = processor.normalize_data(test_data, 'minmax')
    print(f"åŸå§‹æ•°æ®èŒƒå›´: [{np.min(test_data):.2f}, {np.max(test_data):.2f}]")
    print(f"å½’ä¸€åŒ–åèŒƒå›´: [{np.min(normalized):.2f}, {np.max(normalized):.2f}]")
    
    # å¹³æ»‘æµ‹è¯•
    noisy_data = [i + np.random.randn() for i in range(50)]
    smoothed = processor.smooth_data(noisy_data, window_size=5)
    print(f"å¹³æ»‘å‰æ–¹å·®: {np.var(noisy_data):.3f}")
    print(f"å¹³æ»‘åæ–¹å·®: {np.var(smoothed):.3f}")
    
    # å¼‚å¸¸å€¼ç§»é™¤æµ‹è¯•
    data_with_outliers = np.concatenate([
        np.random.randn(90),
        np.array([10, -10, 15, -15])  # å¼‚å¸¸å€¼
    ])
    cleaned_data = processor.remove_outliers(data_with_outliers)
    print(f"ç§»é™¤å¼‚å¸¸å€¼å‰: {len(data_with_outliers)} ä¸ªæ•°æ®ç‚¹")
    print(f"ç§»é™¤å¼‚å¸¸å€¼å: {len(cleaned_data)} ä¸ªæ•°æ®ç‚¹")
    
    print("âœ… æ•°æ®å¤„ç†å™¨æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_data_processor()