"""
ğŸ¨ è®­ç»ƒå¯è§†åŒ–é›†æˆç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨é«˜ç«¯å¯è§†åŒ–å™¨

ä½¿ç”¨æ–¹æ³•ï¼š
python utils/visualizer_demo.py
"""

import numpy as np
import time
from advanced_training_visualizer import create_visualizer


def demo_training_with_visualization():
    """æ¼”ç¤ºå¸¦å¯è§†åŒ–çš„è®­ç»ƒæµç¨‹"""
    
    print("ğŸ¯ å¯åŠ¨é«˜ç«¯è®­ç»ƒå¯è§†åŒ–æ¼”ç¤º...")
    print("=" * 60)
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = create_visualizer(max_history=500)
    visualizer.start(interval=500)  # 500ms åˆ·æ–°ä¸€æ¬¡
    
    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    num_episodes = 1000
    
    for episode in range(num_episodes):
        # ===== æ¨¡æ‹Ÿè®­ç»ƒä¸€ä¸ª episode =====
        
        # æ¨¡æ‹Ÿå¥–åŠ±é€æ¸æå‡
        base_reward = 50 + episode * 0.15
        noise = np.random.randn() * 10
        reward = base_reward + noise
        
        # æ¨¡æ‹ŸæŸå¤±é€æ¸ä¸‹é™
        loss = 1.0 / (1 + episode * 0.01) + np.random.randn() * 0.05
        loss = max(0.001, loss)  # ä¿è¯éè´Ÿ
        
        # æ¨¡æ‹Ÿç¼“å­˜å‘½ä¸­ç‡é€æ¸æå‡
        hit_rate = min(0.92, 0.55 + episode * 0.0008) + np.random.randn() * 0.03
        hit_rate = np.clip(hit_rate, 0, 1)
        
        # æ¨¡æ‹Ÿå»¶è¿Ÿé€æ¸é™ä½
        delay = max(20, 120 - episode * 0.08 + np.random.randn() * 8)
        
        # æ¨¡æ‹Ÿèƒ½è€—é€æ¸é™ä½
        energy = max(10, 55 - episode * 0.025 + np.random.randn() * 3)
        
        # æ¨¡æ‹ŸæˆåŠŸç‡æå‡
        success_rate = min(0.98, 0.75 + episode * 0.0005)
        success_rate = np.clip(success_rate, 0, 1)
        
        # æ¨¡æ‹ŸåŠ¨ä½œå‘é‡ï¼ˆ10ç»´è¿ç»­åŠ¨ä½œï¼‰
        action = np.random.randn(10) * 0.5
        
        # æ¨¡æ‹Ÿæ¢¯åº¦èŒƒæ•°ï¼ˆé€æ¸æ”¶æ•›ï¼‰
        gradient_norm = 2.0 / (1 + episode * 0.015) * (1 + np.random.randn() * 0.2)
        gradient_norm = max(0.001, gradient_norm)
        
        # ===== æ›´æ–°å¯è§†åŒ– =====
        metrics = {
            'reward': reward,
            'loss': loss,
            'hit_rate': hit_rate,
            'delay': delay,
            'energy': energy,
            'success_rate': success_rate,
            'action': action,
            'gradient_norm': gradient_norm
        }
        
        visualizer.update(episode, metrics)
        
        # æ¯10ä¸ªepisodeæ‰“å°ä¸€æ¬¡è¿›åº¦
        if episode % 10 == 0:
            print(f"Episode {episode:4d} | "
                  f"Reward: {reward:7.2f} | "
                  f"Loss: {loss:6.4f} | "
                  f"Hit Rate: {hit_rate*100:5.1f}% | "
                  f"Delay: {delay:6.2f}ms")
        
        # æ§åˆ¶è®­ç»ƒé€Ÿåº¦ï¼ˆå®é™…è®­ç»ƒä¸­ä¸éœ€è¦ï¼‰
        time.sleep(0.03)
        
        # æ¯100ä¸ªepisodeè‡ªåŠ¨ä¿å­˜ä¸€æ¬¡
        if episode > 0 and episode % 100 == 0:
            visualizer.save(f"checkpoint_episode_{episode}.png")
    
    print("=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("   å¯è§†åŒ–çª—å£å°†ä¿æŒæ‰“å¼€çŠ¶æ€")
    print("   æŒ‰ 's' ä¿å­˜æœ€ç»ˆå›¾åƒï¼ŒæŒ‰ 'q' é€€å‡º")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    visualizer.save("final_training_result.png")
    
    # ä¿æŒçª—å£æ‰“å¼€
    import matplotlib.pyplot as plt
    plt.show()


def integration_example():
    """
    é›†æˆåˆ°å®é™…è®­ç»ƒä»£ç çš„ç¤ºä¾‹
    """
    code_example = '''
# ========== åœ¨è®­ç»ƒè„šæœ¬ä¸­é›†æˆå¯è§†åŒ– ==========

from utils.advanced_training_visualizer import create_visualizer

def train_agent():
    # 1. åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = create_visualizer(max_history=500)
    visualizer.start(interval=1000)  # æ¯ç§’æ›´æ–°ä¸€æ¬¡
    
    # 2. è®­ç»ƒå¾ªç¯
    for episode in range(num_episodes):
        # ... ä½ çš„è®­ç»ƒä»£ç  ...
        
        state = env.reset()
        episode_reward = 0
        losses = []
        
        for step in range(max_steps):
            action = agent.select_action(state)
            next_state, reward, done, info = env.step(action)
            
            # è®­ç»ƒagent
            loss = agent.train(state, action, reward, next_state, done)
            losses.append(loss)
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        # 3. æ›´æ–°å¯è§†åŒ–ï¼ˆæ¯ä¸ªepisodeä¸€æ¬¡ï¼‰
        metrics = {
            'reward': episode_reward,
            'loss': np.mean(losses) if losses else 0,
            'hit_rate': info.get('cache_hit_rate', 0),
            'delay': info.get('avg_delay', 0),
            'energy': info.get('total_energy', 0),
            'success_rate': info.get('success_rate', 0),
            'action': action,  # æœ€åä¸€ä¸ªåŠ¨ä½œ
            'gradient_norm': agent.get_gradient_norm()  # å¦‚æœå¯ç”¨
        }
        visualizer.update(episode, metrics)
        
        # 4. å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if episode % 100 == 0:
            visualizer.save(f"checkpoint_{episode}.png")
    
    # 5. ä¿å­˜æœ€ç»ˆç»“æœ
    visualizer.save("final_result.png")
    
    return visualizer

# ========== ä½¿ç”¨ç¤ºä¾‹ ==========
if __name__ == "__main__":
    visualizer = train_agent()
    
    # è®­ç»ƒç»“æŸåä¿æŒå¯è§†åŒ–çª—å£æ‰“å¼€
    import matplotlib.pyplot as plt
    plt.show()
'''
    
    print("=" * 60)
    print("ğŸ“š é›†æˆç¤ºä¾‹ä»£ç ï¼š")
    print("=" * 60)
    print(code_example)
    print("=" * 60)


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'example':
        # æ˜¾ç¤ºé›†æˆç¤ºä¾‹
        integration_example()
    else:
        # è¿è¡Œæ¼”ç¤º
        demo_training_with_visualization()
