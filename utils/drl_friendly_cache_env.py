#!/usr/bin/env python3
"""
DRLå‹å¥½çš„ç¼“å­˜ç¯å¢ƒ
æ”¯æŒå¯æ§ç°å®åº¦çš„æ¸è¿›å¼è®­ç»ƒ
"""

import numpy as np
import time
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from enum import Enum
import random

class RealismLevel(Enum):
    """ç°å®åº¦ç­‰çº§"""
    MINIMAL = "minimal"          # æœ€å°åŒ–ï¼šå›ºå®šæ¨¡å¼ï¼Œä¾¿äºå­¦ä¹ 
    BASIC = "basic"              # åŸºç¡€ï¼šç®€å•æ—¶é—´æ¨¡å¼
    MODERATE = "moderate"        # ä¸­ç­‰ï¼šç”¨æˆ·ç±»å‹å·®å¼‚
    REALISTIC = "realistic"      # ç°å®ï¼šå®Œæ•´è¡Œä¸ºæ¨¡å¼
    CHAOTIC = "chaotic"          # æ··æ²Œï¼šé«˜éšæœºæ€§æµ‹è¯•

@dataclass
class CacheEnvironmentConfig:
    """ç¼“å­˜ç¯å¢ƒé…ç½®"""
    realism_level: RealismLevel = RealismLevel.MINIMAL
    num_content_types: int = 4           # å†…å®¹ç±»å‹æ•°é‡
    cache_capacity: int = 10             # ç¼“å­˜å®¹é‡(é¡¹ç›®æ•°)
    episode_length: int = 100            # Episodeé•¿åº¦
    request_frequency: float = 0.1       # è¯·æ±‚é¢‘ç‡
    reward_shaping: bool = True          # å¥–åŠ±å¡‘å½¢
    state_simplification: bool = True    # çŠ¶æ€ç®€åŒ–
    user_behavior_noise: float = 0.1     # ç”¨æˆ·è¡Œä¸ºå™ªå£°
    temporal_patterns: bool = False      # æ—¶é—´æ¨¡å¼
    user_diversity: bool = False         # ç”¨æˆ·å¤šæ ·æ€§

class DRLFriendlyCacheEnvironment:
    """DRLå‹å¥½çš„ç¼“å­˜ç¯å¢ƒ"""
    
    def __init__(self, config: CacheEnvironmentConfig):
        self.config = config
        self.current_step = 0
        self.episode_step = 0
        
        # çŠ¶æ€ç»´åº¦è®¾è®¡
        self._setup_state_space()
        
        # åŠ¨ä½œç©ºé—´è®¾è®¡  
        self._setup_action_space()
        
        # å†…å®¹è¯·æ±‚æ¨¡å¼
        self._setup_request_patterns()
        
        # ç¼“å­˜çŠ¶æ€
        self.cache_items = []  # ç®€åŒ–ä¸ºåˆ—è¡¨
        self.cache_ages = []   # ç¼“å­˜å¹´é¾„
        self.cache_access_counts = []  # è®¿é—®æ¬¡æ•°
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'episode_reward': 0.0,
            'hit_rate_history': []
        }
        
        print(f"ğŸ¤– DRLç¼“å­˜ç¯å¢ƒåˆå§‹åŒ– - ç°å®åº¦: {config.realism_level.value}")
    
    def _setup_state_space(self):
        """è®¾ç½®çŠ¶æ€ç©ºé—´"""
        # åŸºç¡€çŠ¶æ€ç»´åº¦
        base_dims = [
            self.config.cache_capacity,      # ç¼“å­˜å ç”¨ç‡ 
            self.config.num_content_types,   # å½“å‰è¯·æ±‚ç±»å‹
            1,                               # æ—¶é—´æ­¥
        ]
        
        # æ ¹æ®ç°å®åº¦å¢åŠ ç»´åº¦
        if self.config.realism_level.value in ['moderate', 'realistic']:
            base_dims.extend([
                1,  # ç”¨æˆ·ç±»å‹
                1,  # æ—¶é—´æ¨¡å¼
            ])
        
        if self.config.realism_level.value in ['realistic', 'chaotic']:
            base_dims.extend([
                1,  # ä½ç½®ç›¸å…³æ€§
                1,  # å†…å®¹æ–°é²œåº¦
            ])
        
        self.state_dim = sum(base_dims)
        print(f"  çŠ¶æ€ç»´åº¦: {self.state_dim}")
    
    def _setup_action_space(self):
        """è®¾ç½®åŠ¨ä½œç©ºé—´"""
        # ç®€åŒ–çš„åŠ¨ä½œè®¾è®¡
        self.actions = {
            0: "ä¸ç¼“å­˜",
            1: "ç¼“å­˜(ä½ä¼˜å…ˆçº§)",  
            2: "ç¼“å­˜(é«˜ä¼˜å…ˆçº§)",
            3: "æ›¿æ¢æœ€æ—§é¡¹ç›®",
            4: "æ›¿æ¢æœ€å°‘ä½¿ç”¨é¡¹ç›®"
        }
        
        self.action_dim = len(self.actions)
        print(f"  åŠ¨ä½œç»´åº¦: {self.action_dim}")
    
    def _setup_request_patterns(self):
        """è®¾ç½®è¯·æ±‚æ¨¡å¼"""
        if self.config.realism_level == RealismLevel.MINIMAL:
            # å›ºå®šå¾ªç¯æ¨¡å¼ï¼Œä¾¿äºå­¦ä¹ 
            self.request_pattern = [0, 1, 2, 3] * 25  # ç®€å•å¾ªç¯
            
        elif self.config.realism_level == RealismLevel.BASIC:
            # åŸºç¡€æ—¶é—´æ¨¡å¼
            morning_pattern = [0, 0, 1, 2]  # æ—©é«˜å³°ï¼šäº¤é€šå¯¼èˆª
            noon_pattern = [3, 3, 2, 1]     # åˆä¼‘ï¼šå¨±ä¹åœè½¦
            self.request_pattern = morning_pattern * 12 + noon_pattern * 13
            
        elif self.config.realism_level == RealismLevel.MODERATE:
            # ä¸­ç­‰å¤æ‚åº¦ï¼š3ç§ç”¨æˆ·ç±»å‹
            commuter_pattern = [0, 0, 1, 1, 2]      # é€šå‹¤æ—
            leisure_pattern = [3, 3, 2, 1, 0]       # ä¼‘é—²ç”¨æˆ·  
            business_pattern = [1, 2, 0, 1, 2]      # å•†åŠ¡äººå£«
            self.request_pattern = (commuter_pattern * 7 + 
                                  leisure_pattern * 6 + 
                                  business_pattern * 7)
        else:
            # ç°å®/æ··æ²Œæ¨¡å¼ï¼šä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒ
            self.request_pattern = None  # åŠ¨æ€ç”Ÿæˆ
    
    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = 0
        self.episode_step = 0
        
        # æ¸…ç©ºç¼“å­˜
        self.cache_items = []
        self.cache_ages = []
        self.cache_access_counts = []
        
        # é‡ç½®ç»Ÿè®¡
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'episode_reward': 0.0,
            'hit_rate_history': []
        }
        
        return self._get_state()
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """ç¯å¢ƒæ­¥è¿›"""
        self.episode_step += 1
        
        # ç”Ÿæˆå†…å®¹è¯·æ±‚
        content_type = self._generate_content_request()
        
        # æ£€æŸ¥ç¼“å­˜å‘½ä¸­
        cache_hit = self._check_cache_hit(content_type)
        
        # æ‰§è¡Œç¼“å­˜åŠ¨ä½œ
        self._execute_cache_action(action, content_type, cache_hit)
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(cache_hit, action, content_type)
        
        # æ›´æ–°ç¼“å­˜çŠ¶æ€
        self._update_cache_state()
        
        # æ›´æ–°ç»Ÿè®¡
        self._update_statistics(cache_hit, reward)
        
        # æ£€æŸ¥episodeç»“æŸ
        done = self.episode_step >= self.config.episode_length
        
        # è·å–æ–°çŠ¶æ€
        next_state = self._get_state()
        
        # ä¿¡æ¯å­—å…¸
        info = {
            'cache_hit': cache_hit,
            'content_type': content_type,
            'hit_rate': self.stats['cache_hits'] / max(1, self.stats['total_requests']),
            'cache_utilization': len(self.cache_items) / self.config.cache_capacity
        }
        
        return next_state, reward, done, info
    
    def _generate_content_request(self) -> int:
        """ç”Ÿæˆå†…å®¹è¯·æ±‚"""
        if self.request_pattern is not None:
            # ä½¿ç”¨é¢„å®šä¹‰æ¨¡å¼
            pattern_idx = self.episode_step % len(self.request_pattern)
            content_type = self.request_pattern[pattern_idx]
        else:
            # ä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒï¼ˆç°å®æ¨¡å¼ï¼‰
            content_type = self._generate_realistic_request()
        
        # æ·»åŠ å™ªå£°
        if np.random.random() < self.config.user_behavior_noise:
            content_type = np.random.randint(0, self.config.num_content_types)
        
        return content_type
    
    def _generate_realistic_request(self) -> int:
        """ç”Ÿæˆç°å®è¯·æ±‚ï¼ˆå¤æ‚æ¨¡å¼ï¼‰"""
        # ç®€åŒ–çš„æ—¶é—´æ¨¡å¼
        hour_of_day = (self.episode_step // 4) % 24
        
        if 7 <= hour_of_day <= 9:  # æ—©é«˜å³°
            probs = [0.4, 0.3, 0.2, 0.1]  # äº¤é€šï¼Œå¯¼èˆªï¼Œåœè½¦ï¼Œå¨±ä¹
        elif 12 <= hour_of_day <= 14:  # åˆä¼‘
            probs = [0.2, 0.2, 0.2, 0.4]  # å‡åŒ€åˆ†å¸ƒ
        elif 17 <= hour_of_day <= 19:  # æ™šé«˜å³°
            probs = [0.5, 0.25, 0.15, 0.1]  # äº¤é€šä¼˜å…ˆ
        else:  # å…¶ä»–æ—¶é—´
            probs = [0.25, 0.25, 0.25, 0.25]  # å‡åŒ€åˆ†å¸ƒ
        
        return np.random.choice(self.config.num_content_types, p=probs)
    
    def _check_cache_hit(self, content_type: int) -> bool:
        """æ£€æŸ¥ç¼“å­˜å‘½ä¸­"""
        if content_type in self.cache_items:
            # æ›´æ–°è®¿é—®è®¡æ•°å’Œå¹´é¾„
            idx = self.cache_items.index(content_type)
            self.cache_access_counts[idx] += 1
            self.cache_ages[idx] = 0  # é‡ç½®å¹´é¾„
            return True
        return False
    
    def _execute_cache_action(self, action: int, content_type: int, cache_hit: bool):
        """æ‰§è¡Œç¼“å­˜åŠ¨ä½œ"""
        if cache_hit:
            return  # å·²å‘½ä¸­ï¼Œæ— éœ€æ“ä½œ
        
        if action == 0:  # ä¸ç¼“å­˜
            return
        
        elif action in [1, 2]:  # ç¼“å­˜ï¼ˆä½/é«˜ä¼˜å…ˆçº§ï¼‰
            if len(self.cache_items) < self.config.cache_capacity:
                # æœ‰ç©ºé—´ï¼Œç›´æ¥æ·»åŠ 
                self.cache_items.append(content_type)
                self.cache_ages.append(0)
                self.cache_access_counts.append(1)
            elif action == 2:  # é«˜ä¼˜å…ˆçº§ï¼Œå¼ºåˆ¶æ›¿æ¢
                self._replace_cache_item(content_type, method='random')
        
        elif action == 3:  # æ›¿æ¢æœ€æ—§é¡¹ç›®
            if len(self.cache_items) >= self.config.cache_capacity:
                self._replace_cache_item(content_type, method='oldest')
        
        elif action == 4:  # æ›¿æ¢æœ€å°‘ä½¿ç”¨é¡¹ç›®
            if len(self.cache_items) >= self.config.cache_capacity:
                self._replace_cache_item(content_type, method='lfu')
    
    def _replace_cache_item(self, new_content: int, method: str):
        """æ›¿æ¢ç¼“å­˜é¡¹ç›®"""
        if not self.cache_items:
            return
        
        if method == 'random':
            idx = np.random.randint(len(self.cache_items))
        elif method == 'oldest':
            idx = np.argmax(self.cache_ages)
        elif method == 'lfu':
            idx = np.argmin(self.cache_access_counts)
        else:
            idx = 0
        
        # æ›¿æ¢
        self.cache_items[idx] = new_content
        self.cache_ages[idx] = 0
        self.cache_access_counts[idx] = 1
    
    def _calculate_reward(self, cache_hit: bool, action: int, content_type: int) -> float:
        """è®¡ç®—å¥–åŠ±"""
        reward = 0.0
        
        # åŸºç¡€å¥–åŠ±ï¼šå‘½ä¸­è·å¾—æ­£å¥–åŠ±
        if cache_hit:
            reward += 1.0
        else:
            reward -= 0.1  # å°çš„è´Ÿå¥–åŠ±
        
        # å¥–åŠ±å¡‘å½¢
        if self.config.reward_shaping:
            # ç¼“å­˜åˆ©ç”¨ç‡å¥–åŠ±
            utilization = len(self.cache_items) / self.config.cache_capacity
            if 0.6 <= utilization <= 0.9:  # é¼“åŠ±åˆç†åˆ©ç”¨ç‡
                reward += 0.1
            
            # åŠ¨ä½œåˆç†æ€§å¥–åŠ±
            if not cache_hit and action in [1, 2]:  # æœªå‘½ä¸­æ—¶ç¼“å­˜
                reward += 0.05
            elif cache_hit and action == 0:  # å‘½ä¸­æ—¶ä¸é‡å¤ç¼“å­˜
                reward += 0.05
        
        return reward
    
    def _update_cache_state(self):
        """æ›´æ–°ç¼“å­˜çŠ¶æ€"""
        # å¢åŠ æ‰€æœ‰é¡¹ç›®å¹´é¾„
        self.cache_ages = [age + 1 for age in self.cache_ages]
    
    def _update_statistics(self, cache_hit: bool, reward: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['total_requests'] += 1
        if cache_hit:
            self.stats['cache_hits'] += 1
        
        self.stats['episode_reward'] += reward
        
        # è®°å½•å‘½ä¸­ç‡å†å²
        hit_rate = self.stats['cache_hits'] / self.stats['total_requests']
        self.stats['hit_rate_history'].append(hit_rate)
    
    def _get_state(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€"""
        state = []
        
        # åŸºç¡€çŠ¶æ€
        cache_utilization = len(self.cache_items) / self.config.cache_capacity
        state.append(cache_utilization)
        
        # ç¼“å­˜å†…å®¹åˆ†å¸ƒ
        content_counts = [0] * self.config.num_content_types
        for item in self.cache_items:
            content_counts[item] += 1
        
        # å½’ä¸€åŒ–å†…å®¹è®¡æ•°
        if self.config.cache_capacity > 0:
            content_counts = [c / self.config.cache_capacity for c in content_counts]
        
        state.extend(content_counts)
        
        # æ—¶é—´æ­¥ï¼ˆå½’ä¸€åŒ–ï¼‰
        time_step = self.episode_step / self.config.episode_length
        state.append(time_step)
        
        # æ ¹æ®ç°å®åº¦æ·»åŠ å…¶ä»–çŠ¶æ€
        if self.config.realism_level.value in ['moderate', 'realistic']:
            # æ·»åŠ æ—¶é—´æ¨¡å¼çŠ¶æ€
            hour = (self.episode_step // 4) % 24
            time_pattern = self._get_time_pattern(hour)
            state.append(time_pattern)
        
        # å¡«å……åˆ°å›ºå®šç»´åº¦
        while len(state) < self.state_dim:
            state.append(0.0)
        
        return np.array(state[:self.state_dim], dtype=np.float32)
    
    def _get_time_pattern(self, hour: int) -> float:
        """è·å–æ—¶é—´æ¨¡å¼ç‰¹å¾"""
        if 7 <= hour <= 9:
            return 0.8  # æ—©é«˜å³°
        elif 12 <= hour <= 14:
            return 0.6  # åˆä¼‘
        elif 17 <= hour <= 19:
            return 0.9  # æ™šé«˜å³°
        else:
            return 0.3  # å…¶ä»–æ—¶é—´
    
    def get_statistics(self) -> Dict:
        """è·å–ç¯å¢ƒç»Ÿè®¡"""
        return {
            'hit_rate': self.stats['cache_hits'] / max(1, self.stats['total_requests']),
            'total_requests': self.stats['total_requests'],
            'cache_utilization': len(self.cache_items) / self.config.cache_capacity,
            'episode_reward': self.stats['episode_reward'],
            'cache_items': len(self.cache_items)
        }


class ProgressiveTrainingManager:
    """æ¸è¿›å¼è®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self):
        self.training_stages = [
            {
                'name': 'Stage 1: åŸºç¡€å­¦ä¹ ',
                'config': CacheEnvironmentConfig(
                    realism_level=RealismLevel.MINIMAL,
                    num_content_types=3,
                    cache_capacity=5,
                    episode_length=50,
                    reward_shaping=True,
                    state_simplification=True
                ),
                'episodes': 1000,
                'success_criteria': {'hit_rate': 0.6}
            },
            {
                'name': 'Stage 2: æ—¶é—´æ¨¡å¼',
                'config': CacheEnvironmentConfig(
                    realism_level=RealismLevel.BASIC,
                    num_content_types=4,
                    cache_capacity=8,
                    episode_length=100,
                    temporal_patterns=True,
                    reward_shaping=True
                ),
                'episodes': 2000,
                'success_criteria': {'hit_rate': 0.5}
            },
            {
                'name': 'Stage 3: ç”¨æˆ·å¤šæ ·æ€§',
                'config': CacheEnvironmentConfig(
                    realism_level=RealismLevel.MODERATE,
                    num_content_types=6,
                    cache_capacity=10,
                    episode_length=150,
                    user_diversity=True,
                    user_behavior_noise=0.1
                ),
                'episodes': 3000,
                'success_criteria': {'hit_rate': 0.45}
            },
            {
                'name': 'Stage 4: ç°å®åœºæ™¯',
                'config': CacheEnvironmentConfig(
                    realism_level=RealismLevel.REALISTIC,
                    num_content_types=8,
                    cache_capacity=15,
                    episode_length=200,
                    user_behavior_noise=0.2,
                    reward_shaping=False  # ç§»é™¤å¥–åŠ±å¡‘å½¢
                ),
                'episodes': 5000,
                'success_criteria': {'hit_rate': 0.4}
            }
        ]
    
    def get_next_stage(self, current_performance: Dict) -> Optional[Dict]:
        """è·å–ä¸‹ä¸€ä¸ªè®­ç»ƒé˜¶æ®µ"""
        for stage in self.training_stages:
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ¡ä»¶
            meets_criteria = True
            for metric, threshold in stage['success_criteria'].items():
                if current_performance.get(metric, 0) < threshold:
                    meets_criteria = False
                    break
            
            if not meets_criteria:
                return stage
        
        return None  # æ‰€æœ‰é˜¶æ®µå®Œæˆ


def test_drl_environment():
    """æµ‹è¯•DRLç¯å¢ƒ"""
    print("ğŸ§ª æµ‹è¯•DRLå‹å¥½çš„ç¼“å­˜ç¯å¢ƒ...")
    
    # æµ‹è¯•ä¸åŒç°å®åº¦çº§åˆ«
    realism_levels = [RealismLevel.MINIMAL, RealismLevel.BASIC, RealismLevel.MODERATE]
    
    for level in realism_levels:
        print(f"\nğŸ¯ æµ‹è¯•ç°å®åº¦: {level.value}")
        
        config = CacheEnvironmentConfig(realism_level=level)
        env = DRLFriendlyCacheEnvironment(config)
        
        # è¿è¡Œå‡ ä¸ªepisode
        total_reward = 0
        for episode in range(3):
            state = env.reset()
            episode_reward = 0
            
            for step in range(20):
                # éšæœºåŠ¨ä½œï¼ˆå®é™…ä¸­ç”¨DRL agentï¼‰
                action = np.random.randint(env.action_dim)
                next_state, reward, done, info = env.step(action)
                
                episode_reward += reward
                state = next_state
                
                if done:
                    break
            
            total_reward += episode_reward
            stats = env.get_statistics()
            print(f"  Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}, å‘½ä¸­ç‡={stats['hit_rate']:.2%}")
        
        avg_reward = total_reward / 3
        print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    
    print("\nâœ… DRLç¯å¢ƒæµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    test_drl_environment()
