"""
æˆ˜ç•¥å±‚ï¼ˆStrategic Layerï¼‰å®ç°
ä½¿ç”¨SACç®—æ³•è¿›è¡Œé«˜å±‚å†³ç­–ï¼šè®¡ç®—å¸è½½ vs å†…å®¹ç¼“å­˜

ä¸»è¦åŠŸèƒ½ï¼š
1. åˆ†ææ•´ä¸ªåŒºåŸŸçš„è½¦è¾†å¯†åº¦ã€ç½‘ç»œè´Ÿè½½ç­‰å®è§‚ä¿¡æ¯
2. å†³å®šå½“å‰æ—¶åˆ»ç³»ç»Ÿåº”è¯¥ä¼˜å…ˆé‡‡å–çš„æ€»ä½“ç­–ç•¥
3. ä¸ºæˆ˜æœ¯å±‚æä¾›é«˜å±‚æŒ‡å¯¼
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Optional
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from single_agent.sac import SACAgent, SACConfig
from hierarchical_learning.core.base_layer import BaseLayer


class StrategicLayer(BaseLayer):
    """æˆ˜ç•¥å±‚ - ä½¿ç”¨SACç®—æ³•è¿›è¡Œé«˜å±‚æˆ˜ç•¥å†³ç­–"""
    
    def __init__(self, config: Dict):
        super().__init__(config)
        
        # æˆ˜ç•¥å±‚çŠ¶æ€ç»´åº¦ï¼šåŒºåŸŸçº§åˆ«çš„å®è§‚ä¿¡æ¯
        # åŒ…æ‹¬ï¼šè½¦è¾†å¯†åº¦ã€ç½‘ç»œè´Ÿè½½ã€RSUçŠ¶æ€ã€UAVçŠ¶æ€ã€å†å²æ€§èƒ½æŒ‡æ ‡ç­‰
        self.strategic_state_dim = config.get('strategic_state_dim', 20)
        
        # æˆ˜ç•¥å±‚åŠ¨ä½œç»´åº¦ï¼šé«˜å±‚ç­–ç•¥é€‰æ‹©
        # åŠ¨ä½œç©ºé—´ï¼š[è®¡ç®—å¸è½½æƒé‡, å†…å®¹ç¼“å­˜æƒé‡, èµ„æºåˆ†é…ç­–ç•¥, ä¼˜å…ˆçº§è®¾ç½®]
        self.strategic_action_dim = config.get('strategic_action_dim', 4)
        
        # SACé…ç½® - å…¨é¢ä¼˜åŒ–
        sac_config = SACConfig(
            hidden_dim=config.get('strategic_hidden_dim', 512),   # å¢å¤§ç½‘ç»œå®¹é‡
            actor_lr=config.get('strategic_actor_lr', 8e-5),      # ä¼˜åŒ–å­¦ä¹ ç‡
            critic_lr=config.get('strategic_critic_lr', 1e-4),    # ç¨é«˜çš„criticå­¦ä¹ ç‡
            alpha_lr=config.get('strategic_alpha_lr', 1e-4),      # æ¸©åº¦å‚æ•°å­¦ä¹ ç‡
            initial_temperature=config.get('strategic_temperature', 0.05),  # æ›´ä½çš„åˆå§‹æ¸©åº¦
            tau=config.get('strategic_tau', 0.002),               # é€‚ä¸­çš„è½¯æ›´æ–°ç‡
            gamma=config.get('strategic_gamma', 0.998),           # æ›´é«˜çš„æŠ˜æ‰£å› å­ï¼ˆé•¿æœŸè§„åˆ’ï¼‰
            batch_size=config.get('strategic_batch_size', 64),    # é™ä½æ‰¹æ¬¡å¤§å°ï¼ŒåŠ é€Ÿè§¦å‘æ›´æ–°
            buffer_size=config.get('strategic_buffer_size', 100000)  # å¢å¤§ç¼“å†²åŒº
        )
        
        # åˆå§‹åŒ–SACæ™ºèƒ½ä½“
        self.sac_agent = SACAgent(
            state_dim=self.strategic_state_dim,
            action_dim=self.strategic_action_dim,
            config=sac_config
        )
        
        # æˆ˜ç•¥å†³ç­–å†å²
        self.decision_history = []
        self.performance_history = []
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.initial_lr = {
            'actor': sac_config.actor_lr,
            'critic': sac_config.critic_lr,
            'alpha': sac_config.alpha_lr
        }
        self.lr_decay_rate = config.get('strategic_lr_decay', 0.9995)
        self.min_lr = config.get('strategic_min_lr', 1e-6)
        self.training_steps = 0
        
        # ç®—æ³•å¢å¼ºåŠŸèƒ½
        self.use_gradient_clipping = config.get('strategic_gradient_clip', True)
        self.gradient_clip_value = config.get('strategic_clip_value', 1.0)
        self.use_adaptive_lr = config.get('strategic_adaptive_lr', True)
        self.lr_patience = config.get('strategic_lr_patience', 100)
        self.performance_window = []
        self.best_performance = float('-inf')
        self.patience_counter = 0
        
        # é«˜çº§æ¢ç´¢ç­–ç•¥
        self.exploration_schedule = config.get('strategic_exploration_schedule', 'linear')
        self.initial_exploration = config.get('strategic_initial_exploration', 0.9)
        self.final_exploration = config.get('strategic_final_exploration', 0.1)
        self.exploration_steps = config.get('strategic_exploration_steps', 50000)
        
    def process_state(self, raw_state: Dict) -> np.ndarray:
        """
        å¤„ç†åŸå§‹ç¯å¢ƒçŠ¶æ€ï¼Œæå–æˆ˜ç•¥å±‚éœ€è¦çš„å®è§‚ä¿¡æ¯
        
        Args:
            raw_state: åŒ…å«è½¦è¾†ã€RSUã€UAVç­‰è¯¦ç»†çŠ¶æ€çš„å­—å…¸
            
        Returns:
            strategic_state: æˆ˜ç•¥å±‚çŠ¶æ€å‘é‡
        """
        strategic_features = []
        
        # 1. è½¦è¾†å¯†åº¦å’Œåˆ†å¸ƒç‰¹å¾
        if 'vehicles' in raw_state:
            vehicles = raw_state['vehicles']
            vehicle_count = len(vehicles)
            
            # è½¦è¾†å¯†åº¦
            strategic_features.append(vehicle_count / 100.0)  # å½’ä¸€åŒ–
            
            # è½¦è¾†è®¡ç®—éœ€æ±‚ç»Ÿè®¡
            total_compute_demand = sum([v.get('compute_demand', 0) for v in vehicles])
            avg_compute_demand = total_compute_demand / max(vehicle_count, 1)
            strategic_features.append(avg_compute_demand / 1000.0)  # å½’ä¸€åŒ–
            
            # è½¦è¾†ç§»åŠ¨æ€§ç»Ÿè®¡
            avg_velocity = np.mean([v.get('velocity', 0) for v in vehicles])
            strategic_features.append(avg_velocity / 30.0)  # å½’ä¸€åŒ–åˆ°0-1
            
        else:
            strategic_features.extend([0.0, 0.0, 0.0])
        
        # 2. RSUç½‘ç»œè´Ÿè½½å’ŒçŠ¶æ€
        if 'rsus' in raw_state:
            rsus = raw_state['rsus']
            rsu_count = len(rsus)
            
            # RSUå¹³å‡è´Ÿè½½
            total_rsu_load = sum([rsu.get('cpu_usage', 0) for rsu in rsus])
            avg_rsu_load = total_rsu_load / max(rsu_count, 1)
            strategic_features.append(avg_rsu_load)
            
            # RSUå¯ç”¨è®¡ç®—èµ„æº
            total_available_compute = sum([rsu.get('available_compute', 0) for rsu in rsus])
            strategic_features.append(total_available_compute / 10000.0)  # å½’ä¸€åŒ–
            
            # RSUç½‘ç»œæ‹¥å¡ç¨‹åº¦
            total_network_load = sum([rsu.get('network_load', 0) for rsu in rsus])
            avg_network_load = total_network_load / max(rsu_count, 1)
            strategic_features.append(avg_network_load)
            
        else:
            strategic_features.extend([0.0, 0.0, 0.0])
        
        # 3. UAVçŠ¶æ€ï¼ˆå›ºå®šä½ç½®ï¼Œä½†çŠ¶æ€å¯å˜ï¼‰
        if 'uavs' in raw_state:
            uavs = raw_state['uavs']
            uav_count = len(uavs)
            
            # UAVå¹³å‡è´Ÿè½½
            total_uav_load = sum([uav.get('cpu_usage', 0) for uav in uavs])
            avg_uav_load = total_uav_load / max(uav_count, 1)
            strategic_features.append(avg_uav_load)
            
            # UAVå¯ç”¨è®¡ç®—èµ„æº
            total_uav_compute = sum([uav.get('available_compute', 0) for uav in uavs])
            strategic_features.append(total_uav_compute / 5000.0)  # å½’ä¸€åŒ–
            
            # UAVè¦†ç›–æ•ˆç‡
            coverage_efficiency = sum([uav.get('coverage_efficiency', 0) for uav in uavs]) / max(uav_count, 1)
            strategic_features.append(coverage_efficiency)
            
        else:
            strategic_features.extend([0.0, 0.0, 0.0])
        
        # 4. ç³»ç»Ÿæ•´ä½“æ€§èƒ½æŒ‡æ ‡
        if 'system_metrics' in raw_state:
            metrics = raw_state['system_metrics']
            
            # æ•´ä½“å»¶è¿Ÿ
            strategic_features.append(metrics.get('avg_latency', 0) / 100.0)  # å½’ä¸€åŒ–
            
            # èƒ½è€—æ•ˆç‡
            strategic_features.append(metrics.get('energy_efficiency', 0))
            
            # æˆåŠŸç‡
            strategic_features.append(metrics.get('success_rate', 0))
            
            # ç½‘ç»œåˆ©ç”¨ç‡
            strategic_features.append(metrics.get('network_utilization', 0))
            
        else:
            strategic_features.extend([0.0, 0.0, 0.0, 0.0])
        
        # 5. å†å²æ€§èƒ½è¶‹åŠ¿ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if len(self.performance_history) > 0:
            recent_performance = np.mean(self.performance_history[-5:])  # æœ€è¿‘5æ­¥çš„å¹³å‡æ€§èƒ½
            strategic_features.append(recent_performance)
        else:
            strategic_features.append(0.0)
        
        # 6. æ—¶é—´ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
        time_of_day = raw_state.get('time_of_day', 0) / 24.0  # å½’ä¸€åŒ–åˆ°0-1
        strategic_features.append(time_of_day)
        
        # ç¡®ä¿ç‰¹å¾å‘é‡é•¿åº¦æ­£ç¡®
        while len(strategic_features) < self.strategic_state_dim:
            strategic_features.append(0.0)
        
        strategic_features = strategic_features[:self.strategic_state_dim]
        
        return np.array(strategic_features, dtype=np.float32)
    
    def get_action(self, processed_state: np.ndarray) -> np.ndarray:
        """
        æ ¹æ®å¤„ç†åçš„æˆ˜ç•¥çŠ¶æ€ç”Ÿæˆé«˜å±‚å†³ç­–
        
        Args:
            processed_state: æˆ˜ç•¥å±‚çŠ¶æ€å‘é‡
            
        Returns:
            strategic_action: æˆ˜ç•¥å±‚åŠ¨ä½œå‘é‡
            [è®¡ç®—å¸è½½æƒé‡, å†…å®¹ç¼“å­˜æƒé‡, èµ„æºåˆ†é…ç­–ç•¥, ä¼˜å…ˆçº§è®¾ç½®]
        """
        action = self.sac_agent.select_action(processed_state, training=True)
        
        # å¯¹åŠ¨ä½œè¿›è¡Œåå¤„ç†ï¼Œç¡®ä¿ç¬¦åˆæˆ˜ç•¥å†³ç­–çš„è¯­ä¹‰
        strategic_action = self._post_process_action(action)
        
        # è®°å½•å†³ç­–å†å²
        self.decision_history.append({
            'state': processed_state.copy(),
            'action': strategic_action.copy(),
            'timestamp': len(self.decision_history)
        })
        
        return strategic_action
    
    def _post_process_action(self, raw_action: np.ndarray) -> np.ndarray:
        """
        å¯¹åŸå§‹åŠ¨ä½œè¿›è¡Œåå¤„ç†ï¼Œç¡®ä¿ç¬¦åˆæˆ˜ç•¥å†³ç­–çš„è¯­ä¹‰
        
        Args:
            raw_action: SACè¾“å‡ºçš„åŸå§‹åŠ¨ä½œ
            
        Returns:
            processed_action: å¤„ç†åçš„æˆ˜ç•¥åŠ¨ä½œ
        """
        processed_action = raw_action.copy()
        
        # 1. è®¡ç®—å¸è½½æƒé‡å’Œå†…å®¹ç¼“å­˜æƒé‡ï¼ˆä½¿ç”¨softmaxç¡®ä¿å’Œä¸º1ï¼‰
        offloading_weight = torch.softmax(torch.tensor([raw_action[0], raw_action[1]]), dim=0)
        processed_action[0] = offloading_weight[0].item()  # è®¡ç®—å¸è½½æƒé‡
        processed_action[1] = offloading_weight[1].item()  # å†…å®¹ç¼“å­˜æƒé‡
        
        # 2. èµ„æºåˆ†é…ç­–ç•¥ï¼ˆæ˜ å°„åˆ°0-1èŒƒå›´ï¼‰
        processed_action[2] = torch.sigmoid(torch.tensor(raw_action[2])).item()
        
        # 3. ä¼˜å…ˆçº§è®¾ç½®ï¼ˆæ˜ å°„åˆ°0-1èŒƒå›´ï¼‰
        processed_action[3] = torch.sigmoid(torch.tensor(raw_action[3])).item()
        
        return processed_action
    
    def train(self, replay_buffer=None) -> Dict[str, float]:
        """
        è®­ç»ƒæˆ˜ç•¥å±‚SACæ¨¡å‹
        
        Args:
            replay_buffer: ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆå¯é€‰ï¼ŒSACæœ‰è‡ªå·±çš„ç¼“å†²åŒºï¼‰
            
        Returns:
            training_stats: è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        if len(self.sac_agent.replay_buffer) < self.sac_agent.config.batch_size:
            return {}
        
        # ä½¿ç”¨SACçš„æ›´æ–°æ–¹æ³•
        training_stats = self.sac_agent.update()
        
        # åº”ç”¨æ¢¯åº¦è£å‰ª
        if self.use_gradient_clipping and training_stats:
            self._apply_gradient_clipping()
        
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦
        self.training_steps += 1
        self._update_learning_rate()
        
        # è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´
        if self.use_adaptive_lr and training_stats:
            self._adaptive_lr_update(training_stats.get('critic_loss', 0.0))
        
        # æ·»åŠ å¢å¼ºç»Ÿè®¡ä¿¡æ¯
        if training_stats:
            training_stats['current_lr'] = self._get_current_lr()
            training_stats['exploration_rate'] = self._get_exploration_rate()
            training_stats['training_steps'] = self.training_steps
        
        return training_stats
    
    def _update_learning_rate(self):
        """æ›´æ–°å­¦ä¹ ç‡ï¼ˆè¡°å‡ï¼‰"""
        if hasattr(self.sac_agent, 'actor_optimizer'):
            for param_group in self.sac_agent.actor_optimizer.param_groups:
                new_lr = max(self.min_lr, param_group['lr'] * self.lr_decay_rate)
                param_group['lr'] = new_lr
        
        if hasattr(self.sac_agent, 'critic_optimizer'):
            for param_group in self.sac_agent.critic_optimizer.param_groups:
                new_lr = max(self.min_lr, param_group['lr'] * self.lr_decay_rate)
                param_group['lr'] = new_lr
                
        if hasattr(self.sac_agent, 'alpha_optimizer'):
            for param_group in self.sac_agent.alpha_optimizer.param_groups:
                new_lr = max(self.min_lr, param_group['lr'] * self.lr_decay_rate)
                param_group['lr'] = new_lr
    
    def _get_current_lr(self) -> float:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        if hasattr(self.sac_agent, 'actor_optimizer'):
            return self.sac_agent.actor_optimizer.param_groups[0]['lr']
        return self.initial_lr['actor']
    
    def _apply_gradient_clipping(self):
        """åº”ç”¨æ¢¯åº¦è£å‰ª"""
        if hasattr(self.sac_agent, 'actor'):
            torch.nn.utils.clip_grad_norm_(self.sac_agent.actor.parameters(), self.gradient_clip_value)
        if hasattr(self.sac_agent, 'critic'):
            torch.nn.utils.clip_grad_norm_(self.sac_agent.critic.parameters(), self.gradient_clip_value)
    
    def _adaptive_lr_update(self, current_loss: float):
        """è‡ªé€‚åº”å­¦ä¹ ç‡æ›´æ–°"""
        self.performance_window.append(current_loss)
        
        # ä¿æŒçª—å£å¤§å°
        if len(self.performance_window) > self.lr_patience:
            self.performance_window.pop(0)
        
        # æ£€æŸ¥æ€§èƒ½æ”¹è¿›
        if len(self.performance_window) >= self.lr_patience:
            avg_performance = np.mean(self.performance_window)
            
            if avg_performance < self.best_performance:
                self.best_performance = avg_performance
                self.patience_counter = 0
            else:
                self.patience_counter += 1
                
                # å¦‚æœæ€§èƒ½æ²¡æœ‰æ”¹è¿›ï¼Œé™ä½å­¦ä¹ ç‡
                if self.patience_counter >= self.lr_patience:
                    self._reduce_learning_rate(0.5)
                    self.patience_counter = 0
                    print(f"ğŸ”„ æˆ˜ç•¥å±‚è‡ªé€‚åº”é™ä½å­¦ä¹ ç‡ï¼Œå½“å‰æŸå¤±: {avg_performance:.4f}")
    
    def _reduce_learning_rate(self, factor: float):
        """é™ä½å­¦ä¹ ç‡"""
        if hasattr(self.sac_agent, 'actor_optimizer'):
            for param_group in self.sac_agent.actor_optimizer.param_groups:
                param_group['lr'] = max(self.min_lr, param_group['lr'] * factor)
        
        if hasattr(self.sac_agent, 'critic_optimizer'):
            for param_group in self.sac_agent.critic_optimizer.param_groups:
                param_group['lr'] = max(self.min_lr, param_group['lr'] * factor)
    
    def _get_exploration_rate(self) -> float:
        """è·å–å½“å‰æ¢ç´¢ç‡"""
        if self.exploration_schedule == 'linear':
            progress = min(1.0, self.training_steps / self.exploration_steps)
            return self.initial_exploration - (self.initial_exploration - self.final_exploration) * progress
        elif self.exploration_schedule == 'exponential':
            decay_factor = np.exp(-self.training_steps / (self.exploration_steps / 3))
            return self.final_exploration + (self.initial_exploration - self.final_exploration) * decay_factor
        else:
            return self.final_exploration
    
    def store_experience(self, state: np.ndarray, action: np.ndarray, 
                        reward: float, next_state: np.ndarray, done: bool):
        """å­˜å‚¨ç»éªŒåˆ°SACçš„å›æ”¾ç¼“å†²åŒº"""
        self.sac_agent.store_experience(state, action, reward, next_state, done)
        
        # æ›´æ–°æ€§èƒ½å†å²
        self.performance_history.append(reward)
        if len(self.performance_history) > 100:  # ä¿æŒå†å²é•¿åº¦
            self.performance_history.pop(0)
    
    def save_model(self, path: str):
        """ä¿å­˜æˆ˜ç•¥å±‚æ¨¡å‹"""
        self.sac_agent.save_model(path)
    
    def load_model(self, path: str):
        """åŠ è½½æˆ˜ç•¥å±‚æ¨¡å‹"""
        self.sac_agent.load_model(path)
    
    def get_strategic_guidance(self) -> Dict[str, float]:
        """
        è·å–å½“å‰çš„æˆ˜ç•¥æŒ‡å¯¼ä¿¡æ¯ï¼Œä¾›æˆ˜æœ¯å±‚ä½¿ç”¨
        
        Returns:
            guidance: æˆ˜ç•¥æŒ‡å¯¼å­—å…¸
        """
        if len(self.decision_history) == 0:
            return {
                'offloading_priority': 0.5,
                'caching_priority': 0.5,
                'resource_allocation_strategy': 0.5,
                'system_priority': 0.5
            }
        
        latest_decision = self.decision_history[-1]['action']
        
        return {
            'offloading_priority': latest_decision[0],
            'caching_priority': latest_decision[1],
            'resource_allocation_strategy': latest_decision[2],
            'system_priority': latest_decision[3]
        }
    
    def get_layer_stats(self) -> Dict[str, float]:
        """è·å–æˆ˜ç•¥å±‚ç»Ÿè®¡ä¿¡æ¯"""
        if len(self.performance_history) == 0:
            return {}
        
        return {
            'avg_performance': np.mean(self.performance_history),
            'recent_performance': np.mean(self.performance_history[-10:]) if len(self.performance_history) >= 10 else np.mean(self.performance_history),
            'decision_count': len(self.decision_history),
            'performance_trend': np.mean(self.performance_history[-5:]) - np.mean(self.performance_history[-10:-5]) if len(self.performance_history) >= 10 else 0.0
        }