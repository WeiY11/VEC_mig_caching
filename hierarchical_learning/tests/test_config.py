"""
åˆ†å±‚å¼ºåŒ–å­¦ä¹ æµ‹è¯•é…ç½®
å®šä¹‰å„ç§æµ‹è¯•åœºæ™¯å’Œå‚æ•°
"""

from dataclasses import dataclass
from typing import Dict, List, Optional, Any


@dataclass
class TestConfig:
    """åŸºç¡€æµ‹è¯•é…ç½®"""
    name: str
    description: str
    num_episodes: int
    max_steps_per_episode: int
    evaluation_frequency: int
    save_results: bool
    generate_plots: bool


@dataclass
class LayerTestConfig(TestConfig):
    """å•å±‚æµ‹è¯•é…ç½®"""
    layer_type: str  # 'strategic', 'tactical', 'operational'
    test_metrics: List[str]
    performance_thresholds: Dict[str, float]


@dataclass
class IntegrationTestConfig(TestConfig):
    """é›†æˆæµ‹è¯•é…ç½®"""
    test_coordination: bool
    test_information_flow: bool
    test_decision_consistency: bool
    coordination_threshold: float
    consistency_threshold: float


@dataclass
class BenchmarkTestConfig(TestConfig):
    """åŸºå‡†æµ‹è¯•é…ç½®"""
    baseline_algorithms: List[str]
    comparison_metrics: List[str]
    statistical_significance: bool
    confidence_level: float


# é¢„å®šä¹‰æµ‹è¯•é…ç½®

# å¿«é€Ÿæµ‹è¯•é…ç½®ï¼ˆç”¨äºå¼€å‘è°ƒè¯•ï¼‰
QUICK_TEST_CONFIG = {
    'strategic': LayerTestConfig(
        name="quick_strategic",
        description="æˆ˜ç•¥å±‚å¿«é€Ÿæµ‹è¯•",
        num_episodes=10,
        max_steps_per_episode=50,
        evaluation_frequency=5,
        save_results=False,
        generate_plots=False,
        layer_type="strategic",
        test_metrics=["episode_rewards", "convergence_speed"],
        performance_thresholds={"avg_reward": 10.0, "convergence_speed": 20}
    ),
    'tactical': LayerTestConfig(
        name="quick_tactical",
        description="æˆ˜æœ¯å±‚å¿«é€Ÿæµ‹è¯•",
        num_episodes=10,
        max_steps_per_episode=50,
        evaluation_frequency=5,
        save_results=False,
        generate_plots=False,
        layer_type="tactical",
        test_metrics=["episode_rewards", "coordination_efficiency"],
        performance_thresholds={"avg_reward": 15.0, "coordination_efficiency": 0.6}
    ),
    'operational': LayerTestConfig(
        name="quick_operational",
        description="æ‰§è¡Œå±‚å¿«é€Ÿæµ‹è¯•",
        num_episodes=10,
        max_steps_per_episode=50,
        evaluation_frequency=5,
        save_results=False,
        generate_plots=False,
        layer_type="operational",
        test_metrics=["episode_rewards", "control_precision"],
        performance_thresholds={"avg_reward": 20.0, "control_precision": 0.7}
    )
}

# æ ‡å‡†æµ‹è¯•é…ç½®
STANDARD_TEST_CONFIG = {
    'strategic': LayerTestConfig(
        name="standard_strategic",
        description="æˆ˜ç•¥å±‚æ ‡å‡†æµ‹è¯•",
        num_episodes=50,
        max_steps_per_episode=100,
        evaluation_frequency=10,
        save_results=True,
        generate_plots=True,
        layer_type="strategic",
        test_metrics=["episode_rewards", "episode_losses", "decision_quality", 
                     "convergence_speed", "stability_score", "exploration_efficiency"],
        performance_thresholds={
            "avg_reward": 30.0,
            "convergence_speed": 40,
            "stability_score": 0.7,
            "exploration_efficiency": 0.5
        }
    ),
    'tactical': LayerTestConfig(
        name="standard_tactical",
        description="æˆ˜æœ¯å±‚æ ‡å‡†æµ‹è¯•",
        num_episodes=50,
        max_steps_per_episode=100,
        evaluation_frequency=10,
        save_results=True,
        generate_plots=True,
        layer_type="tactical",
        test_metrics=["episode_rewards", "episode_losses", "coordination_efficiency",
                     "load_balance_score", "communication_overhead", "convergence_speed", "multi_agent_sync"],
        performance_thresholds={
            "avg_reward": 40.0,
            "coordination_efficiency": 0.7,
            "load_balance_score": 0.6,
            "multi_agent_sync": 0.8
        }
    ),
    'operational': LayerTestConfig(
        name="standard_operational",
        description="æ‰§è¡Œå±‚æ ‡å‡†æµ‹è¯•",
        num_episodes=50,
        max_steps_per_episode=100,
        evaluation_frequency=10,
        save_results=True,
        generate_plots=True,
        layer_type="operational",
        test_metrics=["episode_rewards", "episode_losses", "control_precision",
                     "response_time", "safety_violations", "energy_efficiency", "control_stability"],
        performance_thresholds={
            "avg_reward": 50.0,
            "control_precision": 0.8,
            "response_time": 0.01,  # 10ms
            "safety_violations": 0.05,
            "energy_efficiency": 0.7
        }
    )
}

# é›†æˆæµ‹è¯•é…ç½®
INTEGRATION_TEST_CONFIG = IntegrationTestConfig(
    name="hierarchical_integration",
    description="åˆ†å±‚ç³»ç»Ÿé›†æˆæµ‹è¯•",
    num_episodes=30,
    max_steps_per_episode=100,
    evaluation_frequency=10,
    save_results=True,
    generate_plots=True,
    test_coordination=True,
    test_information_flow=True,
    test_decision_consistency=True,
    coordination_threshold=0.7,
    consistency_threshold=0.8
)

# åŸºå‡†æµ‹è¯•é…ç½®
BENCHMARK_TEST_CONFIG = BenchmarkTestConfig(
    name="performance_benchmark",
    description="æ€§èƒ½åŸºå‡†æµ‹è¯•",
    num_episodes=20,
    max_steps_per_episode=100,
    evaluation_frequency=5,
    save_results=True,
    generate_plots=True,
    baseline_algorithms=["random", "greedy", "single_agent"],
    comparison_metrics=["avg_reward", "avg_latency", "success_rate", "energy_efficiency"],
    statistical_significance=True,
    confidence_level=0.95
)

# å‹åŠ›æµ‹è¯•é…ç½®
STRESS_TEST_CONFIG = {
    'high_load': TestConfig(
        name="high_load_stress",
        description="é«˜è´Ÿè½½å‹åŠ›æµ‹è¯•",
        num_episodes=100,
        max_steps_per_episode=200,
        evaluation_frequency=20,
        save_results=True,
        generate_plots=True
    ),
    'long_duration': TestConfig(
        name="long_duration_stress",
        description="é•¿æ—¶é—´è¿è¡Œæµ‹è¯•",
        num_episodes=500,
        max_steps_per_episode=100,
        evaluation_frequency=50,
        save_results=True,
        generate_plots=True
    ),
    'resource_limited': TestConfig(
        name="resource_limited_stress",
        description="èµ„æºå—é™æµ‹è¯•",
        num_episodes=50,
        max_steps_per_episode=100,
        evaluation_frequency=10,
        save_results=True,
        generate_plots=True
    )
}

# ç ”ç©¶æµ‹è¯•é…ç½®ï¼ˆç”¨äºè®ºæ–‡å®éªŒï¼‰
RESEARCH_TEST_CONFIG = {
    'ablation_study': TestConfig(
        name="ablation_study",
        description="æ¶ˆèç ”ç©¶æµ‹è¯•",
        num_episodes=100,
        max_steps_per_episode=150,
        evaluation_frequency=25,
        save_results=True,
        generate_plots=True
    ),
    'parameter_sensitivity': TestConfig(
        name="parameter_sensitivity",
        description="å‚æ•°æ•æ„Ÿæ€§åˆ†æ",
        num_episodes=200,
        max_steps_per_episode=100,
        evaluation_frequency=20,
        save_results=True,
        generate_plots=True
    ),
    'scalability_test': TestConfig(
        name="scalability_test",
        description="å¯æ‰©å±•æ€§æµ‹è¯•",
        num_episodes=150,
        max_steps_per_episode=120,
        evaluation_frequency=30,
        save_results=True,
        generate_plots=True
    )
}


def get_test_config(config_type: str, test_name: str) -> Optional[TestConfig]:
    """
    è·å–æŒ‡å®šçš„æµ‹è¯•é…ç½®
    
    Args:
        config_type: é…ç½®ç±»å‹ - 'quick', 'standard', 'integration', 'benchmark', 'stress', 'research'
        test_name: æµ‹è¯•åç§°
    
    Returns:
        å¯¹åº”çš„æµ‹è¯•é…ç½®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
    """
    config_map = {
        'quick': QUICK_TEST_CONFIG,
        'standard': STANDARD_TEST_CONFIG,
        'integration': {'integration': INTEGRATION_TEST_CONFIG},
        'benchmark': {'benchmark': BENCHMARK_TEST_CONFIG},
        'stress': STRESS_TEST_CONFIG,
        'research': RESEARCH_TEST_CONFIG
    }
    
    if config_type in config_map:
        configs = config_map[config_type]
        return configs.get(test_name)
    
    return None


def list_available_configs() -> Dict[str, List[str]]:
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æµ‹è¯•é…ç½®
    
    Returns:
        é…ç½®ç±»å‹åˆ°æµ‹è¯•åç§°åˆ—è¡¨çš„æ˜ å°„
    """
    return {
        'quick': list(QUICK_TEST_CONFIG.keys()),
        'standard': list(STANDARD_TEST_CONFIG.keys()),
        'integration': ['integration'],
        'benchmark': ['benchmark'],
        'stress': list(STRESS_TEST_CONFIG.keys()),
        'research': list(RESEARCH_TEST_CONFIG.keys())
    }


def validate_test_config(config: TestConfig) -> List[str]:
    """
    éªŒè¯æµ‹è¯•é…ç½®çš„æœ‰æ•ˆæ€§
    
    Args:
        config: è¦éªŒè¯çš„æµ‹è¯•é…ç½®
    
    Returns:
        éªŒè¯é”™è¯¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºåˆ™é…ç½®æœ‰æ•ˆ
    """
    errors = []
    
    # åŸºæœ¬éªŒè¯
    if config.num_episodes <= 0:
        errors.append("num_episodes must be positive")
    
    if config.max_steps_per_episode <= 0:
        errors.append("max_steps_per_episode must be positive")
    
    if config.evaluation_frequency <= 0:
        errors.append("evaluation_frequency must be positive")
    
    if config.evaluation_frequency > config.num_episodes:
        errors.append("evaluation_frequency cannot be greater than num_episodes")
    
    # å±‚æµ‹è¯•ç‰¹å®šéªŒè¯
    if isinstance(config, LayerTestConfig):
        valid_layers = ['strategic', 'tactical', 'operational']
        if config.layer_type not in valid_layers:
            errors.append(f"layer_type must be one of {valid_layers}")
        
        if not config.test_metrics:
            errors.append("test_metrics cannot be empty")
        
        if not config.performance_thresholds:
            errors.append("performance_thresholds cannot be empty")
    
    # é›†æˆæµ‹è¯•ç‰¹å®šéªŒè¯
    if isinstance(config, IntegrationTestConfig):
        if not (0 <= config.coordination_threshold <= 1):
            errors.append("coordination_threshold must be between 0 and 1")
        
        if not (0 <= config.consistency_threshold <= 1):
            errors.append("consistency_threshold must be between 0 and 1")
    
    # åŸºå‡†æµ‹è¯•ç‰¹å®šéªŒè¯
    if isinstance(config, BenchmarkTestConfig):
        if not config.baseline_algorithms:
            errors.append("baseline_algorithms cannot be empty")
        
        if not config.comparison_metrics:
            errors.append("comparison_metrics cannot be empty")
        
        if config.statistical_significance and not (0 < config.confidence_level < 1):
            errors.append("confidence_level must be between 0 and 1")
    
    return errors


def create_custom_test_config(
    name: str,
    description: str,
    num_episodes: int,
    max_steps_per_episode: int = 100,
    evaluation_frequency: int = 10,
    save_results: bool = True,
    generate_plots: bool = True,
    **kwargs
) -> TestConfig:
    """
    åˆ›å»ºè‡ªå®šä¹‰æµ‹è¯•é…ç½®
    
    Args:
        name: æµ‹è¯•åç§°
        description: æµ‹è¯•æè¿°
        num_episodes: æµ‹è¯•å›åˆæ•°
        max_steps_per_episode: æ¯å›åˆæœ€å¤§æ­¥æ•°
        evaluation_frequency: è¯„ä¼°é¢‘ç‡
        save_results: æ˜¯å¦ä¿å­˜ç»“æœ
        generate_plots: æ˜¯å¦ç”Ÿæˆå›¾è¡¨
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
    
    Returns:
        è‡ªå®šä¹‰æµ‹è¯•é…ç½®
    """
    config = TestConfig(
        name=name,
        description=description,
        num_episodes=num_episodes,
        max_steps_per_episode=max_steps_per_episode,
        evaluation_frequency=evaluation_frequency,
        save_results=save_results,
        generate_plots=generate_plots
    )
    
    # éªŒè¯é…ç½®
    errors = validate_test_config(config)
    if errors:
        raise ValueError(f"Invalid test configuration: {', '.join(errors)}")
    
    return config


# æµ‹è¯•åœºæ™¯å®šä¹‰
TEST_SCENARIOS = {
    'development': {
        'description': 'å¼€å‘é˜¶æ®µæµ‹è¯•åœºæ™¯',
        'configs': ['quick'],
        'recommended_order': ['strategic', 'tactical', 'operational']
    },
    'validation': {
        'description': 'éªŒè¯é˜¶æ®µæµ‹è¯•åœºæ™¯',
        'configs': ['standard', 'integration'],
        'recommended_order': ['strategic', 'tactical', 'operational', 'integration']
    },
    'performance': {
        'description': 'æ€§èƒ½è¯„ä¼°åœºæ™¯',
        'configs': ['benchmark', 'stress'],
        'recommended_order': ['benchmark', 'high_load', 'long_duration']
    },
    'research': {
        'description': 'ç ”ç©¶å®éªŒåœºæ™¯',
        'configs': ['research'],
        'recommended_order': ['ablation_study', 'parameter_sensitivity', 'scalability_test']
    }
}


def get_test_scenario(scenario_name: str) -> Optional[Dict]:
    """
    è·å–æµ‹è¯•åœºæ™¯é…ç½®
    
    Args:
        scenario_name: åœºæ™¯åç§°
    
    Returns:
        åœºæ™¯é…ç½®å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
    """
    return TEST_SCENARIOS.get(scenario_name)


def print_test_config_summary(config: TestConfig):
    """
    æ‰“å°æµ‹è¯•é…ç½®æ‘˜è¦
    
    Args:
        config: æµ‹è¯•é…ç½®
    """
    print(f"ğŸ“‹ æµ‹è¯•é…ç½®: {config.name}")
    print(f"   æè¿°: {config.description}")
    print(f"   å›åˆæ•°: {config.num_episodes}")
    print(f"   æ¯å›åˆæ­¥æ•°: {config.max_steps_per_episode}")
    print(f"   è¯„ä¼°é¢‘ç‡: {config.evaluation_frequency}")
    print(f"   ä¿å­˜ç»“æœ: {'æ˜¯' if config.save_results else 'å¦'}")
    print(f"   ç”Ÿæˆå›¾è¡¨: {'æ˜¯' if config.generate_plots else 'å¦'}")
    
    if isinstance(config, LayerTestConfig):
        print(f"   æµ‹è¯•å±‚: {config.layer_type}")
        print(f"   æµ‹è¯•æŒ‡æ ‡: {', '.join(config.test_metrics)}")
        print(f"   æ€§èƒ½é˜ˆå€¼: {config.performance_thresholds}")
    
    elif isinstance(config, IntegrationTestConfig):
        print(f"   æµ‹è¯•åè°ƒæ€§: {'æ˜¯' if config.test_coordination else 'å¦'}")
        print(f"   æµ‹è¯•ä¿¡æ¯æµ: {'æ˜¯' if config.test_information_flow else 'å¦'}")
        print(f"   æµ‹è¯•å†³ç­–ä¸€è‡´æ€§: {'æ˜¯' if config.test_decision_consistency else 'å¦'}")
    
    elif isinstance(config, BenchmarkTestConfig):
        print(f"   åŸºå‡†ç®—æ³•: {', '.join(config.baseline_algorithms)}")
        print(f"   æ¯”è¾ƒæŒ‡æ ‡: {', '.join(config.comparison_metrics)}")
        print(f"   ç»Ÿè®¡æ˜¾è‘—æ€§: {'æ˜¯' if config.statistical_significance else 'å¦'}")