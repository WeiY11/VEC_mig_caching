"""
åˆ†å±‚å¼ºåŒ–å­¦ä¹ åˆ†é˜¶æ®µæµ‹è¯•æ¨¡å—
æ”¯æŒå„å±‚ç‹¬ç«‹éªŒè¯å’Œæ•´ä½“ç³»ç»Ÿæµ‹è¯•

ä¸»è¦åŠŸèƒ½ï¼š
1. æˆ˜ç•¥å±‚ç‹¬ç«‹æµ‹è¯•
2. æˆ˜æœ¯å±‚ç‹¬ç«‹æµ‹è¯•  
3. æ‰§è¡Œå±‚ç‹¬ç«‹æµ‹è¯•
4. åˆ†å±‚é›†æˆæµ‹è¯•
5. æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import json
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥åˆ†å±‚å­¦ä¹ æ¨¡å—
from hierarchical_learning.core.hierarchical_environment import HierarchicalEnvironment
from hierarchical_learning.core.strategic_layer import StrategicLayer
from hierarchical_learning.core.tactical_layer import TacticalLayer
from hierarchical_learning.core.operational_layer import OperationalLayer
from hierarchical_learning.config.hierarchical_config import create_hierarchical_config

# å¯¼å…¥ç°æœ‰æ¨¡å—ç”¨äºå¯¹æ¯”
from evaluation.system_simulator import CompleteSystemSimulator
from algorithms.matd3 import MATD3Environment
from single_agent.sac import SACEnvironment
from single_agent.td3 import TD3Environment
from config import config


class HierarchicalTester:
    """åˆ†å±‚å¼ºåŒ–å­¦ä¹ æµ‹è¯•å™¨"""
    
    def __init__(self, config_type: str = "research"):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            config_type: é…ç½®ç±»å‹ - "default", "lightweight", "performance", "research"
        """
        self.config_type = config_type
        self.hierarchical_config = create_hierarchical_config(config_type)
        
        # åˆ›å»ºåˆ†å±‚ç¯å¢ƒ
        env_config = {
            'num_rsus': self.hierarchical_config.num_rsus,
            'num_uavs': self.hierarchical_config.num_uavs,
            'num_vehicles': self.hierarchical_config.num_vehicles,
            'area_size': (self.hierarchical_config.area_width, self.hierarchical_config.area_height),
            'max_episode_steps': self.hierarchical_config.max_episode_steps,
            'strategic_config': self.hierarchical_config.strategic_config.__dict__,
            'tactical_config': self.hierarchical_config.tactical_config.__dict__,
            'operational_config': self.hierarchical_config.operational_config.__dict__
        }
        
        self.hierarchical_env = HierarchicalEnvironment(env_config)
        
        # åˆ›å»ºå¯¹æ¯”ç¯å¢ƒ
        self.simulator = CompleteSystemSimulator()
        
        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.test_results = {}
        
        print(f"ğŸ§ª åˆ†å±‚æµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ - é…ç½®ç±»å‹: {config_type}")
    
    def test_strategic_layer(self, num_episodes: int = 50) -> Dict:
        """æµ‹è¯•æˆ˜ç•¥å±‚ç‹¬ç«‹æ€§èƒ½"""
        print(f"ğŸ¯ å¼€å§‹æˆ˜ç•¥å±‚ç‹¬ç«‹æµ‹è¯• ({num_episodes} å›åˆ)")
        
        strategic_layer = self.hierarchical_env.strategic_layer
        test_results = {
            'episode_rewards': [],
            'episode_losses': [],
            'decision_quality': [],
            'convergence_speed': 0,
            'stability_score': 0.0,
            'exploration_efficiency': 0.0
        }
        
        # è®°å½•åˆå§‹æ€§èƒ½
        initial_performance = []
        final_performance = []
        
        for episode in range(num_episodes):
            # é‡ç½®ç¯å¢ƒ
            states = self.hierarchical_env.reset()
            strategic_state = states['strategic']
            
            episode_reward = 0.0
            episode_losses = []
            decisions = []
            
            for step in range(100):  # æ¯å›åˆ100æ­¥
                # è·å–æˆ˜ç•¥å†³ç­–
                strategic_action = strategic_layer.get_action(strategic_state)
                decisions.append(strategic_action)
                
                # æ¨¡æ‹Ÿç¯å¢ƒåé¦ˆ
                next_states, rewards, done, info = self.hierarchical_env.step()
                strategic_reward = rewards.get('strategic', 0.0)
                episode_reward += strategic_reward
                
                # å­˜å‚¨ç»éªŒ
                strategic_layer.store_experience(
                    strategic_state, strategic_action, strategic_reward,
                    next_states['strategic'], done
                )
                
                # è®­ç»ƒ
                if hasattr(strategic_layer, 'sac_agent') and hasattr(strategic_layer.sac_agent, 'replay_buffer'):
                    if len(strategic_layer.sac_agent.replay_buffer) >= 32:
                        train_stats = strategic_layer.train()
                        if train_stats and 'actor_loss' in train_stats:
                            episode_losses.append(train_stats['actor_loss'])
                        elif train_stats and 'loss' in train_stats:
                            episode_losses.append(train_stats['loss'])
                
                strategic_state = next_states['strategic']
                
                if done:
                    break
            
            # è®°å½•å›åˆç»“æœ
            test_results['episode_rewards'].append(episode_reward)
            if episode_losses:
                test_results['episode_losses'].append(np.mean(episode_losses))
            
            # è®¡ç®—å†³ç­–è´¨é‡ï¼ˆåŠ¨ä½œçš„æ–¹å·®ï¼Œè¶Šå°è¶Šç¨³å®šï¼‰
            if decisions:
                decision_variance = np.var([np.mean(action) for action in decisions])
                test_results['decision_quality'].append(1.0 / (1.0 + decision_variance))
            
            # è®°å½•æ€§èƒ½ç”¨äºæ”¶æ•›åˆ†æ
            if episode < 10:
                initial_performance.append(episode_reward)
            elif episode >= num_episodes - 10:
                final_performance.append(episode_reward)
            
            if (episode + 1) % 10 == 0:
                avg_reward = np.mean(test_results['episode_rewards'][-10:])
                print(f"  æˆ˜ç•¥å±‚æµ‹è¯•è¿›åº¦: {episode + 1}/{num_episodes}, æœ€è¿‘10å›åˆå¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        
        # è®¡ç®—æ”¶æ•›é€Ÿåº¦ï¼ˆæ€§èƒ½æå‡çš„å›åˆæ•°ï¼‰
        if len(test_results['episode_rewards']) > 20:
            rewards = test_results['episode_rewards']
            for i in range(10, len(rewards)):
                if np.mean(rewards[i-10:i]) > np.mean(rewards[:10]) * 1.1:
                    test_results['convergence_speed'] = i
                    break
        
        # è®¡ç®—ç¨³å®šæ€§åˆ†æ•°
        if len(test_results['episode_rewards']) > 10:
            final_rewards = test_results['episode_rewards'][-10:]
            test_results['stability_score'] = 1.0 / (1.0 + np.std(final_rewards))
        
        # è®¡ç®—æ¢ç´¢æ•ˆç‡
        if initial_performance and final_performance:
            improvement = np.mean(final_performance) - np.mean(initial_performance)
            test_results['exploration_efficiency'] = max(0.0, improvement / abs(np.mean(initial_performance)))
        
        print(f"âœ… æˆ˜ç•¥å±‚æµ‹è¯•å®Œæˆ:")
        print(f"   å¹³å‡å¥–åŠ±: {np.mean(test_results['episode_rewards']):.2f}")
        print(f"   æ”¶æ•›é€Ÿåº¦: {test_results['convergence_speed']} å›åˆ")
        print(f"   ç¨³å®šæ€§åˆ†æ•°: {test_results['stability_score']:.3f}")
        print(f"   æ¢ç´¢æ•ˆç‡: {test_results['exploration_efficiency']:.3f}")
        
        return test_results
    
    def test_tactical_layer(self, num_episodes: int = 50) -> Dict:
        """æµ‹è¯•æˆ˜æœ¯å±‚ç‹¬ç«‹æ€§èƒ½"""
        print(f"ğŸ¯ å¼€å§‹æˆ˜æœ¯å±‚ç‹¬ç«‹æµ‹è¯• ({num_episodes} å›åˆ)")
        
        tactical_layer = self.hierarchical_env.tactical_layer
        test_results = {
            'episode_rewards': [],
            'episode_losses': [],
            'coordination_efficiency': [],
            'load_balance_score': [],
            'communication_overhead': [],
            'convergence_speed': 0,
            'multi_agent_sync': 0.0
        }
        
        for episode in range(num_episodes):
            # é‡ç½®ç¯å¢ƒ
            states = self.hierarchical_env.reset()
            tactical_state = states['tactical']
            
            episode_rewards = {agent_id: 0.0 for agent_id in tactical_layer.agents.keys()}
            episode_losses = []
            coordination_scores = []
            load_balances = []
            
            for step in range(100):  # æ¯å›åˆ100æ­¥
                # è·å–æˆ˜æœ¯å†³ç­–
                tactical_actions = tactical_layer.get_action(tactical_state)
                
                # æ¨¡æ‹Ÿç¯å¢ƒåé¦ˆ
                next_states, rewards, done, info = self.hierarchical_env.step()
                tactical_rewards = rewards.get('tactical', {})
                
                # ç´¯ç§¯å¥–åŠ±
                if isinstance(tactical_rewards, dict):
                    for agent_id, reward in tactical_rewards.items():
                        if agent_id in episode_rewards:
                            episode_rewards[agent_id] += reward
                else:
                    # å¦‚æœtactical_rewardsä¸æ˜¯å­—å…¸ï¼Œä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ†é…ç›¸åŒå¥–åŠ±
                    reward_per_agent = tactical_rewards / len(episode_rewards) if len(episode_rewards) > 0 else 0
                    for agent_id in episode_rewards.keys():
                        episode_rewards[agent_id] += reward_per_agent
                
                # å­˜å‚¨ç»éªŒ
                if isinstance(tactical_rewards, dict):
                    done_dict = {agent_id: done for agent_id in tactical_rewards.keys()}
                else:
                    done_dict = {agent_id: done for agent_id in tactical_layer.agents.keys()}
                    tactical_rewards = {agent_id: reward_per_agent for agent_id in tactical_layer.agents.keys()}
                
                tactical_layer.store_experience(
                    tactical_state, tactical_actions, tactical_rewards,
                    next_states['tactical'], done_dict
                )
                
                # è®­ç»ƒ
                train_stats = tactical_layer.train()
                if train_stats:
                    losses = []
                    for stats in train_stats.values():
                        if isinstance(stats, dict):
                            if 'actor_loss' in stats:
                                losses.append(stats['actor_loss'])
                            elif 'loss' in stats:
                                losses.append(stats['loss'])
                    if losses:
                        episode_losses.append(np.mean(losses))
                
                # è®¡ç®—åè°ƒæ•ˆç‡ï¼ˆåŠ¨ä½œç›¸ä¼¼æ€§ï¼‰
                if isinstance(tactical_actions, dict) and len(tactical_actions) > 1:
                    actions_list = list(tactical_actions.values())
                    if len(actions_list) > 1:
                        action_similarity = 1.0 - np.std([np.mean(action) for action in actions_list])
                        coordination_scores.append(max(0.0, action_similarity))
                
                # è®¡ç®—è´Ÿè½½å‡è¡¡ï¼ˆå¥–åŠ±åˆ†å¸ƒçš„å‡åŒ€æ€§ï¼‰
                if tactical_rewards:
                    reward_values = list(tactical_rewards.values())
                    if len(reward_values) > 1:
                        load_balance = 1.0 / (1.0 + np.std(reward_values))
                        load_balances.append(load_balance)
                
                tactical_state = next_states['tactical']
                
                if done:
                    break
            
            # è®°å½•å›åˆç»“æœ
            total_reward = sum(episode_rewards.values())
            test_results['episode_rewards'].append(total_reward)
            
            if episode_losses:
                test_results['episode_losses'].append(np.mean(episode_losses))
            
            if coordination_scores:
                test_results['coordination_efficiency'].append(np.mean(coordination_scores))
            
            if load_balances:
                test_results['load_balance_score'].append(np.mean(load_balances))
            
            # é€šä¿¡å¼€é”€ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            comm_overhead = len(tactical_actions) * 0.1  # å‡è®¾æ¯ä¸ªæ™ºèƒ½ä½“é€šä¿¡å¼€é”€0.1
            test_results['communication_overhead'].append(comm_overhead)
            
            if (episode + 1) % 10 == 0:
                avg_reward = np.mean(test_results['episode_rewards'][-10:])
                print(f"  æˆ˜æœ¯å±‚æµ‹è¯•è¿›åº¦: {episode + 1}/{num_episodes}, æœ€è¿‘10å›åˆå¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        
        # è®¡ç®—æ”¶æ•›é€Ÿåº¦
        if len(test_results['episode_rewards']) > 20:
            rewards = test_results['episode_rewards']
            for i in range(10, len(rewards)):
                if np.mean(rewards[i-10:i]) > np.mean(rewards[:10]) * 1.1:
                    test_results['convergence_speed'] = i
                    break
        
        # è®¡ç®—å¤šæ™ºèƒ½ä½“åŒæ­¥æ€§
        if test_results['coordination_efficiency']:
            test_results['multi_agent_sync'] = np.mean(test_results['coordination_efficiency'])
        
        print(f"âœ… æˆ˜æœ¯å±‚æµ‹è¯•å®Œæˆ:")
        print(f"   å¹³å‡å¥–åŠ±: {np.mean(test_results['episode_rewards']):.2f}")
        print(f"   åè°ƒæ•ˆç‡: {np.mean(test_results['coordination_efficiency']):.3f}")
        print(f"   è´Ÿè½½å‡è¡¡: {np.mean(test_results['load_balance_score']):.3f}")
        print(f"   å¤šæ™ºèƒ½ä½“åŒæ­¥æ€§: {test_results['multi_agent_sync']:.3f}")
        
        return test_results
    
    def test_operational_layer(self, num_episodes: int = 50) -> Dict:
        """æµ‹è¯•æ‰§è¡Œå±‚ç‹¬ç«‹æ€§èƒ½"""
        print(f"ğŸ¯ å¼€å§‹æ‰§è¡Œå±‚ç‹¬ç«‹æµ‹è¯• ({num_episodes} å›åˆ)")
        
        operational_layer = self.hierarchical_env.operational_layer
        test_results = {
            'episode_rewards': [],
            'episode_losses': [],
            'control_precision': [],
            'response_time': [],
            'safety_violations': [],
            'energy_efficiency': [],
            'convergence_speed': 0,
            'control_stability': 0.0
        }
        
        for episode in range(num_episodes):
            # é‡ç½®ç¯å¢ƒ
            states = self.hierarchical_env.reset()
            operational_state = states['operational']
            
            episode_rewards = {agent_id: 0.0 for agent_id in operational_layer.agents.keys()}
            episode_losses = []
            control_precisions = []
            response_times = []
            safety_checks = []
            energy_costs = []
            
            for step in range(100):  # æ¯å›åˆ100æ­¥
                step_start_time = time.time()
                
                # è·å–æ‰§è¡Œå±‚æ§åˆ¶åŠ¨ä½œ
                operational_actions = operational_layer.get_action(operational_state)
                
                response_time = time.time() - step_start_time
                response_times.append(response_time)
                
                # æ¨¡æ‹Ÿç¯å¢ƒåé¦ˆ
                next_states, rewards, done, info = self.hierarchical_env.step()
                operational_rewards = rewards.get('operational', {})
                
                # ç´¯ç§¯å¥–åŠ±
                if isinstance(operational_rewards, dict):
                    for agent_id, reward in operational_rewards.items():
                        if agent_id in episode_rewards:
                            episode_rewards[agent_id] += reward
                else:
                    # å¦‚æœoperational_rewardsä¸æ˜¯å­—å…¸ï¼Œä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ†é…ç›¸åŒå¥–åŠ±
                    reward_per_agent = operational_rewards / len(episode_rewards) if len(episode_rewards) > 0 else 0
                    for agent_id in episode_rewards.keys():
                        episode_rewards[agent_id] += reward_per_agent
                
                # å­˜å‚¨ç»éªŒ
                if isinstance(operational_rewards, dict):
                    done_dict = {agent_id: done for agent_id in operational_rewards.keys()}
                else:
                    done_dict = {agent_id: done for agent_id in operational_layer.agents.keys()}
                    operational_rewards = {agent_id: reward_per_agent for agent_id in operational_layer.agents.keys()}
                
                operational_layer.store_experience(
                    operational_state, operational_actions, operational_rewards,
                    next_states['operational'], done_dict
                )
                
                # è®­ç»ƒ
                train_stats = operational_layer.train()
                if train_stats:
                    losses = []
                    for stats in train_stats.values():
                        if isinstance(stats, dict):
                            if 'actor_loss' in stats:
                                losses.append(stats['actor_loss'])
                            elif 'loss' in stats:
                                losses.append(stats['loss'])
                    if losses:
                        episode_losses.append(np.mean(losses))
                
                # è®¡ç®—æ§åˆ¶ç²¾åº¦ï¼ˆåŠ¨ä½œä¸ç›®æ ‡çš„åå·®ï¼‰
                if isinstance(operational_actions, dict):
                    action_precision = []
                    for agent_id, action in operational_actions.items():
                        # å‡è®¾ç›®æ ‡åŠ¨ä½œä¸º0.5ï¼ˆä¸­ç­‰å¼ºåº¦ï¼‰
                        target_action = np.full_like(action, 0.5)
                        precision = 1.0 / (1.0 + np.mean(np.abs(action - target_action)))
                        action_precision.append(precision)
                    
                    if action_precision:
                        control_precisions.append(np.mean(action_precision))
                
                # å®‰å…¨æ€§æ£€æŸ¥ï¼ˆåŠ¨ä½œæ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ï¼‰
                safety_violation = 0
                if isinstance(operational_actions, dict):
                    for action in operational_actions.values():
                        if np.any(action < 0) or np.any(action > 1):
                            safety_violation = 1
                            break
                safety_checks.append(safety_violation)
                
                # èƒ½æ•ˆè®¡ç®—ï¼ˆç®€åŒ–ï¼‰
                if isinstance(operational_actions, dict):
                    total_energy = sum([np.sum(action) for action in operational_actions.values()])
                    energy_efficiency = 1.0 / (1.0 + total_energy)
                    energy_costs.append(energy_efficiency)
                
                operational_state = next_states['operational']
                
                if done:
                    break
            
            # è®°å½•å›åˆç»“æœ
            total_reward = sum(episode_rewards.values())
            test_results['episode_rewards'].append(total_reward)
            
            if episode_losses:
                test_results['episode_losses'].append(np.mean(episode_losses))
            
            if control_precisions:
                test_results['control_precision'].append(np.mean(control_precisions))
            
            if response_times:
                test_results['response_time'].append(np.mean(response_times))
            
            if safety_checks:
                test_results['safety_violations'].append(np.mean(safety_checks))
            
            if energy_costs:
                test_results['energy_efficiency'].append(np.mean(energy_costs))
            
            if (episode + 1) % 10 == 0:
                avg_reward = np.mean(test_results['episode_rewards'][-10:])
                print(f"  æ‰§è¡Œå±‚æµ‹è¯•è¿›åº¦: {episode + 1}/{num_episodes}, æœ€è¿‘10å›åˆå¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        
        # è®¡ç®—æ”¶æ•›é€Ÿåº¦
        if len(test_results['episode_rewards']) > 20:
            rewards = test_results['episode_rewards']
            for i in range(10, len(rewards)):
                if np.mean(rewards[i-10:i]) > np.mean(rewards[:10]) * 1.1:
                    test_results['convergence_speed'] = i
                    break
        
        # è®¡ç®—æ§åˆ¶ç¨³å®šæ€§
        if test_results['control_precision']:
            test_results['control_stability'] = 1.0 / (1.0 + np.std(test_results['control_precision']))
        
        print(f"âœ… æ‰§è¡Œå±‚æµ‹è¯•å®Œæˆ:")
        print(f"   å¹³å‡å¥–åŠ±: {np.mean(test_results['episode_rewards']):.2f}")
        print(f"   æ§åˆ¶ç²¾åº¦: {np.mean(test_results['control_precision']):.3f}")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {np.mean(test_results['response_time'])*1000:.2f} ms")
        print(f"   å®‰å…¨è¿è§„ç‡: {np.mean(test_results['safety_violations']):.3f}")
        print(f"   èƒ½æ•ˆ: {np.mean(test_results['energy_efficiency']):.3f}")
        
        return test_results
    
    def test_hierarchical_integration(self, num_episodes: int = 30) -> Dict:
        """æµ‹è¯•åˆ†å±‚é›†æˆæ€§èƒ½"""
        print(f"ğŸ¯ å¼€å§‹åˆ†å±‚é›†æˆæµ‹è¯• ({num_episodes} å›åˆ)")
        
        test_results = {
            'episode_rewards': {'strategic': [], 'tactical': [], 'operational': [], 'total': []},
            'layer_coordination': [],
            'information_flow': [],
            'decision_consistency': [],
            'overall_performance': [],
            'convergence_speed': 0,
            'integration_efficiency': 0.0
        }
        
        for episode in range(num_episodes):
            # é‡ç½®ç¯å¢ƒ
            states = self.hierarchical_env.reset()
            
            episode_rewards = {'strategic': 0.0, 'tactical': 0.0, 'operational': 0.0, 'total': 0.0}
            coordination_scores = []
            decision_records = {'strategic': [], 'tactical': [], 'operational': []}
            
            for step in range(100):  # æ¯å›åˆ100æ­¥
                # æ‰§è¡Œåˆ†å±‚å†³ç­–
                next_states, rewards, done, info = self.hierarchical_env.step()
                
                # ç´¯ç§¯å„å±‚å¥–åŠ±
                for layer, reward in rewards.items():
                    if isinstance(reward, (int, float)):
                        episode_rewards[layer] += reward
                
                episode_rewards['total'] = sum([r for r in episode_rewards.values() if isinstance(r, (int, float))])
                
                # è®°å½•å†³ç­–ä¿¡æ¯
                strategic_guidance = self.hierarchical_env.strategic_layer.get_strategic_guidance()
                tactical_instructions = self.hierarchical_env.tactical_layer.get_tactical_instructions()
                control_commands = self.hierarchical_env.operational_layer.get_control_commands()
                
                decision_records['strategic'].append(strategic_guidance)
                decision_records['tactical'].append(tactical_instructions)
                decision_records['operational'].append(control_commands)
                
                # è®¡ç®—å±‚é—´åè°ƒæ€§
                if step > 0:
                    # ç®€åŒ–çš„åè°ƒæ€§è®¡ç®—ï¼šæ£€æŸ¥å†³ç­–çš„ä¸€è‡´æ€§
                    coordination_score = self._calculate_layer_coordination(
                        strategic_guidance, tactical_instructions, control_commands
                    )
                    coordination_scores.append(coordination_score)
                
                # è®­ç»ƒå„å±‚
                training_results = self.hierarchical_env.train_step()
                
                states = next_states
                
                if done:
                    break
            
            # è®°å½•å›åˆç»“æœ
            for layer in episode_rewards.keys():
                test_results['episode_rewards'][layer].append(episode_rewards[layer])
            
            if coordination_scores:
                test_results['layer_coordination'].append(np.mean(coordination_scores))
            
            # è®¡ç®—ä¿¡æ¯æµæ•ˆç‡
            info_flow_efficiency = self._calculate_information_flow_efficiency(decision_records)
            test_results['information_flow'].append(info_flow_efficiency)
            
            # è®¡ç®—å†³ç­–ä¸€è‡´æ€§
            decision_consistency = self._calculate_decision_consistency(decision_records)
            test_results['decision_consistency'].append(decision_consistency)
            
            # è®¡ç®—æ•´ä½“æ€§èƒ½
            overall_perf = episode_rewards['total'] / max(1, step)
            test_results['overall_performance'].append(overall_perf)
            
            if (episode + 1) % 10 == 0:
                avg_total_reward = np.mean(test_results['episode_rewards']['total'][-10:])
                print(f"  é›†æˆæµ‹è¯•è¿›åº¦: {episode + 1}/{num_episodes}, æœ€è¿‘10å›åˆå¹³å‡æ€»å¥–åŠ±: {avg_total_reward:.2f}")
        
        # è®¡ç®—æ”¶æ•›é€Ÿåº¦
        if len(test_results['episode_rewards']['total']) > 15:
            rewards = test_results['episode_rewards']['total']
            for i in range(5, len(rewards)):
                if np.mean(rewards[i-5:i]) > np.mean(rewards[:5]) * 1.1:
                    test_results['convergence_speed'] = i
                    break
        
        # è®¡ç®—é›†æˆæ•ˆç‡
        if test_results['layer_coordination']:
            test_results['integration_efficiency'] = np.mean(test_results['layer_coordination'])
        
        print(f"âœ… åˆ†å±‚é›†æˆæµ‹è¯•å®Œæˆ:")
        print(f"   å¹³å‡æ€»å¥–åŠ±: {np.mean(test_results['episode_rewards']['total']):.2f}")
        print(f"   å±‚é—´åè°ƒæ€§: {np.mean(test_results['layer_coordination']):.3f}")
        print(f"   ä¿¡æ¯æµæ•ˆç‡: {np.mean(test_results['information_flow']):.3f}")
        print(f"   å†³ç­–ä¸€è‡´æ€§: {np.mean(test_results['decision_consistency']):.3f}")
        print(f"   é›†æˆæ•ˆç‡: {test_results['integration_efficiency']:.3f}")
        
        return test_results
    
    def _calculate_layer_coordination(self, strategic_guidance: Dict, 
                                    tactical_instructions: Dict, 
                                    control_commands: Dict) -> float:
        """è®¡ç®—å±‚é—´åè°ƒæ€§"""
        # ç®€åŒ–çš„åè°ƒæ€§è®¡ç®—
        coordination_score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # æ£€æŸ¥æˆ˜ç•¥æŒ‡å¯¼ä¸æˆ˜æœ¯æŒ‡ä»¤çš„ä¸€è‡´æ€§
        if strategic_guidance and tactical_instructions:
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„ä¸€è‡´æ€§æ£€æŸ¥é€»è¾‘
            coordination_score += 0.2
        
        # æ£€æŸ¥æˆ˜æœ¯æŒ‡ä»¤ä¸æ§åˆ¶å‘½ä»¤çš„ä¸€è‡´æ€§
        if tactical_instructions and control_commands:
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„ä¸€è‡´æ€§æ£€æŸ¥é€»è¾‘
            coordination_score += 0.3
        
        return min(1.0, coordination_score)
    
    def _calculate_information_flow_efficiency(self, decision_records: Dict) -> float:
        """è®¡ç®—ä¿¡æ¯æµæ•ˆç‡"""
        # ç®€åŒ–è®¡ç®—ï¼šåŸºäºå†³ç­–è®°å½•çš„å®Œæ•´æ€§
        efficiency = 0.0
        
        for layer, records in decision_records.items():
            if records:
                # æ£€æŸ¥ä¿¡æ¯çš„è¿ç»­æ€§å’Œå®Œæ•´æ€§
                non_empty_records = [r for r in records if r]
                if non_empty_records:
                    efficiency += len(non_empty_records) / len(records)
        
        return efficiency / len(decision_records) if decision_records else 0.0
    
    def _calculate_decision_consistency(self, decision_records: Dict) -> float:
        """è®¡ç®—å†³ç­–ä¸€è‡´æ€§"""
        # ç®€åŒ–è®¡ç®—ï¼šæ£€æŸ¥å†³ç­–çš„ç¨³å®šæ€§
        consistency = 0.0
        
        for layer, records in decision_records.items():
            if len(records) > 1:
                # è®¡ç®—å†³ç­–å˜åŒ–çš„å¹³æ»‘æ€§
                changes = 0
                for i in range(1, len(records)):
                    if records[i] != records[i-1]:
                        changes += 1
                
                layer_consistency = 1.0 - (changes / (len(records) - 1))
                consistency += layer_consistency
        
        return consistency / len(decision_records) if decision_records else 0.0
    
    def benchmark_performance(self, num_episodes: int = 20) -> Dict:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"ğŸ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯• ({num_episodes} å›åˆ)")
        
        # æµ‹è¯•åˆ†å±‚ç³»ç»Ÿ
        hierarchical_results = self._run_benchmark_episodes(num_episodes, "hierarchical")
        
        # æµ‹è¯•å•ä¸€ç®—æ³•å¯¹æ¯”
        print("ğŸ”„ è¿è¡Œå¯¹æ¯”ç®—æ³•æµ‹è¯•...")
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ ä¸å…¶ä»–ç®—æ³•çš„å¯¹æ¯”æµ‹è¯•
        # ç”±äºæ—¶é—´é™åˆ¶ï¼Œæš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        baseline_results = {
            'avg_reward': np.random.uniform(50, 80),
            'avg_latency': np.random.uniform(30, 50),
            'success_rate': np.random.uniform(0.7, 0.9),
            'energy_efficiency': np.random.uniform(0.6, 0.8)
        }
        
        # è®¡ç®—æ€§èƒ½æå‡
        performance_improvement = {
            'reward_improvement': (hierarchical_results['avg_reward'] - baseline_results['avg_reward']) / baseline_results['avg_reward'],
            'latency_improvement': (baseline_results['avg_latency'] - hierarchical_results['avg_latency']) / baseline_results['avg_latency'],
            'success_rate_improvement': (hierarchical_results['success_rate'] - baseline_results['success_rate']) / baseline_results['success_rate'],
            'energy_improvement': (hierarchical_results['energy_efficiency'] - baseline_results['energy_efficiency']) / baseline_results['energy_efficiency']
        }
        
        benchmark_results = {
            'hierarchical_results': hierarchical_results,
            'baseline_results': baseline_results,
            'performance_improvement': performance_improvement,
            'overall_improvement': np.mean(list(performance_improvement.values()))
        }
        
        print(f"ğŸ“Š åŸºå‡†æµ‹è¯•å®Œæˆ:")
        print(f"   å¥–åŠ±æå‡: {performance_improvement['reward_improvement']*100:.1f}%")
        print(f"   å»¶è¿Ÿæ”¹å–„: {performance_improvement['latency_improvement']*100:.1f}%")
        print(f"   æˆåŠŸç‡æå‡: {performance_improvement['success_rate_improvement']*100:.1f}%")
        print(f"   èƒ½æ•ˆæå‡: {performance_improvement['energy_improvement']*100:.1f}%")
        print(f"   æ•´ä½“æ€§èƒ½æå‡: {benchmark_results['overall_improvement']*100:.1f}%")
        
        return benchmark_results
    
    def _run_benchmark_episodes(self, num_episodes: int, algorithm_type: str) -> Dict:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•å›åˆ"""
        total_rewards = []
        latencies = []
        success_rates = []
        energy_consumptions = []
        
        for episode in range(num_episodes):
            # é‡ç½®ç¯å¢ƒ
            states = self.hierarchical_env.reset()
            
            episode_reward = 0.0
            episode_latencies = []
            episode_successes = 0
            episode_energy = 0.0
            step_count = 0
            
            for step in range(100):
                # æ‰§è¡Œç¯å¢ƒæ­¥éª¤
                next_states, rewards, done, info = self.hierarchical_env.step()
                
                # ç´¯ç§¯å¥–åŠ±
                total_reward = sum([r for r in rewards.values() if isinstance(r, (int, float))])
                episode_reward += total_reward
                
                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                performance_metrics = info.get('performance_metrics', {})
                if 'total_latency' in performance_metrics:
                    episode_latencies.append(performance_metrics['total_latency'])
                if 'success_rate' in performance_metrics:
                    episode_successes += performance_metrics['success_rate']
                if 'total_energy' in performance_metrics:
                    episode_energy += performance_metrics['total_energy']
                
                step_count += 1
                states = next_states
                
                if done:
                    break
            
            # è®°å½•å›åˆç»“æœ
            total_rewards.append(episode_reward)
            if episode_latencies:
                latencies.append(np.mean(episode_latencies))
            success_rates.append(episode_successes / step_count if step_count > 0 else 0)
            energy_consumptions.append(episode_energy)
        
        return {
            'avg_reward': np.mean(total_rewards),
            'avg_latency': np.mean(latencies) if latencies else 0,
            'success_rate': np.mean(success_rates),
            'energy_efficiency': 1.0 / (1.0 + np.mean(energy_consumptions)) if energy_consumptions else 0.5
        }
    
    def run_comprehensive_test(self) -> Dict:
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹åˆ†å±‚å¼ºåŒ–å­¦ä¹ ç»¼åˆæµ‹è¯•")
        
        comprehensive_results = {}
        
        # 1. æˆ˜ç•¥å±‚æµ‹è¯•
        comprehensive_results['strategic_test'] = self.test_strategic_layer(30)
        
        # 2. æˆ˜æœ¯å±‚æµ‹è¯•
        comprehensive_results['tactical_test'] = self.test_tactical_layer(30)
        
        # 3. æ‰§è¡Œå±‚æµ‹è¯•
        comprehensive_results['operational_test'] = self.test_operational_layer(30)
        
        # 4. åˆ†å±‚é›†æˆæµ‹è¯•
        comprehensive_results['integration_test'] = self.test_hierarchical_integration(20)
        
        # 5. æ€§èƒ½åŸºå‡†æµ‹è¯•
        comprehensive_results['benchmark_test'] = self.benchmark_performance(15)
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        self.save_test_results(comprehensive_results)
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_test_report(comprehensive_results)
        
        print("ğŸ‰ ç»¼åˆæµ‹è¯•å®Œæˆ!")
        
        return comprehensive_results
    
    def save_test_results(self, results: Dict):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"hierarchical_test_results_{self.config_type}_{timestamp}.json"
        filepath = os.path.join("test_results", filename)
        
        os.makedirs("test_results", exist_ok=True)
        
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, dict):
                return {key: convert_numpy(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            else:
                return obj
        
        results_serializable = convert_numpy(results)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results_serializable, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    
    def generate_test_report(self, results: Dict):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'åˆ†å±‚å¼ºåŒ–å­¦ä¹ æµ‹è¯•æŠ¥å‘Š - {self.config_type.upper()}', fontsize=16)
        
        # å„å±‚å¥–åŠ±å¯¹æ¯”
        layers = ['strategic', 'tactical', 'operational']
        layer_rewards = []
        for layer in layers:
            test_key = f'{layer}_test'
            if test_key in results and 'episode_rewards' in results[test_key]:
                layer_rewards.append(np.mean(results[test_key]['episode_rewards']))
            else:
                layer_rewards.append(0)
        
        axes[0, 0].bar(layers, layer_rewards, color=['red', 'green', 'blue'])
        axes[0, 0].set_title('å„å±‚å¹³å‡å¥–åŠ±')
        axes[0, 0].set_ylabel('å¹³å‡å¥–åŠ±')
        
        # æ”¶æ•›é€Ÿåº¦å¯¹æ¯”
        convergence_speeds = []
        for layer in layers:
            test_key = f'{layer}_test'
            if test_key in results and 'convergence_speed' in results[test_key]:
                convergence_speeds.append(results[test_key]['convergence_speed'])
            else:
                convergence_speeds.append(0)
        
        axes[0, 1].bar(layers, convergence_speeds, color=['red', 'green', 'blue'])
        axes[0, 1].set_title('æ”¶æ•›é€Ÿåº¦ (å›åˆæ•°)')
        axes[0, 1].set_ylabel('æ”¶æ•›å›åˆæ•°')
        
        # é›†æˆæµ‹è¯•ç»“æœ
        if 'integration_test' in results:
            integration_data = results['integration_test']
            if 'episode_rewards' in integration_data and 'total' in integration_data['episode_rewards']:
                axes[0, 2].plot(integration_data['episode_rewards']['total'])
                axes[0, 2].set_title('é›†æˆæµ‹è¯•æ€»å¥–åŠ±')
                axes[0, 2].set_xlabel('å›åˆ')
                axes[0, 2].set_ylabel('æ€»å¥–åŠ±')
        
        # æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        if 'benchmark_test' in results:
            benchmark_data = results['benchmark_test']
            if 'performance_improvement' in benchmark_data:
                improvements = benchmark_data['performance_improvement']
                metrics = list(improvements.keys())
                values = [improvements[metric] * 100 for metric in metrics]
                
                axes[1, 0].bar(range(len(metrics)), values)
                axes[1, 0].set_title('æ€§èƒ½æå‡ (%)')
                axes[1, 0].set_xticks(range(len(metrics)))
                axes[1, 0].set_xticklabels([m.replace('_improvement', '') for m in metrics], rotation=45)
                axes[1, 0].set_ylabel('æå‡ç™¾åˆ†æ¯”')
        
        # å±‚é—´åè°ƒæ€§
        if 'integration_test' in results and 'layer_coordination' in results['integration_test']:
            coordination_data = results['integration_test']['layer_coordination']
            axes[1, 1].plot(coordination_data)
            axes[1, 1].set_title('å±‚é—´åè°ƒæ€§')
            axes[1, 1].set_xlabel('å›åˆ')
            axes[1, 1].set_ylabel('åè°ƒæ€§åˆ†æ•°')
        
        # ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
        if 'benchmark_test' in results:
            benchmark_data = results['benchmark_test']
            if 'hierarchical_results' in benchmark_data:
                hierarchical_perf = benchmark_data['hierarchical_results']
                
                # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
                categories = ['å¥–åŠ±', 'å»¶è¿Ÿ', 'æˆåŠŸç‡', 'èƒ½æ•ˆ']
                values = [
                    hierarchical_perf.get('avg_reward', 0) / 100,  # å½’ä¸€åŒ–
                    1 - hierarchical_perf.get('avg_latency', 50) / 100,  # å»¶è¿Ÿè¶Šä½è¶Šå¥½
                    hierarchical_perf.get('success_rate', 0),
                    hierarchical_perf.get('energy_efficiency', 0)
                ]
                
                # ç®€åŒ–çš„æ¡å½¢å›¾ä»£æ›¿é›·è¾¾å›¾
                axes[1, 2].bar(categories, values)
                axes[1, 2].set_title('ç»¼åˆæ€§èƒ½')
                axes[1, 2].set_ylabel('å½’ä¸€åŒ–åˆ†æ•°')
                axes[1, 2].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        os.makedirs("test_plots", exist_ok=True)
        plot_filename = f"test_plots/hierarchical_test_report_{self.config_type}_{timestamp}.png"
        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šå›¾è¡¨å·²ä¿å­˜åˆ°: {plot_filename}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='åˆ†å±‚å¼ºåŒ–å­¦ä¹ æµ‹è¯•è„šæœ¬')
    parser.add_argument('--config', type=str, default='research',
                       choices=['default', 'lightweight', 'performance', 'research'],
                       help='é…ç½®ç±»å‹')
    parser.add_argument('--test', type=str, default='comprehensive',
                       choices=['strategic', 'tactical', 'operational', 'integration', 'benchmark', 'comprehensive'],
                       help='æµ‹è¯•ç±»å‹')
    parser.add_argument('--episodes', type=int, default=50,
                       help='æµ‹è¯•å›åˆæ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs('test_results', exist_ok=True)
    os.makedirs('test_plots', exist_ok=True)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = HierarchicalTester(args.config)
    
    # è¿è¡ŒæŒ‡å®šæµ‹è¯•
    if args.test == 'strategic':
        results = tester.test_strategic_layer(args.episodes)
    elif args.test == 'tactical':
        results = tester.test_tactical_layer(args.episodes)
    elif args.test == 'operational':
        results = tester.test_operational_layer(args.episodes)
    elif args.test == 'integration':
        results = tester.test_hierarchical_integration(args.episodes)
    elif args.test == 'benchmark':
        results = tester.benchmark_performance(args.episodes)
    elif args.test == 'comprehensive':
        results = tester.run_comprehensive_test()
    
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()