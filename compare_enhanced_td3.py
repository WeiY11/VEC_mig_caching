#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TD3ç³»åˆ—ç®—æ³•å¯¹æ¯”å®éªŒè„šæœ¬

è‡ªåŠ¨è¿è¡Œå¹¶å¯¹æ¯”ä»¥ä¸‹4ä¸ªç®—æ³•ï¼š
1. TD3 - æ ‡å‡†Twin Delayed DDPG
2. CAM_TD3 - Cache-Aware Migration TD3
3. ENHANCED_TD3 - å¢å¼ºå‹TD3ï¼ˆæ‰€æœ‰5é¡¹ä¼˜åŒ–ï¼‰
4. ENHANCED_CAM_TD3 - å¢å¼ºå‹CAM_TD3ï¼ˆé˜Ÿåˆ—ç„¦ç‚¹ä¼˜åŒ–ï¼‰

ä½¿ç”¨æ–¹æ³•:
    # å¿«é€Ÿæµ‹è¯•ï¼ˆ50è½®ï¼‰
    python compare_enhanced_td3.py --mode quick
    
    # æ ‡å‡†å®éªŒï¼ˆ500è½®ï¼‰
    python compare_enhanced_td3.py --mode standard
    
    # è‡ªå®šä¹‰é…ç½®
    python compare_enhanced_td3.py --episodes 800 --num-vehicles 12
"""

import os
import sys
import subprocess
import json
import time
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# é˜²æ­¢ç»ˆç«¯ç¼–ç å¯¼è‡´çš„è¾“å‡ºå¼‚å¸¸ï¼ˆç‰¹åˆ«æ˜¯åŒ…å« emoji æ—¶ï¼‰
for _stream in (sys.stdout, sys.stderr):
    try:
        _stream.reconfigure(encoding='utf-8', errors='replace')
    except Exception:
        pass

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False


class TD3ComparisonRunner:
    """TD3ç³»åˆ—ç®—æ³•å¯¹æ¯”å®éªŒè¿è¡Œå™¨"""
    
    # å®éªŒé…ç½®
    ALGORITHMS = ['TD3', 'CAM_TD3', 'ENHANCED_TD3', 'ENHANCED_CAM_TD3']
    
    ALGORITHM_LABELS = {
        'TD3': 'TD3 (æ ‡å‡†)',
        'CAM_TD3': 'CAM-TD3 (ç¼“å­˜æ„ŸçŸ¥)',
        'ENHANCED_TD3': 'Enhanced TD3 (å…¨ä¼˜åŒ–)',
        'ENHANCED_CAM_TD3': 'Enhanced CAM-TD3 (é˜Ÿåˆ—ç„¦ç‚¹)',
    }
    
    ALGORITHM_COLORS = {
        'TD3': '#1f77b4',
        'CAM_TD3': '#ff7f0e',
        'ENHANCED_TD3': '#2ca02c',
        'ENHANCED_CAM_TD3': '#d62728',
    }
    
    def __init__(
        self,
        episodes: int = 500,
        num_vehicles: int = 12,
        seed: int = 42,
        output_dir: str = 'results/td3_comparison',
        silent_mode: bool = True,
    ):
        self.episodes = episodes
        self.num_vehicles = num_vehicles
        self.seed = seed
        self.output_dir = Path(output_dir)
        self.silent_mode = silent_mode
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.run_dir = self.output_dir / f'run_{self.timestamp}'
        self.run_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»“æœå­˜å‚¨
        self.results = {}
        self.training_data = {}
        
        print("=" * 80)
        print("ğŸš€ TD3ç³»åˆ—ç®—æ³•å¯¹æ¯”å®éªŒ")
        print("=" * 80)
        print(f"å®éªŒé…ç½®:")
        print(f"  è®­ç»ƒè½®æ¬¡: {episodes}")
        print(f"  è½¦è¾†æ•°é‡: {num_vehicles}")
        print(f"  éšæœºç§å­: {seed}")
        print(f"  è¾“å‡ºç›®å½•: {self.run_dir}")
        print("=" * 80)
    
    def run_single_algorithm(self, algorithm: str) -> Optional[Dict]:
        """è¿è¡Œå•ä¸ªç®—æ³•"""
        print(f"\n{'='*80}")
        print(f"ğŸ”¬ å¼€å§‹è®­ç»ƒ: {self.ALGORITHM_LABELS[algorithm]}")
        print(f"{'='*80}")
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            sys.executable,
            'train_single_agent.py',
            '--algorithm', algorithm,
            '--episodes', str(self.episodes),
            '--num-vehicles', str(self.num_vehicles),
            '--seed', str(self.seed),
        ]
        
        if self.silent_mode:
            cmd.append('--silent-mode')
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        try:
            # è¿è¡Œè®­ç»ƒ
            print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                check=True,
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='replace'
            )
            
            # è®°å½•ç»“æŸæ—¶é—´
            elapsed_time = time.time() - start_time
            
            print(f"âœ… {self.ALGORITHM_LABELS[algorithm]} è®­ç»ƒå®Œæˆ!")
            print(f"â±ï¸  ç”¨æ—¶: {elapsed_time/60:.1f} åˆ†é’Ÿ")
            
            # æŸ¥æ‰¾è®­ç»ƒç»“æœæ–‡ä»¶
            result_file = self._find_latest_result_file(algorithm)
            if result_file:
                print(f"ğŸ“ ç»“æœæ–‡ä»¶: {result_file}")
                with open(result_file, 'r', encoding='utf-8') as f:
                    training_data = json.load(f)
                
                # ä¿å­˜ç»“æœ
                self.training_data[algorithm] = training_data
                self.results[algorithm] = {
                    'success': True,
                    'elapsed_time': elapsed_time,
                    'result_file': str(result_file),
                    'final_metrics': self._extract_final_metrics(training_data)
                }
                
                return self.results[algorithm]
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
                return None
                
        except subprocess.CalledProcessError as e:
            elapsed_time = time.time() - start_time
            print(f"âŒ {self.ALGORITHM_LABELS[algorithm]} è®­ç»ƒå¤±è´¥!")
            print(f"é”™è¯¯ä¿¡æ¯: {e.stderr[:500]}")
            
            self.results[algorithm] = {
                'success': False,
                'elapsed_time': elapsed_time,
                'error': str(e)
            }
            return None
        except Exception as e:
            print(f"âŒ è¿è¡Œè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
            return None
    
    def _find_latest_result_file(self, algorithm: str) -> Optional[Path]:
        """æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒç»“æœæ–‡ä»¶"""
        algo_dir = Path('results') / 'single_agent' / algorithm.lower()
        
        if not algo_dir.exists():
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰ training_results*.json æ–‡ä»¶
        result_files = list(algo_dir.glob('training_results_*.json'))
        
        if not result_files:
            return None
        
        # è¿”å›æœ€æ–°çš„æ–‡ä»¶
        return max(result_files, key=lambda p: p.stat().st_mtime)
    
    def _extract_final_metrics(self, training_data: Dict) -> Dict:
        """??????"""
        metrics: Dict[str, float] = {}
        episode_metrics = training_data.get('episode_metrics', {})
        final_perf = training_data.get('final_performance', {})

        def _get_series(key: str) -> Optional[List]:
            # ???episode_metrics??????????
            if isinstance(episode_metrics.get(key), list):
                return episode_metrics[key]
            if isinstance(training_data.get(key), list):
                return training_data[key]
            return None

        def _mean_last(series: List[float]) -> float:
            last_n = max(1, len(series) // 10)
            return float(np.mean(series[-last_n:]))

        # Episode reward
        rewards = _get_series('episode_rewards') or training_data.get('episode_rewards')
        if isinstance(rewards, list) and len(rewards) > 0:
            metrics['final_reward'] = _mean_last(rewards)
            metrics['reward_std'] = float(np.std(rewards[-max(1, len(rewards) // 10):]))
        elif isinstance(final_perf.get('avg_episode_reward'), (int, float)):
            metrics['final_reward'] = float(final_perf['avg_episode_reward'])

        # ????
        metric_keys = [
            'avg_delay', 'total_energy', 'task_completion_rate',
            'cache_hit_rate', 'data_loss_ratio_bytes',
            'migration_success_rate', 'queue_overload_rate'
        ]

        for key in metric_keys:
            series = _get_series(key)
            if isinstance(series, list) and len(series) > 0:
                metrics[f'final_{key}'] = _mean_last(series)

        # ??????
        if isinstance(final_perf.get('avg_delay'), (int, float)):
            metrics.setdefault('final_avg_delay', float(final_perf['avg_delay']))
        if isinstance(final_perf.get('avg_energy'), (int, float)):
            metrics.setdefault('final_total_energy', float(final_perf['avg_energy']))
        if isinstance(final_perf.get('avg_completion'), (int, float)):
            metrics.setdefault('final_task_completion_rate', float(final_perf['avg_completion']))

        return metrics

    def run_all_algorithms(self):
        """è¿è¡Œæ‰€æœ‰ç®—æ³•"""
        print(f"\nğŸ¯ å°†ä¾æ¬¡è¿è¡Œ {len(self.ALGORITHMS)} ä¸ªç®—æ³•")
        print(f"é¢„è®¡æ€»ç”¨æ—¶: {len(self.ALGORITHMS) * self.episodes * 0.5 / 60:.0f} - {len(self.ALGORITHMS) * self.episodes * 1.0 / 60:.0f} åˆ†é’Ÿ\n")
        
        for i, algorithm in enumerate(self.ALGORITHMS, 1):
            print(f"\nè¿›åº¦: {i}/{len(self.ALGORITHMS)}")
            self.run_single_algorithm(algorithm)
            
            # ç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿æ–‡ä»¶å†™å…¥å®Œæˆ
            time.sleep(2)
        
        print(f"\n{'='*80}")
        print("âœ… æ‰€æœ‰ç®—æ³•è®­ç»ƒå®Œæˆ!")
        print(f"{'='*80}")
    
    def generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print(f"\nğŸ“Š ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
        
        # 1. ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
        self._generate_comparison_table()
        
        # 2. ç”Ÿæˆè®­ç»ƒæ›²çº¿å¯¹æ¯”å›¾
        self._generate_training_curves()
        
        # 3. ç”Ÿæˆæ€§èƒ½é›·è¾¾å›¾
        self._generate_radar_chart()
        
        # 4. ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
        self._generate_text_summary()
        
        print(f"\nâœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.run_dir}")
    
    def _generate_comparison_table(self):
        """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼"""
        # æ”¶é›†æ•°æ®
        table_data = []
        for algo in self.ALGORITHMS:
            if algo in self.results and self.results[algo]['success']:
                metrics = self.results[algo]['final_metrics']
                row = {
                    'ç®—æ³•': self.ALGORITHM_LABELS[algo],
                    'æœ€ç»ˆå¥–åŠ±': f"{metrics.get('final_reward', 0):.2f}",
                    'ä»»åŠ¡å®Œæˆç‡': f"{metrics.get('final_task_completion_rate', 0)*100:.1f}%",
                    'å¹³å‡æ—¶å»¶(s)': f"{metrics.get('final_avg_delay', 0):.3f}",
                    'æ€»èƒ½è€—(J)': f"{metrics.get('final_total_energy', 0):.1f}",
                    'ç¼“å­˜å‘½ä¸­ç‡': f"{metrics.get('final_cache_hit_rate', 0)*100:.1f}%",
                    'æ•°æ®ä¸¢å¤±ç‡': f"{metrics.get('final_data_loss_ratio_bytes', 0)*100:.2f}%",
                    'è®­ç»ƒç”¨æ—¶(åˆ†)': f"{self.results[algo]['elapsed_time']/60:.1f}",
                }
                table_data.append(row)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(table_data)
        
        # ä¿å­˜ä¸ºCSV
        csv_path = self.run_dir / 'comparison_table.csv'
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"  âœ“ å¯¹æ¯”è¡¨æ ¼: {csv_path}")
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        print(f"\n{'='*100}")
        print("ğŸ“Š æ€§èƒ½å¯¹æ¯”è¡¨")
        print(f"{'='*100}")
        print(df.to_string(index=False))
        print(f"{'='*100}\n")
    
    def _generate_training_curves(self):
        """?????????"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('??????', fontsize=16, fontweight='bold')

        # ?????
        metrics_to_plot = [
            ('episode_rewards', '????', axes[0, 0]),
            ('avg_delay', '???? (s)', axes[0, 1]),
            ('cache_hit_rate', '?????', axes[1, 0]),
            ('task_completion_rate', '?????', axes[1, 1]),
        ]

        for metric_key, metric_label, ax in metrics_to_plot:
            for algo in self.ALGORITHMS:
                data = self.training_data.get(algo)
                if not data:
                    continue

                # ???episode_metrics?????????
                if metric_key == 'episode_rewards':
                    values = data.get('episode_rewards')
                else:
                    values = data.get('episode_metrics', {}).get(metric_key) or data.get(metric_key)

                if not isinstance(values, list) or len(values) == 0:
                    continue

                episodes = range(1, len(values) + 1)

                # ???????????
                ax.plot(
                    episodes,
                    values,
                    color=self.ALGORITHM_COLORS[algo],
                    alpha=0.2,
                    linewidth=1,
                )

                # ??????
                if len(values) > 10:
                    window = min(50, len(values) // 10)
                    smoothed = pd.Series(values).rolling(window=window, min_periods=1).mean()
                    ax.plot(
                        episodes,
                        smoothed,
                        color=self.ALGORITHM_COLORS[algo],
                        label=self.ALGORITHM_LABELS[algo],
                        linewidth=2,
                    )

            ax.set_xlabel('Episode')
            ax.set_ylabel(metric_label)
            ax.set_title(metric_label)
            ax.legend(loc='best', fontsize=8)
            ax.grid(True, alpha=0.3)

        plt.tight_layout()

        # ????
        fig_path = self.run_dir / 'training_curves.png'
        plt.savefig(fig_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"  ?????: {fig_path}")

    def _generate_radar_chart(self):
        """ç”Ÿæˆæ€§èƒ½é›·è¾¾å›¾"""
        # å®šä¹‰æŒ‡æ ‡ï¼ˆå½’ä¸€åŒ–åˆ°0-1ï¼‰
        metrics_config = [
            ('final_task_completion_rate', 'ä»»åŠ¡å®Œæˆç‡', 1.0),
            ('final_cache_hit_rate', 'ç¼“å­˜å‘½ä¸­ç‡', 1.0),
            ('final_avg_delay', 'æ—¶å»¶', 0.0),  # è¶Šä½è¶Šå¥½ï¼Œéœ€è¦åè½¬
            ('final_total_energy', 'èƒ½è€—', 0.0),  # è¶Šä½è¶Šå¥½ï¼Œéœ€è¦åè½¬
            ('final_data_loss_ratio_bytes', 'æ•°æ®ä¸¢å¤±', 0.0),  # è¶Šä½è¶Šå¥½ï¼Œéœ€è¦åè½¬
        ]
        
        # æ”¶é›†æ•°æ®
        radar_data = {}
        for algo in self.ALGORITHMS:
            if algo in self.results and self.results[algo]['success']:
                radar_data[algo] = []
        
        # å½’ä¸€åŒ–æ•°æ®
        for metric_key, metric_label, better_higher in metrics_config:
            values = []
            for algo in radar_data.keys():
                val = self.results[algo]['final_metrics'].get(metric_key, 0)
                values.append(val)
            
            # å½’ä¸€åŒ–åˆ°0-1
            if len(values) > 0:
                min_val = min(values)
                max_val = max(values)
                range_val = max_val - min_val if max_val > min_val else 1.0
                
                for algo, val in zip(radar_data.keys(), values):
                    normalized = (val - min_val) / range_val
                    # å¦‚æœè¶Šä½è¶Šå¥½ï¼Œåè½¬
                    if not better_higher:
                        normalized = 1.0 - normalized
                    radar_data[algo].append(normalized)
        
        # ç»˜åˆ¶é›·è¾¾å›¾
        labels = [label for _, label, _ in metrics_config]
        num_vars = len(labels)
        
        angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆ
        
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        for algo, values in radar_data.items():
            values_plot = values + values[:1]  # é—­åˆ
            ax.plot(angles, values_plot, 
                   color=self.ALGORITHM_COLORS[algo],
                   linewidth=2, label=self.ALGORITHM_LABELS[algo])
            ax.fill(angles, values_plot, 
                   color=self.ALGORITHM_COLORS[algo], alpha=0.15)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(labels, fontsize=10)
        ax.set_ylim(0, 1)
        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
        ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=8)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
        ax.set_title('æ€§èƒ½é›·è¾¾å›¾ï¼ˆå€¼è¶Šå¤§è¶Šå¥½ï¼‰', fontsize=14, fontweight='bold', pad=20)
        ax.grid(True)
        
        # ä¿å­˜
        fig_path = self.run_dir / 'performance_radar.png'
        plt.savefig(fig_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ“ æ€§èƒ½é›·è¾¾å›¾: {fig_path}")
    
    def _generate_text_summary(self):
        """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦"""
        summary_path = self.run_dir / 'summary.txt'
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("TD3ç³»åˆ—ç®—æ³•å¯¹æ¯”å®éªŒæ€»ç»“\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"å®éªŒæ—¶é—´: {self.timestamp}\n")
            f.write(f"è®­ç»ƒè½®æ¬¡: {self.episodes}\n")
            f.write(f"è½¦è¾†æ•°é‡: {self.num_vehicles}\n")
            f.write(f"éšæœºç§å­: {self.seed}\n\n")
            
            f.write("=" * 80 + "\n")
            f.write("ç®—æ³•æ€§èƒ½æ’å\n")
            f.write("=" * 80 + "\n\n")
            
            # æŒ‰æœ€ç»ˆå¥–åŠ±æ’åº
            ranking = []
            for algo in self.ALGORITHMS:
                if algo in self.results and self.results[algo]['success']:
                    reward = self.results[algo]['final_metrics'].get('final_reward', -float('inf'))
                    ranking.append((algo, reward))
            
            ranking.sort(key=lambda x: x[1], reverse=True)
            
            for rank, (algo, reward) in enumerate(ranking, 1):
                f.write(f"{rank}. {self.ALGORITHM_LABELS[algo]}\n")
                f.write(f"   æœ€ç»ˆå¥–åŠ±: {reward:.2f}\n")
                
                metrics = self.results[algo]['final_metrics']
                f.write(f"   ä»»åŠ¡å®Œæˆç‡: {metrics.get('final_task_completion_rate', 0)*100:.1f}%\n")
                f.write(f"   å¹³å‡æ—¶å»¶: {metrics.get('final_avg_delay', 0):.3f}s\n")
                f.write(f"   ç¼“å­˜å‘½ä¸­ç‡: {metrics.get('final_cache_hit_rate', 0)*100:.1f}%\n")
                f.write(f"   è®­ç»ƒç”¨æ—¶: {self.results[algo]['elapsed_time']/60:.1f}åˆ†é’Ÿ\n\n")
        
        print(f"  âœ“ æ–‡æœ¬æ‘˜è¦: {summary_path}")
    
    def save_experiment_config(self):
        """ä¿å­˜å®éªŒé…ç½®"""
        config_path = self.run_dir / 'experiment_config.json'
        
        config = {
            'timestamp': self.timestamp,
            'episodes': self.episodes,
            'num_vehicles': self.num_vehicles,
            'seed': self.seed,
            'algorithms': self.ALGORITHMS,
            'results_summary': {
                algo: {
                    'success': self.results[algo]['success'],
                    'elapsed_time': self.results[algo]['elapsed_time'],
                }
                for algo in self.ALGORITHMS
                if algo in self.results
            }
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"  âœ“ å®éªŒé…ç½®: {config_path}")


def main():
    parser = argparse.ArgumentParser(
        description='TD3ç³»åˆ—ç®—æ³•å¯¹æ¯”å®éªŒ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å¿«é€Ÿæµ‹è¯•ï¼ˆ50è½®ï¼Œçº¦5-10åˆ†é’Ÿï¼‰
  python compare_enhanced_td3.py --mode quick
  
  # æ ‡å‡†å®éªŒï¼ˆ500è½®ï¼Œçº¦1-2å°æ—¶ï¼‰
  python compare_enhanced_td3.py --mode standard
  
  # å®Œæ•´å®éªŒï¼ˆ1500è½®ï¼Œçº¦3-5å°æ—¶ï¼‰
  python compare_enhanced_td3.py --mode full
  
  # è‡ªå®šä¹‰é…ç½®
  python compare_enhanced_td3.py --episodes 800 --num-vehicles 16 --seed 123
        """
    )
    
    parser.add_argument('--mode', type=str, choices=['quick', 'standard', 'full'],
                       help='å®éªŒæ¨¡å¼ï¼ˆå¿«é€Ÿ/æ ‡å‡†/å®Œæ•´ï¼‰')
    parser.add_argument('--episodes', type=int, help='è®­ç»ƒè½®æ¬¡')
    parser.add_argument('--num-vehicles', type=int, default=12, help='è½¦è¾†æ•°é‡')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--output-dir', type=str, default='results/td3_comparison',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--no-silent', action='store_true',
                       help='ç¦ç”¨é™é»˜æ¨¡å¼ï¼ˆæ˜¾ç¤ºè®­ç»ƒè¯¦æƒ…ï¼‰')
    
    args = parser.parse_args()
    
    # æ ¹æ®æ¨¡å¼è®¾ç½®episodes
    if args.mode:
        mode_episodes = {'quick': 50, 'standard': 500, 'full': 1500}
        episodes = mode_episodes[args.mode]
    else:
        episodes = args.episodes if args.episodes else 500
    
    # åˆ›å»ºè¿è¡Œå™¨
    runner = TD3ComparisonRunner(
        episodes=episodes,
        num_vehicles=args.num_vehicles,
        seed=args.seed,
        output_dir=args.output_dir,
        silent_mode=not args.no_silent,
    )
    
    # è¿è¡Œå®éªŒ
    runner.run_all_algorithms()
    
    # ç”ŸæˆæŠ¥å‘Š
    runner.generate_comparison_report()
    
    # ä¿å­˜é…ç½®
    runner.save_experiment_config()
    
    print(f"\n{'='*80}")
    print("ğŸ‰ å®éªŒå®Œæˆ!")
    print(f"{'='*80}")
    print(f"ğŸ“ æŸ¥çœ‹ç»“æœ: {runner.run_dir}")
    print(f"ğŸ“Š å¯¹æ¯”è¡¨æ ¼: {runner.run_dir / 'comparison_table.csv'}")
    print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿: {runner.run_dir / 'training_curves.png'}")
    print(f"ğŸ¯ æ€§èƒ½é›·è¾¾å›¾: {runner.run_dir / 'performance_radar.png'}")
    print(f"ğŸ“ æ–‡æœ¬æ‘˜è¦: {runner.run_dir / 'summary.txt'}")
    print(f"{'='*80}\n")


if __name__ == '__main__':
    main()
