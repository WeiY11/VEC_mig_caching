"""
å•æ™ºèƒ½ä½“ç®—æ³•è®­ç»ƒè„šæœ¬
æ”¯æŒDDPGã€TD3ã€DQNã€PPOã€SACç­‰ç®—æ³•çš„è®­ç»ƒå’Œæ¯”è¾ƒ

ä½¿ç”¨æ–¹æ³•:
python train_single_agent.py --algorithm DDPG --episodes 200
python train_single_agent.py --algorithm TD3 --episodes 200  
python train_single_agent.py --algorithm DQN --episodes 200
python train_single_agent.py --algorithm PPO --episodes 200
python train_single_agent.py --algorithm SAC --episodes 200
python train_single_agent.py --compare --episodes 200  # æ¯”è¾ƒæ‰€æœ‰ç®—æ³•
"""
import os
import argparse
import numpy as np
import matplotlib.pyplot as plt
import json
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from config import config
from evaluation.test_complete_system import CompleteSystemSimulator
from utils import MovingAverage

# å¯¼å…¥å„ç§å•æ™ºèƒ½ä½“ç®—æ³•
from single_agent.ddpg import DDPGEnvironment
from single_agent.td3 import TD3Environment
from single_agent.dqn import DQNEnvironment
from single_agent.ppo import PPOEnvironment
from single_agent.sac import SACEnvironment


def generate_timestamp() -> str:
    """ç”Ÿæˆæ—¶é—´æˆ³"""
    if config.experiment.use_timestamp:
        return datetime.now().strftime(config.experiment.timestamp_format)
    else:
        return ""

def get_timestamped_filename(base_name: str, extension: str = ".json") -> str:
    """è·å–å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å"""
    timestamp = generate_timestamp()
    if timestamp:
        name_parts = base_name.split('.')
        if len(name_parts) > 1:
            base = '.'.join(name_parts[:-1])
            return f"{base}_{timestamp}{extension}"
        else:
            return f"{base_name}_{timestamp}{extension}"
    else:
        return f"{base_name}{extension}"


class SingleAgentTrainingEnvironment:
    """å•æ™ºèƒ½ä½“è®­ç»ƒç¯å¢ƒåŸºç±»"""
    
    def __init__(self, algorithm: str):
        self.algorithm = algorithm.upper()
        self.simulator = CompleteSystemSimulator()
        
        # æ ¹æ®ç®—æ³•åˆ›å»ºç›¸åº”ç¯å¢ƒ
        if self.algorithm == "DDPG":
            self.agent_env = DDPGEnvironment()
        elif self.algorithm == "TD3":
            self.agent_env = TD3Environment()
        elif self.algorithm == "DQN":
            self.agent_env = DQNEnvironment()
        elif self.algorithm == "PPO":
            self.agent_env = PPOEnvironment()
        elif self.algorithm == "SAC":
            self.agent_env = SACEnvironment()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_losses = {}
        self.episode_metrics = {
            'avg_delay': [],
            'total_energy': [],
            'task_completion_rate': [],
            'cache_hit_rate': [],
            'migration_success_rate': []
        }
        
        # æ€§èƒ½è¿½è¸ªå™¨
        self.performance_tracker = {
            'recent_rewards': MovingAverage(100),
            'recent_delays': MovingAverage(100),
            'recent_energy': MovingAverage(100),
            'recent_completion': MovingAverage(100)
        }
        
        print(f"âœ“ {self.algorithm}è®­ç»ƒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ ç®—æ³•ç±»å‹: å•æ™ºèƒ½ä½“")
    
    def reset_environment(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒå¹¶è¿”å›åˆå§‹çŠ¶æ€"""
        # é‡ç½®ä»¿çœŸå™¨çŠ¶æ€
        self.simulator._setup_scenario()
        
        # æ”¶é›†ç³»ç»ŸçŠ¶æ€
        node_states = {}
        
        # è½¦è¾†çŠ¶æ€
        for i, vehicle in enumerate(self.simulator.vehicles):
            # ç”Ÿæˆè½¦è¾†çŠ¶æ€
            vehicle_state = np.array([
                vehicle['position'][0] / 1000,  # å½’ä¸€åŒ–ä½ç½®x
                vehicle['position'][1] / 1000,  # å½’ä¸€åŒ–ä½ç½®y
                vehicle['velocity'] / 50,       # å½’ä¸€åŒ–é€Ÿåº¦
                len(vehicle.get('tasks', [])) / 10,  # å½’ä¸€åŒ–ä»»åŠ¡æ•°
                vehicle.get('energy_consumed', 0) / 1000  # å½’ä¸€åŒ–èƒ½è€—
            ])
            node_states[f'vehicle_{i}'] = vehicle_state
        
        # RSUçŠ¶æ€
        for i, rsu in enumerate(self.simulator.rsus):
            rsu_state = np.array([
                rsu['position'][0] / 1000,  # å½’ä¸€åŒ–ä½ç½®x
                rsu['position'][1] / 1000,  # å½’ä¸€åŒ–ä½ç½®y
                len(rsu.get('cache', {})) / rsu.get('cache_capacity', 100),  # ç¼“å­˜åˆ©ç”¨ç‡
                len(rsu.get('computation_queue', [])) / 10,  # å½’ä¸€åŒ–é˜Ÿåˆ—é•¿åº¦
                rsu.get('energy_consumed', 0) / 1000  # å½’ä¸€åŒ–èƒ½è€—
            ])
            node_states[f'rsu_{i}'] = rsu_state
        
        # UAVçŠ¶æ€
        for i, uav in enumerate(self.simulator.uavs):
            uav_state = np.array([
                uav['position'][0] / 1000,  # å½’ä¸€åŒ–ä½ç½®x
                uav['position'][1] / 1000,  # å½’ä¸€åŒ–ä½ç½®y
                uav['position'][2] / 200,   # å½’ä¸€åŒ–é«˜åº¦
                len(uav.get('cache', {})) / uav.get('cache_capacity', 100),  # ç¼“å­˜åˆ©ç”¨ç‡
                uav.get('energy_consumed', 0) / 1000  # å½’ä¸€åŒ–èƒ½è€—
            ])
            node_states[f'uav_{i}'] = uav_state
        
        # åˆå§‹ç³»ç»ŸæŒ‡æ ‡
        system_metrics = {
            'avg_task_delay': 0.0,
            'total_energy_consumption': 0.0,
            'data_loss_rate': 0.0,
            'cache_hit_rate': 0.0,
            'migration_success_rate': 0.0
        }
        
        # ğŸ”§ ä¿®å¤ï¼šé‡ç½®èƒ½è€—è¿½è¸ªå™¨ï¼Œé¿å…è·¨episodeç´¯ç§¯
        if hasattr(self, '_last_total_energy'):
            delattr(self, '_last_total_energy')
        
        # è·å–åˆå§‹çŠ¶æ€å‘é‡
        state = self.agent_env.get_state_vector(node_states, system_metrics)
        
        return state
    
    def step(self, action, state, actions_dict: Optional[Dict] = None) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡Œä¸€æ­¥ä»¿çœŸï¼Œåº”ç”¨æ™ºèƒ½ä½“åŠ¨ä½œåˆ°ä»¿çœŸå™¨"""
        # æ„é€ ä¼ é€’ç»™ä»¿çœŸå™¨çš„åŠ¨ä½œï¼ˆå°†è¿ç»­åŠ¨ä½œæ˜ å°„ä¸ºæœ¬åœ°/RSU/UAVåå¥½ï¼‰
        sim_actions = self._build_simulator_actions(actions_dict)
        
        # æ‰§è¡Œä»¿çœŸæ­¥éª¤ï¼ˆä¼ å…¥åŠ¨ä½œï¼‰
        step_stats = self.simulator.run_simulation_step(0, sim_actions)
        
        # æ”¶é›†ä¸‹ä¸€æ­¥çŠ¶æ€
        node_states = {}
        
        # è½¦è¾†çŠ¶æ€
        for i, vehicle in enumerate(self.simulator.vehicles):
            vehicle_state = np.array([
                vehicle['position'][0] / 1000,
                vehicle['position'][1] / 1000,
                vehicle['velocity'] / 50,
                len(vehicle.get('tasks', [])) / 10,
                vehicle.get('energy_consumed', 0) / 1000
            ])
            node_states[f'vehicle_{i}'] = vehicle_state
        
        # RSUçŠ¶æ€
        for i, rsu in enumerate(self.simulator.rsus):
            rsu_state = np.array([
                rsu['position'][0] / 1000,
                rsu['position'][1] / 1000,
                len(rsu.get('cache', {})) / rsu.get('cache_capacity', 100),
                len(rsu.get('computation_queue', [])) / 10,
                rsu.get('energy_consumed', 0) / 1000
            ])
            node_states[f'rsu_{i}'] = rsu_state
        
        # UAVçŠ¶æ€
        for i, uav in enumerate(self.simulator.uavs):
            uav_state = np.array([
                uav['position'][0] / 1000,
                uav['position'][1] / 1000,
                uav['position'][2] / 200,
                len(uav.get('cache', {})) / uav.get('cache_capacity', 100),
                uav.get('energy_consumed', 0) / 1000
            ])
            node_states[f'uav_{i}'] = uav_state
        
        # è®¡ç®—ç³»ç»ŸæŒ‡æ ‡
        system_metrics = self._calculate_system_metrics(step_stats)
        
        # è·å–ä¸‹ä¸€çŠ¶æ€
        next_state = self.agent_env.get_state_vector(node_states, system_metrics)
        
        # è®¡ç®—å¥–åŠ±
        reward = self.agent_env.calculate_reward(system_metrics)
        
        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = False  # å•æ™ºèƒ½ä½“ç¯å¢ƒé€šå¸¸ä¸ä¼šæå‰ç»“æŸ
        
        # é™„åŠ ä¿¡æ¯
        info = {
            'step_stats': step_stats,
            'system_metrics': system_metrics
        }
        
        return next_state, reward, done, info
    
    def _calculate_system_metrics(self, step_stats: Dict) -> Dict:
        """è®¡ç®—ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ - æœ€ç»ˆä¿®å¤ç‰ˆï¼Œç¡®ä¿æ•°å€¼åœ¨åˆç†èŒƒå›´"""
        import numpy as np
        
        # å®‰å…¨è·å–æ•°å€¼
        def safe_get(key: str, default: float = 0.0) -> float:
            value = step_stats.get(key, default)
            if np.isnan(value) or np.isinf(value):
                return default
            return max(0.0, value)  # ç¡®ä¿éè´Ÿ
        
        # ğŸ”§ ä¿®å¤ï¼šä»»åŠ¡å®Œæˆç‡è®¡ç®— - ä½¿ç”¨ç´¯è®¡ç»Ÿè®¡è€Œéå•æ­¥æ•°æ®
        # generated_tasks æ˜¯æœ¬æ­¥æ–°ç”Ÿæˆçš„ï¼Œprocessed_tasks æ˜¯ç´¯è®¡å®Œæˆçš„
        new_generated = max(1, int(safe_get('generated_tasks', 1)))  # æœ¬æ­¥ç”Ÿæˆ
        total_processed = int(safe_get('processed_tasks', 0))  # ç´¯è®¡å®Œæˆ
        total_dropped = int(safe_get('dropped_tasks', 0))  # ç´¯è®¡ä¸¢å¼ƒ
        
        # ç´¯è®¡æ€»ä»»åŠ¡æ•° = å·²å®Œæˆ + å·²ä¸¢å¼ƒ + åœ¨åˆ¶ä¸­
        active_count = int(safe_get('active_tasks_count', 0))  # åœ¨åˆ¶ä»»åŠ¡æ•°
        total_generated = total_processed + total_dropped + active_count
        
        # å®Œæˆç‡ = å·²å®Œæˆ / æ€»ä»»åŠ¡æ•°
        completion_rate = min(1.0, total_processed / max(1, total_generated)) if total_generated > 0 else 0.0
        
        cache_hits = int(safe_get('cache_hits', 0))
        cache_misses = int(safe_get('cache_misses', 0))
        cache_requests = max(1, cache_hits + cache_misses)
        cache_hit_rate = cache_hits / cache_requests
        
        # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨è®¡ç®—å¹³å‡å»¶è¿Ÿ - ä½¿ç”¨ç´¯è®¡ç»Ÿè®¡
        total_delay = safe_get('total_delay', 0.0)
        processed_for_delay = max(1, total_processed)  # ä½¿ç”¨ç´¯è®¡å®Œæˆæ•°
        avg_delay = total_delay / processed_for_delay
        
        # é™åˆ¶å»¶è¿Ÿåœ¨åˆç†èŒƒå›´å†…ï¼ˆå…³é”®ä¿®å¤ï¼‰
        avg_delay = np.clip(avg_delay, 0.01, 5.0)  # æ‰©å¤§åˆ°0.01-5.0ç§’èŒƒå›´ï¼Œé€‚åº”è·¨æ—¶éš™å¤„ç†
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨èƒ½è€—å¢é‡è€Œéç´¯ç§¯å€¼ï¼Œç¡®ä¿å¥–åŠ±è®¡ç®—æ­£ç¡®
        current_total_energy = safe_get('total_energy', 0.0)
        
        # è®¡ç®—æœ¬æ­¥èƒ½è€—å¢é‡
        if not hasattr(self, '_last_total_energy'):
            self._last_total_energy = 0.0
        
        step_energy = current_total_energy - self._last_total_energy
        step_energy = max(0.0, step_energy)  # ç¡®ä¿éè´Ÿ
        self._last_total_energy = current_total_energy
        
        # è®°å½•ç´¯ç§¯èƒ½è€—ç”¨äºæ˜¾ç¤ºï¼Œä½†ç”¨å¢é‡è®¡ç®—å¥–åŠ±
        total_energy = step_energy  # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¥–åŠ±ä½¿ç”¨å¢é‡èƒ½è€—
        
        # ğŸ”§ ä¿®å¤ï¼šæ•°æ®ä¸¢å¤±ç‡è®¡ç®— - ä½¿ç”¨ç´¯è®¡ç»Ÿè®¡
        data_loss_rate = min(1.0, total_dropped / max(1, total_generated)) if total_generated > 0 else 0.0
        
        # è¿ç§»æˆåŠŸç‡ï¼ˆæ¥è‡ªä»¿çœŸå™¨ç»Ÿè®¡ï¼‰
        migrations_executed = int(safe_get('migrations_executed', 0))
        migrations_successful = int(safe_get('migrations_successful', 0))
        migration_success_rate = (migrations_successful / migrations_executed) if migrations_executed > 0 else 0.0
        
        return {
            'avg_task_delay': avg_delay,
            'total_energy_consumption': total_energy,
            'data_loss_rate': data_loss_rate,
            'task_completion_rate': completion_rate,
            'cache_hit_rate': cache_hit_rate,
            'migration_success_rate': migration_success_rate
        }
    
    def run_episode(self, episode: int, max_steps: Optional[int] = None) -> Dict:
        """è¿è¡Œä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒè½®æ¬¡"""
        # ä½¿ç”¨é…ç½®ä¸­çš„æœ€å¤§æ­¥æ•°
        if max_steps is None:
            max_steps = config.experiment.max_steps_per_episode
        
        # é‡ç½®ç¯å¢ƒ
        state = self.reset_environment()
        
        episode_reward = 0.0
        episode_info = {}
        step = 0
        info = {}  # åˆå§‹åŒ–infoå˜é‡
        
        # PPOéœ€è¦ç‰¹æ®Šå¤„ç†
        if self.algorithm == "PPO":
            return self._run_ppo_episode(episode, max_steps)
        
        for step in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            if self.algorithm == "DQN":
                # DQNè¿”å›ç¦»æ•£åŠ¨ä½œ
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    # å¤„ç†å¯èƒ½çš„å…ƒç»„è¿”å›
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                        
                # éœ€è¦å°†åŠ¨ä½œæ˜ å°„å›å…¨å±€åŠ¨ä½œç´¢å¼•
                action_idx = self._encode_discrete_action(actions_dict)
                action = action_idx
            else:
                # è¿ç»­åŠ¨ä½œç®—æ³•
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    # å¤„ç†å¯èƒ½çš„å…ƒç»„è¿”å›
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                action = self._encode_continuous_action(actions_dict)
            
            # æ‰§è¡ŒåŠ¨ä½œï¼ˆå°†åŠ¨ä½œå­—å…¸ä¼ å…¥ä»¥å½±å“ä»¿çœŸå™¨å¸è½½åå¥½ï¼‰
            next_state, reward, done, info = self.step(action, state, actions_dict)
            
            # åˆå§‹åŒ–training_info
            training_info = {}
            
            # è®­ç»ƒæ™ºèƒ½ä½“ - æ‰€æœ‰ç®—æ³•ç°åœ¨éƒ½æ”¯æŒUnionç±»å‹ç»Ÿä¸€æ¥å£
            # ç¡®ä¿actionç±»å‹å®‰å…¨è½¬æ¢
            if self.algorithm == "DQN":
                # DQNé¦–é€‰æ•´æ•°åŠ¨ä½œï¼Œä½†æ¥å—Unionç±»å‹
                safe_action = self._safe_int_conversion(action)
                training_info = self.agent_env.train_step(state, safe_action, reward, next_state, done)
            elif self.algorithm in ["DDPG", "TD3", "SAC"]:
                # è¿ç»­åŠ¨ä½œç®—æ³•é¦–é€‰numpyæ•°ç»„ï¼Œä½†æ¥å—Unionç±»å‹
                safe_action = action if isinstance(action, np.ndarray) else np.array([action], dtype=np.float32)
                training_info = self.agent_env.train_step(state, safe_action, reward, next_state, done)
            elif self.algorithm == "PPO":
                # PPOä½¿ç”¨ç‰¹æ®Šçš„episodeçº§åˆ«è®­ç»ƒï¼Œtrain_stepä¸ºå ä½ç¬¦
                # ä¿æŒåŸactionç±»å‹å³å¯ï¼Œå› ä¸ºPPOçš„train_stepä¸åšå®é™…å¤„ç†
                training_info = self.agent_env.train_step(state, action, reward, next_state, done)
            else:
                # å…¶ä»–ç®—æ³•çš„é»˜è®¤å¤„ç†
                training_info = {'message': f'Unknown algorithm: {self.algorithm}'}
            
            episode_info = training_info
            
            # æ›´æ–°çŠ¶æ€
            state = next_state
            episode_reward += reward
            
            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            if done:
                break
        
        # è®°å½•è½®æ¬¡ç»Ÿè®¡
        system_metrics = info.get('system_metrics', {})
        
        return {
            'episode_reward': episode_reward,
            'avg_reward': episode_reward,
            'episode_info': episode_info,
            'system_metrics': system_metrics,
            'steps': step + 1
        }
    
    def _run_ppo_episode(self, episode: int, max_steps: int = 100) -> Dict:
        """è¿è¡ŒPPOä¸“ç”¨episode"""
        state = self.reset_environment()
        episode_reward = 0.0
        
        # åˆå§‹åŒ–å˜é‡
        done = False
        step = 0
        info = {}
        
        for step in range(max_steps):
            # è·å–åŠ¨ä½œã€å¯¹æ•°æ¦‚ç‡å’Œä»·å€¼
            if hasattr(self.agent_env, 'get_actions'):
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, tuple) and len(actions_result) == 3:
                    actions_dict, log_prob, value = actions_result
                else:
                    # å¦‚æœä¸æ˜¯å…ƒç»„ï¼Œå°±ä½¿ç”¨é»˜è®¤å€¼
                    actions_dict = actions_result if isinstance(actions_result, dict) else {}
                    log_prob = 0.0
                    value = 0.0
            else:
                actions_dict = {}
                log_prob = 0.0
                value = 0.0
                
            action = self._encode_continuous_action(actions_dict)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.step(action, state, actions_dict)
            
            # å­˜å‚¨ç»éªŒ - æ‰€æœ‰ç®—æ³•éƒ½æ”¯æŒç»Ÿä¸€æ¥å£
            # ç¡®ä¿å‚æ•°ç±»å‹æ­£ç¡®
            log_prob_float = float(log_prob) if not isinstance(log_prob, float) else log_prob
            value_float = float(value) if not isinstance(value, float) else value
            # ä½¿ç”¨å‘½åå‚æ•°é¿å…ä½ç½®å‚æ•°é¡ºåºé—®é¢˜
            self.agent_env.store_experience(
                state=state, 
                action=action, 
                reward=reward, 
                next_state=next_state, 
                done=done, 
                log_prob=log_prob_float, 
                value=value_float
            )
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        # Episodeç»“æŸåè¿›è¡ŒPPOæ›´æ–°
        last_value = 0.0
        if not done:
            if hasattr(self.agent_env, 'get_actions'):
                actions_result = self.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, tuple) and len(actions_result) >= 3:
                    _, _, last_value = actions_result
                else:
                    last_value = 0.0
        
        # ç¡®ä¿ last_value ä¸º float ç±»å‹
        last_value_float = float(last_value) if not isinstance(last_value, float) else last_value
        
        # è¿›è¡Œæ›´æ–° - æ‰€æœ‰ç®—æ³•éƒ½æ”¯æŒç»Ÿä¸€æ¥å£
        training_info = self.agent_env.update(last_value_float)
        
        system_metrics = info.get('system_metrics', {})
        
        return {
            'episode_reward': episode_reward,
            'avg_reward': episode_reward,
            'episode_info': training_info,
            'system_metrics': system_metrics,
            'steps': step + 1
        }

    def _build_simulator_actions(self, actions_dict: Optional[Dict]) -> Optional[Dict]:
        """å°†ç®—æ³•åŠ¨ä½œå­—å…¸è½¬æ¢ä¸ºä»¿çœŸå™¨å¯æ¶ˆè´¹çš„ç®€å•æ§åˆ¶ä¿¡å·ã€‚
        è½»é‡æ˜ å°„ï¼š
        - vehicle_agent å‰3ç»´ â†’ æœ¬åœ°/RSU/UAV æ¦‚ç‡
        - rsu_agent å‰Nç»´(N=RSUæ•°) â†’ æŒ‡å®šRSUæ¦‚ç‡
        - uav_agent å‰Mç»´(M=UAVæ•°) â†’ æŒ‡å®šUAVæ¦‚ç‡
        """
        if not isinstance(actions_dict, dict):
            return None
        vehicle_action = actions_dict.get('vehicle_agent')
        if vehicle_action is None:
            return None
        try:
            import numpy as np
            # å–å‰ä¸‰ç»´ï¼Œæ˜ å°„åˆ°[0,1]å¹¶softmaxä¸ºæ¦‚ç‡
            raw = np.array(vehicle_action[:3], dtype=np.float32).reshape(-1)
            # æ•°å€¼å®‰å…¨
            raw = np.clip(raw, -5.0, 5.0)
            exp = np.exp(raw - np.max(raw))
            probs = exp / np.sum(exp)
            sim_actions = {
                'vehicle_offload_pref': {
                    'local': float(probs[0]),
                    'rsu': float(probs[1] if probs.size > 1 else 0.33),
                    'uav': float(probs[2] if probs.size > 2 else 0.34)
                }
            }
            # RSUé€‰æ‹©æ¦‚ç‡
            num_rsus = len(getattr(self.simulator, 'rsus', []))
            rsu_action = actions_dict.get('rsu_agent')
            if isinstance(rsu_action, (list, tuple, np.ndarray)) and num_rsus > 0:
                rsu_raw = np.array(rsu_action[:num_rsus], dtype=np.float32)
                rsu_raw = np.clip(rsu_raw, -5.0, 5.0)
                rsu_exp = np.exp(rsu_raw - np.max(rsu_raw))
                rsu_probs = rsu_exp / np.sum(rsu_exp)
                sim_actions['rsu_selection_probs'] = [float(x) for x in rsu_probs]
            # UAVé€‰æ‹©æ¦‚ç‡
            num_uavs = len(getattr(self.simulator, 'uavs', []))
            uav_action = actions_dict.get('uav_agent')
            if isinstance(uav_action, (list, tuple, np.ndarray)) and num_uavs > 0:
                uav_raw = np.array(uav_action[:num_uavs], dtype=np.float32)
                uav_raw = np.clip(uav_raw, -5.0, 5.0)
                uav_exp = np.exp(uav_raw - np.max(uav_raw))
                uav_probs = uav_exp / np.sum(uav_exp)
                sim_actions['uav_selection_probs'] = [float(x) for x in uav_probs]
            return sim_actions
        except Exception:
            return None
    
    def _encode_continuous_action(self, actions_dict) -> np.ndarray:
        """å°†åŠ¨ä½œå­—å…¸ç¼–ç ä¸ºè¿ç»­åŠ¨ä½œå‘é‡"""
        # å¤„ç†å¯èƒ½çš„ä¸åŒè¾“å…¥ç±»å‹
        if not isinstance(actions_dict, dict):
            # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œè¿”å›é»˜è®¤åŠ¨ä½œ
            return np.zeros(30)  # 3ä¸ªæ™ºèƒ½ä½“ * 10ç»´åŠ¨ä½œ
        
        action_list = []
        for agent_type in ['vehicle_agent', 'rsu_agent', 'uav_agent']:
            if agent_type in actions_dict:
                action_list.append(actions_dict[agent_type])
            else:
                action_list.append(np.zeros(10))  # é»˜è®¤åŠ¨ä½œ
        
        return np.concatenate(action_list)
    
    def _encode_discrete_action(self, actions_dict) -> int:
        """å°†åŠ¨ä½œå­—å…¸ç¼–ç ä¸ºç¦»æ•£åŠ¨ä½œç´¢å¼•"""
        # å¤„ç†å¯èƒ½çš„ä¸åŒè¾“å…¥ç±»å‹
        if not isinstance(actions_dict, dict):
            return 0  # é»˜è®¤åŠ¨ä½œç´¢å¼•
        
        # ç®€åŒ–å®ç°ï¼šå°†æ¯ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œç»„åˆæˆä¸€ä¸ªç´¢å¼•
        vehicle_action = actions_dict.get('vehicle_agent', 0)
        rsu_action = actions_dict.get('rsu_agent', 0)
        uav_action = actions_dict.get('uav_agent', 0)
        
        # å®‰å…¨åœ°å°†åŠ¨ä½œè½¬æ¢ä¸ºæ•´æ•°
        def safe_int_conversion(value):
            if isinstance(value, (int, np.integer)):
                return int(value)
            elif isinstance(value, np.ndarray):
                if value.size == 1:
                    return int(value.item())
                else:
                    return int(value[0])  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
            elif isinstance(value, (float, np.floating)):
                return int(value)
            else:
                return 0
        
        vehicle_action = safe_int_conversion(vehicle_action)
        rsu_action = safe_int_conversion(rsu_action)
        uav_action = safe_int_conversion(uav_action)
        
        # 5^3 = 125 ç§ç»„åˆ
        return vehicle_action * 25 + rsu_action * 5 + uav_action
    
    def _safe_int_conversion(self, value) -> int:
        """å®‰å…¨åœ°å°†ä¸åŒç±»å‹è½¬æ¢ä¸ºæ•´æ•°"""
        if isinstance(value, (int, np.integer)):
            return int(value)
        elif isinstance(value, np.ndarray):
            if value.size == 1:
                return int(value.item())
            else:
                return int(value[0])  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
        elif isinstance(value, (float, np.floating)):
            return int(round(value))
        else:
            return 0  # å®‰å…¨å›é€€å€¼


def train_single_algorithm(algorithm: str, num_episodes: Optional[int] = None, eval_interval: Optional[int] = None, 
                          save_interval: Optional[int] = None) -> Dict:
    """è®­ç»ƒå•ä¸ªç®—æ³•"""
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    if num_episodes is None:
        num_episodes = config.experiment.num_episodes
    
    # ğŸ”§ è‡ªåŠ¨è°ƒæ•´è¯„ä¼°é—´éš”å’Œä¿å­˜é—´éš”
    def auto_adjust_intervals(total_episodes: int):
        """æ ¹æ®æ€»è½®æ•°è‡ªåŠ¨è°ƒæ•´é—´éš”"""
        # è¯„ä¼°é—´éš”ï¼šæ€»è½®æ•°çš„5-8%ï¼ŒèŒƒå›´[10, 100]
        auto_eval = max(10, min(100, int(total_episodes * 0.06)))
        
        # ä¿å­˜é—´éš”ï¼šæ€»è½®æ•°çš„15-20%ï¼ŒèŒƒå›´[50, 500]  
        auto_save = max(50, min(500, int(total_episodes * 0.18)))
        
        return auto_eval, auto_save
    
    # åº”ç”¨è‡ªåŠ¨è°ƒæ•´ï¼ˆä»…å½“ç”¨æˆ·æœªæŒ‡å®šæ—¶ï¼‰
    if eval_interval is None or save_interval is None:
        auto_eval, auto_save = auto_adjust_intervals(num_episodes)
        if eval_interval is None:
            eval_interval = auto_eval
        if save_interval is None:
            save_interval = auto_save
    
    # æœ€ç»ˆå›é€€åˆ°é…ç½®é»˜è®¤å€¼
    if eval_interval is None:
        eval_interval = config.experiment.eval_interval
    if save_interval is None:
        save_interval = config.experiment.save_interval
    
    print(f"\nğŸš€ å¼€å§‹{algorithm}å•æ™ºèƒ½ä½“ç®—æ³•è®­ç»ƒ")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    training_env = SingleAgentTrainingEnvironment(algorithm)
    
    print(f"è®­ç»ƒé…ç½®:")
    print(f"  ç®—æ³•: {algorithm}")
    print(f"  æ€»è½®æ¬¡: {num_episodes}")
    print(f"  è¯„ä¼°é—´éš”: {eval_interval} (è‡ªåŠ¨è°ƒæ•´)" if eval_interval != config.experiment.eval_interval else f"  è¯„ä¼°é—´éš”: {eval_interval}")
    print(f"  ä¿å­˜é—´éš”: {save_interval} (è‡ªåŠ¨è°ƒæ•´)" if save_interval != config.experiment.save_interval else f"  ä¿å­˜é—´éš”: {save_interval}")
    print("-" * 60)
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(f"results/single_agent/{algorithm.lower()}", exist_ok=True)
    os.makedirs(f"results/models/single_agent/{algorithm.lower()}", exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    best_avg_reward = -100.0  # ä½¿ç”¨æœ‰é™çš„åˆå§‹å€¼è€Œä¸æ˜¯-inf
    training_start_time = time.time()
    
    for episode in range(1, num_episodes + 1):
        episode_start_time = time.time()
        
        # è¿è¡Œè®­ç»ƒè½®æ¬¡
        episode_result = training_env.run_episode(episode)
        
        # è®°å½•è®­ç»ƒæ•°æ®
        training_env.episode_rewards.append(episode_result['avg_reward'])
        
        # æ›´æ–°æ€§èƒ½è¿½è¸ªå™¨
        training_env.performance_tracker['recent_rewards'].update(episode_result['avg_reward'])
        
        system_metrics = episode_result['system_metrics']
        training_env.performance_tracker['recent_delays'].update(system_metrics.get('avg_task_delay', 0))
        training_env.performance_tracker['recent_energy'].update(system_metrics.get('total_energy_consumption', 0))
        training_env.performance_tracker['recent_completion'].update(system_metrics.get('task_completion_rate', 0))
        
        # ğŸ”§ ä¿®å¤ï¼šè®°å½•æŒ‡æ ‡ - è§£å†³é”®åä¸åŒ¹é…é—®é¢˜
        metric_mapping = {
            'avg_task_delay': 'avg_delay',
            'total_energy_consumption': 'total_energy',
            'task_completion_rate': 'task_completion_rate',
            'cache_hit_rate': 'cache_hit_rate', 
            'migration_success_rate': 'migration_success_rate'
        }
        
        for system_key, episode_key in metric_mapping.items():
            if system_key in system_metrics and episode_key in training_env.episode_metrics:
                training_env.episode_metrics[episode_key].append(system_metrics[system_key])
                print(f"âœ… è®°å½•æŒ‡æ ‡ {episode_key}: {system_metrics[system_key]:.3f}")  # è°ƒè¯•ä¿¡æ¯
        
        episode_time = time.time() - episode_start_time
        
        # å®šæœŸè¾“å‡ºè¿›åº¦
        if episode % 10 == 0:
            avg_reward = training_env.performance_tracker['recent_rewards'].get_average()
            avg_delay = training_env.performance_tracker['recent_delays'].get_average()
            avg_completion = training_env.performance_tracker['recent_completion'].get_average()
            
            print(f"è½®æ¬¡ {episode:4d}/{num_episodes}:")
            print(f"  å¹³å‡å¥–åŠ±: {avg_reward:8.3f}")
            print(f"  å¹³å‡æ—¶å»¶: {avg_delay:8.3f}s")
            print(f"  å®Œæˆç‡:   {avg_completion:8.1%}")
            print(f"  è½®æ¬¡ç”¨æ—¶: {episode_time:6.3f}s")
        
        # è¯„ä¼°æ¨¡å‹
        if episode % eval_interval == 0:
            eval_result = evaluate_single_model(algorithm, training_env, episode)
            print(f"\nğŸ“Š è½®æ¬¡ {episode} è¯„ä¼°ç»“æœ:")
            print(f"  è¯„ä¼°å¥–åŠ±: {eval_result['avg_reward']:.3f}")
            print(f"  è¯„ä¼°æ—¶å»¶: {eval_result['avg_delay']:.3f}s")
            print(f"  è¯„ä¼°å®Œæˆç‡: {eval_result['completion_rate']:.1%}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if eval_result['avg_reward'] > best_avg_reward:
                best_avg_reward = eval_result['avg_reward']
                training_env.agent_env.save_models(f"results/models/single_agent/{algorithm.lower()}/best_model")
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (å¥–åŠ±: {best_avg_reward:.3f})")
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if episode % save_interval == 0:
            training_env.agent_env.save_models(f"results/models/single_agent/{algorithm.lower()}/checkpoint_{episode}")
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: checkpoint_{episode}")
    
    # è®­ç»ƒå®Œæˆ
    total_training_time = time.time() - training_start_time
    print("\n" + "=" * 60)
    print(f"ğŸ‰ {algorithm}è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_training_time/3600:.2f} å°æ—¶")
    print(f"ğŸ† æœ€ä½³å¹³å‡å¥–åŠ±: {best_avg_reward:.3f}")
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    results = save_single_training_results(algorithm, training_env, total_training_time)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_single_training_curves(algorithm, training_env)
    
    return results


def evaluate_single_model(algorithm: str, training_env: SingleAgentTrainingEnvironment, 
                         episode: int, num_eval_episodes: int = 5) -> Dict:
    """è¯„ä¼°å•æ™ºèƒ½ä½“æ¨¡å‹æ€§èƒ½ - ä¿®å¤ç‰ˆï¼Œé˜²æ­¢infå’Œnan"""
    import numpy as np
    
    eval_rewards = []
    eval_delays = []
    eval_completions = []
    
    def safe_value(value: float, default: float = 0.0, max_val: float = 1e6) -> float:
        """å®‰å…¨å¤„ç†æ•°å€¼ï¼Œé˜²æ­¢infå’Œnan"""
        if np.isnan(value) or np.isinf(value):
            return default
        return np.clip(value, -max_val, max_val)
    
    for _ in range(num_eval_episodes):
        state = training_env.reset_environment()
        episode_reward = 0.0
        episode_delay = 0.0
        episode_completion = 0.0
        steps = 0
        
        for step in range(50):  # è¾ƒçŸ­çš„è¯„ä¼°è½®æ¬¡
            if algorithm == "DQN":
                actions_result = training_env.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                action = training_env._encode_discrete_action(actions_dict)
            else:
                actions_result = training_env.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, tuple):  # PPOè¿”å›å…ƒç»„
                    actions_dict = actions_result[0]
                elif isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    actions_dict = {}
                action = training_env._encode_continuous_action(actions_dict)
            
            # è¯„ä¼°æ—¶ä¹Ÿä¼ å…¥åŠ¨ä½œå­—å…¸ï¼Œç¡®ä¿åå¥½ç”Ÿæ•ˆ
            next_state, reward, done, info = training_env.step(action, state, actions_dict)
            
            # å®‰å…¨å¤„ç†å¥–åŠ±å’ŒæŒ‡æ ‡
            safe_reward = safe_value(reward, -1.0, 100.0)
            episode_reward += safe_reward
            
            system_metrics = info['system_metrics']
            safe_delay = safe_value(system_metrics.get('avg_task_delay', 0), 0.0, 10.0)
            safe_completion = safe_value(system_metrics.get('task_completion_rate', 0), 0.0, 1.0)
            
            episode_delay += safe_delay
            episode_completion += safe_completion
            steps += 1
            
            state = next_state
            
            if done:
                break
        
        # å®‰å…¨è®¡ç®—å¹³å‡å€¼
        steps = max(1, steps)  # é˜²æ­¢é™¤é›¶
        eval_rewards.append(safe_value(episode_reward / steps, -10.0, 10.0))
        eval_delays.append(safe_value(episode_delay / steps, 0.0, 10.0))
        eval_completions.append(safe_value(episode_completion / steps, 0.0, 1.0))
    
    # å®‰å…¨è®¡ç®—æœ€ç»ˆç»“æœ
    if len(eval_rewards) == 0:
        return {'avg_reward': -1.0, 'avg_delay': 1.0, 'completion_rate': 0.0}
    
    avg_reward = safe_value(float(np.mean(eval_rewards)), -10.0, 10.0)
    avg_delay = safe_value(float(np.mean(eval_delays)), 0.0, 10.0)
    avg_completion = safe_value(float(np.mean(eval_completions)), 0.0, 1.0)
    
    return {
        'avg_reward': avg_reward,
        'avg_delay': avg_delay,
        'completion_rate': avg_completion
    }


def save_single_training_results(algorithm: str, training_env: SingleAgentTrainingEnvironment, 
                                training_time: float) -> Dict:
    """ä¿å­˜è®­ç»ƒç»“æœ"""
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = generate_timestamp()
    
    results = {
        'algorithm': algorithm,
        'agent_type': 'single_agent',
        'timestamp': timestamp,
        'training_start_time': datetime.now().isoformat(),
        'training_config': {
            'num_episodes': len(training_env.episode_rewards),
            'training_time_hours': training_time / 3600,
            'max_steps_per_episode': config.experiment.max_steps_per_episode
        },
        'episode_rewards': training_env.episode_rewards,
        'episode_metrics': training_env.episode_metrics,
        'final_performance': {
            'avg_reward': training_env.performance_tracker['recent_rewards'].get_average(),
            'avg_delay': training_env.performance_tracker['recent_delays'].get_average(),
            'avg_completion': training_env.performance_tracker['recent_completion'].get_average()
        }
    }
    
    # ä½¿ç”¨æ—¶é—´æˆ³æ–‡ä»¶å
    filename = get_timestamped_filename("training_results")
    filepath = f"results/single_agent/{algorithm.lower()}/{filename}"
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ {algorithm}è®­ç»ƒç»“æœå·²ä¿å­˜åˆ° {filepath}")
    
    return results


def plot_single_training_curves(algorithm: str, training_env: SingleAgentTrainingEnvironment):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿ - ç®€æ´ä¼˜ç¾ç‰ˆ"""
    
    # ğŸ¨ ä½¿ç”¨æ–°çš„ç®€æ´å¯è§†åŒ–ç³»ç»Ÿ
    from visualization.clean_charts import create_training_chart, cleanup_old_charts, plot_objective_function_breakdown
    
    # åˆ›å»ºç®—æ³•ç›®å½•
    algorithm_dir = f"results/single_agent/{algorithm.lower()}"
    
    # æ¸…ç†æ—§çš„å†—ä½™å›¾è¡¨
    cleanup_old_charts(algorithm_dir)
    
    # ç”Ÿæˆæ ¸å¿ƒå›¾è¡¨
    chart_path = f"{algorithm_dir}/training_overview.png"
    create_training_chart(training_env, algorithm, chart_path)
    
    # ğŸ¯ ç”Ÿæˆç›®æ ‡å‡½æ•°åˆ†è§£å›¾ï¼ˆæ˜¾ç¤ºæ—¶å»¶ã€èƒ½è€—ã€æ•°æ®ä¸¢å¤±çš„æƒé‡è´¡çŒ®ï¼‰
    objective_path = f"{algorithm_dir}/objective_analysis.png"
    plot_objective_function_breakdown(training_env, algorithm, objective_path)
    
    print(f"ğŸ“ˆ {algorithm} è®­ç»ƒå¯è§†åŒ–å·²å®Œæˆ")
    print(f"   è®­ç»ƒæ€»è§ˆ: {chart_path}")
    print(f"   ç›®æ ‡åˆ†æ: {objective_path}")
    
    # ç”Ÿæˆè®­ç»ƒæ€»ç»“
    from visualization.clean_charts import get_summary_text
    summary = get_summary_text(training_env, algorithm)
    print(f"\n{summary}")


def compare_single_algorithms(algorithms: List[str], num_episodes: Optional[int] = None) -> Dict:
    """æ¯”è¾ƒå¤šä¸ªå•æ™ºèƒ½ä½“ç®—æ³•çš„æ€§èƒ½"""
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    if num_episodes is None:
        num_episodes = config.experiment.num_episodes
    
    print("\nğŸ”¥ å¼€å§‹å•æ™ºèƒ½ä½“ç®—æ³•æ€§èƒ½æ¯”è¾ƒ")
    print("=" * 60)
    
    results = {}
    
    # è®­ç»ƒæ‰€æœ‰ç®—æ³•
    for algorithm in algorithms:
        print(f"\nå¼€å§‹è®­ç»ƒ {algorithm}...")
        results[algorithm] = train_single_algorithm(algorithm, num_episodes)
    
    # ğŸ¨ ç”Ÿæˆç®€æ´çš„å¯¹æ¯”å›¾è¡¨
    from visualization.clean_charts import create_comparison_chart
    timestamp = generate_timestamp()
    comparison_chart_path = f"results/single_agent_comparison_{timestamp}.png" if timestamp else "results/single_agent_comparison.png"
    create_comparison_chart(results, comparison_chart_path)
    
    # ä¿å­˜æ¯”è¾ƒç»“æœ
    timestamp = generate_timestamp()
    comparison_results = {
        'algorithms': algorithms,
        'agent_type': 'single_agent',
        'num_episodes': num_episodes,
        'timestamp': timestamp,
        'comparison_time': datetime.now().isoformat(),
        'results': results,
        'summary': {}
    }
    
    # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
    for algorithm, result in results.items():
        final_perf = result['final_performance']
        comparison_results['summary'][algorithm] = {
            'final_avg_reward': final_perf['avg_reward'],
            'final_avg_delay': final_perf['avg_delay'],
            'final_completion_rate': final_perf['avg_completion'],
            'training_time_hours': result['training_config']['training_time_hours']
        }
    
    # ä½¿ç”¨æ—¶é—´æˆ³æ–‡ä»¶å
    comparison_filename = get_timestamped_filename("single_agent_comparison")
    with open(f"results/{comparison_filename}", "w", encoding="utf-8") as f:
        json.dump(comparison_results, f, indent=2, ensure_ascii=False)
    
    print("\nğŸ¯ å•æ™ºèƒ½ä½“ç®—æ³•æ¯”è¾ƒå®Œæˆï¼")
    print(f"ğŸ“„ æ¯”è¾ƒç»“æœå·²ä¿å­˜åˆ° results/{comparison_filename}")
    print(f"ğŸ“Š å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ° {comparison_chart_path}")
    
    return comparison_results




def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å•æ™ºèƒ½ä½“ç®—æ³•è®­ç»ƒè„šæœ¬')
    parser.add_argument('--algorithm', type=str, choices=['DDPG', 'TD3', 'DQN', 'PPO', 'SAC'],
                       help='é€‰æ‹©è®­ç»ƒç®—æ³•')
    parser.add_argument('--episodes', type=int, default=None, help=f'è®­ç»ƒè½®æ¬¡ (é»˜è®¤: {config.experiment.num_episodes})')
    parser.add_argument('--eval_interval', type=int, default=None, help=f'è¯„ä¼°é—´éš” (é»˜è®¤: {config.experiment.eval_interval})')
    parser.add_argument('--save_interval', type=int, default=None, help=f'ä¿å­˜é—´éš” (é»˜è®¤: {config.experiment.save_interval})')
    parser.add_argument('--compare', action='store_true', help='æ¯”è¾ƒæ‰€æœ‰ç®—æ³•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs("results/single_agent", exist_ok=True)
    
    if args.compare:
        # æ¯”è¾ƒæ‰€æœ‰ç®—æ³•
        algorithms = ['DDPG', 'TD3', 'DQN', 'PPO', 'SAC']
        compare_single_algorithms(algorithms, args.episodes)
    elif args.algorithm:
        # è®­ç»ƒå•ä¸ªç®—æ³•
        train_single_algorithm(args.algorithm, args.episodes, args.eval_interval, args.save_interval)
    else:
        print("è¯·æŒ‡å®š --algorithm æˆ–ä½¿ç”¨ --compare æ ‡å¿—")
        print("ä½¿ç”¨ python train_single_agent.py --help æŸ¥çœ‹å¸®åŠ©")


if __name__ == "__main__":
    main()