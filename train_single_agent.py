"""

ğŸğŸ–¥ï¸ğŸ“š
cd offloading_strategy_comparison
# 1. æµ‹è¯•ï¼ˆ1åˆ†é’Ÿï¼‰
python test_offloading_strategies.py
# 2. å¿«é€Ÿå®éªŒï¼ˆ10åˆ†é’Ÿï¼‰
python run_offloading_comparison.py --mode vehicle --episodes 5
# 3. å®Œæ•´å®éªŒï¼ˆ3-4å°æ—¶ï¼Œè®ºæ–‡ç”¨ï¼‰
python run_offloading_comparison.py --mode all --episodes 50
# 4. ç”Ÿæˆå›¾è¡¨
python visualize_offloading_comparison.py --results all_experiments_*.json --mode all

å•æ™ºèƒ½ä½“ç®—æ³•è®­ç»ƒè„šæœ¬
æ”¯æŒDDPGã€TD3ã€TD3-LEã€DQNã€PPOã€SACç­‰ç®—æ³•çš„è®­ç»ƒå’Œæ¯”è¾ƒ
ä½¿ç”¨æ–¹æ³•:
python train_single_agent.py --algorithm TD3 --episodes 200
python train_single_agent.py --algorithm TD3 --episodes 200 --seed 123 --num-vehicles 16
python train_single_agent.py --algorithm DDPG --episodes 200
python train_single_agent.py --algorithm PPO --episodes 150 --seed 3407
python train_single_agent.py --algorithm TD3-LE --episodes 200  # å»¶æ—¶-èƒ½è€—ååŒä¼˜åŒ–
python train_single_agent.py --compare --episodes 200  # æ¯”è¾ƒæ‰€æœ‰ç®—æ³•
ğŸš€ å¢å¼ºç¼“å­˜æ¨¡å¼ (é»˜è®¤å¯ç”¨ - åˆ†å±‚L1/L2 + è‡ªé€‚åº”çƒ­åº¦ç­–ç•¥ + RSUåä½œ):
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 8
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 12
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 16
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 20
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 24
python train_single_agent.py --algorithm TD3-LE --episodes 1600 --num-vehicles 12
ğŸ”§ ç¦ç”¨å¢å¼ºç¼“å­˜ (å¦‚éœ€baselineå¯¹æ¯”):
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 20 --no-enhanced-cache

ğŸŒ å®æ—¶å¯è§†åŒ–:
python train_single_agent.py --algorithm TD3 --episodes 200 --realtime-vis
python train_single_agent.py --algorithm DDPG --episodes 100 --realtime-vis --vis-port 8080

ğŸ“Š æ‰¹é‡å®éªŒè„šæœ¬:
python experiments/run_td3_seed_sweep.py --seeds 42 2025 3407 --episodes 200
python experiments/run_td3_vehicle_sweep.py --vehicles 8 12 16 --episodes 200
python experiments/run_td3_vehicle_sweep.py --vehicles 8 12 16 20 24 --episodes 800
ğŸ ç”Ÿæˆå­¦æœ¯å›¾è¡¨:
python generate_academic_charts.py results/single_agent/td3/training_results_20251007_220900.json

""" 
import os
import sys
import random

# ğŸ”§ ä¿®å¤Windowsç¼–ç é—®é¢˜
if sys.platform == 'win32':
    try:
        if hasattr(sys.stdout, 'reconfigure'):
            sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        elif hasattr(sys.stdout, 'buffer'):
            import io
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=True)
    except Exception:
        pass
    try:
        if hasattr(sys.stderr, 'reconfigure'):
            sys.stderr.reconfigure(encoding='utf-8', errors='replace')
        elif hasattr(sys.stderr, 'buffer'):
            import io
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace', line_buffering=True)
    except Exception:
        pass

import argparse
import json
from fixed_topology_optimizer import FixedTopologyOptimizer
import numpy as np
import matplotlib.pyplot as plt
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from config import config
from evaluation.system_simulator import CompleteSystemSimulator
try:
    from evaluation.enhanced_system_simulator import EnhancedSystemSimulator
    ENHANCED_CACHE_AVAILABLE = True
except ImportError:
    ENHANCED_CACHE_AVAILABLE = False
    print("[Warning] Enhanced cache system not available, using standard simulator")
from utils import MovingAverage
# ğŸ¤– å¯¼å…¥è‡ªé€‚åº”æ§åˆ¶ç»„ä»¶
from utils.adaptive_control import AdaptiveCacheController, AdaptiveMigrationController, map_agent_actions_to_params

# å¯¼å…¥å„ç§å•æ™ºèƒ½ä½“ç®—æ³•
from single_agent.ddpg import DDPGEnvironment
from single_agent.td3 import TD3Environment
from single_agent.td3_latency_energy import TD3LatencyEnergyEnvironment
from single_agent.dqn import DQNEnvironment
from single_agent.ppo import PPOEnvironment
from single_agent.sac import SACEnvironment

# å¯¼å…¥HTMLæŠ¥å‘Šç”Ÿæˆå™¨
from utils.html_report_generator import HTMLReportGenerator

# ğŸŒ å¯¼å…¥å®æ—¶å¯è§†åŒ–æ¨¡å—
try:
    from realtime_visualization import create_visualizer
    REALTIME_AVAILABLE = True
except ImportError:
    REALTIME_AVAILABLE = False
    print("âš ï¸  å®æ—¶å¯è§†åŒ–åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·è¿è¡Œ: pip install flask flask-socketio")

# å°è¯•å¯¼å…¥PyTorchä»¥è®¾ç½®éšæœºç§å­ï¼›å¦‚æœä¸å¯ç”¨åˆ™è·³è¿‡
try:
    import torch
except ImportError:  # pragma: no cover - å®¹é”™å¤„ç†
    torch = None


def _apply_global_seed_from_env():
    """æ ¹æ®ç¯å¢ƒå˜é‡RANDOM_SEEDè®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§"""
    seed_env = os.environ.get('RANDOM_SEED')
    if not seed_env:
        return
    try:
        seed = int(seed_env)
    except ValueError:
        print(f"âš ï¸  RANDOM_SEED ç¯å¢ƒå˜é‡æ— æ•ˆ: {seed_env}")
        return

    random.seed(seed)
    np.random.seed(seed)
    if torch is not None:
        torch.manual_seed(seed)
        if torch.cuda.is_available():  # pragma: no cover - GPUå¯é€‰
            torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True  # type: ignore[attr-defined]
        torch.backends.cudnn.benchmark = False  # type: ignore[attr-defined]
    config.random_seed = seed
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"ğŸ” å…¨å±€éšæœºç§å­å·²è®¾ç½®ä¸º {seed}")


def _build_scenario_config() -> Dict[str, Any]:
    """æ„å»ºæ¨¡æ‹Ÿç¯å¢ƒé…ç½®ï¼Œå…è®¸é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–é»˜è®¤å€¼"""
    scenario = {
        "num_vehicles": 12,
        "num_rsus": 4,
        "num_uavs": 2,
        "task_arrival_rate": 1.8,
        "time_slot": 0.2,
        "simulation_time": 1000,
        "computation_capacity": 800,
        "bandwidth": 15,
        "cache_capacity": 80,
        "transmission_power": 0.15,
        "computation_power": 1.2,
        "high_load_mode": True,
        "task_complexity_multiplier": 1.5,
        "rsu_load_divisor": 4.0,
        "uav_load_divisor": 2.0,
        "enhanced_task_generation": True,
    }

    override_env = os.environ.get('TRAINING_SCENARIO_OVERRIDES')
    if override_env:
        try:
            overrides = json.loads(override_env)
            if isinstance(overrides, dict):
                scenario.update(overrides)
            else:
                print("âš ï¸  TRAINING_SCENARIO_OVERRIDES éœ€ä¸ºJSONå¯¹è±¡ï¼Œå·²å¿½ç•¥ã€‚")
        except json.JSONDecodeError as exc:
            print(f"âš ï¸  TRAINING_SCENARIO_OVERRIDES è§£æå¤±è´¥: {exc}")

    return scenario


_apply_global_seed_from_env()


def generate_timestamp() -> str:
    """ç”Ÿæˆæ—¶é—´æˆ³"""
    if config.experiment.use_timestamp:
        return datetime.now().strftime(config.experiment.timestamp_format)
    else:
        return ""

def get_timestamped_filename(base_name: str, extension: str = ".json") -> str:
    """è·å–å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å"""
    timestamp = generate_timestamp()
    if timestamp:
        name_parts = base_name.split('.')
        if len(name_parts) > 1:
            base = '.'.join(name_parts[:-1])
            return f"{base}_{timestamp}{extension}"
        else:
            return f"{base_name}_{timestamp}{extension}"
    else:
        return f"{base_name}{extension}"


class SingleAgentTrainingEnvironment:
    """å•æ™ºèƒ½ä½“è®­ç»ƒç¯å¢ƒåŸºç±»"""
    
    def __init__(self, algorithm: str, override_scenario: Optional[Dict[str, Any]] = None, 
                 use_enhanced_cache: bool = False):
        self.input_algorithm = algorithm
        normalized_algorithm = algorithm.upper().replace('-', '_')
        alias_map = {
            "TD3LE": "TD3_LATENCY_ENERGY",
            "TD3_LE": "TD3_LATENCY_ENERGY",
            "TD3LATENCY": "TD3_LATENCY_ENERGY",
            "TD3_LATENCY": "TD3_LATENCY_ENERGY",
            "TD3_LATENCY_ENERGY": "TD3_LATENCY_ENERGY",
        }
        alias_key = normalized_algorithm.replace('_', '')
        self.algorithm = alias_map.get(normalized_algorithm, alias_map.get(alias_key, normalized_algorithm))
        scenario_config = _build_scenario_config()
        # åº”ç”¨å¤–éƒ¨è¦†ç›–
        if override_scenario:
            scenario_config.update(override_scenario)
            scenario_config['override_topology'] = True
        
        # é€‰æ‹©ä»¿çœŸå™¨ç±»å‹
        self.use_enhanced_cache = use_enhanced_cache and ENHANCED_CACHE_AVAILABLE
        if self.use_enhanced_cache:
            print("ğŸš€ [Training] Using Enhanced Cache System (Default) with:")
            print("   - Hierarchical L1/L2 caching (3GB + 7GB)")
            print("   - Adaptive HeatBasedCacheStrategy")
            print("   - Inter-RSU collaboration")
            self.simulator = EnhancedSystemSimulator(scenario_config)
        else:
            self.simulator = CompleteSystemSimulator(scenario_config)
        
        # ğŸ¤– åˆå§‹åŒ–è‡ªé€‚åº”æ§åˆ¶ç»„ä»¶
        self.adaptive_cache_controller = AdaptiveCacheController()
        self.adaptive_migration_controller = AdaptiveMigrationController()
        print(f"ğŸ¤– å·²å¯ç”¨è‡ªé€‚åº”ç¼“å­˜å’Œè¿ç§»æ§åˆ¶åŠŸèƒ½")
        
        # ä»ä»¿çœŸå™¨è·å–å®é™…ç½‘ç»œæ‹“æ‰‘å‚æ•°
        num_vehicles = len(self.simulator.vehicles)
        num_rsus = len(self.simulator.rsus)
        num_uavs = len(self.simulator.uavs)
        
        # åº”ç”¨å›ºå®šæ‹“æ‰‘çš„å‚æ•°ä¼˜åŒ–ï¼ˆä¿æŒ4 RSU + 2 UAVï¼‰
        if self.algorithm in {"TD3", "TD3_LATENCY_ENERGY"}:
            topology_optimizer = FixedTopologyOptimizer()
            opt_params = topology_optimizer.get_optimized_params(num_vehicles)
            
            # åº”ç”¨ä¼˜åŒ–çš„è¶…å‚æ•°åˆ°TD3é…ç½®
            os.environ['TD3_HIDDEN_DIM'] = str(opt_params.get('hidden_dim', 400))
            os.environ['TD3_ACTOR_LR'] = str(opt_params.get('actor_lr', 1e-4))
            os.environ['TD3_CRITIC_LR'] = str(opt_params.get('critic_lr', 8e-5))
            os.environ['TD3_BATCH_SIZE'] = str(opt_params.get('batch_size', 256))
            
            print(f"[FIXED-TOPOLOGY] è½¦è¾†æ•°:{num_vehicles} â†’ Hidden:{opt_params['hidden_dim']}, LR:{opt_params['actor_lr']:.1e}, Batch:{opt_params['batch_size']}")
            print(f"[FIXED-TOPOLOGY] ä¿æŒå›ºå®š: RSU=4, UAV=2ï¼ˆéªŒè¯ç®—æ³•ç­–ç•¥æœ‰æ•ˆæ€§ï¼‰")
        
        # ğŸ”§ ä¼˜åŒ–ï¼šæ‰€æœ‰ç®—æ³•ç»Ÿä¸€ä¼ å…¥æ‹“æ‰‘å‚æ•°ï¼Œå®ç°åŠ¨æ€é€‚é…
        if self.algorithm == "DDPG":
            self.agent_env = DDPGEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "TD3":
            self.agent_env = TD3Environment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "TD3_LATENCY_ENERGY":
            self.agent_env = TD3LatencyEnergyEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "DQN":
            self.agent_env = DQNEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "PPO":
            self.agent_env = PPOEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "SAC":
            self.agent_env = SACEnvironment(num_vehicles, num_rsus, num_uavs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_losses = {}
        self.episode_metrics = {
            'avg_delay': [],
            'total_energy': [],
            'data_loss_bytes': [],
            'data_loss_ratio_bytes': [],
            'task_completion_rate': [],
            'cache_hit_rate': [],
            'cache_utilization': [],
            'cache_evictions': [],
            'cache_eviction_rate': [],
            'cache_requests': [],
            'cache_collaborative_writes': [],
            'local_cache_hits': [],
            'migration_avg_cost': [],
            'migration_avg_delay_saved': [],
            'migration_success_rate': [],
            'episode_steps': [],  # ğŸ”§ æ–°å¢ï¼šè®°å½•æ¯ä¸ªepisodeçš„å®é™…æ­¥æ•°
            'task_type_queue_share_1': [],
            'task_type_queue_share_2': [],
            'task_type_queue_share_3': [],
            'task_type_queue_share_4': [],
            'task_type_deadline_norm_1': [],
            'task_type_deadline_norm_2': [],
            'task_type_deadline_norm_3': [],
            'task_type_deadline_norm_4': [],
            'task_type_drop_rate_1': [],
            'task_type_drop_rate_2': [],
            'task_type_drop_rate_3': [],
            'task_type_drop_rate_4': []
        }
        
        # æ€§èƒ½è¿½è¸ªå™¨
        self.performance_tracker = {
            'recent_rewards': MovingAverage(100),
            'recent_delays': MovingAverage(100),
            'recent_energy': MovingAverage(100),
            'recent_completion': MovingAverage(100)
        }
        
        print(f"âœ“ {self.algorithm}è®­ç»ƒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ ç®—æ³•ç±»å‹: å•æ™ºèƒ½ä½“")
    
    def _calculate_correct_cache_utilization(self, cache: Dict, cache_capacity_mb: float) -> float:
        """
        ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—ç¼“å­˜åˆ©ç”¨ç‡
        
        Args:
            cache: ç¼“å­˜å­—å…¸
            cache_capacity_mb: ç¼“å­˜å®¹é‡(MB)
        Returns:
            ç¼“å­˜åˆ©ç”¨ç‡ [0.0, 1.0]
        """
        if not cache or cache_capacity_mb <= 0:
            return 0.0
        
        total_used_mb = 0.0
        for item in cache.values():
            if isinstance(item, dict) and 'size' in item:
                total_used_mb += float(item.get('size', 0.0))
            else:
                # å…¼å®¹æ—§æ ¼å¼ï¼Œä½¿ç”¨realisticå¤§å°
                total_used_mb += 1.0  # é»˜è®¤1MB
        
        utilization = total_used_mb / cache_capacity_mb
        return min(1.0, max(0.0, utilization))
    
    def reset_environment(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒå¹¶è¿”å›åˆå§‹çŠ¶æ€"""
        # é‡ç½®ä»¿çœŸå™¨çŠ¶æ€
        self.simulator._setup_scenario()
        
        # æ”¶é›†ç³»ç»ŸçŠ¶æ€
        node_states = {}
        
        # è½¦è¾†çŠ¶æ€ï¼ˆä¸stepä¿æŒä¸€è‡´çš„å½’ä¸€åŒ–æ–¹å¼ï¼‰
        for i, vehicle in enumerate(self.simulator.vehicles):
            vehicle_state = np.array([
                np.clip(vehicle['position'][0] / 1000, 0.0, 1.0),
                np.clip(vehicle['position'][1] / 1000, 0.0, 1.0),
                np.clip(vehicle['velocity'] / 50, 0.0, 1.0),
                np.clip(len(vehicle.get('tasks', [])) / 20.0, 0.0, 1.0),
                np.clip(vehicle.get('energy_consumed', 0) / 1000.0, 0.0, 1.0)
            ])
            node_states[f'vehicle_{i}'] = vehicle_state

        # RSUçŠ¶æ€ï¼ˆç»Ÿä¸€å½’ä¸€åŒ–/è£å‰ªï¼‰
        for i, rsu in enumerate(self.simulator.rsus):
            rsu_state = np.array([
                np.clip(rsu['position'][0] / 1000, 0.0, 1.0),
                np.clip(rsu['position'][1] / 1000, 0.0, 1.0),
                self._calculate_correct_cache_utilization(rsu.get('cache', {}), rsu.get('cache_capacity', 1000.0)),
                np.clip(len(rsu.get('computation_queue', [])) / 20.0, 0.0, 1.0),
                np.clip(rsu.get('energy_consumed', 0) / 1000.0, 0.0, 1.0)
            ])
            node_states[f'rsu_{i}'] = rsu_state

        # UAVçŠ¶æ€ï¼ˆç»Ÿä¸€å½’ä¸€åŒ–/è£å‰ªï¼‰
        for i, uav in enumerate(self.simulator.uavs):
            uav_state = np.array([
                np.clip(uav['position'][0] / 1000, 0.0, 1.0),
                np.clip(uav['position'][1] / 1000, 0.0, 1.0),
                np.clip(uav['position'][2] / 200, 0.0, 1.0),
                self._calculate_correct_cache_utilization(uav.get('cache', {}), uav.get('cache_capacity', 200.0)),
                np.clip(uav.get('energy_consumed', 0) / 1000.0, 0.0, 1.0)
            ])
            node_states[f'uav_{i}'] = uav_state
        
        # åˆå§‹ç³»ç»ŸæŒ‡æ ‡
        system_metrics = {
            'avg_task_delay': 0.0,
            'total_energy_consumption': 0.0,
            'data_loss_bytes': 0.0,
            'data_loss_ratio_bytes': 0.0,
            'cache_hit_rate': 0.0,
            'migration_success_rate': 0.0
        }
        
        # ğŸ”§ ä¿®å¤ï¼šé‡ç½®èƒ½è€—è¿½è¸ªå™¨ï¼Œé¿å…è·¨episodeç´¯ç§¯
        if hasattr(self, '_last_total_energy'):
            delattr(self, '_last_total_energy')
        # è®¾ç½®æœ¬episodeèƒ½è€—åŸºçº¿ï¼ˆç”¨äºè®¡ç®—å¢é‡èƒ½è€—ï¼‰
        self._episode_energy_base = 0.0
        
        # è·å–åˆå§‹çŠ¶æ€å‘é‡
        state = self.agent_env.get_state_vector(node_states, system_metrics)
        
        return state
    
    def step(self, action, state, actions_dict: Optional[Dict] = None) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡Œä¸€æ­¥ä»¿çœŸï¼Œåº”ç”¨æ™ºèƒ½ä½“åŠ¨ä½œåˆ°ä»¿çœŸå™¨"""
        # æ„é€ ä¼ é€’ç»™ä»¿çœŸå™¨çš„åŠ¨ä½œï¼ˆå°†è¿ç»­åŠ¨ä½œæ˜ å°„ä¸ºæœ¬åœ°/RSU/UAVåå¥½ï¼‰
        sim_actions = self._build_simulator_actions(actions_dict)
        
        # æ‰§è¡Œä»¿çœŸæ­¥éª¤ï¼ˆä¼ å…¥åŠ¨ä½œï¼‰
        step_stats = self.simulator.run_simulation_step(0, sim_actions)
        
        # æ”¶é›†ä¸‹ä¸€æ­¥çŠ¶æ€
        node_states = {}
        
        # è½¦è¾†çŠ¶æ€ (5ç»´ - ç»Ÿä¸€å½’ä¸€åŒ–)
        for i, vehicle in enumerate(self.simulator.vehicles):
            vehicle_state = np.array([
                np.clip(vehicle['position'][0] / 1000, 0.0, 1.0),  # ä½ç½®x
                np.clip(vehicle['position'][1] / 1000, 0.0, 1.0),  # ä½ç½®y
                np.clip(vehicle['velocity'] / 50, 0.0, 1.0),  # é€Ÿåº¦
                np.clip(len(vehicle.get('tasks', [])) / 20.0, 0.0, 1.0),  # é˜Ÿåˆ—ï¼ˆæ‰©å¤§èŒƒå›´åˆ°20ï¼‰
                np.clip(vehicle.get('energy_consumed', 0) / 1000.0, 0.0, 1.0)  # èƒ½è€—
            ])
            node_states[f'vehicle_{i}'] = vehicle_state
        
        # RSUçŠ¶æ€ (5ç»´ - æ¸…ç†ç‰ˆï¼Œç§»é™¤æ§åˆ¶å‚æ•°)
        for i, rsu in enumerate(self.simulator.rsus):
            # æ ‡å‡†åŒ–å½’ä¸€åŒ–ï¼šç¡®ä¿æ‰€æœ‰å€¼åœ¨[0,1]èŒƒå›´
            rsu_state = np.array([
                np.clip(rsu['position'][0] / 1000, 0.0, 1.0),  # ä½ç½®x
                np.clip(rsu['position'][1] / 1000, 0.0, 1.0),  # ä½ç½®y
                self._calculate_correct_cache_utilization(rsu.get('cache', {}), rsu.get('cache_capacity', 1000.0)),  # ç¼“å­˜åˆ©ç”¨ç‡
                np.clip(len(rsu.get('computation_queue', [])) / 20.0, 0.0, 1.0),  # é˜Ÿåˆ—åˆ©ç”¨ç‡ï¼ˆæ‰©å¤§èŒƒå›´åˆ°20ï¼‰
                np.clip(rsu.get('energy_consumed', 0) / 1000.0, 0.0, 1.0)  # èƒ½è€—
            ])
            node_states[f'rsu_{i}'] = rsu_state
        
        # UAVçŠ¶æ€ (5ç»´ - æ¸…ç†ç‰ˆï¼Œç§»é™¤æ§åˆ¶å‚æ•°)
        for i, uav in enumerate(self.simulator.uavs):
            # æ ‡å‡†åŒ–å½’ä¸€åŒ–ï¼šç¡®ä¿æ‰€æœ‰å€¼åœ¨[0,1]èŒƒå›´
            uav_state = np.array([
                np.clip(uav['position'][0] / 1000, 0.0, 1.0),  # ä½ç½®x
                np.clip(uav['position'][1] / 1000, 0.0, 1.0),  # ä½ç½®y
                np.clip(uav['position'][2] / 200, 0.0, 1.0),   # ä½ç½®zï¼ˆé«˜åº¦ï¼‰
                self._calculate_correct_cache_utilization(uav.get('cache', {}), uav.get('cache_capacity', 200.0)),  # ç¼“å­˜åˆ©ç”¨ç‡
                np.clip(uav.get('energy_consumed', 0) / 1000.0, 0.0, 1.0)  # èƒ½è€—
            ])
            node_states[f'uav_{i}'] = uav_state
        
        # è®¡ç®—ç³»ç»ŸæŒ‡æ ‡
        system_metrics = self._calculate_system_metrics(step_stats)
        
        # è·å–ä¸‹ä¸€çŠ¶æ€
        next_state = self.agent_env.get_state_vector(node_states, system_metrics)
        
        # ğŸ”§ å¢å¼ºï¼šè®¡ç®—åŒ…å«å­ç³»ç»ŸæŒ‡æ ‡çš„å¥–åŠ±
        cache_metrics = self.adaptive_cache_controller.get_cache_metrics()
        migration_metrics = self.adaptive_migration_controller.get_migration_metrics()
        
        reward = self.agent_env.calculate_reward(system_metrics, cache_metrics, migration_metrics)
        
        task_type_queue = system_metrics.get('task_type_queue_distribution', [])
        task_type_deadline = system_metrics.get('task_type_deadline_remaining', [])
        task_type_drop = system_metrics.get('task_type_drop_rate', [])
        for idx in range(4):
            queue_val = float(task_type_queue[idx]) if idx < len(task_type_queue) else 0.0
            deadline_val = float(task_type_deadline[idx]) if idx < len(task_type_deadline) else 0.0
            drop_val = float(task_type_drop[idx]) if idx < len(task_type_drop) else 0.0
            self.episode_metrics[f'task_type_queue_share_{idx+1}'].append(queue_val)
            self.episode_metrics[f'task_type_deadline_norm_{idx+1}'].append(deadline_val)
            self.episode_metrics[f'task_type_drop_rate_{idx+1}'].append(drop_val)
        
        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = False  # å•æ™ºèƒ½ä½“ç¯å¢ƒé€šå¸¸ä¸ä¼šæå‰ç»“æŸ
        
        # é™„åŠ ä¿¡æ¯
        info = {
            'step_stats': step_stats,
            'system_metrics': system_metrics
        }
        
        return next_state, reward, done, info
    
    def _calculate_system_metrics(self, step_stats: Dict) -> Dict:
        """è®¡ç®—ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ - æœ€ç»ˆä¿®å¤ç‰ˆï¼Œç¡®ä¿æ•°å€¼åœ¨åˆç†èŒƒå›´"""
        import numpy as np
        
        # å®‰å…¨è·å–æ•°å€¼
        def safe_get(key: str, default: float = 0.0) -> float:
            value = step_stats.get(key, default)
            if np.isnan(value) or np.isinf(value):
                return default
            return max(0.0, value)  # ç¡®ä¿éè´Ÿ
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨episodeçº§åˆ«ç»Ÿè®¡è€Œéç´¯ç§¯ç»Ÿè®¡ï¼Œé¿å…å¥–åŠ±ç´¯ç§¯æ¶åŒ–
        # è®¡ç®—æœ¬episodeçš„å¢é‡ç»Ÿè®¡
        total_processed = int(safe_get('processed_tasks', 0))  # ç´¯è®¡å®Œæˆ
        total_dropped = int(safe_get('dropped_tasks', 0))  # ç´¯è®¡ä¸¢å¼ƒï¼ˆæ•°é‡ï¼‰
        
        # è®¡ç®—æœ¬episodeå¢é‡
        episode_processed = total_processed - getattr(self, '_episode_processed_base', 0)
        episode_dropped = total_dropped - getattr(self, '_episode_dropped_base', 0)
        
        # æ•°æ®ä¸¢å¤±é‡ï¼šä½¿ç”¨æœ¬episodeå¢é‡
        current_generated_bytes = float(step_stats.get('generated_data_bytes', 0.0))
        current_dropped_bytes = float(step_stats.get('dropped_data_bytes', 0.0))
        episode_generated_bytes = current_generated_bytes - getattr(self, '_episode_generated_bytes_base', 0.0)
        episode_dropped_bytes = current_dropped_bytes - getattr(self, '_episode_dropped_bytes_base', 0.0)
        
        # è®¡ç®—æœ¬episodeä»»åŠ¡æ€»æ•°å’Œå®Œæˆç‡ï¼ˆé¿å…ç´¯ç§¯æ•ˆåº”ï¼‰
        episode_total = episode_processed + episode_dropped
        completion_rate = episode_processed / max(1, episode_total) if episode_total > 0 else 0.5
        
        cache_hits = int(safe_get('cache_hits', 0))
        cache_misses = int(safe_get('cache_misses', 0))
        cache_requests = max(1, cache_hits + cache_misses)
        cache_hit_rate = cache_hits / cache_requests
        local_cache_hits = int(safe_get('local_cache_hits', 0))
        
        # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨è®¡ç®—å¹³å‡å»¶è¿Ÿ - ä½¿ç”¨ç´¯è®¡ç»Ÿè®¡
        total_delay = safe_get('total_delay', 0.0)
        processed_for_delay = max(1, total_processed)  # ä½¿ç”¨ç´¯è®¡å®Œæˆæ•°
        avg_delay = total_delay / processed_for_delay
        
        # é™åˆ¶å»¶è¿Ÿåœ¨åˆç†èŒƒå›´å†…ï¼ˆå…³é”®ä¿®å¤ï¼‰
        avg_delay = np.clip(avg_delay, 0.01, 5.0)  # æ‰©å¤§åˆ°0.01-5.0ç§’èŒƒå›´ï¼Œé€‚åº”è·¨æ—¶éš™å¤„ç†
        
        # ğŸ”§ ä¿®å¤èƒ½è€—è®¡ç®—ï¼šä½¿ç”¨çœŸå®ç´¯ç§¯èƒ½è€—å¹¶è½¬æ¢ä¸ºæœ¬episodeå¢é‡
        current_total_energy = safe_get('total_energy', 0.0)

        # è‡ªé€‚åº”æ§åˆ¶å™¨ç»Ÿè®¡ï¼ˆç”¨äºå¥–åŠ±ä¸æŒ‡æ ‡å½’ä¸€åŒ–ï¼‰
        cache_metrics = self.adaptive_cache_controller.get_cache_metrics()
        migration_metrics = self.adaptive_migration_controller.get_migration_metrics()
        cache_total_requests = int(cache_metrics.get('total_requests', 0) or 0)
        cache_total_evictions = int(cache_metrics.get('evicted_items', 0) or 0)
        cache_total_collab = int(cache_metrics.get('collaborative_writes', 0) or 0)
        
        # åˆå§‹åŒ–æœ¬episodeå„é¡¹ç»Ÿè®¡åŸºçº¿
        if not hasattr(self, '_episode_energy_base_initialized'):
            self._episode_energy_base = current_total_energy
            self._episode_processed_base = total_processed
            self._episode_dropped_base = total_dropped
            self._episode_generated_bytes_base = current_generated_bytes
            self._episode_dropped_bytes_base = current_dropped_bytes
            self._episode_cache_requests_base = cache_total_requests
            self._episode_cache_evictions_base = cache_total_evictions
            self._episode_cache_collab_base = cache_total_collab
            self._episode_energy_base_initialized = True
        
        # è®¡ç®—æœ¬episodeå¢é‡èƒ½è€—ï¼ˆé˜²æ­¢è´Ÿå€¼ä¸å¼‚å¸¸ï¼‰
        if current_total_energy <= 0.0:
            # ä»¿çœŸå™¨èƒ½è€—å¼‚å¸¸æ—¶çš„ä¿åº•ä¼°ç®—
            completed_tasks = self.simulator.stats.get('completed_tasks', 0) if hasattr(self, 'simulator') else 0
            estimated_energy = max(0.0, completed_tasks * 15.0)
            total_energy = estimated_energy
            print(f"âš ï¸ ä»¿çœŸå™¨èƒ½è€—ä¸º0ï¼Œä½¿ç”¨ä¼°ç®—èƒ½è€—: {total_energy:.1f}J")
        else:
            episode_incremental_energy = max(0.0, current_total_energy - getattr(self, '_episode_energy_base', 0.0))
            total_energy = episode_incremental_energy
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨episodeçº§åˆ«æ•°æ®ä¸¢å¤±é‡ï¼Œé¿å…ç´¯ç§¯æ•ˆåº”
        data_loss_bytes = max(0.0, episode_dropped_bytes)
        data_generated_bytes = max(1.0, episode_generated_bytes)
        data_loss_ratio_bytes = min(1.0, data_loss_bytes / data_generated_bytes) if data_generated_bytes > 0 else 0.0
        
        # è¿ç§»æˆåŠŸç‡ï¼ˆæ¥è‡ªä»¿çœŸå™¨ç»Ÿè®¡ï¼‰
        migrations_executed = int(safe_get('migrations_executed', 0))
        migrations_successful = int(safe_get('migrations_successful', 0))
        migration_success_rate = (migrations_successful / migrations_executed) if migrations_executed > 0 else 0.0
        
        # ğŸ”§ è°ƒè¯•è¿ç§»ç»Ÿè®¡
        if migrations_executed > 0:
            print(f"ğŸ” è¿ç§»ç»Ÿè®¡: æ‰§è¡Œ{migrations_executed}æ¬¡, æˆåŠŸ{migrations_successful}æ¬¡, æˆåŠŸç‡{migration_success_rate:.1%}")

        episode_cache_requests = max(
            0,
            cache_total_requests - getattr(self, '_episode_cache_requests_base', 0)
        )
        episode_cache_evictions = max(
            0,
            cache_total_evictions - getattr(self, '_episode_cache_evictions_base', 0)
        )
        episode_cache_collab = max(
            0,
            cache_total_collab - getattr(self, '_episode_cache_collab_base', 0)
        )
        cache_eviction_rate = (
            episode_cache_evictions / episode_cache_requests
            if episode_cache_requests > 0 else 0.0
        )

        def _normalize_vector(key: str, length: int = 4, clip: bool = True) -> List[float]:
            raw = step_stats.get(key)
            if isinstance(raw, np.ndarray):
                values = raw.tolist()
            elif isinstance(raw, (list, tuple)):
                values = list(raw)
            else:
                values = []
            values = [float(v) for v in values[:length]]
            if len(values) < length:
                values.extend([0.0] * (length - len(values)))
            if clip:
                values = [float(np.clip(v, 0.0, 1.0)) for v in values]
            else:
                values = [float(max(0.0, v)) for v in values]
            return values

        queue_distribution = _normalize_vector('task_type_queue_distribution')
        active_distribution = _normalize_vector('task_type_active_distribution')
        deadline_remaining = _normalize_vector('task_type_deadline_remaining')
        queue_counts = _normalize_vector('task_type_queue_counts', clip=False)
        active_counts = _normalize_vector('task_type_active_counts', clip=False)

        task_generation_stats = step_stats.get('task_generation')
        gen_by_type = task_generation_stats.get('by_type', {}) if isinstance(task_generation_stats, dict) else {}
        drop_stats = step_stats.get('drop_stats')
        drop_by_type = drop_stats.get('by_type', {}) if isinstance(drop_stats, dict) else {}

        total_generated_by_type = sum(float(gen_by_type.get(t, 0.0)) for t in range(1, 5))
        generated_share: List[float] = []
        drop_rate: List[float] = []
        for task_type in range(1, 5):
            generated = float(gen_by_type.get(task_type, 0.0))
            dropped = float(drop_by_type.get(task_type, 0.0))
            drop_rate.append(float(np.clip(dropped / generated, 0.0, 1.0)) if generated > 0.0 else 0.0)
            generated_share.append(
                float(np.clip(generated / total_generated_by_type, 0.0, 1.0)) if total_generated_by_type > 0.0 else 0.0
            )

        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šèƒ½è€—ä¸è¿ç§»æ•æ„ŸåŒºé—´
        current_episode = getattr(self, '_current_episode', 0)
        if current_episode > 0 and (current_episode % 50 == 0 or avg_delay > 0.2 or migration_success_rate < 0.9):
            print(
                f"[è°ƒè¯•] Episode {current_episode:04d}: å»¶è¿Ÿ {avg_delay:.3f}s, èƒ½è€— {total_energy:.2f}J, "
                f"å®Œæˆç‡ {completion_rate:.1%}, è¿ç§»æˆåŠŸç‡ {migration_success_rate:.1%}, "
                f"ç¼“å­˜å‘½ä¸­ {cache_hit_rate:.1%}"
            )

        # ğŸ¤– æ›´æ–°ç¼“å­˜æ§åˆ¶å™¨ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰å®é™…æ•°æ®ï¼‰
        if cache_hit_rate > 0:
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—ç¼“å­˜ç»Ÿè®¡
            total_utilization = 0.0
            for rsu in self.simulator.rsus:
                utilization = self._calculate_correct_cache_utilization(
                    rsu.get('cache', {}), 
                    rsu.get('cache_capacity', 1000.0)
                )
                total_utilization += utilization
            
            self.adaptive_cache_controller.cache_stats['current_utilization'] = (
                total_utilization / max(1, len(self.simulator.rsus))
            )
        
        return {
            'avg_task_delay': avg_delay,
            'total_energy_consumption': total_energy,
            'data_loss_bytes': data_loss_bytes,
            'data_loss_ratio_bytes': data_loss_ratio_bytes,
            'task_completion_rate': completion_rate,
            'cache_hit_rate': cache_hit_rate,
            'local_cache_hits': local_cache_hits,
            'migration_success_rate': migration_success_rate,
            'dropped_tasks': episode_dropped,
            # ğŸ¤– æ–°å¢è‡ªé€‚åº”æ§åˆ¶æŒ‡æ ‡
            'adaptive_cache_effectiveness': cache_metrics.get('effectiveness', 0.0),
            'adaptive_migration_effectiveness': migration_metrics.get('effectiveness', 0.0),
            'migration_avg_cost': migration_metrics.get('avg_cost', 0.0),
            'migration_avg_delay_saved': migration_metrics.get('avg_delay_saved', 0.0),
            'cache_utilization': cache_metrics.get('utilization', 0.0),
            'cache_evictions': episode_cache_evictions,
            'cache_eviction_rate': cache_eviction_rate,
            'cache_requests': episode_cache_requests,
            'cache_collaborative_writes': episode_cache_collab,
            'adaptive_cache_params': cache_metrics.get('agent_params', {}),
            'adaptive_migration_params': migration_metrics.get('agent_params', {}),
            'task_type_queue_distribution': queue_distribution,
            'task_type_active_distribution': active_distribution,
            'task_type_deadline_remaining': deadline_remaining,
            'task_type_queue_counts': queue_counts,
            'task_type_active_counts': active_counts,
            'task_type_drop_rate': drop_rate,
            'task_type_generated_share': generated_share
        }
    
    def run_episode(self, episode: int, max_steps: Optional[int] = None) -> Dict:
        """è¿è¡Œä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒè½®æ¬¡"""
        # ä½¿ç”¨é…ç½®ä¸­çš„æœ€å¤§æ­¥æ•°
        if max_steps is None:
            max_steps = config.experiment.max_steps_per_episode
        
        # é‡ç½®ç¯å¢ƒ
        state = self.reset_environment()
        
        # ğŸ”§ ä¿å­˜å½“å‰episodeç¼–å·
        self._current_episode = episode
        
        # ğŸ”§ é‡ç½®episodeæ­¥æ•°è·Ÿè¸ªï¼Œä¿®å¤èƒ½è€—è®¡ç®—
        self._current_episode_step = 0
        
        # é‡ç½®episodeç»Ÿè®¡åŸºçº¿æ ‡è®°
        if hasattr(self, '_episode_energy_base_initialized'):
            delattr(self, '_episode_energy_base_initialized')
        
        episode_reward = 0.0
        episode_info = {}
        step = 0
        info = {}  # åˆå§‹åŒ–infoå˜é‡
        
        # PPOéœ€è¦ç‰¹æ®Šå¤„ç†
        if self.algorithm == "PPO":
            return self._run_ppo_episode(episode, max_steps)
        
        for step in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            if self.algorithm == "DQN":
                # DQNè¿”å›ç¦»æ•£åŠ¨ä½œ
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    # å¤„ç†å¯èƒ½çš„å…ƒç»„è¿”å›
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                        
                # éœ€è¦å°†åŠ¨ä½œæ˜ å°„å›å…¨å±€åŠ¨ä½œç´¢å¼•
                action_idx = self._encode_discrete_action(actions_dict)
                action = action_idx
            else:
                # è¿ç»­åŠ¨ä½œç®—æ³•
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    # å¤„ç†å¯èƒ½çš„å…ƒç»„è¿”å›
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                action = self._encode_continuous_action(actions_dict)
            
            # ğŸ”§ æ›´æ–°episodeæ­¥æ•°è®¡æ•°å™¨
            self._current_episode_step += 1
            
            # æ‰§è¡ŒåŠ¨ä½œï¼ˆå°†åŠ¨ä½œå­—å…¸ä¼ å…¥ä»¥å½±å“ä»¿çœŸå™¨å¸è½½åå¥½ï¼‰
            next_state, reward, done, info = self.step(action, state, actions_dict)
            
            # åˆå§‹åŒ–training_info
            training_info = {}
            
            # è®­ç»ƒæ™ºèƒ½ä½“ - æ‰€æœ‰ç®—æ³•ç°åœ¨éƒ½æ”¯æŒUnionç±»å‹ç»Ÿä¸€æ¥å£
            # ç¡®ä¿actionç±»å‹å®‰å…¨è½¬æ¢
            if self.algorithm == "DQN":
                # DQNé¦–é€‰æ•´æ•°åŠ¨ä½œï¼Œä½†æ¥å—Unionç±»å‹
                safe_action = self._safe_int_conversion(action)
                training_info = self.agent_env.train_step(state, safe_action, reward, next_state, done)
            elif self.algorithm in ["DDPG", "TD3", "TD3_LATENCY_ENERGY", "SAC"]:
                # è¿ç»­åŠ¨ä½œç®—æ³•é¦–é€‰numpyæ•°ç»„ï¼Œä½†æ¥å—Unionç±»å‹
                safe_action = action if isinstance(action, np.ndarray) else np.array([action], dtype=np.float32)
                training_info = self.agent_env.train_step(state, safe_action, reward, next_state, done)
            elif self.algorithm == "PPO":
                # PPOä½¿ç”¨ç‰¹æ®Šçš„episodeçº§åˆ«è®­ç»ƒï¼Œtrain_stepä¸ºå ä½ç¬¦
                # ä¿æŒåŸactionç±»å‹å³å¯ï¼Œå› ä¸ºPPOçš„train_stepä¸åšå®é™…å¤„ç†
                training_info = self.agent_env.train_step(state, action, reward, next_state, done)
            else:
                # å…¶ä»–ç®—æ³•çš„é»˜è®¤å¤„ç†
                training_info = {'message': f'Unknown algorithm: {self.algorithm}'}
            
            episode_info = training_info
            
            # æ›´æ–°çŠ¶æ€
            state = next_state
            episode_reward += reward
            
            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            if done:
                break
        
        # è®°å½•è½®æ¬¡ç»Ÿè®¡
        system_metrics = info.get('system_metrics', {})
        
        return {
            'episode_reward': episode_reward,
            'avg_reward': episode_reward,
            'episode_info': episode_info,
            'system_metrics': system_metrics,
            'steps': step + 1
        }
    
    def _run_ppo_episode(self, episode: int, max_steps: int = 100) -> Dict:
        """è¿è¡ŒPPOä¸“ç”¨episode"""
        state = self.reset_environment()
        episode_reward = 0.0
        
        # åˆå§‹åŒ–å˜é‡
        done = False
        step = 0
        info = {}
        
        for step in range(max_steps):
            # è·å–åŠ¨ä½œã€å¯¹æ•°æ¦‚ç‡å’Œä»·å€¼
            if hasattr(self.agent_env, 'get_actions'):
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, tuple) and len(actions_result) == 3:
                    actions_dict, log_prob, value = actions_result
                else:
                    # å¦‚æœä¸æ˜¯å…ƒç»„ï¼Œå°±ä½¿ç”¨é»˜è®¤å€¼
                    actions_dict = actions_result if isinstance(actions_result, dict) else {}
                    log_prob = 0.0
                    value = 0.0
            else:
                actions_dict = {}
                log_prob = 0.0
                value = 0.0
                
            action = self._encode_continuous_action(actions_dict)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.step(action, state, actions_dict)
            
            # å­˜å‚¨ç»éªŒ - æ‰€æœ‰ç®—æ³•éƒ½æ”¯æŒç»Ÿä¸€æ¥å£
            # ç¡®ä¿å‚æ•°ç±»å‹æ­£ç¡®
            log_prob_float = float(log_prob) if not isinstance(log_prob, float) else log_prob
            value_float = float(value) if not isinstance(value, float) else value
            # ä½¿ç”¨å‘½åå‚æ•°é¿å…ä½ç½®å‚æ•°é¡ºåºé—®é¢˜
            self.agent_env.store_experience(
                state=state, 
                action=action, 
                reward=reward, 
                next_state=next_state, 
                done=done, 
                log_prob=log_prob_float, 
                value=value_float
            )
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        # ğŸ”§ PPOæ›´æ–°ç­–ç•¥ä¿®å¤ï¼šç´¯ç§¯å¤šä¸ªepisodeåå†æ›´æ–°
        last_value = 0.0
        if not done:
            if hasattr(self.agent_env, 'get_actions'):
                actions_result = self.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, tuple) and len(actions_result) >= 3:
                    _, _, last_value = actions_result
                else:
                    last_value = 0.0
        
        # ç¡®ä¿ last_value ä¸º float ç±»å‹
        last_value_float = float(last_value) if not isinstance(last_value, float) else last_value
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ›´æ–°ï¼ˆæ¯Nä¸ªepisodeæˆ–bufferå¿«æ»¡æ—¶ï¼‰
        ppo_config = self.agent_env.config
        should_update = (
            episode % ppo_config.update_frequency == 0 or  # æ¯Nä¸ªepisode
            self.agent_env.buffer.size >= ppo_config.buffer_size * 0.9  # bufferæ¥è¿‘æ»¡
        )
        
        # è¿›è¡Œæ›´æ–°
        if should_update:
            training_info = self.agent_env.update(last_value_float, force_update=True)
        else:
            training_info = self.agent_env.update(last_value_float, force_update=False)
        
        system_metrics = info.get('system_metrics', {})
        
        return {
            'episode_reward': episode_reward,
            'avg_reward': episode_reward,
            'episode_info': training_info,
            'system_metrics': system_metrics,
            'steps': step + 1
        }

    def _build_simulator_actions(self, actions_dict: Optional[Dict]) -> Optional[Dict]:
        """å°†ç®—æ³•åŠ¨ä½œå­—å…¸è½¬æ¢ä¸ºä»¿çœŸå™¨å¯æ¶ˆè´¹çš„ç®€å•æ§åˆ¶ä¿¡å·ã€‚
        ğŸ¤– æ‰©å±•æ”¯æŒ18ç»´åŠ¨ä½œç©ºé—´ï¼š
        - vehicle_agent å‰11ç»´ â†’ åŸæœ‰ä»»åŠ¡åˆ†é…å’ŒèŠ‚ç‚¹é€‰æ‹©
        - vehicle_agent å8ç»´ â†’ ç¼“å­˜è¿ç§»å‚æ•°æ§åˆ¶
        """
        if not isinstance(actions_dict, dict):
            return None
        vehicle_action = actions_dict.get('vehicle_agent')
        if vehicle_action is None:
            return None
        try:
            import numpy as np
            
            # =============== åŸæœ‰11ç»´åŠ¨ä½œé€»è¾‘ (ä¿æŒå…¼å®¹) ===============
            # å–å‰ä¸‰ç»´ï¼Œæ˜ å°„åˆ°[0,1]å¹¶softmaxä¸ºæ¦‚ç‡
            raw = np.array(vehicle_action[:3], dtype=np.float32).reshape(-1)
            # æ•°å€¼å®‰å…¨
            raw = np.clip(raw, -5.0, 5.0)
            exp = np.exp(raw - np.max(raw))
            probs = exp / np.sum(exp)
            sim_actions = {
                'vehicle_offload_pref': {
                    'local': float(probs[0]),
                    'rsu': float(probs[1] if probs.size > 1 else 0.33),
                    'uav': float(probs[2] if probs.size > 2 else 0.34)
                }
            }
            # RSUé€‰æ‹©æ¦‚ç‡
            num_rsus = len(getattr(self.simulator, 'rsus', []))
            rsu_action = actions_dict.get('rsu_agent')
            if isinstance(rsu_action, (list, tuple, np.ndarray)) and num_rsus > 0:
                rsu_raw = np.array(rsu_action[:num_rsus], dtype=np.float32)
                rsu_raw = np.clip(rsu_raw, -5.0, 5.0)
                rsu_exp = np.exp(rsu_raw - np.max(rsu_raw))
                rsu_probs = rsu_exp / np.sum(rsu_exp)
                sim_actions['rsu_selection_probs'] = [float(x) for x in rsu_probs]
            # UAVé€‰æ‹©æ¦‚ç‡
            num_uavs = len(getattr(self.simulator, 'uavs', []))
            uav_action = actions_dict.get('uav_agent')
            if isinstance(uav_action, (list, tuple, np.ndarray)) and num_uavs > 0:
                uav_raw = np.array(uav_action[:num_uavs], dtype=np.float32)
                uav_raw = np.clip(uav_raw, -5.0, 5.0)
                uav_exp = np.exp(uav_raw - np.max(uav_raw))
                uav_probs = uav_exp / np.sum(uav_exp)
                sim_actions['uav_selection_probs'] = [float(x) for x in uav_probs]
            
            # ğŸ¤– =============== æ–°å¢7ç»´ç¼“å­˜è¿ç§»æ§åˆ¶ ===============
            if isinstance(vehicle_action, (list, tuple, np.ndarray)):
                vehicle_action_array = np.array(vehicle_action, dtype=np.float32)
                control_start = 3 + num_rsus + num_uavs
                control_end = control_start + 8
                if vehicle_action_array.size >= control_end:
                    cache_migration_actions = vehicle_action_array[control_start:control_end]
                elif vehicle_action_array.size > control_start:
                    # è‹¥é•¿åº¦ä¸è¶³7ç»´ï¼Œåšå®‰å…¨è¡¥é›¶
                    cache_migration_actions = np.zeros(8, dtype=np.float32)
                    available = vehicle_action_array[control_start:]
                    cache_migration_actions[:min(available.size, 8)] = available[:8]
                else:
                    cache_migration_actions = np.zeros(8, dtype=np.float32)

                cache_migration_actions = np.clip(cache_migration_actions, -1.0, 1.0)

                # æ˜ å°„ä¸ºå‚æ•°å­—å…¸
                cache_params, migration_params = map_agent_actions_to_params(cache_migration_actions)

                # æ›´æ–°è‡ªé€‚åº”æ§åˆ¶å™¨å‚æ•°
                self.adaptive_cache_controller.update_agent_params(cache_params)
                self.adaptive_migration_controller.update_agent_params(migration_params)

                # å°†è‡ªé€‚åº”å‚æ•°ä¼ é€’ç»™ä»¿çœŸå™¨
                sim_actions.update({
                    'adaptive_cache_params': cache_params,
                    'adaptive_migration_params': migration_params,
                    'cache_controller': self.adaptive_cache_controller,
                    'migration_controller': self.adaptive_migration_controller
                })
            
            return sim_actions
        except Exception as e:
            print(f"âš ï¸ åŠ¨ä½œæ„é€ å¼‚å¸¸: {e}")
            return None
    
    def _encode_continuous_action(self, actions_dict) -> np.ndarray:
        """
        ğŸ¤– å°†åŠ¨ä½œå­—å…¸ç¼–ç ä¸ºè¿ç»­åŠ¨ä½œå‘é‡ - åŠ¨æ€é€‚é…åŠ¨ä½œç»´åº¦
        """
        # å¤„ç†å¯èƒ½çš„ä¸åŒè¾“å…¥ç±»å‹
        action_dim = getattr(self.agent_env, 'action_dim', 18)
        if not isinstance(actions_dict, dict):
            # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œè¿”å›é»˜è®¤åŠ¨ä½œç»´åº¦
            return np.zeros(action_dim, dtype=np.float32)

        # ğŸ¤– åªä½¿ç”¨vehicle_agentçš„å®Œæ•´åŠ¨ä½œå‘é‡
        vehicle_action = actions_dict.get('vehicle_agent')
        if isinstance(vehicle_action, (list, tuple, np.ndarray)):
            vehicle_action = np.array(vehicle_action, dtype=np.float32)
            if vehicle_action.size >= action_dim:
                return vehicle_action[:action_dim]
            action = np.zeros(action_dim, dtype=np.float32)
            action[:vehicle_action.size] = vehicle_action
            return action

        # é»˜è®¤è¿”å›å…¨é›¶åŠ¨ä½œ
        return np.zeros(action_dim, dtype=np.float32)
    
    def _build_actions_from_vector(self, action_vector: np.ndarray) -> Dict[str, np.ndarray]:
        """å°†è¿ç»­åŠ¨ä½œå‘é‡æ¢å¤ä¸ºä»¿çœŸå™¨éœ€è¦çš„åŠ¨ä½œå­—å…¸ï¼ˆåŠ¨æ€ç»´åº¦ï¼‰"""
        import numpy as np

        if not isinstance(action_vector, np.ndarray):
            action_vector = np.array(action_vector, dtype=np.float32)

        action_dim = getattr(self.agent_env, 'action_dim', action_vector.size)
        if action_vector.size < action_dim:
            padded = np.zeros(action_dim, dtype=np.float32)
            padded[:action_vector.size] = action_vector
            action_vector = padded
        else:
            action_vector = action_vector.astype(np.float32)[:action_dim]

        num_rsus = len(getattr(self.simulator, 'rsus', []))
        num_uavs = len(getattr(self.simulator, 'uavs', []))
        rsu_start = 3
        rsu_end = rsu_start + num_rsus
        uav_end = rsu_end + num_uavs

        return {
            'vehicle_agent': action_vector,
            'rsu_agent': action_vector[rsu_start:rsu_end],
            'uav_agent': action_vector[rsu_end:uav_end]
        }

    def _encode_discrete_action(self, actions_dict) -> int:
        """å°†åŠ¨ä½œå­—å…¸ç¼–ç ä¸ºç¦»æ•£åŠ¨ä½œç´¢å¼•"""
        # å¤„ç†å¯èƒ½çš„ä¸åŒè¾“å…¥ç±»å‹
        if not isinstance(actions_dict, dict):
            return 0  # é»˜è®¤åŠ¨ä½œç´¢å¼•
        
        # ç®€åŒ–å®ç°ï¼šå°†æ¯ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œç»„åˆæˆä¸€ä¸ªç´¢å¼•
        vehicle_action = actions_dict.get('vehicle_agent', 0)
        rsu_action = actions_dict.get('rsu_agent', 0)
        uav_action = actions_dict.get('uav_agent', 0)
        
        # å®‰å…¨åœ°å°†åŠ¨ä½œè½¬æ¢ä¸ºæ•´æ•°
        def safe_int_conversion(value):
            if isinstance(value, (int, np.integer)):
                return int(value)
            elif isinstance(value, np.ndarray):
                if value.size == 1:
                    return int(value.item())
                else:
                    return int(value[0])  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
            elif isinstance(value, (float, np.floating)):
                return int(value)
            else:
                return 0
        
        vehicle_action = safe_int_conversion(vehicle_action)
        rsu_action = safe_int_conversion(rsu_action)
        uav_action = safe_int_conversion(uav_action)
        
        # 5^3 = 125 ç§ç»„åˆ
        return vehicle_action * 25 + rsu_action * 5 + uav_action
    
    def _safe_int_conversion(self, value) -> int:
        """å®‰å…¨åœ°å°†ä¸åŒç±»å‹è½¬æ¢ä¸ºæ•´æ•°"""
        if isinstance(value, (int, np.integer)):
            return int(value)
        elif isinstance(value, np.ndarray):
            if value.size == 1:
                return int(value.item())
            else:
                return int(value[0])  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
        elif isinstance(value, (float, np.floating)):
            return int(round(value))
        else:
            return 0  # å®‰å…¨å›é€€å€¼


def train_single_algorithm(algorithm: str, num_episodes: Optional[int] = None, eval_interval: Optional[int] = None, 
                          save_interval: Optional[int] = None, enable_realtime_vis: bool = False, 
                          vis_port: int = 5000, silent_mode: bool = False, override_scenario: Optional[Dict[str, Any]] = None,
                          use_enhanced_cache: bool = False) -> Dict:
    """è®­ç»ƒå•ä¸ªç®—æ³•
    
    Args:
        algorithm: ç®—æ³•åç§°
        num_episodes: è®­ç»ƒè½®æ¬¡
        eval_interval: è¯„ä¼°é—´éš”
        save_interval: ä¿å­˜é—´éš”
        enable_realtime_vis: æ˜¯å¦å¯ç”¨å®æ—¶å¯è§†åŒ–
        vis_port: å¯è§†åŒ–æœåŠ¡å™¨ç«¯å£
        silent_mode: é™é»˜æ¨¡å¼ï¼Œè·³è¿‡ç”¨æˆ·äº¤äº’ï¼ˆç”¨äºæ‰¹é‡å®éªŒï¼‰
    """
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    if num_episodes is None:
        num_episodes = config.experiment.num_episodes
    
    # ğŸ”§ è‡ªåŠ¨è°ƒæ•´è¯„ä¼°é—´éš”å’Œä¿å­˜é—´éš”
    def auto_adjust_intervals(total_episodes: int):
        """æ ¹æ®æ€»è½®æ•°è‡ªåŠ¨è°ƒæ•´é—´éš”"""
        # è¯„ä¼°é—´éš”ï¼šæ€»è½®æ•°çš„5-8%ï¼ŒèŒƒå›´[10, 100]
        auto_eval = max(10, min(100, int(total_episodes * 0.06)))
        
        # ä¿å­˜é—´éš”ï¼šæ€»è½®æ•°çš„15-20%ï¼ŒèŒƒå›´[50, 500]  
        auto_save = max(50, min(500, int(total_episodes * 0.18)))
        
        return auto_eval, auto_save
    
    # åº”ç”¨è‡ªåŠ¨è°ƒæ•´ï¼ˆä»…å½“ç”¨æˆ·æœªæŒ‡å®šæ—¶ï¼‰
    if eval_interval is None or save_interval is None:
        auto_eval, auto_save = auto_adjust_intervals(num_episodes)
        if eval_interval is None:
            eval_interval = auto_eval
        if save_interval is None:
            save_interval = auto_save
    
    # æœ€ç»ˆå›é€€åˆ°é…ç½®é»˜è®¤å€¼
    if eval_interval is None:
        eval_interval = config.experiment.eval_interval
    if save_interval is None:
        save_interval = config.experiment.save_interval
    
    print(f"\n>> å¼€å§‹{algorithm}å•æ™ºèƒ½ä½“ç®—æ³•è®­ç»ƒ")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒï¼ˆåº”ç”¨é¢å¤–åœºæ™¯è¦†ç›–ï¼‰
    training_env = SingleAgentTrainingEnvironment(algorithm, override_scenario=override_scenario, 
                                                  use_enhanced_cache=use_enhanced_cache)
    canonical_algorithm = training_env.algorithm
    if canonical_algorithm != algorithm:
        print(f"âš™ï¸  è§„èŒƒåŒ–ç®—æ³•æ ‡è¯†: {canonical_algorithm}")
    algorithm = canonical_algorithm
    
    # ğŸŒ åˆ›å»ºå®æ—¶å¯è§†åŒ–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    visualizer = None
    if enable_realtime_vis and REALTIME_AVAILABLE:
        print(f"ğŸŒ å¯åŠ¨å®æ—¶å¯è§†åŒ–æœåŠ¡å™¨ (ç«¯å£: {vis_port})")
        visualizer = create_visualizer(
            algorithm=algorithm,
            total_episodes=num_episodes,
            port=vis_port,
            auto_open=True
        )
        print(f"âœ… å®æ—¶å¯è§†åŒ–å·²å¯ç”¨ï¼Œè®¿é—® http://localhost:{vis_port}")
    elif enable_realtime_vis and not REALTIME_AVAILABLE:
        print("âš ï¸  å®æ—¶å¯è§†åŒ–æœªå¯ç”¨ï¼ˆç¼ºå°‘ä¾èµ–åŒ…ï¼‰")
    
    print(f"è®­ç»ƒé…ç½®:")
    print(f"  ç®—æ³•: {algorithm}")
    print(f"  æ€»è½®æ¬¡: {num_episodes}")
    print(f"  è¯„ä¼°é—´éš”: {eval_interval} (è‡ªåŠ¨è°ƒæ•´)" if eval_interval != config.experiment.eval_interval else f"  è¯„ä¼°é—´éš”: {eval_interval}")
    print(f"  ä¿å­˜é—´éš”: {save_interval} (è‡ªåŠ¨è°ƒæ•´)" if save_interval != config.experiment.save_interval else f"  ä¿å­˜é—´éš”: {save_interval}")
    print(f"  å®æ—¶å¯è§†åŒ–: {'å¯ç”¨ âœ“' if visualizer else 'ç¦ç”¨'}")
    print("-" * 60)
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(f"results/single_agent/{algorithm.lower()}", exist_ok=True)
    os.makedirs(f"results/models/single_agent/{algorithm.lower()}", exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    # ğŸ”§ ä¿®å¤ï¼šper-stepå¥–åŠ±èŒƒå›´çº¦ä¸º-2.0åˆ°-0.5ï¼Œåˆå§‹å€¼åº”ç›¸åº”è°ƒæ•´
    best_avg_reward = -10.0  # per-stepå¥–åŠ±åˆå§‹é˜ˆå€¼ï¼ˆè´Ÿå€¼è¶Šå¤§è¶Šå¥½ï¼‰
    training_start_time = time.time()
    
    for episode in range(1, num_episodes + 1):
        episode_start_time = time.time()
        
        # è¿è¡Œè®­ç»ƒè½®æ¬¡
        episode_result = training_env.run_episode(episode)
        
        # è®°å½•è®­ç»ƒæ•°æ®
        training_env.episode_rewards.append(episode_result['avg_reward'])
        
        # ğŸ”§ æ–°å¢ï¼šè®°å½•å®é™…æ­¥æ•°
        episode_steps = episode_result.get('steps', config.experiment.max_steps_per_episode)
        training_env.episode_metrics['episode_steps'].append(episode_steps)
        
        # æ›´æ–°æ€§èƒ½è¿½è¸ªå™¨
        training_env.performance_tracker['recent_rewards'].update(episode_result['avg_reward'])
        
        system_metrics = episode_result['system_metrics']
        training_env.performance_tracker['recent_delays'].update(system_metrics.get('avg_task_delay', 0))
        training_env.performance_tracker['recent_energy'].update(system_metrics.get('total_energy_consumption', 0))
        training_env.performance_tracker['recent_completion'].update(system_metrics.get('task_completion_rate', 0))
        
        # ğŸ”§ ä¿®å¤ï¼šè®°å½•æŒ‡æ ‡ - è§£å†³é”®åä¸åŒ¹é…é—®é¢˜
        metric_mapping = {
            'avg_task_delay': 'avg_delay',
            'total_energy_consumption': 'total_energy',
            'data_loss_bytes': 'data_loss_bytes',
            'data_loss_ratio_bytes': 'data_loss_ratio_bytes',
            'task_completion_rate': 'task_completion_rate',
            'cache_hit_rate': 'cache_hit_rate', 
            'cache_utilization': 'cache_utilization',
            'cache_evictions': 'cache_evictions',
            'cache_eviction_rate': 'cache_eviction_rate',
            'cache_requests': 'cache_requests',
            'cache_collaborative_writes': 'cache_collaborative_writes',
            'local_cache_hits': 'local_cache_hits',
            'migration_success_rate': 'migration_success_rate',
            'migration_avg_cost': 'migration_avg_cost',
            'migration_avg_delay_saved': 'migration_avg_delay_saved'
        }
        
        for system_key, episode_key in metric_mapping.items():
            if system_key in system_metrics and episode_key in training_env.episode_metrics:
                training_env.episode_metrics[episode_key].append(system_metrics[system_key])
                # print(f"âœ… è®°å½•æŒ‡æ ‡ {episode_key}: {system_metrics[system_key]:.3f}")  # è°ƒè¯•ä¿¡æ¯ï¼ˆå‡å°‘è¾“å‡ºï¼‰
        
        # ğŸŒ æ›´æ–°å®æ—¶å¯è§†åŒ–
        if visualizer:
            vis_metrics = {
                'avg_delay': system_metrics.get('avg_task_delay', 0),
                'total_energy': system_metrics.get('total_energy_consumption', 0),
                'task_completion_rate': system_metrics.get('task_completion_rate', 0),
                'cache_hit_rate': system_metrics.get('cache_hit_rate', 0),
                'data_loss_ratio_bytes': system_metrics.get('data_loss_ratio_bytes', 0),
                'migration_success_rate': system_metrics.get('migration_success_rate', 0)
            }
            visualizer.update(episode, episode_result['avg_reward'], vis_metrics)
        
        episode_time = time.time() - episode_start_time
        
        # å®šæœŸè¾“å‡ºè¿›åº¦
        if episode % 10 == 0:
            avg_reward = training_env.performance_tracker['recent_rewards'].get_average()
            avg_delay = training_env.performance_tracker['recent_delays'].get_average()
            avg_completion = training_env.performance_tracker['recent_completion'].get_average()
            
            print(f"è½®æ¬¡ {episode:4d}/{num_episodes}:")
            print(f"  Per-Stepå¥–åŠ±: {avg_reward:8.3f}")
            print(f"  å¹³å‡æ—¶å»¶: {avg_delay:8.3f}s")
            print(f"  å®Œæˆç‡:   {avg_completion:8.1%}")
            print(f"  è½®æ¬¡ç”¨æ—¶: {episode_time:6.3f}s")
        
        # è¯„ä¼°æ¨¡å‹
        if episode % eval_interval == 0:
            eval_result = evaluate_single_model(algorithm, training_env, episode)
            print(f"\nğŸ“Š è½®æ¬¡ {episode} è¯„ä¼°ç»“æœ:")
            print(f"  Per-Stepå¥–åŠ±: {eval_result['avg_reward']:.3f}")
            print(f"  è¯„ä¼°æ—¶å»¶: {eval_result['avg_delay']:.3f}s")
            print(f"  è¯„ä¼°å®Œæˆç‡: {eval_result['completion_rate']:.1%}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if eval_result['avg_reward'] > best_avg_reward:
                best_avg_reward = eval_result['avg_reward']
                training_env.agent_env.save_models(f"results/models/single_agent/{algorithm.lower()}/best_model")
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Per-Stepå¥–åŠ±: {best_avg_reward:.3f})")
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if episode % save_interval == 0:
            training_env.agent_env.save_models(f"results/models/single_agent/{algorithm.lower()}/checkpoint_{episode}")
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: checkpoint_{episode}")
    
    # è®­ç»ƒå®Œæˆ
    total_training_time = time.time() - training_start_time
    
    # ğŸŒ æ ‡è®°å®æ—¶å¯è§†åŒ–å®Œæˆ
    if visualizer:
        visualizer.complete()
        print(f"âœ… å®æ—¶å¯è§†åŒ–å·²æ ‡è®°å®Œæˆ")
    
    print("\n" + "=" * 60)
    print(f"ğŸ‰ {algorithm}è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_training_time/3600:.2f} å°æ—¶")
    print(f"ğŸ† æœ€ä½³Per-Stepå¥–åŠ±: {best_avg_reward:.3f}")
    
    # æ”¶é›†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯ç”¨äºæŠ¥å‘Š
    simulator_stats = {}
    
    # ğŸ¢ æ˜¾ç¤ºä¸­å¤®RSUè°ƒåº¦å™¨æŠ¥å‘Š
    try:
        central_report = training_env.simulator.get_central_scheduling_report()
        if central_report.get('status') != 'not_available' and central_report.get('status') != 'error':
            print(f"\nğŸ¢ ä¸­å¤®RSUéª¨å¹²è°ƒåº¦å™¨æ€»ç»“:")
            print(f"   ğŸ“Š è°ƒåº¦è°ƒç”¨æ¬¡æ•°: {central_report.get('scheduling_calls', 0)}")
            
            scheduler_status = central_report.get('central_scheduler_status', {})
            if 'global_metrics' in scheduler_status:
                metrics = scheduler_status['global_metrics']
                print(f"   âš–ï¸ è´Ÿè½½å‡è¡¡æŒ‡æ•°: {metrics.get('load_balance_index', 0.0):.3f}")
                print(f"   ğŸ’š ç³»ç»Ÿå¥åº·çŠ¶æ€: {scheduler_status.get('system_health', 'N/A')}")
                
                # æ”¶é›†è°ƒåº¦å™¨ç»Ÿè®¡ä¿¡æ¯
                simulator_stats['scheduling_calls'] = central_report.get('scheduling_calls', 0)
                simulator_stats['load_balance_index'] = metrics.get('load_balance_index', 0.0)
                simulator_stats['system_health'] = scheduler_status.get('system_health', 'N/A')
            
            # æ˜¾ç¤ºå„RSUè´Ÿè½½åˆ†å¸ƒ
            rsu_details = central_report.get('rsu_details', {})
            if rsu_details:
                print(f"   ğŸ“¡ å„RSUè´Ÿè½½çŠ¶æ€:")
                for rsu_id, details in rsu_details.items():
                    print(f"      {rsu_id}: CPUè´Ÿè½½={details['cpu_usage']:.1%}, ä»»åŠ¡é˜Ÿåˆ—={details['queue_length']}")
        else:
            print(f"ğŸ“‹ ä¸­å¤®è°ƒåº¦å™¨çŠ¶æ€: {central_report.get('message', 'æœªå¯ç”¨')}")
        
        # ğŸ”Œ æ˜¾ç¤ºæœ‰çº¿å›ä¼ ç½‘ç»œç»Ÿè®¡
        rsu_migration_delay = training_env.simulator.stats.get('rsu_migration_delay', 0.0)
        rsu_migration_energy = training_env.simulator.stats.get('rsu_migration_energy', 0.0)
        rsu_migration_data = training_env.simulator.stats.get('rsu_migration_data', 0.0)
        backhaul_collection_delay = training_env.simulator.stats.get('backhaul_collection_delay', 0.0)
        backhaul_command_delay = training_env.simulator.stats.get('backhaul_command_delay', 0.0)
        backhaul_total_energy = training_env.simulator.stats.get('backhaul_total_energy', 0.0)
        
        # ğŸš— æ˜¾ç¤ºå„ç§è¿ç§»ç»Ÿè®¡
        handover_migrations = training_env.simulator.stats.get('handover_migrations', 0)
        uav_migration_count = training_env.simulator.stats.get('uav_migration_count', 0)
        uav_migration_distance = training_env.simulator.stats.get('uav_migration_distance', 0.0)
        
        # æ”¶é›†è¿ç§»ç»Ÿè®¡ä¿¡æ¯
        simulator_stats['rsu_migration_delay'] = rsu_migration_delay
        simulator_stats['rsu_migration_energy'] = rsu_migration_energy
        simulator_stats['rsu_migration_data'] = rsu_migration_data
        simulator_stats['backhaul_total_energy'] = backhaul_total_energy
        simulator_stats['handover_migrations'] = handover_migrations
        simulator_stats['uav_migration_count'] = uav_migration_count
        
        if rsu_migration_data > 0 or backhaul_total_energy > 0 or handover_migrations > 0 or uav_migration_count > 0:
            print(f"\nğŸ”Œ æœ‰çº¿å›ä¼ ç½‘ç»œä¸è¿ç§»ç»Ÿè®¡:")
            print(f"   ğŸ“¡ RSUè¿ç§»æ•°æ®: {rsu_migration_data:.1f}MB")
            print(f"   â±ï¸ RSUè¿ç§»å»¶è¿Ÿ: {rsu_migration_delay*1000:.1f}ms")
            print(f"   âš¡ RSUè¿ç§»èƒ½è€—: {rsu_migration_energy:.2f}J")
            print(f"   ğŸ“Š ä¿¡æ¯æ”¶é›†å»¶è¿Ÿ: {backhaul_collection_delay*1000:.1f}ms")
            print(f"   ğŸ“¤ æŒ‡ä»¤åˆ†å‘å»¶è¿Ÿ: {backhaul_command_delay*1000:.1f}ms")
            print(f"   ğŸ”‹ å›ä¼ ç½‘ç»œæ€»èƒ½è€—: {backhaul_total_energy:.2f}J")
            if handover_migrations > 0:
                print(f"   ğŸš— è½¦è¾†è·Ÿéšè¿ç§»: {handover_migrations} æ¬¡")
            if uav_migration_count > 0:
                avg_distance = uav_migration_distance / uav_migration_count if uav_migration_count > 0 else 0
                print(f"   ğŸš UAVè¿ç§»: {uav_migration_count} æ¬¡, å¹³å‡è·ç¦»{avg_distance:.1f}m")
    except Exception as e:
        print(f"âš ï¸ ä¸­å¤®è°ƒåº¦æŠ¥å‘Šè·å–å¤±è´¥: {e}")
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    results = save_single_training_results(algorithm, training_env, total_training_time, override_scenario=override_scenario)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_single_training_curves(algorithm, training_env)
    
    # ç”ŸæˆHTMLè®­ç»ƒæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
    
    try:
        report_generator = HTMLReportGenerator()
        html_content = report_generator.generate_full_report(
            algorithm=algorithm,
            training_env=training_env,
            training_time=total_training_time,
            results=results,
            simulator_stats=simulator_stats
        )
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
        timestamp = generate_timestamp()
        report_filename = f"training_report_{timestamp}.html" if timestamp else "training_report.html"
        report_path = f"results/single_agent/{algorithm.lower()}/{report_filename}"
        
        print(f"âœ… è®­ç»ƒæŠ¥å‘Šå·²ç”Ÿæˆ")
        print(f"ğŸ“„ æŠ¥å‘ŠåŒ…å«:")
        print(f"   - æ‰§è¡Œæ‘˜è¦ä¸å…³é”®æŒ‡æ ‡")
        print(f"   - è®­ç»ƒé…ç½®è¯¦æƒ…")
        print(f"   - æ€§èƒ½æŒ‡æ ‡å¯è§†åŒ–å›¾è¡¨")
        print(f"   - è¯¦ç»†çš„ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯")
        print(f"   - è‡ªé€‚åº”æ§åˆ¶å™¨åˆ†æ")
        print(f"   - ä¼˜åŒ–å»ºè®®ä¸ç»“è®º")
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ä¿å­˜æŠ¥å‘Šï¼ˆé™é»˜æ¨¡å¼ä¸‹è‡ªåŠ¨ä¿å­˜ï¼‰
        if silent_mode:
            # é™é»˜æ¨¡å¼ï¼šè‡ªåŠ¨ä¿å­˜ï¼Œä¸æ‰“å¼€æµè§ˆå™¨
            if report_generator.save_report(html_content, report_path):
                print(f"âœ… æŠ¥å‘Šå·²è‡ªåŠ¨ä¿å­˜åˆ°: {report_path}")
            else:
                print("âŒ æŠ¥å‘Šä¿å­˜å¤±è´¥")
        else:
            # äº¤äº’æ¨¡å¼ï¼šè¯¢é—®ç”¨æˆ·
            print("\n" + "-" * 60)
            save_choice = input("ğŸ’¾ æ˜¯å¦ä¿å­˜HTMLè®­ç»ƒæŠ¥å‘Š? (y/n, é»˜è®¤y): ").strip().lower()
            
            if save_choice in ['', 'y', 'yes', 'æ˜¯']:
                if report_generator.save_report(html_content, report_path):
                    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
                    print(f"ğŸ’¡ æç¤º: ä½¿ç”¨æµè§ˆå™¨æ‰“å¼€è¯¥æ–‡ä»¶å³å¯æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š")
                    
                    # å°è¯•è‡ªåŠ¨æ‰“å¼€æŠ¥å‘Šï¼ˆå¯é€‰ï¼‰
                    auto_open = input("ğŸŒ æ˜¯å¦åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŠ¥å‘Š? (y/n, é»˜è®¤n): ").strip().lower()
                    if auto_open in ['y', 'yes', 'æ˜¯']:
                        import webbrowser
                        abs_path = os.path.abspath(report_path)
                        webbrowser.open(f'file://{abs_path}')
                        print("âœ… æŠ¥å‘Šå·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€")
                else:
                    print("âŒ æŠ¥å‘Šä¿å­˜å¤±è´¥")
            else:
                print("â„¹ï¸ æŠ¥å‘Šæœªä¿å­˜")
                print(f"ğŸ’¡ å¦‚éœ€æŸ¥çœ‹ï¼Œè¯·æ‰‹åŠ¨è¿è¡ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½")
    
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        print("è®­ç»ƒæ•°æ®å·²æ­£å¸¸ä¿å­˜ï¼Œå¯ç¨åæ‰‹åŠ¨ç”ŸæˆæŠ¥å‘Š")
    
    return results


def evaluate_single_model(algorithm: str, training_env: SingleAgentTrainingEnvironment, 
                         episode: int, num_eval_episodes: int = 5) -> Dict:
    """è¯„ä¼°å•æ™ºèƒ½ä½“æ¨¡å‹æ€§èƒ½ - ä¿®å¤ç‰ˆï¼Œé˜²æ­¢infå’Œnan"""
    import numpy as np
    
    eval_rewards = []
    eval_delays = []
    eval_completions = []
    
    def safe_value(value: float, default: float = 0.0, max_val: float = 1e6) -> float:
        """å®‰å…¨å¤„ç†æ•°å€¼ï¼Œé˜²æ­¢infå’Œnan"""
        if np.isnan(value) or np.isinf(value):
            return default
        return np.clip(value, -max_val, max_val)
    
    for _ in range(num_eval_episodes):
        state = training_env.reset_environment()
        episode_reward = 0.0
        episode_delay = 0.0
        episode_completion = 0.0
        steps = 0
        
        for step in range(50):  # è¾ƒçŸ­çš„è¯„ä¼°è½®æ¬¡
            if algorithm == "DQN":
                actions_result = training_env.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                action = training_env._encode_discrete_action(actions_dict)
            else:
                actions_result = training_env.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, tuple):  # PPOè¿”å›å…ƒç»„
                    actions_dict = actions_result[0]
                elif isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    actions_dict = {}
                action = training_env._encode_continuous_action(actions_dict)
            
            # è¯„ä¼°æ—¶ä¹Ÿä¼ å…¥åŠ¨ä½œå­—å…¸ï¼Œç¡®ä¿åå¥½ç”Ÿæ•ˆ
            next_state, reward, done, info = training_env.step(action, state, actions_dict)
            
            # å®‰å…¨å¤„ç†å¥–åŠ±å’ŒæŒ‡æ ‡
            safe_reward = safe_value(reward, -10.0, 120.0)
            episode_reward += safe_reward
            
            system_metrics = info['system_metrics']
            safe_delay = safe_value(system_metrics.get('avg_task_delay', 0), 0.0, 10.0)
            safe_completion = safe_value(system_metrics.get('task_completion_rate', 0), 0.0, 1.0)
            
            episode_delay += safe_delay
            episode_completion += safe_completion
            steps += 1
            
            state = next_state
            
            if done:
                break
        
        # å®‰å…¨è®¡ç®—å¹³å‡å€¼
        steps = max(1, steps)  # é˜²æ­¢é™¤é›¶
        eval_rewards.append(safe_value(episode_reward / steps, -20.0, 80.0))
        eval_delays.append(safe_value(episode_delay / steps, 0.0, 10.0))
        eval_completions.append(safe_value(episode_completion / steps, 0.0, 1.0))
    
    # å®‰å…¨è®¡ç®—æœ€ç»ˆç»“æœ
    if len(eval_rewards) == 0:
        return {'avg_reward': -1.0, 'avg_delay': 1.0, 'completion_rate': 0.0}
    
    avg_reward = safe_value(float(np.mean(eval_rewards)), -20.0, 80.0)
    avg_delay = safe_value(float(np.mean(eval_delays)), 0.0, 10.0)
    avg_completion = safe_value(float(np.mean(eval_completions)), 0.0, 1.0)
    
    return {
        'avg_reward': avg_reward,
        'avg_delay': avg_delay,
        'completion_rate': avg_completion
    }


def _calculate_stable_delay_average(training_env: SingleAgentTrainingEnvironment) -> float:
    """
    è®¡ç®—ç¨³å®šçš„æ—¶å»¶å¹³å‡å€¼ï¼Œé¿å…MovingAverage(100)çš„è®­ç»ƒæ³¢åŠ¨å½±å“
    
    ç­–ç•¥ï¼š
    1. ä¼˜å…ˆä½¿ç”¨episode_metricsä¸­çš„å®Œæ•´æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    2. ä½¿ç”¨å50%çš„æ•°æ®ï¼ˆæ’é™¤å‰æœŸå­¦ä¹ é˜¶æ®µï¼‰
    3. å¦‚æœæ•°æ®ä¸è¶³ï¼Œå›é€€åˆ°MovingAverage(100)
    
    Returns:
        float: ç¨³å®šçš„å¹³å‡æ—¶å»¶
    """
    # å°è¯•ä»episode_metricsè·å–å®Œæ•´æ—¶å»¶æ•°æ®
    if hasattr(training_env, 'episode_metrics') and 'avg_delay' in training_env.episode_metrics:
        delay_history = training_env.episode_metrics['avg_delay']
        
        if len(delay_history) >= 100:
            # ä½¿ç”¨å50%çš„æ•°æ®ï¼ˆæ›´æˆç†Ÿçš„ç­–ç•¥ï¼‰
            half_point = len(delay_history) // 2
            converged_delays = delay_history[half_point:]
            return float(np.mean(converged_delays))
        elif len(delay_history) >= 50:
            # å¦‚æœä¸è¶³100è½®ï¼Œä½¿ç”¨å30è½®
            return float(np.mean(delay_history[-30:]))
        elif len(delay_history) > 0:
            # æ•°æ®å¾ˆå°‘ï¼Œä½¿ç”¨å…¨éƒ¨
            return float(np.mean(delay_history))
    
    # å›é€€ï¼šä½¿ç”¨MovingAverage
    return training_env.performance_tracker['recent_delays'].get_average()


def _calculate_stable_completion_average(training_env: SingleAgentTrainingEnvironment) -> float:
    """
    è®¡ç®—ç¨³å®šçš„å®Œæˆç‡å¹³å‡å€¼
    
    Returns:
        float: ç¨³å®šçš„å¹³å‡å®Œæˆç‡
    """
    # å°è¯•ä»episode_metricsè·å–å®Œæ•´å®Œæˆç‡æ•°æ®
    if hasattr(training_env, 'episode_metrics') and 'task_completion_rate' in training_env.episode_metrics:
        completion_history = training_env.episode_metrics['task_completion_rate']
        
        if len(completion_history) >= 100:
            # ä½¿ç”¨å50%çš„æ•°æ®
            half_point = len(completion_history) // 2
            converged_completions = completion_history[half_point:]
            return float(np.mean(converged_completions))
        elif len(completion_history) >= 50:
            # å¦‚æœä¸è¶³100è½®ï¼Œä½¿ç”¨å30è½®
            return float(np.mean(completion_history[-30:]))
        elif len(completion_history) > 0:
            # æ•°æ®å¾ˆå°‘ï¼Œä½¿ç”¨å…¨éƒ¨
            return float(np.mean(completion_history))
    
    # å›é€€ï¼šä½¿ç”¨MovingAverage
    return training_env.performance_tracker['recent_completion'].get_average()


def save_single_training_results(algorithm: str, training_env: SingleAgentTrainingEnvironment, 
                                training_time: float,
                                override_scenario: Optional[Dict[str, Any]] = None) -> Dict:
    """ä¿å­˜è®­ç»ƒç»“æœ"""
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = generate_timestamp()
    
    # ğŸ”§ åŒæ—¶æä¾›Episodeæ€»å¥–åŠ±å’ŒPer-Stepå¹³å‡å¥–åŠ±
    recent_episode_reward = training_env.performance_tracker['recent_rewards'].get_average()
    
    # ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨å®é™…å¹³å‡æ­¥æ•°è®¡ç®— avg_step_reward
    if 'episode_steps' in training_env.episode_metrics and training_env.episode_metrics['episode_steps']:
        # ä½¿ç”¨æœ€è¿‘100ä¸ªepisodeçš„å¹³å‡æ­¥æ•°
        recent_steps = training_env.episode_metrics['episode_steps'][-100:]
        avg_steps_per_episode = sum(recent_steps) / len(recent_steps)
    else:
        # å›é€€åˆ°é…ç½®çš„é»˜è®¤å€¼
        avg_steps_per_episode = config.experiment.max_steps_per_episode
    
    avg_step_reward = recent_episode_reward / avg_steps_per_episode
    
    # è·å–ç½‘ç»œæ‹“æ‰‘ä¿¡æ¯
    num_vehicles = len(training_env.simulator.vehicles)
    num_rsus = len(training_env.simulator.rsus)
    num_uavs = len(training_env.simulator.uavs)
    state_dim = getattr(training_env.agent_env, 'state_dim', 'N/A')
    
    # ğŸ†• ä¿®å¤ï¼šæ”¶é›†å®Œæ•´çš„ç³»ç»Ÿé…ç½®å‚æ•°ï¼ˆç”¨äºHTMLæŠ¥å‘Šæ˜¾ç¤ºï¼‰
    # ç›´æ¥ä½¿ç”¨å·²å¯¼å…¥çš„configå¯¹è±¡
    
    results = {
        'algorithm': algorithm,
        'agent_type': 'single_agent',
        'timestamp': timestamp,
        'training_start_time': datetime.now().isoformat(),
        'network_topology': {
            'num_vehicles': num_vehicles,
            'num_rsus': num_rsus,
            'num_uavs': num_uavs,
        },
        'state_dim': state_dim,
        'override_scenario': override_scenario,
        'training_config': {
            'num_episodes': len(training_env.episode_rewards),
            'training_time_hours': training_time / 3600,
            'max_steps_per_episode': config.experiment.max_steps_per_episode
        },
        # ğŸ†• æ·»åŠ ç³»ç»Ÿé…ç½®å‚æ•°ï¼ˆHTMLæŠ¥å‘Šéœ€è¦ï¼‰
        'system_config': {
            'num_vehicles': num_vehicles,
            'num_rsus': num_rsus,
            'num_uavs': num_uavs,
            'simulation_time': config.simulation_time,
            'time_slot': config.time_slot,
            'device': str(config.device),
            'random_seed': config.random_seed,
        },
        # ğŸ†• æ·»åŠ ç½‘ç»œé…ç½®å‚æ•°
        'network_config': {
            'bandwidth': config.network.bandwidth,
            'carrier_frequency': config.communication.carrier_frequency,
            'coverage_radius': config.network.coverage_radius,
        },
        # ğŸ†• æ·»åŠ é€šä¿¡é…ç½®å‚æ•°
        'communication_config': {
            'vehicle_tx_power': config.communication.vehicle_tx_power,
            'rsu_tx_power': config.communication.rsu_tx_power,
            'uav_tx_power': config.communication.uav_tx_power,
            'antenna_gain_vehicle': config.communication.antenna_gain_vehicle,
            'antenna_gain_rsu': config.communication.antenna_gain_rsu,
            'antenna_gain_uav': config.communication.antenna_gain_uav,
        },
        # ğŸ†• æ·»åŠ è®¡ç®—èƒ½åŠ›å‚æ•°
        'compute_config': {
            'vehicle_cpu_freq': config.compute.vehicle_cpu_freq,
            'rsu_cpu_freq': config.compute.rsu_cpu_freq,
            'uav_cpu_freq': config.compute.uav_cpu_freq,
            'vehicle_memory': getattr(config.compute, 'vehicle_memory', 4e9),
            'rsu_memory': getattr(config.compute, 'rsu_memory', 32e9),
            'uav_memory': getattr(config.compute, 'uav_memory', 16e9),
            'vehicle_static_power': config.compute.vehicle_static_power,
            'rsu_static_power': config.compute.rsu_static_power,
            'uav_static_power': getattr(config.compute, 'uav_static_power', 20.0),
        },
        # ğŸ†• æ·»åŠ ä»»åŠ¡å’Œè¿ç§»å‚æ•°
        'task_migration_config': {
            'task_arrival_rate': config.task.arrival_rate,
            'task_size_mean': sum(config.task.data_size_range) / 2,
            'task_size_std': (config.task.data_size_range[1] - config.task.data_size_range[0]) / 4,
            'task_cpu_cycles_mean': sum(config.task.compute_cycles_range) / 2,
            'task_cpu_cycles_std': (config.task.compute_cycles_range[1] - config.task.compute_cycles_range[0]) / 4,
            'task_deadline_mean': sum(config.task.deadline_range) / 2,
            'cache_capacity_rsu': config.cache.rsu_cache_capacity,
            'cache_capacity_uav': config.cache.uav_cache_capacity,
            'migration_threshold': getattr(config.migration, 'threshold', 0.8),
        },
        'episode_rewards': training_env.episode_rewards,
        'episode_metrics': training_env.episode_metrics,
        'final_performance': {
            # æä¾›ä¸¤ç§å¥–åŠ±æŒ‡æ ‡ï¼Œç”¨é€”ä¸åŒ
            'avg_episode_reward': recent_episode_reward,  # Episodeæ€»å¥–åŠ±ï¼ˆè®­ç»ƒç›®æ ‡ï¼‰
            'avg_step_reward': avg_step_reward,           # æ¯æ­¥å¹³å‡å¥–åŠ±ï¼ˆå¯¹æ¯”è¯„ä¼°ï¼‰
            'avg_reward': avg_step_reward,  # å‘åå…¼å®¹ï¼šé»˜è®¤ä½¿ç”¨per-stepï¼ˆä¸å¯è§†åŒ–ä¸€è‡´ï¼‰
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ›´ç¨³å®šçš„å¹³å‡æ–¹æ³•ï¼Œé¿å…MovingAverage(100)çš„æ³¢åŠ¨å½±å“
            'avg_delay': _calculate_stable_delay_average(training_env),
            'avg_completion': _calculate_stable_completion_average(training_env)
        }
    }
    
    print(f"ğŸ“Š æ”¶é›†çš„é…ç½®å‚æ•°:")
    print(f"   ç³»ç»Ÿæ‹“æ‰‘: {num_vehicles}è½¦è¾†, {num_rsus}RSU, {num_uavs}UAV")
    print(f"   ç½‘ç»œé…ç½®: å¸¦å®½{config.network.bandwidth/1e6:.0f}MHz, é¢‘ç‡{config.communication.carrier_frequency/1e9:.1f}GHz")
    print(f"   ä»»åŠ¡å‚æ•°: åˆ°è¾¾ç‡{config.task.arrival_rate:.1f}, æ•°æ®é‡{sum(config.task.data_size_range)/2/1e6:.1f}MB")
    
    # ä½¿ç”¨æ—¶é—´æˆ³æ–‡ä»¶å
    filename = get_timestamped_filename("training_results")
    filepath = f"results/single_agent/{algorithm.lower()}/{filename}"
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ {algorithm}è®­ç»ƒç»“æœå·²ä¿å­˜åˆ° {filepath}")
    
    return results


def plot_single_training_curves(algorithm: str, training_env: SingleAgentTrainingEnvironment):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿ - ç®€æ´ä¼˜ç¾ç‰ˆ"""
    
    # ğŸ¨ ä½¿ç”¨æ–°çš„ç®€æ´å¯è§†åŒ–ç³»ç»Ÿ
    from visualization.clean_charts import create_training_chart, cleanup_old_charts, plot_objective_function_breakdown
    
    # åˆ›å»ºç®—æ³•ç›®å½•
    algorithm_dir = f"results/single_agent/{algorithm.lower()}"
    
    # æ¸…ç†æ—§çš„å†—ä½™å›¾è¡¨
    cleanup_old_charts(algorithm_dir)
    
    # ç”Ÿæˆæ ¸å¿ƒå›¾è¡¨
    chart_path = f"{algorithm_dir}/training_overview.png"
    create_training_chart(training_env, algorithm, chart_path)
    
    # ğŸ¯ ç”Ÿæˆç›®æ ‡å‡½æ•°åˆ†è§£å›¾ï¼ˆæ˜¾ç¤ºæ—¶å»¶ã€èƒ½è€—ä¸¤é¡¹æ ¸å¿ƒç›®æ ‡çš„æƒé‡è´¡çŒ®ï¼‰
    objective_path = f"{algorithm_dir}/objective_analysis.png"
    plot_objective_function_breakdown(training_env, algorithm, objective_path)
    
    print(f"ğŸ“ˆ {algorithm} è®­ç»ƒå¯è§†åŒ–å·²å®Œæˆ")
    print(f"   è®­ç»ƒæ€»è§ˆ: {chart_path}")
    print(f"   ç›®æ ‡åˆ†æ: {objective_path}")
    
    # ç”Ÿæˆè®­ç»ƒæ€»ç»“
    from visualization.clean_charts import get_summary_text
    summary = get_summary_text(training_env, algorithm)
    print(f"\n{summary}")


def compare_single_algorithms(algorithms: List[str], num_episodes: Optional[int] = None) -> Dict:
    """æ¯”è¾ƒå¤šä¸ªå•æ™ºèƒ½ä½“ç®—æ³•çš„æ€§èƒ½"""
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    if num_episodes is None:
        num_episodes = config.experiment.num_episodes
    
    print("\nğŸ”¥ å¼€å§‹å•æ™ºèƒ½ä½“ç®—æ³•æ€§èƒ½æ¯”è¾ƒ")
    print("=" * 60)
    
    results = {}
    
    # è®­ç»ƒæ‰€æœ‰ç®—æ³•
    for algorithm in algorithms:
        print(f"\nå¼€å§‹è®­ç»ƒ {algorithm}...")
        results[algorithm] = train_single_algorithm(algorithm, num_episodes)
    
    # ğŸ¨ ç”Ÿæˆç®€æ´çš„å¯¹æ¯”å›¾è¡¨
    from visualization.clean_charts import create_comparison_chart
    timestamp = generate_timestamp()
    comparison_chart_path = f"results/single_agent_comparison_{timestamp}.png" if timestamp else "results/single_agent_comparison.png"
    create_comparison_chart(results, comparison_chart_path)
    
    # ä¿å­˜æ¯”è¾ƒç»“æœ
    timestamp = generate_timestamp()
    comparison_results = {
        'algorithms': algorithms,
        'agent_type': 'single_agent',
        'num_episodes': num_episodes,
        'timestamp': timestamp,
        'comparison_time': datetime.now().isoformat(),
        'results': results,
        'summary': {}
    }
    
    # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
    for algorithm, result in results.items():
        final_perf = result['final_performance']
        comparison_results['summary'][algorithm] = {
            'final_avg_reward': final_perf['avg_reward'],
            'final_avg_delay': final_perf['avg_delay'],
            'final_completion_rate': final_perf['avg_completion'],
            'training_time_hours': result['training_config']['training_time_hours']
        }
    
    # ä½¿ç”¨æ—¶é—´æˆ³æ–‡ä»¶å
    comparison_filename = get_timestamped_filename("single_agent_comparison")
    with open(f"results/{comparison_filename}", "w", encoding="utf-8") as f:
        json.dump(comparison_results, f, indent=2, ensure_ascii=False)
    
    print("\nğŸ¯ å•æ™ºèƒ½ä½“ç®—æ³•æ¯”è¾ƒå®Œæˆï¼")
    print(f"ğŸ“„ æ¯”è¾ƒç»“æœå·²ä¿å­˜åˆ° results/{comparison_filename}")
    print(f"ğŸ“Š å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ° {comparison_chart_path}")
    
    return comparison_results




def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å•æ™ºèƒ½ä½“ç®—æ³•è®­ç»ƒè„šæœ¬')
    parser.add_argument('--algorithm', type=str, choices=['DDPG', 'TD3', 'TD3-LE', 'TD3_LE', 'TD3_LATENCY_ENERGY', 'DQN', 'PPO', 'SAC'],
                       help='é€‰æ‹©è®­ç»ƒç®—æ³•')
    parser.add_argument('--episodes', type=int, default=None, help=f'è®­ç»ƒè½®æ¬¡ (é»˜è®¤: {config.experiment.num_episodes})')
    parser.add_argument('--eval_interval', type=int, default=None, help=f'è¯„ä¼°é—´éš” (é»˜è®¤: {config.experiment.eval_interval})')
    parser.add_argument('--save_interval', type=int, default=None, help=f'ä¿å­˜é—´éš” (é»˜è®¤: {config.experiment.save_interval})')
    parser.add_argument('--compare', action='store_true', help='æ¯”è¾ƒæ‰€æœ‰ç®—æ³•')
    parser.add_argument('--seed', type=int, default=None, help='è¦†ç›–éšæœºç§å­ (é»˜è®¤è¯»å–configæˆ–ç¯å¢ƒå˜é‡)')
    parser.add_argument('--num-vehicles', type=int, default=None, help='è¦†ç›–è½¦è¾†æ•°é‡ç”¨äºå®éªŒ')
    # ğŸŒ å®æ—¶å¯è§†åŒ–å‚æ•°
    parser.add_argument('--realtime-vis', action='store_true', help='å¯ç”¨å®æ—¶å¯è§†åŒ–')
    parser.add_argument('--vis-port', type=int, default=5000, help='å®æ—¶å¯è§†åŒ–æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 5000)')
    # ğŸš€ å¢å¼ºç¼“å­˜å‚æ•°ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    parser.add_argument('--no-enhanced-cache', action='store_true', 
                       help='ç¦ç”¨å¢å¼ºç¼“å­˜ç³»ç»Ÿï¼ˆé»˜è®¤å¯ç”¨åˆ†å±‚L1/L2 + çƒ­åº¦ç­–ç•¥ + RSUåä½œï¼‰')
    
    args = parser.parse_args()

    if args.seed is not None:
        os.environ['RANDOM_SEED'] = str(args.seed)
        _apply_global_seed_from_env()

    # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®æ„å»ºoverride_scenarioå‚æ•°
    override_scenario = None
    if args.num_vehicles is not None:
        override_scenario = {
            "num_vehicles": args.num_vehicles,
        }
        # åŒæ—¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå‘åå…¼å®¹ï¼‰
        os.environ['TRAINING_SCENARIO_OVERRIDES'] = json.dumps(override_scenario)
        print(f"ğŸ“‹ è¦†ç›–å‚æ•°: è½¦è¾†æ•° = {args.num_vehicles}")
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs("results/single_agent", exist_ok=True)
    
    if args.compare:
        # æ¯”è¾ƒæ‰€æœ‰ç®—æ³•
        algorithms = ['DDPG', 'TD3', 'TD3-LE', 'DQN', 'PPO', 'SAC']
        compare_single_algorithms(algorithms, args.episodes)
    elif args.algorithm:
        # è®­ç»ƒå•ä¸ªç®—æ³• - ğŸ”§ ä¼ é€’override_scenarioå‚æ•°
        train_single_algorithm(
            args.algorithm, 
            args.episodes, 
            args.eval_interval, 
            args.save_interval,
            enable_realtime_vis=args.realtime_vis,
            vis_port=args.vis_port,
            override_scenario=override_scenario,  # ğŸ”§ æ–°å¢ï¼šä¼ é€’è¦†ç›–å‚æ•°
            use_enhanced_cache=not args.no_enhanced_cache  # ğŸš€ é»˜è®¤å¯ç”¨å¢å¼ºç¼“å­˜
        )
    else:
        print("è¯·æŒ‡å®š --algorithm æˆ–ä½¿ç”¨ --compare æ ‡å¿—")
        print("ä½¿ç”¨ python train_single_agent.py --help æŸ¥çœ‹å¸®åŠ©")


if __name__ == "__main__":
    main()
