"""
ğŸ¯ CAMTD3è®­ç»ƒè„šæœ¬ï¼ˆCache-Aware Migration with Twin Delayed DDPGï¼‰

ã€ç³»ç»Ÿæ¶æ„ã€‘
CAMTD3 = åŸºäºä¸­å¤®èµ„æºåˆ†é…çš„ç¼“å­˜æ„ŸçŸ¥ä»»åŠ¡è¿ç§»ç³»ç»Ÿ
â”œâ”€â”€ Phase 1: ä¸­å¤®æ™ºèƒ½ä½“èµ„æºåˆ†é…å†³ç­–ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
â”‚   â”œâ”€â”€ çŠ¶æ€ç©ºé—´: 80ç»´ï¼ˆè½¦è¾†+RSU+UAVå…¨å±€çŠ¶æ€ï¼‰
â”‚   â”œâ”€â”€ åŠ¨ä½œç©ºé—´: 30ç»´ï¼ˆå¸¦å®½+è®¡ç®—èµ„æºåˆ†é…å‘é‡ï¼‰
â”‚   â””â”€â”€ ç®—æ³•: TD3/SAC/DDPG/PPO
â”œâ”€â”€ Phase 2: æœ¬åœ°ä»»åŠ¡æ‰§è¡Œ
â”‚   â”œâ”€â”€ ç¼“å­˜å†³ç­–ï¼ˆCache-Awareï¼‰
â”‚   â”œâ”€â”€ ä»»åŠ¡è¿ç§»ï¼ˆMigrationï¼‰
â”‚   â””â”€â”€ ä»»åŠ¡è°ƒåº¦
python train_single_agent.py --algorithm OPTIMIZED_TD3 --episodes 1000 --num-vehicles 12 --seed 42

Queue-aware Replay
â€¢è®­ç»ƒæ•ˆç‡æå‡35%
â€¢å¿«é€Ÿå­¦ä¹ é«˜è´Ÿè½½åœºæ™¯
â€¢é’ˆå¯¹VECé˜Ÿåˆ—ç®¡ç†ç—›ç‚¹
GNN Attention
â€¢ç¼“å­˜å‘½ä¸­ç‡æå‡20%
â€¢æ™ºèƒ½å­¦ä¹ èŠ‚ç‚¹åä½œå…³ç³»
â€¢é€‚åº”åŠ¨æ€æ‹“æ‰‘å˜åŒ–

ã€ä½¿ç”¨æ–¹æ³•ã€‘
# CAMTD3æ ‡å‡†è®­ç»ƒï¼ˆé»˜è®¤æ¨¡å¼ï¼‰
python train_single_agent.py --algorithm TD3 --episodes 200
python train_single_agent.py --algorithm SAC --episodes 200

ğŸğŸ–¥ï¸ğŸ“š

å•æ™ºèƒ½ä½“ç®—æ³•è®­ç»ƒè„šæœ¬
æ”¯æŒDDPGã€TD3ã€TD3-LEã€DQNã€PPOã€SACç­‰ç®—æ³•çš„è®­ç»ƒå’Œæ¯”è¾ƒ
python train_single_agent.py --compare --episodes 200  # æ¯”è¾ƒæ‰€æœ‰ç®—æ³•
ğŸš€ å¢å¼ºç¼“å­˜æ¨¡å¼ (é»˜è®¤å¯ç”¨ - åˆ†å±‚L1/L2 + è‡ªé€‚åº”çƒ­åº¦ç­–ç•¥ + RSUåä½œ):
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 8
python train_single_agent.py --algorithm TD3 --episodes 1000 --num-vehicles 12
python train_single_agent.py --algorithm TD3 --episodes 800 --num-vehicles 12 --silent-mode  # é™é»˜ä¿å­˜ç»“æœ
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 16
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 20
python train_single_agent.py --algorithm TD3 --episodes 1600 --num-vehicles 24
python train_single_agent.py --algorithm TD3-LE --episodes 1600 --num-vehicles 12
python train_single_agent.py --algorithm SAC --episodes 800
python train_single_agent.py --algorithm PPO --episodes 800

ğŸŒ å®æ—¶å¯è§†åŒ–:
python train_single_agent.py --algorithm DDPG --episodes 100 --realtime-vis --vis-port 8080

ğŸ ç”Ÿæˆå­¦æœ¯å›¾è¡¨:
python generate_academic_charts.py results/single_agent/td3/training_results_20251007_220900.json

åˆ°è¾¾ç‡å¯¹æ¯”ï¼špython experiments/arrival_rate_analysis/run_td3_arrival_rate_sweep_silent.py --rates 1.0 1.5 2.0 2.5 3.0 3.5 --episodes 800


""" 
import os
import sys
import random

# ğŸ”§ ä¿®å¤Windowsç¼–ç é—®é¢˜
if sys.platform == 'win32':
    try:
        if hasattr(sys.stdout, 'reconfigure'):
            sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        elif hasattr(sys.stdout, 'buffer'):
            import io
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=True)
    except Exception:
        pass
    try:
        if hasattr(sys.stderr, 'reconfigure'):
            sys.stderr.reconfigure(encoding='utf-8', errors='replace')
        elif hasattr(sys.stderr, 'buffer'):
            import io
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace', line_buffering=True)
    except Exception:
        pass

import argparse
import json
from tools.fixed_topology_optimizer import FixedTopologyOptimizer
import numpy as np
import matplotlib.pyplot as plt
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from config import config
from evaluation.system_simulator import CompleteSystemSimulator
try:
    from evaluation.enhanced_system_simulator import EnhancedSystemSimulator
    ENHANCED_CACHE_AVAILABLE = True
except ImportError:
    ENHANCED_CACHE_AVAILABLE = False
    print("[Warning] Enhanced cache system not available, using standard simulator")
from utils import MovingAverage
from utils.normalization_utils import (
    normalize_distribution,
    normalize_feature_vector,
    normalize_ratio,
    normalize_scalar,
)
# ğŸ¤– å¯¼å…¥è‡ªé€‚åº”æ§åˆ¶ç»„ä»¶
from utils.adaptive_control import AdaptiveCacheController, AdaptiveMigrationController, map_agent_actions_to_params
from decision.strategy_coordinator import StrategyCoordinator
from utils.unified_reward_calculator import update_reward_targets, _general_reward_calculator

# å¯¼å…¥å„ç§å•æ™ºèƒ½ä½“ç®—æ³•
from single_agent.ddpg import DDPGEnvironment
from single_agent.td3 import TD3Environment
from single_agent.td3_hybrid_fusion import CAMTD3Environment
from single_agent.td3_latency_energy import TD3LatencyEnergyEnvironment
from single_agent.dqn import DQNEnvironment
from single_agent.ppo import PPOEnvironment
from single_agent.sac import SACEnvironment
from single_agent.optimized_td3_wrapper import OptimizedTD3Environment

# å¯¼å…¥HTMLæŠ¥å‘Šç”Ÿæˆå™¨
from utils.html_report_generator import HTMLReportGenerator

# ğŸŒ å¯¼å…¥å®æ—¶å¯è§†åŒ–æ¨¡å—
# try:
#     from scripts.visualize.realtime_visualization import create_visualizer
#     REALTIME_AVAILABLE = True
# except ImportError:
#     try:
#         from scripts.visualize.realtime_visualization_simple import create_visualizer
#         REALTIME_AVAILABLE = True
#     except ImportError:
#         REALTIME_AVAILABLE = False
#     print("âš ï¸  å®æ—¶å¯è§†åŒ–åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·è¿è¡Œ: pip install flask flask-socketio")
REALTIME_AVAILABLE = False

# å°è¯•å¯¼å…¥PyTorchä»¥è®¾ç½®éšæœºç§å­ï¼›å¦‚æœä¸å¯ç”¨åˆ™è·³è¿‡
try:
    import torch
except ImportError:  # pragma: no cover - å®¹é”™å¤„ç†
    torch = None


def _apply_global_seed_from_env():
    """æ ¹æ®ç¯å¢ƒå˜é‡RANDOM_SEEDè®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§"""
    seed_env = os.environ.get('RANDOM_SEED')
    if not seed_env:
        return
    try:
        seed = int(seed_env)
    except ValueError:
        print(f"âš ï¸  RANDOM_SEED ç¯å¢ƒå˜é‡æ— æ•ˆ: {seed_env}")
        return

    random.seed(seed)
    np.random.seed(seed)
    if torch is not None:
        torch.manual_seed(seed)
        if torch.cuda.is_available():  # pragma: no cover - GPUå¯é€‰
            torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True  # type: ignore[attr-defined]
        torch.backends.cudnn.benchmark = False  # type: ignore[attr-defined]
    config.random_seed = seed
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"ğŸ” å…¨å±€éšæœºç§å­å·²è®¾ç½®ä¸º {seed}")


def _maybe_apply_reward_smoothing_from_env():
    """Optionally enable reward smoothing via environment variables.

    RL_SMOOTH_DELAY, RL_SMOOTH_ENERGY, RL_SMOOTH_ALPHA can be provided.
    """
    try:
        d = os.environ.get('RL_SMOOTH_DELAY')
        e = os.environ.get('RL_SMOOTH_ENERGY')
        a = os.environ.get('RL_SMOOTH_ALPHA')
        if d is not None:
            setattr(config.rl, 'reward_smooth_delay_weight', float(d))
        if e is not None:
            setattr(config.rl, 'reward_smooth_energy_weight', float(e))
        if a is not None:
            setattr(config.rl, 'reward_smooth_alpha', float(a))
    except Exception:
        pass

def _build_scenario_config() -> Dict[str, Any]:
    """æ„å»ºæ¨¡æ‹Ÿç¯å¢ƒé…ç½®ï¼Œå…è®¸é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–é»˜è®¤å€¼"""
    # ğŸ”§ æ”¯æŒä»ç¯å¢ƒå˜é‡è¦†ç›–ä»»åŠ¡åˆ°è¾¾ç‡ï¼ˆç”¨äºå‚æ•°æ•æ„Ÿæ€§åˆ†æï¼‰
    task_arrival_rate = getattr(getattr(config, "task", None), "arrival_rate", 1.8)
    if os.environ.get('TASK_ARRIVAL_RATE'):
        try:
            task_arrival_rate = float(os.environ.get('TASK_ARRIVAL_RATE'))
            print(f"ğŸ”§ ä»ç¯å¢ƒå˜é‡è¦†ç›–ä»»åŠ¡åˆ°è¾¾ç‡: {task_arrival_rate} tasks/s")
        except ValueError:
            print(f"âš ï¸  ç¯å¢ƒå˜é‡TASK_ARRIVAL_RATEæ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼")

    def _get_or_default(obj: Optional[Any], attr: str, default: Any) -> Any:
        return getattr(obj, attr, default) if obj is not None else default

    network_cfg = getattr(config, "network", None)
    vehicle_cfg = getattr(network_cfg, "vehicle_config", {}) if network_cfg else {}
    rsu_cfg = getattr(network_cfg, "rsu_config", {}) if network_cfg else {}
    uav_cfg = getattr(network_cfg, "uav_config", {}) if network_cfg else {}
    comm_cfg = getattr(network_cfg, "communication_config", {}) if network_cfg else {}
    compute_cfg = getattr(config, "compute", None)
    service_cfg = getattr(config, "service", None)
    communication_cfg = getattr(config, "communication", None)

    def _normalize_bandwidth(value: Optional[float], fallback: float) -> float:
        if value is None:
            return fallback
        bw = float(value)
        if bw < 1e3:  # assume MHz â†’ Hz
            bw *= 1e6
        return bw

    scenario = {
        "num_vehicles": getattr(config, "num_vehicles", vehicle_cfg.get('num_vehicles', 12)),
        "num_rsus": getattr(config, "num_rsus", rsu_cfg.get('num_rsus', 4)),
        "num_uavs": getattr(config, "num_uavs", uav_cfg.get('num_uavs', 2)),
        "task_arrival_rate": task_arrival_rate,
        "time_slot": getattr(config, "time_slot", _get_or_default(network_cfg, 'time_slot_duration', 0.1)),
        "simulation_time": getattr(config, "simulation_time", 1000),
        "computation_capacity": float(vehicle_cfg.get('computation_capacity', 1000)),
        "bandwidth": _normalize_bandwidth(
            comm_cfg.get('bandwidth'),
            _get_or_default(communication_cfg, 'total_bandwidth', 50e6),
        ),
        "coverage_radius": float(rsu_cfg.get('coverage_radius', 300)),
        "cache_capacity": float(rsu_cfg.get('cache_capacity', 120)),
        "transmission_power": float(vehicle_cfg.get('transmission_power', 0.15)),
        "computation_power": float(_get_or_default(compute_cfg, 'vehicle_static_power', 1.2)),
        "thermal_noise_density": float(comm_cfg.get('thermal_noise_density', -174.0)),
        "noise_figure": float(_get_or_default(communication_cfg, 'noise_figure', 9.0)),
        "high_load_mode": getattr(getattr(config, "task", None), "high_load_mode", False),
        "task_complexity_multiplier": float(
            getattr(getattr(config, "task", None), "complexity_multiplier", 1.1)
        ),
        "rsu_load_divisor": float(_get_or_default(service_cfg, 'rsu_queue_boost_divisor', 4.0)),
        "uav_load_divisor": float(_get_or_default(service_cfg, 'uav_queue_boost_divisor', 2.0)),
        "enhanced_task_generation": True,
    }

    override_env = os.environ.get('TRAINING_SCENARIO_OVERRIDES')
    if override_env:
        try:
            overrides = json.loads(override_env)
            if isinstance(overrides, dict):
                scenario.update(overrides)
            else:
                print("âš ï¸  TRAINING_SCENARIO_OVERRIDES éœ€ä¸ºJSONå¯¹è±¡ï¼Œå·²å¿½ç•¥ã€‚")
        except json.JSONDecodeError as exc:
            print(f"âš ï¸  TRAINING_SCENARIO_OVERRIDES è§£æå¤±è´¥: {exc}")

    return scenario


_apply_global_seed_from_env()
_maybe_apply_reward_smoothing_from_env()


def generate_timestamp() -> str:
    """ç”Ÿæˆæ—¶é—´æˆ³"""
    if config.experiment.use_timestamp:
        return datetime.now().strftime(config.experiment.timestamp_format)
    else:
        return ""

def get_timestamped_filename(base_name: str, extension: str = ".json") -> str:
    """è·å–å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å"""
    timestamp = generate_timestamp()
    if timestamp:
        name_parts = base_name.split('.')
        if len(name_parts) > 1:
            base = '.'.join(name_parts[:-1])
            return f"{base}_{timestamp}{extension}"
        else:
            return f"{base_name}_{timestamp}{extension}"
    else:
        return f"{base_name}{extension}"


class SingleAgentTrainingEnvironment:
    """å•æ™ºèƒ½ä½“è®­ç»ƒç¯å¢ƒåŸºç±»"""
    
    def _apply_optimized_td3_defaults(self) -> None:
        """
        ã€åŠŸèƒ½ã€‘è‹¥å½“å‰ç®—æ³•ä¸ºOPTIMIZED_TD3ï¼Œåˆ™æ”¾å®½å¥–åŠ±æƒé‡ä¸ç›®æ ‡ï¼Œé™ä½è®­ç»ƒæŒ¯è¡ã€‚
        ã€è¯´æ˜ã€‘ä»…åœ¨æœªé€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–æ—¶ç”Ÿæ•ˆï¼Œç¡®ä¿å…¼å®¹è®ºæ–‡ç»Ÿä¸€å¥–åŠ±ã€‚
        """
        if not hasattr(self, 'algorithm') and hasattr(self, 'input_algorithm'):
            alg = str(self.input_algorithm).upper()
        else:
            alg = getattr(self, 'algorithm', '').upper()
        if alg != "OPTIMIZED_TD3":
            return
        rl = getattr(config, "rl", None)
        if rl is None:
            return

        overridden_keys = []

        def _set_if_absent(env_key: str, attr: str, value: float, use_max: bool = False) -> None:
            if os.environ.get(env_key) is not None:
                return
            current = float(getattr(rl, attr, 0.0) or 0.0)
            if use_max:
                if value > current:
                    setattr(rl, attr, value)
                    overridden_keys.append(f"{attr}={value} (max)")
            elif current == 0.0:
                setattr(rl, attr, value)
                overridden_keys.append(f"{attr}={value} (default)")

        def _force_override(env_key: str, attr: str, value: float) -> None:
            if os.environ.get(env_key) is not None:
                return
            setattr(rl, attr, float(value))
            overridden_keys.append(f"{attr}={value}")

        # ğŸš« ç¦ç”¨æ‰€æœ‰è¦†ç›–ï¼Œä½¿ç”¨system_config.pyä¸­çš„ä¼˜åŒ–æƒé‡
        # _force_override("RL_USE_DYNAMIC_REWARD_NORMALIZATION", "use_dynamic_reward_normalization", 0.0)
        # _force_override("RL_WEIGHT_LOSS_RATIO", "reward_weight_loss_ratio", 1.0)
        # _force_override("RL_WEIGHT_CACHE", "reward_weight_cache", 0.35)
        # _force_override("RL_WEIGHT_CACHE_BONUS", "reward_weight_cache_bonus", 0.8)
        # _force_override("RL_WEIGHT_CACHE_PRESSURE", "reward_weight_cache_pressure", 0.8)
        # _force_override("RL_WEIGHT_OFFLOAD_BONUS", "reward_weight_offload_bonus", 0.8)
        # _force_override("RL_WEIGHT_COMPLETION_GAP", "reward_weight_completion_gap", 0.95)
        # _force_override("RL_PENALTY_DROPPED", "reward_penalty_dropped", 0.35)
        # _force_override("RL_WEIGHT_QUEUE_OVERLOAD", "reward_weight_queue_overload", 1.2)
        # _force_override("RL_WEIGHT_REMOTE_REJECT", "reward_weight_remote_reject", 0.45)
        # _force_override("RL_LATENCY_TARGET", "latency_target", 2.5)
        # _force_override("RL_LATENCY_UPPER_TOL", "latency_upper_tolerance", 5.0)
        # _force_override("RL_ENERGY_TARGET", "energy_target", 20000.0)
        # _force_override("RL_ENERGY_UPPER_TOL", "energy_upper_tolerance", 35000.0)
        # _force_override("RL_SMOOTH_DELAY", "reward_smooth_delay_weight", 0.35)
        # _force_override("RL_SMOOTH_ENERGY", "reward_smooth_energy_weight", 0.45)
        # _force_override("RL_SMOOTH_ALPHA", "reward_smooth_alpha", 0.12)

        # ğŸš« ç¦ç”¨è¿™äº›è¦†ç›–ï¼Œä½¿ç”¨system_config.pyä¸­çš„ä¼˜åŒ–å€¼
        # _set_if_absent("RL_WEIGHT_COMPLETION_GAP", "reward_weight_completion_gap", 0.7)
        # _set_if_absent("RL_PENALTY_DROPPED", "reward_penalty_dropped", 0.15, use_max=True)
        # _set_if_absent("RL_WEIGHT_QUEUE_OVERLOAD", "reward_weight_queue_overload", 0.8, use_max=True)
        # _set_if_absent("RL_WEIGHT_REMOTE_REJECT", "reward_weight_remote_reject", 0.25, use_max=True)

        if overridden_keys:
            print(f"\nâš¡ OPTIMIZED_TD3 Configuration Overrides:")
            for k in overridden_keys:
                print(f"   - {k}")
            print("")

        # âœ… å¯ç”¨update_reward_targetsï¼Œä½¿ç”¨system_config.pyä¸­çš„ä¼˜åŒ–ç›®æ ‡å€¼
        # ç¡®ä¿å…¨å±€å•ä¾‹è®¡ç®—å™¨ä½¿ç”¨æ­£ç¡®çš„å½’ä¸€åŒ–ç›®æ ‡
        # ğŸ”§ 2024-12-02 æ¿€è¿›ç®€åŒ–ï¼šé™ä½å½’ä¸€åŒ–ç›®æ ‡ï¼Œå¢å¼ºæ ¸å¿ƒä¿¡å·
        try:
            update_reward_targets(
                latency_target=float(getattr(rl, "latency_target", 1.5)),
                energy_target=float(getattr(rl, "energy_target", 200.0)),
            )
        except Exception:
            pass

    def __init__(
        self,
        algorithm: str,
        override_scenario: Optional[Dict[str, Any]] = None,
        use_enhanced_cache: bool = False,
        disable_migration: bool = False,
        enforce_offload_mode: Optional[str] = None,
        fixed_offload_policy: Optional[str] = None,
        joint_controller: bool = False,
        simulation_only: bool = False,
    ):
        self.input_algorithm = algorithm
        self.simulation_only = simulation_only
        normalized_algorithm = algorithm.upper().replace('-', '_')
        alias_map = {
            "TD3LE": "TD3_LATENCY_ENERGY",
            "TD3_LE": "TD3_LATENCY_ENERGY",
            "TD3LATENCY": "TD3_LATENCY_ENERGY",
            "TD3_LATENCY": "TD3_LATENCY_ENERGY",
            "TD3_LATENCY_ENERGY": "TD3_LATENCY_ENERGY",
            "CAMTD3": "CAM_TD3",
            "CAM_TD3": "CAM_TD3",
            "HYBRID_EDGE-TD3": "CAM_TD3",
            "OPTIMIZEDTD3": "OPTIMIZED_TD3",
            "OPTIMIZED-TD3": "OPTIMIZED_TD3",
        }
        alias_key = normalized_algorithm.replace('_', '')
        self.algorithm = alias_map.get(normalized_algorithm, alias_map.get(alias_key, normalized_algorithm))
        self._apply_optimized_td3_defaults()
        scenario_config = _build_scenario_config()
        # åº”ç”¨å¤–éƒ¨è¦†ç›–
        central_env_value = os.environ.get('CENTRAL_RESOURCE', '')
        self.central_resource_enabled = central_env_value.strip() in {'1', 'true', 'True'}
        self.joint_controller = bool(joint_controller)
        if self.joint_controller and not self.central_resource_enabled:
            os.environ['CENTRAL_RESOURCE'] = '1'
            self.central_resource_enabled = True

        if override_scenario:
            scenario_config.update(override_scenario)
            scenario_config['override_topology'] = True
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šåŠ¨æ€ä¿®æ”¹å…¨å±€configä»¥æ”¯æŒå‚æ•°è¦†ç›–
            # åŸå› ï¼šNodeç±»ä½¿ç”¨å…¨å±€configè€Œéscenario_config
            network_cfg = getattr(config, "network", None)

            def _sync_topology(attr_name: str, component_attr: str, dict_key: str, value: int) -> None:
                setattr(config, attr_name, value)
                if network_cfg is not None:
                    setattr(network_cfg, attr_name, value)
                    component_cfg = getattr(network_cfg, component_attr, None)
                    if isinstance(component_cfg, dict):
                        component_cfg[dict_key] = value
            
            # æ‹“æ‰‘æ•°é‡å‚æ•°
            if 'num_vehicles' in override_scenario:
                num_vehicles_override = int(override_scenario['num_vehicles'])
                _sync_topology('num_vehicles', 'vehicle_config', 'num_vehicles', num_vehicles_override)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®è½¦è¾†æ•°é‡: {num_vehicles_override}")
            if 'num_rsus' in override_scenario:
                num_rsus_override = int(override_scenario['num_rsus'])
                _sync_topology('num_rsus', 'rsu_config', 'num_rsus', num_rsus_override)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®RSUæ•°é‡: {num_rsus_override}")
            if 'num_uavs' in override_scenario:
                num_uav_override = int(override_scenario['num_uavs'])
                _sync_topology('num_uavs', 'uav_config', 'num_uavs', num_uav_override)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®UAVæ•°é‡: {num_uav_override}")

            # å¸¦å®½å‚æ•°
            if 'bandwidth' in override_scenario or 'total_bandwidth' in override_scenario:
                bw_value = override_scenario.get('total_bandwidth') or override_scenario.get('bandwidth')
                if bw_value:
                    config.communication.total_bandwidth = float(bw_value)
                    network_comm_cfg = getattr(network_cfg, "communication_config", None)
                    if isinstance(network_comm_cfg, dict):
                        network_comm_cfg['bandwidth'] = float(bw_value)
                    print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®å¸¦å®½: {float(bw_value)/1e6:.1f} MHz")
            
            # ğŸ¯ æ€»èµ„æºæ± å‚æ•°ï¼ˆä¼˜å…ˆçº§é«˜äºå•èŠ‚ç‚¹é¢‘ç‡ï¼‰
            if 'total_vehicle_compute' in override_scenario:
                total_compute = float(override_scenario['total_vehicle_compute'])
                config.compute.total_vehicle_compute = total_compute
                # è‡ªåŠ¨è®¡ç®—æ¯è½¦å¹³å‡é¢‘ç‡
                avg_freq = total_compute / config.num_vehicles
                config.compute.vehicle_initial_freq = avg_freq
                config.compute.vehicle_default_freq = avg_freq
                config.compute.vehicle_cpu_freq = avg_freq
                config.compute.vehicle_cpu_freq_range = (avg_freq, avg_freq)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®æ€»æœ¬åœ°è®¡ç®—: {total_compute/1e9:.1f} GHz (æ¯è½¦{avg_freq/1e9:.3f} GHz)")
            
            if 'total_rsu_compute' in override_scenario:
                total_compute = float(override_scenario['total_rsu_compute'])
                config.compute.total_rsu_compute = total_compute
                avg_freq = total_compute / config.num_rsus
                config.compute.rsu_initial_freq = avg_freq
                config.compute.rsu_default_freq = avg_freq
                config.compute.rsu_cpu_freq = avg_freq
                config.compute.rsu_cpu_freq_range = (avg_freq, avg_freq)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®æ€»RSUè®¡ç®—: {total_compute/1e9:.1f} GHz (æ¯RSU{avg_freq/1e9:.1f} GHz)")
            
            if 'total_uav_compute' in override_scenario:
                total_compute = float(override_scenario['total_uav_compute'])
                config.compute.total_uav_compute = total_compute
                avg_freq = total_compute / config.num_uavs
                config.compute.uav_initial_freq = avg_freq
                config.compute.uav_default_freq = avg_freq
                config.compute.uav_cpu_freq = avg_freq
                config.compute.uav_cpu_freq_range = (avg_freq, avg_freq)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®æ€»UAVè®¡ç®—: {total_compute/1e9:.1f} GHz (æ¯UAV{avg_freq/1e9:.1f} GHz)")
            
            # CPUé¢‘ç‡å‚æ•°ï¼ˆå•èŠ‚ç‚¹é¢‘ç‡ï¼Œå…¼å®¹æ—§ä»£ç ï¼‰
            if 'vehicle_cpu_freq' in override_scenario and 'total_vehicle_compute' not in override_scenario:
                freq_value = override_scenario['vehicle_cpu_freq']
                # æ›´æ–°èŒƒå›´å’Œé»˜è®¤å€¼
                config.compute.vehicle_cpu_freq_range = (freq_value, freq_value)
                config.compute.vehicle_cpu_freq = freq_value
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®è½¦è¾†CPUé¢‘ç‡: {float(freq_value)/1e9:.2f} GHz")
            
            if 'rsu_cpu_freq' in override_scenario and 'total_rsu_compute' not in override_scenario:
                freq_value = override_scenario['rsu_cpu_freq']
                config.compute.rsu_cpu_freq_range = (freq_value, freq_value)
                config.compute.rsu_cpu_freq = freq_value
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®RSU CPUé¢‘ç‡: {float(freq_value)/1e9:.2f} GHz")
            
            if 'uav_cpu_freq' in override_scenario and 'total_uav_compute' not in override_scenario:
                freq_value = override_scenario['uav_cpu_freq']
                config.compute.uav_cpu_freq_range = (freq_value, freq_value)
                config.compute.uav_cpu_freq = freq_value
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®UAV CPUé¢‘ç‡: {float(freq_value)/1e9:.2f} GHz")
            
            # ä»»åŠ¡æ•°æ®å¤§å°å‚æ•°
            if 'task_data_size_min_kb' in override_scenario or 'task_data_size_max_kb' in override_scenario:
                min_kb = override_scenario.get('task_data_size_min_kb')
                max_kb = override_scenario.get('task_data_size_max_kb')
                if min_kb is not None and max_kb is not None:
                    # è½¬æ¢ä¸ºå­—èŠ‚
                    min_bytes = float(min_kb) * 1024
                    max_bytes = float(max_kb) * 1024
                    config.task.data_size_range = (min_bytes, max_bytes)
                    config.task.task_data_size_range = (min_bytes, max_bytes)
                    print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®ä»»åŠ¡æ•°æ®å¤§å°: {min_kb}-{max_kb} KB")
            
            # ä»»åŠ¡å¤æ‚åº¦å‚æ•°
            if 'task_complexity_multiplier' in override_scenario:
                multiplier = override_scenario['task_complexity_multiplier']
                # é€šè¿‡ç¯å¢ƒå˜é‡ä¼ é€’ç»™TaskConfig
                os.environ['TASK_COMPLEXITY_MULTIPLIER'] = str(multiplier)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®ä»»åŠ¡å¤æ‚åº¦å€æ•°: {multiplier}x")
            
            if 'task_compute_density' in override_scenario:
                density = override_scenario['task_compute_density']
                config.task.task_compute_density = float(density)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®ä»»åŠ¡è®¡ç®—å¯†åº¦: {density} cycles/bit")
            
            # ç¼“å­˜å®¹é‡å‚æ•°
            if 'cache_capacity' in override_scenario:
                capacity_mb = override_scenario['cache_capacity']
                # é€šè¿‡ç¯å¢ƒå˜é‡ä¼ é€’ï¼ˆå½±å“æ‰€æœ‰èŠ‚ç‚¹ï¼‰
                os.environ['CACHE_CAPACITY_MB'] = str(capacity_mb)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®ç¼“å­˜å®¹é‡: {capacity_mb} MB")

            # æœåŠ¡èƒ½åŠ›å‚æ•°
            if 'rsu_base_service' in override_scenario:
                value = int(override_scenario['rsu_base_service'])
                config.service.rsu_base_service = value
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®RSUåŸºç¡€æœåŠ¡èƒ½åŠ›: {value}")
            if 'rsu_max_service' in override_scenario:
                value = int(override_scenario['rsu_max_service'])
                config.service.rsu_max_service = value
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®RSUæœ€å¤§æœåŠ¡èƒ½åŠ›: {value}")
            if 'rsu_work_capacity' in override_scenario:
                value = float(override_scenario['rsu_work_capacity'])
                config.service.rsu_work_capacity = value
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®RSUå·¥ä½œå®¹é‡: {value}")
            if 'uav_base_service' in override_scenario:
                value = int(override_scenario['uav_base_service'])
                config.service.uav_base_service = value
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®UAVåŸºç¡€æœåŠ¡èƒ½åŠ›: {value}")
            if 'uav_max_service' in override_scenario:
                value = int(override_scenario['uav_max_service'])
                config.service.uav_max_service = value
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®UAVæœ€å¤§æœåŠ¡èƒ½åŠ›: {value}")
            if 'uav_work_capacity' in override_scenario:
                value = float(override_scenario['uav_work_capacity'])
                config.service.uav_work_capacity = value
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®UAVå·¥ä½œå®¹é‡: {value}")
            
            # ä»»åŠ¡åˆ°è¾¾ç‡å‚æ•°
            if 'task_arrival_rate' in override_scenario:
                arrival_rate = override_scenario['task_arrival_rate']
                config.task.arrival_rate = float(arrival_rate)
                # åŒæ—¶è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å…¼å®¹æ—§ä»£ç 
                os.environ['TASK_ARRIVAL_RATE'] = str(arrival_rate)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®ä»»åŠ¡åˆ°è¾¾ç‡: {arrival_rate} tasks/s")
            
            # å•ä¸€ä»»åŠ¡æ•°æ®å¤§å°å‚æ•°ï¼ˆç”¨äºæ··åˆè´Ÿè½½å®éªŒï¼‰
            if 'task_data_size_kb' in override_scenario:
                size_kb = override_scenario['task_data_size_kb']
                size_bytes = float(size_kb) * 1024
                config.task.data_size_range = (size_bytes, size_bytes)
                config.task.task_data_size_range = (size_bytes, size_bytes)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®ä»»åŠ¡æ•°æ®å¤§å°: {size_kb} KB")
            
            # é€šä¿¡å‚æ•°ï¼ˆå™ªå£°åŠŸç‡ã€è·¯å¾„æŸè€—ï¼‰
            if 'noise_power_dbm' in override_scenario:
                noise_power = override_scenario['noise_power_dbm']
                config.communication.noise_power_dbm = float(noise_power)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®å™ªå£°åŠŸç‡: {noise_power} dBm")
            
            if 'path_loss_exponent' in override_scenario:
                exponent = override_scenario['path_loss_exponent']
                config.communication.path_loss_exponent = float(exponent)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®è·¯å¾„æŸè€—æŒ‡æ•°: {exponent}")
            
            # èµ„æºå¼‚æ„æ€§å‚æ•°
            if 'heterogeneity_level' in override_scenario:
                hetero_level = override_scenario['heterogeneity_level']
                os.environ['HETEROGENEITY_LEVEL'] = str(hetero_level)
                print(f"ğŸ”§ [Override] åŠ¨æ€è®¾ç½®èµ„æºå¼‚æ„æ€§çº§åˆ«: {hetero_level}")
        
        mode_aliases = {
            'local': 'local_only',
            'local_only': 'local_only',
            'remote': 'remote_only',
            'remote_only': 'remote_only',
            '': ''
        }
        forced_mode_input = (
            enforce_offload_mode
            or scenario_config.get('forced_offload_mode')
            or os.environ.get('FORCE_OFFLOAD_MODE', '')
        )
        requested_mode = mode_aliases.get(str(forced_mode_input).strip().lower(), '')
        if requested_mode not in {'', 'local_only', 'remote_only'}:
            print(f"âš ï¸ æœªè¯†åˆ«çš„å¼ºåˆ¶å¸è½½æ¨¡å¼: {forced_mode_input}, å°†å¿½ç•¥ã€‚")
            requested_mode = ''
        self.enforce_offload_mode = requested_mode
        if self.enforce_offload_mode:
            scenario_config['forced_offload_mode'] = self.enforce_offload_mode
            if self.enforce_offload_mode == 'remote_only':
                scenario_config.setdefault('allow_local_processing', False)
            elif self.enforce_offload_mode == 'local_only':
                scenario_config.setdefault('allow_local_processing', True)

        if self.enforce_offload_mode == 'local_only':
            print("ğŸ§· å¼ºåˆ¶å¸è½½æ¨¡å¼: å…¨éƒ¨æœ¬åœ°å¤„ç†ï¼ˆLocal-Onlyï¼‰")
        elif self.enforce_offload_mode == 'remote_only':
            print("ğŸ§· å¼ºåˆ¶å¸è½½æ¨¡å¼: å…¨éƒ¨è¿œç«¯æ‰§è¡Œï¼ˆRemote-Onlyï¼‰")
        
        # ğŸ¯ å›ºå®šå¸è½½ç­–ç•¥åˆå§‹åŒ–
        self.fixed_offload_policy = None
        self.fixed_policy_name = None
        if fixed_offload_policy:
            try:
                import sys
                import importlib.util
                from pathlib import Path
                
                # åŠ¨æ€æ·»åŠ  experiments ç›®å½•åˆ° Python è·¯å¾„
                exp_path = Path(__file__).parent / 'experiments'
                if str(exp_path) not in sys.path:
                    sys.path.insert(0, str(exp_path))
                
                # ä½¿ç”¨ importlib åŠ¨æ€å¯¼å…¥æ¨¡å—ï¼ˆé¿å…é™æ€åˆ†æè­¦å‘Šï¼‰
                module_path = exp_path / 'fallback_baselines.py'
                if module_path.exists():
                    spec = importlib.util.spec_from_file_location("fallback_baselines", module_path)
                    if spec and spec.loader:
                        fallback_module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(fallback_module)
                        create_baseline_algorithm = fallback_module.create_baseline_algorithm
                    else:
                        raise ImportError(f"æ— æ³•åŠ è½½æ¨¡å— {module_path}")
                else:
                    raise ImportError(f"æ¨¡å—æ–‡ä»¶ä¸å­˜åœ¨: {module_path}")
                
                self.fixed_offload_policy = create_baseline_algorithm(fixed_offload_policy)
                self.fixed_policy_name = fixed_offload_policy
                print(f"ğŸ² å›ºå®šå¸è½½ç­–ç•¥: {fixed_offload_policy} (å¸è½½å†³ç­–ä¸ç”±æ™ºèƒ½ä½“å­¦ä¹ )")
                print(f"   å…¶ä»–å†³ç­–ï¼ˆç¼“å­˜ã€è¿ç§»ã€èµ„æºåˆ†é…ï¼‰ä»ç”±æ™ºèƒ½ä½“å­¦ä¹ ")
            except Exception as e:
                print(f"âš ï¸  æ— æ³•åˆ›å»ºå›ºå®šç­–ç•¥ '{fixed_offload_policy}': {e}")
                print(f"   å°†ä½¿ç”¨æ™ºèƒ½ä½“å­¦ä¹ å¸è½½å†³ç­–")
                self.fixed_offload_policy = None
        
        # é€‰æ‹©ä»¿çœŸå™¨ç±»å‹
        self.use_enhanced_cache = use_enhanced_cache and ENHANCED_CACHE_AVAILABLE
        env_disable_migration = os.environ.get("DISABLE_MIGRATION", "").strip() == "1"
        self.disable_migration = disable_migration or env_disable_migration
        if self.use_enhanced_cache:
            print("ğŸš€ [Training] Using Enhanced Cache System (Default) with:")
            print("   - Hierarchical L1/L2 caching (3GB + 7GB)")
            print("   - Adaptive HeatBasedCacheStrategy")
            print("   - Inter-RSU collaboration")
            self.simulator = EnhancedSystemSimulator(scenario_config)
        else:
            self.simulator = CompleteSystemSimulator(scenario_config)
        
        # ğŸ¤– åˆå§‹åŒ–è‡ªé€‚åº”æ§åˆ¶ç»„ä»¶
        self.adaptive_cache_controller = AdaptiveCacheController()
        self.adaptive_migration_controller = AdaptiveMigrationController()
        if self.disable_migration:
            print("ğŸ¤– è‡ªé€‚åº”ç¼“å­˜å·²å¯ç”¨ï¼›è¿ç§»æ§åˆ¶å·²ç¦ç”¨ï¼ˆDISABLE_MIGRATION æ¨¡å¼ï¼‰")
        else:
            print(f"ğŸ¤– å·²å¯ç”¨è‡ªé€‚åº”ç¼“å­˜å’Œè¿ç§»æ§åˆ¶åŠŸèƒ½")

        self.strategy_coordinator = StrategyCoordinator(
            self.adaptive_cache_controller,
            None if self.disable_migration else self.adaptive_migration_controller
        )
        self.strategy_coordinator.register_simulator(self.simulator)
        setattr(self.simulator, 'strategy_coordinator', self.strategy_coordinator)
        
        # ä»ä»¿çœŸå™¨è·å–å®é™…ç½‘ç»œæ‹“æ‰‘å‚æ•°
        num_vehicles = len(self.simulator.vehicles)
        num_rsus = len(self.simulator.rsus)
        num_uavs = len(self.simulator.uavs)
        self.num_vehicles = num_vehicles
        self.num_rsus = num_rsus
        self.num_uavs = num_uavs
        
        # ğŸ¯ æ›´æ–°å›ºå®šç­–ç•¥çš„ç¯å¢ƒä¿¡æ¯
        if self.fixed_offload_policy is not None:
            try:
                # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„ç¯å¢ƒå¯¹è±¡ä¾›å›ºå®šç­–ç•¥ä½¿ç”¨
                class SimpleEnv:
                    def __init__(self, simulator):
                        self.simulator = simulator
                        self.agent_env = type('obj', (object,), {
                            'action_dim': 18,  # é»˜è®¤actionç»´åº¦
                        })()
                
                simple_env = SimpleEnv(self.simulator)
                self.fixed_offload_policy.update_environment(simple_env)
                print(f"   å›ºå®šç­–ç•¥å·²æ›´æ–°ç¯å¢ƒä¿¡æ¯: {num_vehicles}è½¦è¾†, {num_rsus}RSU, {num_uavs}UAV")
            except Exception as e:
                print(f"âš ï¸  å›ºå®šç­–ç•¥æ›´æ–°ç¯å¢ƒå¤±è´¥: {e}")
        
        # åº”ç”¨å›ºå®šæ‹“æ‰‘çš„å‚æ•°ä¼˜åŒ–ï¼ˆä¿æŒ4 RSU + 2 UAVï¼‰
        if self.algorithm in {"TD3", "TD3_LATENCY_ENERGY"}:
            topology_optimizer = FixedTopologyOptimizer()
            opt_params = topology_optimizer.get_optimized_params(num_vehicles)
            
            # åº”ç”¨ä¼˜åŒ–çš„è¶…å‚æ•°åˆ°TD3é…ç½®
            os.environ['TD3_HIDDEN_DIM'] = str(opt_params.get('hidden_dim', 400))
            os.environ['TD3_ACTOR_LR'] = str(opt_params.get('actor_lr', 1e-4))
            os.environ['TD3_CRITIC_LR'] = str(opt_params.get('critic_lr', 8e-5))
            os.environ['TD3_BATCH_SIZE'] = str(opt_params.get('batch_size', 256))
            
            print(f"[FIXED-TOPOLOGY] è½¦è¾†æ•°:{num_vehicles} â†’ Hidden:{opt_params['hidden_dim']}, LR:{opt_params['actor_lr']:.1e}, Batch:{opt_params['batch_size']}")
            print(f"[FIXED-TOPOLOGY] ä¿æŒå›ºå®š: RSU=4, UAV=2ï¼ˆéªŒè¯ç®—æ³•ç­–ç•¥æœ‰æ•ˆæ€§ï¼‰")
        
        # ğŸ”§ ä¼˜åŒ–ï¼šæ‰€æœ‰ç®—æ³•ç»Ÿä¸€ä¼ å…¥æ‹“æ‰‘å‚æ•°ï¼Œå®ç°åŠ¨æ€é€‚é…
        if self.algorithm == "DDPG":
            self.agent_env = DDPGEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "TD3":
            self.agent_env = TD3Environment(
                num_vehicles,
                num_rsus,
                num_uavs,
                use_central_resource=self.central_resource_enabled,
            )
        elif self.algorithm == "TD3_LATENCY_ENERGY":
            self.agent_env = TD3LatencyEnergyEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "CAM_TD3":
            self.agent_env = CAMTD3Environment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "DQN":
            self.agent_env = DQNEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "PPO":
            self.agent_env = PPOEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "SAC":
            self.agent_env = SACEnvironment(num_vehicles, num_rsus, num_uavs)
        elif self.algorithm == "OPTIMIZED_TD3":
            self.agent_env = OptimizedTD3Environment(
                num_vehicles,
                num_rsus,
                num_uavs,
                use_central_resource=self.central_resource_enabled,
                simulation_only=self.simulation_only
            )
            if not self.simulation_only:
                print(f"[OptimizedTD3] ä½¿ç”¨ç²¾ç®€ä¼˜åŒ–é…ç½® (Queue-aware Replay + GNN Attention)")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")

        # ğŸ¯ ä¸­å¤®èµ„æºåˆ†é…æ¨¡å¼æ—¥å¿—
        import sys
        print(f"\n[èµ„æºåˆ†é…æ¨¡å¼æ£€æŸ¥]", file=sys.stderr)
        print(f"  CENTRAL_RESOURCE ç¯å¢ƒå˜é‡: '{central_env_value}'", file=sys.stderr)
        print(f"  use_central_resource: {self.central_resource_enabled}", file=sys.stderr)
        
        self.central_resource_action_dim = getattr(self.agent_env, 'central_resource_action_dim', 0)
        self.central_resource_state_dim = getattr(self.agent_env, 'central_state_dim', 0)
        self.base_action_dim = getattr(self.agent_env, 'base_action_dim', getattr(self.agent_env, 'action_dim', 0) - self.central_resource_action_dim)
        
        if self.central_resource_enabled and self.central_resource_action_dim > 0:
            print(f"âœ… å¯ç”¨ä¸­å¤®èµ„æºåˆ†é…æ¶æ„ï¼šPhase 1(å†³ç­–) + Phase 2(æ‰§è¡Œ)", file=sys.stderr)
            print(f"   ç¯å¢ƒç±»å‹: {type(self.agent_env).__name__}", file=sys.stderr)
            print(f"   åŸºç¡€åŠ¨ä½œç»´åº¦: {self.base_action_dim}", file=sys.stderr)
            print(f"   ä¸­å¤®èµ„æºåŠ¨ä½œç»´åº¦: {self.central_resource_action_dim}", file=sys.stderr)
            if self.central_resource_state_dim:
                print(f"   çŠ¶æ€æ‰©å±•ç»´åº¦: +{self.central_resource_state_dim}", file=sys.stderr)
        else:
            print(f"  ä½¿ç”¨æ ‡å‡†æ¨¡å¼ï¼ˆå‡åŒ€èµ„æºåˆ†é…ï¼‰", file=sys.stderr)
        
        # ğŸ§  è‹¥æŒ‡å®šäº†é˜¶æ®µä¸€ç®—æ³•ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ï¼‰ï¼Œç”¨DualStageå°è£…å™¨ç»„åˆä¸¤ä¸ªé˜¶æ®µ
        stage1_alg = os.environ.get('STAGE1_ALG', '').strip().lower()
        if stage1_alg:
            try:
                from single_agent.dual_stage_controller import DualStageControllerEnv
                self.agent_env = DualStageControllerEnv(self.agent_env, self.simulator, stage1_strategy=stage1_alg)
                print(f"ğŸ§  å¯ç”¨ä¸¤é˜¶æ®µæ§åˆ¶ï¼šStage1={stage1_alg} + Stage2={self.algorithm}")
                # Two-stage planner inside simulator becomes redundant
                os.environ['TWO_STAGE_MODE'] = '0'
            except Exception as e:
                print(f"âš ï¸ ä¸¤é˜¶æ®µæ§åˆ¶å°è£…å¤±è´¥ï¼Œå›é€€åˆ°å•ç®—æ³•: {e}")
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_losses = {}
        self.episode_metrics = {
            'avg_delay': [],
            'total_energy': [],
            'data_loss_bytes': [],
            'data_loss_ratio_bytes': [],
            'task_completion_rate': [],
            'cache_hit_rate': [],
            'cache_utilization': [],
            'cache_evictions': [],
            'cache_eviction_rate': [],
            'cache_requests': [],
            'cache_collaborative_writes': [],
            'local_cache_hits': [],
            'migration_avg_cost': [],
            'migration_avg_delay_saved': [],
            'migration_success_rate': [],
            'queue_rho_sum': [],
            'queue_rho_max': [],
            'queue_overload_flag': [],
            'queue_overload_events': [],
            'episode_steps': [],  # ğŸ”§ æ–°å¢ï¼šè®°å½•æ¯ä¸ªepisodeçš„å®é™…æ­¥æ•°
            'task_type_queue_share_1': [],
            'task_type_queue_share_2': [],
            'task_type_queue_share_3': [],
            'task_type_queue_share_4': [],
            'task_type_deadline_norm_1': [],
            'task_type_deadline_norm_2': [],
            'task_type_deadline_norm_3': [],
            'task_type_deadline_norm_4': [],
            'task_type_drop_rate_1': [],
            'task_type_drop_rate_2': [],
            'task_type_drop_rate_3': [],
            'task_type_drop_rate_4': [],
            'task_type_queue_share_ep_1': [],
            'task_type_queue_share_ep_2': [],
            'task_type_queue_share_ep_3': [],
            'task_type_queue_share_ep_4': [],
            'rsu_hotspot_mean': [],
            'rsu_hotspot_peak': [],
            'rsu_hotspot_mean_series': [],
            'rsu_hotspot_peak_series': [],
            'mm1_queue_error': [],
            'mm1_delay_error': [],
            'normalized_delay': [],
            'normalized_energy': [],
            'normalized_reward': []
        }
        
        # æ€§èƒ½è¿½è¸ªå™¨
        self.performance_tracker = {
            'recent_rewards': MovingAverage(100),
            'recent_step_rewards': MovingAverage(100),
            'recent_delays': MovingAverage(100),
            'recent_energy': MovingAverage(100),
            'recent_completion': MovingAverage(100)
        }
        self._reward_baseline: Dict[str, float] = {}
        self._energy_target_per_vehicle = float(os.environ.get('ENERGY_TARGET_PER_VEHICLE', '75.0'))  # ğŸ”§ 220 â†’ 75 (ä½¿å¯å‘å¼ç›®æ ‡ = 75Ã—12 = 900J)
        self._dynamic_energy_target = float(getattr(config.rl, 'energy_target', 1200.0))
        heuristic_energy_target = max(
            self._dynamic_energy_target,
            self.num_vehicles * self._energy_target_per_vehicle
        )
        if heuristic_energy_target > self._dynamic_energy_target * 1.05:
            self._dynamic_energy_target = heuristic_energy_target
            update_reward_targets(energy_target=heuristic_energy_target)
            print(
                f"âš–ï¸ åŠ¨æ€è°ƒæ•´èƒ½è€—ç›®æ ‡: {heuristic_energy_target:.1f}J "
                f"(è½¦è¾†æ•°={self.num_vehicles}, æ¯è½¦é¢„ç®—={self._energy_target_per_vehicle:.1f}J)"
            )
        self._energy_target_ema = self._dynamic_energy_target
        self._energy_target_warmup = max(40, int(config.experiment.num_episodes * 0.1))
        self._last_energy_target_update = 0
        self._reward_smoothing_alpha = float(getattr(config.rl, 'reward_smooth_alpha', 0.35))
        self._reward_ema_delay: Optional[float] = None
        self._reward_ema_energy: Optional[float] = None
        self._episode_counters_initialized = False
        
        print(f"âœ“ {self.algorithm}è®­ç»ƒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ ç®—æ³•ç±»å‹: å•æ™ºèƒ½ä½“")
        

    
    def _calculate_correct_cache_utilization(self, cache: Dict, cache_capacity_mb: float) -> float:
        """
        ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—ç¼“å­˜åˆ©ç”¨ç‡
        
        Args:
            cache: ç¼“å­˜å­—å…¸
            cache_capacity_mb: ç¼“å­˜å®¹é‡(MB)
        Returns:
            ç¼“å­˜åˆ©ç”¨ç‡ [0.0, 1.0]
        """
        if not cache or cache_capacity_mb <= 0:
            return 0.0
        
        total_used_mb = 0.0
        for item in cache.values():
            if isinstance(item, dict) and 'size' in item:
                total_used_mb += float(item.get('size', 0.0))
            else:
                # å…¼å®¹æ—§æ ¼å¼ï¼Œä½¿ç”¨realisticå¤§å°
                total_used_mb += 1.0  # é»˜è®¤1MB
        
        utilization = total_used_mb / cache_capacity_mb
        return min(1.0, max(0.0, utilization))
    
    def _initialize_episode_counters(self, stats: Optional[Dict[str, Any]] = None) -> None:
        """Reset per-episode baseline counters to avoid carrying over cumulative stats."""
        stats_dict: Dict[str, Any]
        if stats is None:
            stats_dict = {}
        else:
            try:
                stats_dict = dict(stats)
            except Exception:
                stats_dict = {}

        self._episode_energy_base = float(stats_dict.get('total_energy', 0.0) or 0.0)
        self._episode_processed_base = int(stats_dict.get('processed_tasks', 0) or 0)
        self._episode_dropped_base = int(stats_dict.get('dropped_tasks', 0) or 0)
        self._episode_generated_bytes_base = float(stats_dict.get('generated_data_bytes', 0.0) or 0.0)
        self._episode_dropped_bytes_base = float(stats_dict.get('dropped_data_bytes', 0.0) or 0.0)
        remote_stats = stats_dict.get('remote_rejections', {})
        if isinstance(remote_stats, dict):
            self._episode_remote_reject_base = int(remote_stats.get('total', 0) or 0)
        else:
            self._episode_remote_reject_base = 0

        # Cache controllers keep their own cumulative counters; snapshot them as the new baseline
        if hasattr(self, 'adaptive_cache_controller'):
            cache_metrics = self.adaptive_cache_controller.get_cache_metrics()
            self._episode_cache_requests_base = int(cache_metrics.get('total_requests', 0) or 0)
            self._episode_cache_evictions_base = int(cache_metrics.get('evicted_items', 0) or 0)
            self._episode_cache_collab_base = int(cache_metrics.get('collaborative_writes', 0) or 0)
        else:
            self._episode_cache_requests_base = 0
            self._episode_cache_evictions_base = 0
            self._episode_cache_collab_base = 0

        self._episode_queue_overload_events_base = int(stats_dict.get('queue_overload_events', 0) or 0)
        delay_buckets = ('delay_processing', 'delay_uplink', 'delay_downlink', 'delay_cache', 'delay_waiting')
        energy_buckets = ('energy_compute', 'energy_transmit_uplink', 'energy_transmit_downlink', 'energy_cache')
        self._episode_delay_component_base = {
            bucket: float(stats_dict.get(bucket, 0.0) or 0.0) for bucket in delay_buckets
        }
        self._episode_energy_component_base = {
            bucket: float(stats_dict.get(bucket, 0.0) or 0.0) for bucket in energy_buckets
        }
        self._episode_queue_overflow_base = int(stats_dict.get('queue_overflow_drops', 0) or 0)
        self._episode_counters_initialized = True

    def _reset_reward_baseline(self, stats: Optional[Dict[str, Any]] = None) -> None:
        """åˆå§‹åŒ–/é‡ç½®å¥–åŠ±å¢é‡åŸºçº¿ã€‚"""
        base = stats or {}
        self._reward_baseline = {
            'processed': int(base.get('processed_tasks', 0) or 0),
            'dropped': int(base.get('dropped_tasks', 0) or 0),
            'delay': float(base.get('total_delay', 0.0) or 0.0),
            'energy': float(base.get('total_energy', 0.0) or 0.0),
            'generated_bytes': float(base.get('generated_data_bytes', 0.0) or 0.0),
            'dropped_bytes': float(base.get('dropped_data_bytes', 0.0) or 0.0),
        }
        self._reward_ema_delay = None
        self._reward_ema_energy = None

    def _build_reward_snapshot(self, stats: Dict[str, Any]) -> Dict[str, float]:
        """åŸºäºç´¯è®¡ç»Ÿè®¡è®¡ç®—å•æ­¥å¥–åŠ±æ‰€éœ€çš„å¢é‡æŒ‡æ ‡ã€‚"""
        baseline = getattr(self, '_reward_baseline', None) or {
            'processed': 0,
            'dropped': 0,
            'delay': 0.0,
            'energy': 0.0,
            'generated_bytes': 0.0,
            'dropped_bytes': 0.0,
        }

        total_processed = int(stats.get('processed_tasks', 0) or 0)
        total_dropped = int(stats.get('dropped_tasks', 0) or 0)
        total_delay = float(stats.get('total_delay', 0.0) or 0.0)
        total_energy = float(stats.get('total_energy', 0.0) or 0.0)
        total_generated = float(stats.get('generated_data_bytes', 0.0) or 0.0)
        total_dropped_bytes = float(stats.get('dropped_data_bytes', 0.0) or 0.0)

        delta_processed = max(0, total_processed - baseline['processed'])
        delta_dropped = max(0, total_dropped - baseline['dropped'])
        delta_delay = max(0.0, total_delay - baseline['delay'])
        delta_energy = max(0.0, total_energy - baseline['energy'])
        
        # ğŸ”§ ä¿®å¤ï¼šå‡å»é™æ€èƒ½è€—ï¼Œåªå¥–åŠ±åŠ¨æ€èƒ½è€—
        # é™æ€åŠŸç‡ = RSUé™æ€ * num_rsus + UAVé™æ€ * num_uavs
        rsu_static = getattr(config.compute, 'rsu_static_power', 25.0)
        uav_static = getattr(config.compute, 'uav_static_power', 2.5)
        static_power = (self.num_rsus * rsu_static) + (self.num_uavs * uav_static)
        time_slot = getattr(config.experiment, 'time_slot', 0.1)
        static_energy_step = static_power * time_slot
        
        # ç¡®ä¿ä¸å‡æˆè´Ÿæ•°
        dynamic_delta_energy = max(0.0, delta_energy - static_energy_step)
        
        delta_generated = max(0.0, total_generated - baseline['generated_bytes'])
        delta_loss_bytes = max(0.0, total_dropped_bytes - baseline['dropped_bytes'])

        if delta_processed > 0:
            avg_delay_for_reward = delta_delay / delta_processed
        else:
            avg_delay_for_reward = 0.0

        completion_total = delta_processed + delta_dropped
        completion_rate = normalize_ratio(delta_processed, completion_total, default=1.0)
        loss_ratio = normalize_ratio(delta_loss_bytes, delta_generated)
        # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨delta_energyï¼Œç§»é™¤å¹³æ»‘å’Œå›é€€é€»è¾‘
        # ä¹‹å‰çš„å›é€€å¯¼è‡´åœ¨æ— ä»»åŠ¡å¤„ç†çš„stepä½¿ç”¨äº†ç´¯ç§¯èƒ½è€—ï¼ˆ~900Jï¼‰ï¼Œå¯¼è‡´å¥–åŠ±å´©å¡Œ
        reward_snapshot = {
            'avg_task_delay': avg_delay_for_reward,
            'total_energy_consumption': dynamic_delta_energy,
            'dropped_tasks': delta_dropped,
            'task_completion_rate': completion_rate,
            'data_loss_bytes': delta_loss_bytes,
            'data_loss_ratio_bytes': loss_ratio,
        }

        self._reward_baseline = {
            'processed': total_processed,
            'dropped': total_dropped,
            'delay': total_delay,
            'energy': total_energy,
            'generated_bytes': total_generated,
            'dropped_bytes': total_dropped_bytes,
        }

        return reward_snapshot

    def _apply_reward_smoothing(self, delay_value: float, energy_per_task: float) -> Tuple[float, float]:
        """å¯¹å¥–åŠ±å…³é”®æŒ‡æ ‡è¿›è¡ŒæŒ‡æ•°å¹³æ»‘ï¼Œå‡å°TD3è®­ç»ƒå™ªå£°ã€‚"""
        if self._reward_smoothing_alpha <= 0.0:
            return delay_value, energy_per_task
        alpha = self._reward_smoothing_alpha
        if self._reward_ema_delay is None:
            self._reward_ema_delay = delay_value
        else:
            self._reward_ema_delay = (1.0 - alpha) * self._reward_ema_delay + alpha * delay_value
        if self._reward_ema_energy is None:
            self._reward_ema_energy = energy_per_task
        else:
            self._reward_ema_energy = (1.0 - alpha) * self._reward_ema_energy + alpha * energy_per_task
        return self._reward_ema_delay, self._reward_ema_energy

    def _maybe_update_dynamic_energy_target(self, episode: int, episode_energy: float) -> None:
        """æ ¹æ®å®é™…èƒ½è€—è‡ªåŠ¨æ”¾å®½ç›®æ ‡ï¼Œé¿å…ä¸å¯è¾¾çº¦æŸå¯¼è‡´æŒ¯è¡ã€‚"""
        if episode_energy <= 0:
            return
        decay = 0.9
        self._energy_target_ema = decay * self._energy_target_ema + (1.0 - decay) * episode_energy
        if episode < self._energy_target_warmup:
            return
        if episode - self._last_energy_target_update < 5:
            return
        target = self._dynamic_energy_target
        ema = self._energy_target_ema
        if ema > target * 1.2:
            new_target = min(ema * 0.95, target * 1.8)
            self._dynamic_energy_target = new_target
            self._last_energy_target_update = episode
            update_reward_targets(energy_target=new_target)
            print(
                f"âš™ï¸ èƒ½è€—EMA {ema:.1f}J è¶…è¿‡ç›®æ ‡ {target:.1f}Jï¼Œ"
                f"è‡ªåŠ¨ä¸Šè°ƒå¥–åŠ±é˜ˆå€¼ -> {new_target:.1f}J (Episode {episode})"
            )

    def reset_environment(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒå¹¶è¿”å›åˆå§‹çŠ¶æ€"""
        # é‡ç½®ä»¿çœŸå™¨çŠ¶æ€
        self._episode_counters_initialized = False
        self.simulator._setup_scenario()
        
        # æ”¶é›†ç³»ç»ŸçŠ¶æ€
        node_states = {}
        
        # è½¦è¾†çŠ¶æ€ï¼ˆä¸stepä¿æŒä¸€è‡´çš„å½’ä¸€åŒ–æ–¹å¼ï¼‰
        for i, vehicle in enumerate(self.simulator.vehicles):
            vehicle_state = np.array([
                normalize_scalar(vehicle['position'][0], 'vehicle_position_range', 1000.0),
                normalize_scalar(vehicle['position'][1], 'vehicle_position_range', 1000.0),
                normalize_scalar(vehicle.get('velocity', 0.0), 'vehicle_speed_range', 50.0),
                normalize_scalar(len(vehicle.get('tasks', [])), 'vehicle_queue_capacity', 20.0),
                normalize_scalar(vehicle.get('energy_consumed', 0.0), 'vehicle_energy_reference', 1000.0),
            ])
            node_states[f'vehicle_{i}'] = vehicle_state

        # RSUçŠ¶æ€ï¼ˆç»Ÿä¸€å½’ä¸€åŒ–/è£å‰ªï¼‰
        for i, rsu in enumerate(self.simulator.rsus):
            rsu_state = np.array([
                normalize_scalar(rsu['position'][0], 'rsu_position_range', 1000.0),
                normalize_scalar(rsu['position'][1], 'rsu_position_range', 1000.0),
                self._calculate_correct_cache_utilization(rsu.get('cache', {}), rsu.get('cache_capacity', 1000.0)),
                normalize_scalar(len(rsu.get('computation_queue', [])), 'rsu_queue_capacity', 20.0),
                normalize_scalar(rsu.get('energy_consumed', 0.0), 'rsu_energy_reference', 1000.0),
            ])
            node_states[f'rsu_{i}'] = rsu_state

        # UAVçŠ¶æ€ï¼ˆç»Ÿä¸€å½’ä¸€åŒ–/è£å‰ªï¼‰
        for i, uav in enumerate(self.simulator.uavs):
            uav_state = np.array([
                normalize_scalar(uav['position'][0], 'uav_position_range', 1000.0),
                normalize_scalar(uav['position'][1], 'uav_position_range', 1000.0),
                normalize_scalar(uav['position'][2], 'uav_altitude_range', 200.0),
                self._calculate_correct_cache_utilization(uav.get('cache', {}), uav.get('cache_capacity', 200.0)),
                normalize_scalar(uav.get('energy_consumed', 0.0), 'uav_energy_reference', 1000.0),
            ])
            node_states[f'uav_{i}'] = uav_state
        
        # åˆå§‹ç³»ç»ŸæŒ‡æ ‡
        system_metrics = {
            'avg_task_delay': 0.0,
            'total_energy_consumption': 0.0,
            'data_loss_bytes': 0.0,
            'data_loss_ratio_bytes': 0.0,
            'cache_hit_rate': 0.0,
            'migration_success_rate': 0.0
        }
        
        # ğŸ”§ ä¿®å¤ï¼šé‡ç½®èƒ½è€—è¿½è¸ªå™¨ï¼Œé¿å…è·¨episodeç´¯ç§¯
        if hasattr(self, '_last_total_energy'):
            delattr(self, '_last_total_energy')

        stats_snapshot = getattr(self.simulator, 'stats', None)
        self._initialize_episode_counters(stats_snapshot)
        self._reset_reward_baseline(stats_snapshot)
        
        resource_state = self._collect_resource_state()
        state = self.agent_env.get_state_vector(node_states, system_metrics, resource_state)
        
        return state

    def step(self, action, state: Optional[np.ndarray] = None, actions_dict: Optional[Dict] = None) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡Œä¸€æ­¥ä»¿çœŸï¼Œåº”ç”¨æ™ºèƒ½ä½“åŠ¨ä½œåˆ°ä»¿çœŸå™¨"""
        # ğŸ¯ å¦‚æœæœªæä¾›actions_dictï¼Œå°è¯•ä»actionåˆ†è§£
        if actions_dict is None and hasattr(self.agent_env, 'decompose_action'):
            try:
                actions_dict = self.agent_env.decompose_action(action)
            except Exception:
                pass


        # ğŸ¯ ä½¿ç”¨å›ºå®šå¸è½½ç­–ç•¥ï¼ˆå¦‚æœè®¾ç½®ï¼‰
        if self.fixed_offload_policy is not None and actions_dict is not None:
            try:
                # ä½¿ç”¨å›ºå®šç­–ç•¥ç”Ÿæˆå¸è½½å†³ç­–
                fixed_action = self.fixed_offload_policy.select_action(state)
                
                # å°†å›ºå®šç­–ç•¥çš„actionè½¬æ¢ä¸ºoffload preference
                # å›ºå®šç­–ç•¥è¿”å›çš„actionæ ¼å¼: [local_score, rsu_score, uav_score, ...]
                if isinstance(fixed_action, np.ndarray) and len(fixed_action) >= 3:
                    local_pref = float(fixed_action[0])
                    rsu_pref = float(fixed_action[1])
                    uav_pref = float(fixed_action[2])
                    
                    # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
                    total = abs(local_pref) + abs(rsu_pref) + abs(uav_pref)
                    if total > 1e-6:
                        local_pref = abs(local_pref) / total
                        rsu_pref = abs(rsu_pref) / total
                        uav_pref = abs(uav_pref) / total
                    else:
                        local_pref, rsu_pref, uav_pref = 0.33, 0.33, 0.34
                    
                    # è¦†ç›–æ™ºèƒ½ä½“çš„å¸è½½å†³ç­–ï¼Œä¿ç•™å…¶ä»–å†³ç­–ï¼ˆç¼“å­˜ã€è¿ç§»ç­‰ï¼‰
                    if 'offload_preference' in actions_dict:
                        actions_dict['offload_preference'] = {
                            'local': local_pref,
                            'rsu': rsu_pref,
                            'uav': uav_pref
                        }
            except Exception as e:
                # å¦‚æœå›ºå®šç­–ç•¥å¤±è´¥ï¼Œå›é€€åˆ°æ™ºèƒ½ä½“å†³ç­–
                pass
        
        # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šç›‘æ§å¸è½½å†³ç­–åˆ†å¸ƒ
        if actions_dict is not None and 'offload_preference' in actions_dict:
            step_count = getattr(self, '_step_counter', 0)
            self._step_counter = step_count + 1
            
            if step_count % 50 == 0:
                offload_pref = actions_dict['offload_preference']
                local_val = offload_pref.get('local', 0.0)
                rsu_val = offload_pref.get('rsu', 0.0)
                uav_val = offload_pref.get('uav', 0.0)
                print(f"ğŸ” [Step {step_count}] å¸è½½åå¥½ â†’ Local:{local_val:.3f}, RSU:{rsu_val:.3f}, UAV:{uav_val:.3f}")
        
        # æ„é€ ä¼ é€’ç»™ä»¿çœŸå™¨çš„åŠ¨ä½œï¼ˆå°†è¿ç»­åŠ¨ä½œæ˜ å°„ä¸ºæœ¬åœ°/RSU/UAVåå¥½ï¼‰
        sim_actions = self._build_simulator_actions(actions_dict)
        
        # æ‰§è¡Œä»¿çœŸæ­¥éª¤ï¼ˆä¼ å…¥åŠ¨ä½œï¼‰
        step_stats = self.simulator.run_simulation_step(0, sim_actions)
        
        # ğŸ”§ å®æ—¶å¯è§†åŒ–ï¼šå‘å°„ä»»åŠ¡äº‹ä»¶
        if getattr(self, 'visualizer', None) is not None:
            step_events = step_stats.get('step_events', [])
            for event in step_events:
                try:
                    self.visualizer.emit_task_event(
                        event_type=event['type'],
                        vehicle_id=event['vehicle_id'],
                        target_id=event['target_id']
                    )
                except Exception:
                    pass
            
            # ğŸ”§ å®æ—¶å¯è§†åŒ–ï¼šæ›´æ–°è½¦è¾†ä½ç½®æ‹“æ‰‘
            vehicle_positions = step_stats.get('vehicle_positions', [])
            if vehicle_positions:
                try:
                    self.visualizer.emit_topology_update(vehicle_positions)
                except Exception:
                    pass
        
        resource_state = self._collect_resource_state()

        
        # æ”¶é›†ä¸‹ä¸€æ­¥çŠ¶æ€
        node_states = {}
        
        # è½¦è¾†çŠ¶æ€ (5ç»´ - ç»Ÿä¸€å½’ä¸€åŒ–)
        for i, vehicle in enumerate(self.simulator.vehicles):
            vehicle_state = np.array([
                normalize_scalar(vehicle['position'][0], 'vehicle_position_range', 1000.0),  # ä½ç½®x
                normalize_scalar(vehicle['position'][1], 'vehicle_position_range', 1000.0),  # ä½ç½®y
                normalize_scalar(vehicle.get('velocity', 0.0), 'vehicle_speed_range', 50.0),  # é€Ÿåº¦
                normalize_scalar(len(vehicle.get('tasks', [])), 'vehicle_queue_capacity', 20.0),  # é˜Ÿåˆ—
                normalize_scalar(vehicle.get('energy_consumed', 0.0), 'vehicle_energy_reference', 1000.0),  # èƒ½è€—
            ])
            node_states[f'vehicle_{i}'] = vehicle_state

        # RSUçŠ¶æ€ (5ç»´ - æ¸…ç†ç‰ˆï¼Œç§»é™¤æ§åˆ¶å‚æ•°)
        for i, rsu in enumerate(self.simulator.rsus):
            # æ ‡å‡†åŒ–å½’ä¸€åŒ–ï¼šç¡®ä¿æ‰€æœ‰å€¼åœ¨[0,1]èŒƒå›´
            rsu_state = np.array([
                normalize_scalar(rsu['position'][0], 'rsu_position_range', 1000.0),  # ä½ç½®x
                normalize_scalar(rsu['position'][1], 'rsu_position_range', 1000.0),  # ä½ç½®y
                self._calculate_correct_cache_utilization(rsu.get('cache', {}), rsu.get('cache_capacity', 1000.0)),  # ç¼“å­˜åˆ©ç”¨ç‡
                normalize_scalar(len(rsu.get('computation_queue', [])), 'rsu_queue_capacity', 20.0),  # é˜Ÿåˆ—åˆ©ç”¨ç‡
                normalize_scalar(rsu.get('energy_consumed', 0.0), 'rsu_energy_reference', 1000.0),  # èƒ½è€—
            ])
            node_states[f'rsu_{i}'] = rsu_state

        # UAVçŠ¶æ€ (5ç»´ - æ¸…ç†ç‰ˆï¼Œç§»é™¤æ§åˆ¶å‚æ•°)
        for i, uav in enumerate(self.simulator.uavs):
            # æ ‡å‡†åŒ–å½’ä¸€åŒ–ï¼šç¡®ä¿æ‰€æœ‰å€¼åœ¨[0,1]èŒƒå›´
            uav_state = np.array([
                normalize_scalar(uav['position'][0], 'uav_position_range', 1000.0),  # ä½ç½®x
                normalize_scalar(uav['position'][1], 'uav_position_range', 1000.0),  # ä½ç½®y
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨é˜Ÿåˆ—åˆ©ç”¨ç‡ä»£æ›¿é«˜åº¦ï¼ˆé«˜åº¦å¯¹å†³ç­–å½±å“å°ï¼Œé˜Ÿåˆ—è´Ÿè½½å…³é”®ï¼‰
                normalize_scalar(len(uav.get('computation_queue', [])), 'uav_queue_capacity', 20.0),   # é˜Ÿåˆ—åˆ©ç”¨ç‡
                self._calculate_correct_cache_utilization(uav.get('cache', {}), uav.get('cache_capacity', 200.0)),  # ç¼“å­˜åˆ©ç”¨ç‡
                normalize_scalar(uav.get('energy_consumed', 0.0), 'uav_energy_reference', 1000.0),  # èƒ½è€—
            ])
            node_states[f'uav_{i}'] = uav_state
        
        # è®¡ç®—ç³»ç»ŸæŒ‡æ ‡
        system_metrics = self._calculate_system_metrics(step_stats)
        
        # è·å–ä¸‹ä¸€çŠ¶æ€
        next_state = self.agent_env.get_state_vector(node_states, system_metrics, resource_state)
        
        # ğŸ”§ å¢å¼ºï¼šè®¡ç®—åŒ…å«å­ç³»ç»ŸæŒ‡æ ‡çš„å¥–åŠ±
        cache_metrics = self.adaptive_cache_controller.get_cache_metrics()
        migration_metrics = self.adaptive_migration_controller.get_migration_metrics()
        if hasattr(self, 'strategy_coordinator') and self.strategy_coordinator is not None:
            try:
                self.strategy_coordinator.observe_step(
                    system_metrics,
                    cache_metrics,
                    migration_metrics,
                    step_stats,
                )
            except Exception as exc:
                print(f"âš ï¸ è”åˆç­–ç•¥åè°ƒå™¨è§‚æµ‹å¼‚å¸¸: {exc}")

        # åé¦ˆå…³é”®ç³»ç»ŸæŒ‡æ ‡ç»™TD3ç­–ç•¥æŒ‡å¯¼æ¨¡å—ï¼Œé©±åŠ¨èƒ½è€—/å»¶è¿Ÿæ¸©åº¦è‡ªé€‚åº”
        agent_core = getattr(self.agent_env, 'agent', None)
        if agent_core is not None and hasattr(agent_core, 'update_guidance_feedback'):
            try:
                agent_core.update_guidance_feedback(system_metrics, cache_metrics, migration_metrics)
            except Exception as exc:
                if getattr(self, '_current_episode', 0) % 200 == 0:
                    print(f"âš ï¸ æŒ‡å¯¼åé¦ˆæ›´æ–°å¤±è´¥: {exc}")

        reward_source = system_metrics.get('reward_snapshot', system_metrics)
        reward, reward_components = self.agent_env.calculate_reward(reward_source, cache_metrics, migration_metrics)
        
        # å°†å¥–åŠ±ç»„ä»¶æ·»åŠ åˆ°step_statsä¾›è°ƒè¯•ä½¿ç”¨
        step_stats['reward_components'] = reward_components
        
        try:
            system_metrics['normalized_reward'] = self._normalize_reward_value(reward)
        except Exception:
            system_metrics['normalized_reward'] = 0.0
        
        task_type_queue = system_metrics.get('task_type_queue_distribution', [])
        task_type_deadline = system_metrics.get('task_type_deadline_remaining', [])
        task_type_drop = system_metrics.get('task_type_drop_rate', [])
        for idx in range(4):
            queue_val = float(task_type_queue[idx]) if idx < len(task_type_queue) else 0.0
            deadline_val = float(task_type_deadline[idx]) if idx < len(task_type_deadline) else 0.0
            drop_val = float(task_type_drop[idx]) if idx < len(task_type_drop) else 0.0
            self.episode_metrics[f'task_type_queue_share_{idx+1}'].append(queue_val)
            self.episode_metrics[f'task_type_deadline_norm_{idx+1}'].append(deadline_val)
            self.episode_metrics[f'task_type_drop_rate_{idx+1}'].append(drop_val)

        hotspot_mean = float(system_metrics.get('rsu_hotspot_mean', 0.0))
        hotspot_peak = float(system_metrics.get('rsu_hotspot_peak', 0.0))
        self.episode_metrics['rsu_hotspot_mean_series'].append(hotspot_mean)
        self.episode_metrics['rsu_hotspot_peak_series'].append(hotspot_peak)
        self.episode_metrics['mm1_queue_error'].append(float(system_metrics.get('mm1_queue_error', 0.0)))
        self.episode_metrics['mm1_delay_error'].append(float(system_metrics.get('mm1_delay_error', 0.0)))
        
        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = False  # å•æ™ºèƒ½ä½“ç¯å¢ƒé€šå¸¸ä¸ä¼šæå‰ç»“æŸ
        
        # é™„åŠ ä¿¡æ¯
        info = {
            'step_stats': step_stats,
            'system_metrics': system_metrics
        }
        
        return next_state, reward, done, info
    
    def _calculate_system_metrics(self, step_stats: Dict) -> Dict:
        """è®¡ç®—ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ - æœ€ç»ˆä¿®å¤ç‰ˆï¼Œç¡®ä¿æ•°å€¼åœ¨åˆç†èŒƒå›´"""
        import numpy as np
        
        # å®‰å…¨è·å–æ•°å€¼
        def safe_get(key: str, default: float = 0.0) -> float:
            value = step_stats.get(key, default)
            if np.isnan(value) or np.isinf(value):
                return default
            return max(0.0, value)  # ç¡®ä¿éè´Ÿ
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨episodeçº§åˆ«ç»Ÿè®¡è€Œéç´¯ç§¯ç»Ÿè®¡ï¼Œé¿å…å¥–åŠ±ç´¯ç§¯æ¶åŒ–
        # è®¡ç®—æœ¬episodeçš„å¢é‡ç»Ÿè®¡
        total_processed = int(safe_get('processed_tasks', 0))  # ç´¯è®¡å®Œæˆ
        total_dropped = int(safe_get('dropped_tasks', 0))  # ç´¯è®¡ä¸¢å¼ƒï¼ˆæ•°é‡ï¼‰
        
        # è®¡ç®—æœ¬episodeå¢é‡
        episode_processed = total_processed - getattr(self, '_episode_processed_base', 0)
        episode_dropped = total_dropped - getattr(self, '_episode_dropped_base', 0)
        
        # æ•°æ®ä¸¢å¤±é‡ï¼šä½¿ç”¨æœ¬episodeå¢é‡
        current_generated_bytes = float(step_stats.get('generated_data_bytes', 0.0))
        current_dropped_bytes = float(step_stats.get('dropped_data_bytes', 0.0))
        episode_generated_bytes = current_generated_bytes - getattr(self, '_episode_generated_bytes_base', 0.0)
        episode_dropped_bytes = current_dropped_bytes - getattr(self, '_episode_dropped_bytes_base', 0.0)
        
        # è®¡ç®—æœ¬episodeä»»åŠ¡æ€»æ•°å’Œå®Œæˆç‡ï¼ˆé¿å…ç´¯ç§¯æ•ˆåº”ï¼‰
        episode_total = episode_processed + episode_dropped
        completion_rate = normalize_ratio(episode_processed, episode_total, default=0.5)
        
        cache_hits = int(safe_get('cache_hits', 0))
        cache_misses = int(safe_get('cache_misses', 0))
        cache_requests_total = cache_hits + cache_misses
        reported_requests = int(step_stats.get('cache_requests', cache_requests_total) or cache_requests_total)
        reported_hit_rate = step_stats.get('cache_hit_rate')
        if reported_requests > 0:
            cache_requests_total = reported_requests
        if isinstance(reported_hit_rate, (int, float)):
            cache_hit_rate = float(np.clip(reported_hit_rate, 0.0, 1.0))
        else:
            cache_hit_rate = normalize_ratio(cache_hits, cache_requests_total)
        local_cache_hits = int(safe_get('local_cache_hits', 0))
        
        # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨è®¡ç®—å¹³å‡å»¶è¿Ÿ - ä½¿ç”¨ç´¯è®¡ç»Ÿè®¡
        total_delay = safe_get('total_delay', 0.0)
        processed_for_delay = max(1, total_processed)  # ä½¿ç”¨ç´¯è®¡å®Œæˆæ•°
        avg_delay = total_delay / processed_for_delay
        
        # é™åˆ¶å»¶è¿Ÿåœ¨åˆç†èŒƒå›´å†…ï¼ˆå…³é”®ä¿®å¤ï¼‰
        avg_delay = np.clip(avg_delay, 0.01, 5.0)  # æ‰©å¤§åˆ°0.01-5.0ç§’èŒƒå›´ï¼Œé€‚åº”è·¨æ—¶éš™å¤„ç†

        delay_base = getattr(self, '_episode_delay_component_base', {})
        delay_processing_total = safe_get('delay_processing', 0.0)
        delay_uplink_total = safe_get('delay_uplink', 0.0)
        delay_downlink_total = safe_get('delay_downlink', 0.0)
        delay_cache_total = safe_get('delay_cache', 0.0)
        delay_wait_total = safe_get('delay_waiting', 0.0)
        def _episode_delay(bucket_total: float, bucket_key: str) -> float:
            return max(0.0, bucket_total - delay_base.get(bucket_key, 0.0))
        episode_delay_processing = _episode_delay(delay_processing_total, 'delay_processing')
        episode_delay_uplink = _episode_delay(delay_uplink_total, 'delay_uplink')
        episode_delay_downlink = _episode_delay(delay_downlink_total, 'delay_downlink')
        episode_delay_cache = _episode_delay(delay_cache_total, 'delay_cache')
        episode_delay_wait = _episode_delay(delay_wait_total, 'delay_waiting')
        delay_denominator = max(1, episode_processed) if episode_processed > 0 else max(1, processed_for_delay)
        avg_processing_delay_component = episode_delay_processing / delay_denominator
        avg_uplink_delay_component = episode_delay_uplink / delay_denominator
        avg_downlink_delay_component = episode_delay_downlink / delay_denominator
        avg_cache_delay_component = episode_delay_cache / delay_denominator
        avg_wait_delay_component = episode_delay_wait / delay_denominator
        
        # ğŸ”§ ä¿®å¤èƒ½è€—è®¡ç®—ï¼šä½¿ç”¨çœŸå®ç´¯ç§¯èƒ½è€—å¹¶è½¬æ¢ä¸ºæœ¬episodeå¢é‡
        current_total_energy = safe_get('total_energy', 0.0)

        if not getattr(self, '_episode_counters_initialized', False):
            self._initialize_episode_counters(step_stats)

        # è‡ªé€‚åº”æ§åˆ¶å™¨ç»Ÿè®¡ï¼ˆç”¨äºå¥–åŠ±ä¸æŒ‡æ ‡å½’ä¸€åŒ–ï¼‰
        cache_metrics = self.adaptive_cache_controller.get_cache_metrics()
        migration_metrics = self.adaptive_migration_controller.get_migration_metrics()
        cache_total_requests = int(cache_metrics.get('total_requests', 0) or 0)
        cache_total_evictions = int(cache_metrics.get('evicted_items', 0) or 0)
        cache_total_collab = int(cache_metrics.get('collaborative_writes', 0) or 0)

        queue_rho_sum = float(step_stats.get('queue_rho_sum', 0.0) or 0.0)
        queue_rho_max = float(step_stats.get('queue_rho_max', 0.0) or 0.0)
        queue_overload_flag = 1.0 if bool(step_stats.get('queue_overload_flag', False)) else 0.0
        queue_rho_by_node = step_stats.get('queue_rho_by_node', {}) or {}
        queue_overloaded_nodes = step_stats.get('queue_overloaded_nodes', {}) or {}
        queue_warning_nodes = step_stats.get('queue_warning_nodes', {}) or {}
        queue_overload_events_total = int(step_stats.get('queue_overload_events', 0) or 0)
        queue_overload_events = max(0, queue_overload_events_total - getattr(self, '_episode_queue_overload_events_base', 0))
        queue_overflow_total = int(step_stats.get('queue_overflow_drops', 0) or 0)
        queue_overflow_drops = max(0, queue_overflow_total - getattr(self, '_episode_queue_overflow_base', 0))
        remote_stats = step_stats.get('remote_rejections', {}) or {}
        remote_total = int(remote_stats.get('total', 0) or 0)
        episode_remote_rejects = max(0, remote_total - getattr(self, '_episode_remote_reject_base', 0))
        remote_rejection_rate = normalize_ratio(episode_remote_rejects, episode_total, default=0.0)

        mm1_predictions_raw = step_stats.get('mm1_predictions', {}) or {}
        mm1_predictions: Dict[str, Dict[str, float]] = {}
        mm1_queue_errors: List[float] = []
        mm1_delay_errors: List[float] = []
        if isinstance(mm1_predictions_raw, dict):
            for node_key, pred in mm1_predictions_raw.items():
                if not isinstance(pred, dict):
                    continue
                arrival_rate = float(pred.get('arrival_rate', 0.0) or 0.0)
                service_rate = float(pred.get('service_rate', 0.0) or 0.0)
                rho_val = pred.get('rho')
                if rho_val is None:
                    rho = np.inf if service_rate <= 0.0 and arrival_rate > 0.0 else 0.0
                else:
                    try:
                        rho = float(rho_val)
                    except (TypeError, ValueError):
                        rho = np.inf
                stable = bool(pred.get('stable', False))
                rho_storable = float(rho) if np.isfinite(rho) else float('inf')
                theoretical_queue = pred.get('theoretical_queue')
                actual_queue = float(pred.get('actual_queue', 0.0) or 0.0)
                theoretical_delay = pred.get('theoretical_delay')
                actual_delay_obs = float(pred.get('actual_delay', 0.0) or 0.0)

                if theoretical_queue is not None:
                    try:
                        theo_queue_val = float(theoretical_queue)
                    except (TypeError, ValueError):
                        theo_queue_val = None
                else:
                    theo_queue_val = None

                if theoretical_delay is not None:
                    try:
                        theo_delay_val = float(theoretical_delay)
                    except (TypeError, ValueError):
                        theo_delay_val = None
                else:
                    theo_delay_val = None

                mm1_predictions[node_key] = {
                    'arrival_rate': arrival_rate,
                    'service_rate': service_rate,
                    'rho': rho_storable,
                    'stable': bool(stable),
                    'theoretical_queue': theo_queue_val,
                    'actual_queue': actual_queue,
                    'theoretical_delay': theo_delay_val,
                    'actual_delay': actual_delay_obs,
                }

                if theo_queue_val is not None:
                    mm1_queue_errors.append(abs(actual_queue - theo_queue_val))
                if theo_delay_val is not None:
                    mm1_delay_errors.append(abs(actual_delay_obs - theo_delay_val))

        mm1_queue_error = float(np.mean(mm1_queue_errors)) if mm1_queue_errors else 0.0
        mm1_delay_error = float(np.mean(mm1_delay_errors)) if mm1_delay_errors else 0.0

        
        # è®¡ç®—æœ¬episodeå¢é‡èƒ½è€—ï¼ˆé˜²æ­¢è´Ÿå€¼ä¸å¼‚å¸¸ï¼‰
        if current_total_energy <= 0.0:
            # ä»¿çœŸå™¨èƒ½è€—å¼‚å¸¸æ—¶çš„ä¿åº•ä¼°ç®—
            completed_tasks = self.simulator.stats.get('completed_tasks', 0) if hasattr(self, 'simulator') else 0
            estimated_energy = max(0.0, completed_tasks * 15.0)
            total_energy = estimated_energy
            print(f"âš ï¸ ä»¿çœŸå™¨èƒ½è€—ä¸º0ï¼Œä½¿ç”¨ä¼°ç®—èƒ½è€—: {total_energy:.1f}J")
        else:
            episode_incremental_energy = max(0.0, current_total_energy - getattr(self, '_episode_energy_base', 0.0))
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šç§»é™¤é™æ€èƒ½è€—åŸºçº¿ï¼Œåªå¥–åŠ±åŠ¨æ€èƒ½è€—ä¼˜åŒ–
            # é™æ€èƒ½è€— = (RSUé™æ€åŠŸç‡ * RSUæ•°é‡ + UAVé™æ€åŠŸç‡ * UAVæ•°é‡) * æŒç»­æ—¶é—´
            # è¿™æ ·å¯ä»¥è®©æ™ºèƒ½ä½“ä¸“æ³¨äºä¼˜åŒ–é‚£ ~200J çš„åŠ¨æ€èƒ½è€—ï¼Œè€Œä¸æ˜¯è¢« ~2000J çš„é™æ€èƒ½è€—æ·¹æ²¡
            rsu_static = getattr(config.compute, 'rsu_static_power', 25.0)
            uav_static = getattr(config.compute, 'uav_static_power', 2.5)
            # è½¦è¾†é™æ€èƒ½è€—é€šå¸¸ä¸è®¡å…¥ç³»ç»Ÿè¿è¥æˆæœ¬ï¼ˆå±äºç”¨æˆ·è®¾å¤‡ï¼‰ï¼Œä½†ä¸ºäº†ä¸¥è°¨ä¹Ÿå¯ä»¥å‡å»
            # è¿™é‡Œä¸»è¦å…³æ³¨åŸºç¡€è®¾æ–½èƒ½è€—
            static_power_total = (self.num_rsus * rsu_static) + (self.num_uavs * uav_static)
            
            # è®¡ç®—å½“å‰episodeå·²è¿è¡Œæ—¶é—´çš„é™æ€èƒ½è€—
            # ä½¿ç”¨ä»¿çœŸå™¨å½“å‰æ—¶é—´ä½œä¸ºæŒç»­æ—¶é—´
            current_duration = self.simulator.current_time
            static_energy_baseline = static_power_total * current_duration
            
            # åŠ¨æ€èƒ½è€— = æ€»èƒ½è€— - é™æ€åŸºçº¿
            # é™åˆ¶ä¸ºéè´Ÿï¼Œé˜²æ­¢å› æµ®ç‚¹è¯¯å·®å‡ºç°è´Ÿå€¼
            dynamic_energy = max(0.0, episode_incremental_energy - static_energy_baseline)
            
            # âš ï¸ ä»ç„¶è®°å½•æ€»èƒ½è€—ç”¨äºå±•ç¤ºï¼Œä½†ä½¿ç”¨åŠ¨æ€èƒ½è€—ç”¨äºå¥–åŠ±è®¡ç®—
            total_energy = dynamic_energy
            # print(f"DEBUG: Total={episode_incremental_energy:.1f}J, Static={static_energy_baseline:.1f}J, Dynamic={dynamic_energy:.1f}J")

        energy_base = getattr(self, '_episode_energy_component_base', {})
        def _episode_energy(bucket_key: str) -> float:
            return max(0.0, safe_get(bucket_key, 0.0) - energy_base.get(bucket_key, 0.0))
        energy_compute_component = _episode_energy('energy_compute')
        energy_tx_uplink_component = _episode_energy('energy_transmit_uplink')
        energy_tx_downlink_component = _episode_energy('energy_transmit_downlink')
        energy_cache_component = _episode_energy('energy_cache')
        energy_denominator = max(1, episode_processed) if episode_processed > 0 else 1
        avg_energy_compute_component = energy_compute_component / energy_denominator
        avg_energy_uplink_component = energy_tx_uplink_component / energy_denominator
        avg_energy_downlink_component = energy_tx_downlink_component / energy_denominator
        avg_energy_cache_component = energy_cache_component / energy_denominator
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨episodeçº§åˆ«æ•°æ®ä¸¢å¤±é‡ï¼Œé¿å…ç´¯ç§¯æ•ˆåº”
        data_loss_bytes = max(0.0, episode_dropped_bytes)
        data_generated_bytes = max(0.0, episode_generated_bytes)
        data_loss_ratio_bytes = normalize_ratio(data_loss_bytes, data_generated_bytes)
        
        # è¿ç§»æˆåŠŸç‡ï¼ˆæ¥è‡ªä»¿çœŸå™¨ç»Ÿè®¡ï¼‰
        migrations_executed = int(safe_get('migrations_executed', 0))
        migrations_successful = int(safe_get('migrations_successful', 0))
        migration_success_rate = normalize_ratio(migrations_successful, migrations_executed)
        
        # ğŸ”§ è°ƒè¯•è¿ç§»ç»Ÿè®¡
        if migrations_executed > 0:
            print(f"ğŸ” è¿ç§»ç»Ÿè®¡: æ‰§è¡Œ{migrations_executed}æ¬¡, æˆåŠŸ{migrations_successful}æ¬¡, æˆåŠŸç‡{migration_success_rate:.1%}")

        episode_cache_requests = max(
            0,
            cache_total_requests - getattr(self, '_episode_cache_requests_base', 0)
        )
        episode_cache_evictions = max(
            0,
            cache_total_evictions - getattr(self, '_episode_cache_evictions_base', 0)
        )
        episode_cache_collab = max(
            0,
            cache_total_collab - getattr(self, '_episode_cache_collab_base', 0)
        )
        cache_eviction_rate = normalize_ratio(episode_cache_evictions, episode_cache_requests)

        def _normalize_vector(key: str, length: int = 4, clip: bool = True) -> List[float]:
            raw = step_stats.get(key)
            if isinstance(raw, (np.ndarray, list, tuple)):
                values = raw
            else:
                values = []
            return normalize_feature_vector(values, length, clip=clip)

        queue_distribution = _normalize_vector('task_type_queue_distribution')
        active_distribution = _normalize_vector('task_type_active_distribution')
        deadline_remaining = _normalize_vector('task_type_deadline_remaining')
        queue_counts = _normalize_vector('task_type_queue_counts', clip=False)
        active_counts = _normalize_vector('task_type_active_counts', clip=False)
        hotspot_list = _normalize_vector(
            'rsu_hotspot_intensity',
            length=getattr(self.simulator, 'num_rsus', 0) or 4
        )
        rsu_hotspot_mean = float(np.mean(hotspot_list)) if hotspot_list else 0.0
        rsu_hotspot_peak = float(np.max(hotspot_list)) if hotspot_list else 0.0

        task_generation_stats = step_stats.get('task_generation')
        gen_by_type = task_generation_stats.get('by_type', {}) if isinstance(task_generation_stats, dict) else {}
        drop_stats = step_stats.get('drop_stats')
        drop_by_type = drop_stats.get('by_type', {}) if isinstance(drop_stats, dict) else {}

        generated_counts: List[float] = []
        drop_rate: List[float] = []
        for task_type in range(1, 5):
            generated = float(gen_by_type.get(task_type, 0.0))
            dropped = float(drop_by_type.get(task_type, 0.0))
            generated_counts.append(generated)
            drop_rate.append(normalize_ratio(dropped, generated))
        generated_share = normalize_distribution(generated_counts) if generated_counts else []

        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šèƒ½è€—ä¸è¿ç§»æ•æ„ŸåŒºé—´
        current_episode = getattr(self, '_current_episode', 0)
        if current_episode > 0 and (current_episode % 50 == 0 or avg_delay > 0.2 or migration_success_rate < 0.9):
            print(
                f"[è°ƒè¯•] Episode {current_episode:04d}: å»¶è¿Ÿ {avg_delay:.3f}s, èƒ½è€— {total_energy:.2f}J, "
                f"å®Œæˆç‡ {completion_rate:.1%}, è¿ç§»æˆåŠŸç‡ {migration_success_rate:.1%}, "
                f"ç¼“å­˜å‘½ä¸­ {cache_hit_rate:.1%}, æ•°æ®æŸå¤± {data_loss_ratio_bytes:.1%}, "
                f"ç¼“å­˜æ·˜æ±°ç‡ {cache_eviction_rate:.1%}"
            )

        # ğŸ¤– æ›´æ–°ç¼“å­˜æ§åˆ¶å™¨ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰å®é™…æ•°æ®ï¼‰
        if cache_hit_rate > 0:
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—ç¼“å­˜ç»Ÿè®¡
            total_utilization = 0.0
            for rsu in self.simulator.rsus:
                utilization = self._calculate_correct_cache_utilization(
                    rsu.get('cache', {}), 
                    rsu.get('cache_capacity', 1000.0)
                )
                total_utilization += utilization
            
            self.adaptive_cache_controller.cache_stats['current_utilization'] = (
                total_utilization / max(1, len(self.simulator.rsus))
            )
        
        latency_target = max(1e-6, getattr(config.rl, 'latency_target', 0.4))
        energy_target = max(1e-6, getattr(config.rl, 'energy_target', 1200.0))

        reward_snapshot = self._build_reward_snapshot(step_stats)

        return {
            'avg_task_delay': avg_delay,
            'total_energy_consumption': total_energy,
            'data_loss_bytes': data_loss_bytes,
            'data_loss_ratio_bytes': data_loss_ratio_bytes,
            'task_completion_rate': completion_rate,
            'cache_hit_rate': cache_hit_rate,
            'local_cache_hits': local_cache_hits,
            'migration_success_rate': migration_success_rate,
            'dropped_tasks': episode_dropped,
            'avg_processing_delay': avg_processing_delay_component,
            'avg_uplink_delay': avg_uplink_delay_component,
            'avg_downlink_delay': avg_downlink_delay_component,
            'avg_cache_delay': avg_cache_delay_component,
            'avg_waiting_delay': avg_wait_delay_component,
            'energy_compute': energy_compute_component,
            'energy_transmit_uplink': energy_tx_uplink_component,
            'energy_transmit_downlink': energy_tx_downlink_component,
            'energy_cache': energy_cache_component,
            'avg_energy_compute': avg_energy_compute_component,
            'avg_energy_uplink': avg_energy_uplink_component,
            'avg_energy_downlink': avg_energy_downlink_component,
            'avg_energy_cache': avg_energy_cache_component,
            # ğŸ¤– æ–°å¢è‡ªé€‚åº”æ§åˆ¶æŒ‡æ ‡
            'adaptive_cache_effectiveness': cache_metrics.get('effectiveness', 0.0),
            'adaptive_migration_effectiveness': migration_metrics.get('effectiveness', 0.0),
            'migration_avg_cost': migration_metrics.get('avg_cost', 0.0),
            'migration_avg_delay_saved': migration_metrics.get('avg_delay_saved', 0.0),
            'cache_utilization': cache_metrics.get('utilization', 0.0),
            'cache_evictions': episode_cache_evictions,
            'cache_eviction_rate': cache_eviction_rate,
            'cache_requests': episode_cache_requests,
            'cache_collaborative_writes': episode_cache_collab,
            'adaptive_cache_params': cache_metrics.get('agent_params', {}),
            'adaptive_migration_params': migration_metrics.get('agent_params', {}),
            'task_type_queue_distribution': queue_distribution,
            'task_type_active_distribution': active_distribution,
            'task_type_deadline_remaining': deadline_remaining,
            'task_type_queue_counts': queue_counts,
            'task_type_active_counts': active_counts,
            'task_type_drop_rate': drop_rate,
            'task_type_generated_share': generated_share,
            'queue_rho_sum': queue_rho_sum,
            'queue_rho_max': queue_rho_max,
            'queue_overload_flag': queue_overload_flag,
            'queue_overload_events': queue_overload_events,
            'queue_rho_by_node': queue_rho_by_node,
            'queue_overloaded_nodes': queue_overloaded_nodes,
            'queue_warning_nodes': queue_warning_nodes,
            'queue_overflow_drops': queue_overflow_drops,
            'mm1_queue_error': mm1_queue_error,
            'mm1_delay_error': mm1_delay_error,
            'mm1_predictions': mm1_predictions,
            'rsu_hotspot_intensity_list': hotspot_list,
            'rsu_hotspot_mean': rsu_hotspot_mean,
            'rsu_hotspot_peak': rsu_hotspot_peak,
            'remote_rejection_count': episode_remote_rejects,
            'remote_rejection_rate': remote_rejection_rate,
            'normalized_delay': avg_delay / latency_target,
            'normalized_energy': total_energy / energy_target,
            'reward_snapshot': reward_snapshot,
        }

    def _normalize_reward_value(self, reward: float) -> float:
        """å°†å¥–åŠ±å€¼è½¬æ¢ä¸ºæ— é‡çº²æ¯”ä¾‹ï¼Œä¾¿äºä¸å…¶ä»–æŒ‡æ ‡å¯¹æ¯”ã€‚"""
        import numpy as np
        rl_config = getattr(config, 'rl', None)
        reward_scale = float(
            getattr(
                rl_config,
                'reward_normalizer',
                getattr(rl_config, 'reward_weight_delay', 1.0)
                + getattr(rl_config, 'reward_weight_energy', 1.0)
            )
        )
        reward_scale = max(reward_scale, 1e-6)
        normalized = -reward / reward_scale
        return float(np.clip(normalized, -5.0, 5.0))
    
    def _record_episode_metrics(self, system_metrics: Dict, episode_steps: Optional[int] = None) -> None:
        """å°†ç³»ç»ŸæŒ‡æ ‡å†™å…¥episode_metricsï¼Œæ–¹ä¾¿åç»­æŠ¥å‘Š/å¯è§†åŒ–ä½¿ç”¨ã€‚"""
        import numpy as np

        metric_mapping = {
            'avg_task_delay': 'avg_delay',
            'total_energy_consumption': 'total_energy',
            'data_loss_bytes': 'data_loss_bytes',
            'data_loss_ratio_bytes': 'data_loss_ratio_bytes',
            'task_completion_rate': 'task_completion_rate',
            'cache_hit_rate': 'cache_hit_rate',
            'cache_utilization': 'cache_utilization',
            'cache_evictions': 'cache_evictions',
            'cache_eviction_rate': 'cache_eviction_rate',
            'cache_requests': 'cache_requests',
            'cache_collaborative_writes': 'cache_collaborative_writes',
            'local_cache_hits': 'local_cache_hits',
            'migration_success_rate': 'migration_success_rate',
            'queue_rho_sum': 'queue_rho_sum',
            'queue_rho_max': 'queue_rho_max',
            'queue_overload_flag': 'queue_overload_flag',
            'queue_overload_events': 'queue_overload_events',
            'migration_avg_cost': 'migration_avg_cost',
            'migration_avg_delay_saved': 'migration_avg_delay_saved',
            'rsu_hotspot_mean': 'rsu_hotspot_mean',
            'rsu_hotspot_peak': 'rsu_hotspot_peak',
            'normalized_delay': 'normalized_delay',
            'normalized_energy': 'normalized_energy',
            'normalized_reward': 'normalized_reward',
            'avg_processing_delay': 'avg_processing_delay',
            'avg_uplink_delay': 'avg_uplink_delay',
            'avg_downlink_delay': 'avg_downlink_delay',
            'avg_cache_delay': 'avg_cache_delay',
            'avg_waiting_delay': 'avg_waiting_delay',
            'energy_compute': 'energy_compute',
            'energy_transmit_uplink': 'energy_transmit_uplink',
            'energy_transmit_downlink': 'energy_transmit_downlink',
            'energy_cache': 'energy_cache',
            'avg_energy_compute': 'avg_energy_compute',
            'avg_energy_uplink': 'avg_energy_uplink',
            'avg_energy_downlink': 'avg_energy_downlink',
            'avg_energy_cache': 'avg_energy_cache',
            'queue_overflow_drops': 'queue_overflow_drops',
        }

        def _coerce_scalar(value: Any) -> Optional[float]:
            if isinstance(value, (list, tuple, dict)):
                return None
            try:
                return float(value)
            except (TypeError, ValueError):
                if isinstance(value, np.ndarray) and value.size == 1:
                    return float(value.item())
                return None

        for system_key, episode_key in metric_mapping.items():
            if episode_key not in self.episode_metrics:
                continue
            scalar_value = _coerce_scalar(system_metrics.get(system_key))
            if scalar_value is None:
                continue
            self.episode_metrics[episode_key].append(scalar_value)

        queue_distribution_ep = system_metrics.get('task_type_queue_distribution')
        if isinstance(queue_distribution_ep, (list, tuple, np.ndarray)):
            for idx, value in enumerate(queue_distribution_ep):
                key = f'task_type_queue_share_ep_{idx+1}'
                if key in self.episode_metrics:
                    coerced = _coerce_scalar(value)
                    if coerced is not None:
                        self.episode_metrics[key].append(coerced)

        if episode_steps is not None and 'episode_steps' in self.episode_metrics:
            self.episode_metrics['episode_steps'].append(int(episode_steps))
    
    def run_episode(self, episode: int, max_steps: Optional[int] = None, visualizer: Optional[Any] = None) -> Dict:
        """è¿è¡Œä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒè½®æ¬¡"""
        # ä½¿ç”¨é…ç½®ä¸­çš„æœ€å¤§æ­¥æ•°
        if max_steps is None:
            max_steps = config.experiment.max_steps_per_episode
        
        # é‡ç½®ç¯å¢ƒ
        self._episode_counters_initialized = False
        state = self.reset_environment()
        
        # ğŸ”§ è®¾ç½®å¯è§†åŒ–å™¨
        self.visualizer = visualizer
        
        # ğŸ”§ ä¿å­˜å½“å‰episodeç¼–å·
        self._current_episode = episode
        
        # ğŸ”§ é‡ç½®episodeæ­¥æ•°è·Ÿè¸ªï¼Œä¿®å¤èƒ½è€—è®¡ç®—
        self._current_episode_step = 0
        
        episode_reward = 0.0
        episode_info = {}
        step = 0
        info = {}  # åˆå§‹åŒ–infoå˜é‡
        
        # PPOéœ€è¦ç‰¹æ®Šå¤„ç†
        if self.algorithm == "PPO":
            return self._run_ppo_episode(episode, max_steps, visualizer)
        
        for step in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            if self.algorithm == "DQN":
                # DQNè¿”å›ç¦»æ•£åŠ¨ä½œ
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    # å¤„ç†å¯èƒ½çš„å…ƒç»„è¿”å›
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                        
                # éœ€è¦å°†åŠ¨ä½œæ˜ å°„å›å…¨å±€åŠ¨ä½œç´¢å¼•
                action_idx = self._encode_discrete_action(actions_dict)
                action = action_idx
            else:
                # è¿ç»­åŠ¨ä½œç®—æ³•
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    # å¤„ç†å¯èƒ½çš„å…ƒç»„è¿”å›
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                action = self._encode_continuous_action(actions_dict)
            
            # ğŸ”§ æ›´æ–°episodeæ­¥æ•°è®¡æ•°å™¨
            self._current_episode_step += 1
            
            # å°†å‘é‡åŠ¨ä½œæ¢å¤ä¸ºå­—å…¸ä¾›æ¨¡æ‹Ÿå™¨æ¶ˆè´¹ï¼ˆé¿å…åŠ¨ä½œè¢«å¿½ç•¥ï¼‰
            sim_actions_dict = actions_dict if isinstance(actions_dict, dict) else self._build_actions_from_vector(action)
            
            # æ‰§è¡ŒåŠ¨ä½œï¼ˆå°†åŠ¨ä½œå­—å…¸ä¼ å…¥ä»¥å½±å“ä»¿çœŸå™¨å¸è½½åå¥½ï¼‰
            next_state, reward, done, info = self.step(action, state, sim_actions_dict)
            
            # ğŸ”§ ä¿®å¤1ï¼šæ›´æ–°é˜Ÿåˆ—æŒ‡æ ‡ï¼ˆé©±åŠ¨Queue-aware Replayï¼‰
            if hasattr(self.agent_env, 'update_queue_metrics'):
                step_stats = info.get('step_stats', {})
                try:
                    self.agent_env.update_queue_metrics(step_stats)
                except Exception as e:
                    if self._current_episode % 100 == 0:  # ä»…æ¯100è½®æŠ¥å‘Šä¸€æ¬¡
                        print(f"âš ï¸ é˜Ÿåˆ—æŒ‡æ ‡æ›´æ–°å¤±è´¥: {e}")
            
            # åˆå§‹åŒ–training_info
            training_info = {}
            
            # è®­ç»ƒæ™ºèƒ½ä½“ - æ‰€æœ‰ç®—æ³•ç°åœ¨éƒ½æ”¯æŒUnionç±»å‹ç»Ÿä¸€æ¥å£
            # ç¡®ä¿actionç±»å‹å®‰å…¨è½¬æ¢
            if self.algorithm == "DQN":
                # DQNé¦–é€‰æ•´æ•°åŠ¨ä½œï¼Œä½†æ¥å—Unionç±»å‹
                safe_action = self._safe_int_conversion(action)
                training_info = self.agent_env.train_step(state, safe_action, reward, next_state, done)
            elif self.algorithm in ["DDPG", "TD3", "TD3_LATENCY_ENERGY", "SAC", "OPTIMIZED_TD3"]:
                # è¿ç»­åŠ¨ä½œç®—æ³•é¦–é€‰numpyæ•°ç»„ï¼Œä½†æ¥å—Unionç±»å‹
                safe_action = action if isinstance(action, np.ndarray) else np.array([action], dtype=np.float32)
                training_info = self.agent_env.train_step(state, safe_action, reward, next_state, done)
            elif self.algorithm == "PPO":
                # PPOä½¿ç”¨ç‰¹æ®Šçš„episodeçº§åˆ«è®­ç»ƒï¼Œtrain_stepä¸ºå ä½ç¬¦
                # ä¿æŒåŸactionç±»å‹å³å¯ï¼Œå› ä¸ºPPOçš„train_stepä¸åšå®é™…å¤„ç†
                training_info = self.agent_env.train_step(state, action, reward, next_state, done)
            else:
                # å…¶ä»–ç®—æ³•çš„é»˜è®¤å¤„ç†
                training_info = {'message': f'Unknown algorithm: {self.algorithm}'}
            
            # ç´¯ç§¯å¥–åŠ±å¹¶ä¿å­˜æœ€æ–°çš„è®­ç»ƒä¿¡æ¯
            episode_reward += reward
            episode_info = training_info

            # æ›´æ–°çŠ¶æ€ï¼›å¦‚æœªæ¥å¼•å…¥æå‰ç»“æŸï¼Œè¿™é‡Œå…¼å®¹ done æ ‡å¿—
            state = next_state
            if done:
                break
            
        # ?? ?????system_metrics?????????episode???
        steps_taken = step + 1  # range ? 0 ??
        system_metrics = info.get('system_metrics', {})
        self._record_episode_metrics(system_metrics, episode_steps=steps_taken)
        
        return {
            'episode_reward': episode_reward,
            'avg_reward': episode_reward,
            'episode_info': episode_info,
            'system_metrics': system_metrics,
            'steps': steps_taken
        }
    
    def _run_ppo_episode(self, episode: int, max_steps: int = 100, visualizer: Optional[Any] = None) -> Dict:
        """è¿è¡ŒPPOä¸“ç”¨episode"""
        state = self.reset_environment()
        self.visualizer = visualizer
        episode_reward = 0.0
        
        # åˆå§‹åŒ–å˜é‡
        done = False
        step = 0
        info = {}
        
        for step in range(max_steps):
            # è·å–åŠ¨ä½œã€å¯¹æ•°æ¦‚ç‡å’Œä»·å€¼
            if hasattr(self.agent_env, 'get_actions'):
                actions_result = self.agent_env.get_actions(state, training=True)
                if isinstance(actions_result, tuple) and len(actions_result) == 3:
                    actions_dict, log_prob, value = actions_result
                else:
                    # å¦‚æœä¸æ˜¯å…ƒç»„ï¼Œå°±ä½¿ç”¨é»˜è®¤å€¼
                    actions_dict = actions_result if isinstance(actions_result, dict) else {}
                    log_prob = 0.0
                    value = 0.0
            else:
                actions_dict = {}
                log_prob = 0.0
                value = 0.0
                
            action = self._encode_continuous_action(actions_dict)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.step(action, state, actions_dict)
            
            # å­˜å‚¨ç»éªŒ - æ‰€æœ‰ç®—æ³•éƒ½æ”¯æŒç»Ÿä¸€æ¥å£
            # ç¡®ä¿å‚æ•°ç±»å‹æ­£ç¡®
            log_prob_float = float(log_prob) if not isinstance(log_prob, float) else log_prob
            value_float = float(value) if not isinstance(value, float) else value
            # ä½¿ç”¨å‘½åå‚æ•°é¿å…ä½ç½®å‚æ•°é¡ºåºé—®é¢˜
            self.agent_env.store_experience(
                state=state, 
                action=action, 
                reward=reward, 
                next_state=next_state, 
                done=done, 
                log_prob=log_prob_float, 
                value=value_float
            )
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        # ğŸ”§ PPOæ›´æ–°ç­–ç•¥ä¿®å¤ï¼šç´¯ç§¯å¤šä¸ªepisodeåå†æ›´æ–°
        last_value = 0.0
        if not done:
            if hasattr(self.agent_env, 'get_actions'):
                actions_result = self.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, tuple) and len(actions_result) >= 3:
                    _, _, last_value = actions_result
                else:
                    last_value = 0.0
        
        # ç¡®ä¿ last_value ä¸º float ç±»å‹
        last_value_float = float(last_value) if not isinstance(last_value, float) else last_value
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ›´æ–°ï¼ˆæ¯Nä¸ªepisodeæˆ–bufferå¿«æ»¡æ—¶ï¼‰
        ppo_config = self.agent_env.config
        should_update = (
            episode % ppo_config.update_frequency == 0 or  # æ¯Nä¸ªepisode
            self.agent_env.agent.buffer.size >= ppo_config.buffer_size * 0.9  # bufferæ¥è¿‘æ»¡
        )
        
        # è¿›è¡Œæ›´æ–°
        # PPOEnvironment.updateåªæ¥å—last_valueå‚æ•°ï¼Œforce_updateåœ¨agentå†…éƒ¨å¤„ç†
        if should_update:
            training_info = self.agent_env.agent.update(last_value_float, force_update=True)
        else:
            training_info = self.agent_env.agent.update(last_value_float, force_update=False)
        
        steps_taken = step + 1  # range ? 0 ??
        system_metrics = info.get('system_metrics', {})
        self._record_episode_metrics(system_metrics, episode_steps=steps_taken)
        
        return {
            'episode_reward': episode_reward,
            'avg_reward': episode_reward,
            'episode_info': training_info,
            'system_metrics': system_metrics,
            'steps': steps_taken
        }

    def _build_simulator_actions(self, actions_dict: Optional[Dict]) -> Optional[Dict]:
        """å°†ç®—æ³•åŠ¨ä½œå­—å…¸è½¬æ¢ä¸ºä»¿çœŸå™¨å¯æ¶ˆè´¹çš„ç®€å•æ§åˆ¶ä¿¡å·ã€‚
        ğŸ¤– æ‰©å±•æ”¯æŒè”åˆåŠ¨ä½œç©ºé—´ï¼š
        - vehicle_agent å‰3ç»´ â†’ åŸæœ‰ä»»åŠ¡åˆ†é…åå¥½
        - ä¸­é—´ num_rsus/num_uavs ç»´ â†’ èŠ‚ç‚¹é€‰æ‹©æƒé‡
        - æœ«å°¾10ç»´ â†’ ç¼“å­˜ã€è¿ç§»åŠè”åŠ¨æ§åˆ¶å‚æ•°
        """
        if not isinstance(actions_dict, dict):
            return None
        vehicle_action = actions_dict.get('vehicle_agent')
        if vehicle_action is None:
            return None
        try:
            import numpy as np
            
            vehicle_action_array = np.array(vehicle_action, dtype=np.float32).reshape(-1)
            expected_dim = getattr(self.agent_env, 'action_dim', vehicle_action_array.size)
            if vehicle_action_array.size < expected_dim:
                padded = np.zeros(expected_dim, dtype=np.float32)
                padded[:vehicle_action_array.size] = vehicle_action_array
                vehicle_action_array = padded
            else:
                vehicle_action_array = vehicle_action_array[:expected_dim]
            
            # =============== åŸæœ‰ä»»åŠ¡åˆ†é…é€»è¾‘ (ä¿æŒå…¼å®¹) ===============
            raw = vehicle_action_array[:3]
            # ğŸ”§ ä¿®å¤ï¼šå°†[-1,1]èŒƒå›´çš„åŠ¨ä½œå€¼æ”¾å¤§åˆ°[-5,5]ï¼Œä½¿softmaxæ›´æ•æ„Ÿ
            # Actorè¾“å‡ºæ˜¯[-1,1]ï¼Œéœ€è¦æ”¾å¤§æ‰èƒ½äº§ç”Ÿæ˜æ˜¾çš„åå¥½å·®å¼‚
            raw = np.clip(raw, -1.0, 1.0) * 5.0  # æ”¾å¤§5å€ï¼š[-1,1] -> [-5,5]
            raw = np.clip(raw, -5.0, 5.0)  # ç¡®ä¿åœ¨[-5,5]èŒƒå›´å†…
            exp = np.exp(raw - np.max(raw))
            probs = exp / np.sum(exp)
            sim_actions = {
                'vehicle_offload_pref': {
                    'local': float(probs[0]),
                    'rsu': float(probs[1] if probs.size > 1 else 0.33),
                    'uav': float(probs[2] if probs.size > 2 else 0.34)
                }
            }
            # RSUé€‰æ‹©æ¦‚ç‡
            num_rsus = self.num_rsus
            rsu_action = actions_dict.get('rsu_agent')
            if isinstance(rsu_action, (list, tuple, np.ndarray)) and num_rsus > 0:
                rsu_raw = np.array(rsu_action[:num_rsus], dtype=np.float32)
            else:
                rsu_raw = vehicle_action_array[3:3 + num_rsus]
            if num_rsus > 0:
                # ğŸ”§ ä¿®å¤ï¼šåŒæ ·æ”¾å¤§RSUé€‰æ‹©æƒé‡
                rsu_raw = np.clip(rsu_raw, -1.0, 1.0) * 5.0  # æ”¾å¤§5å€
                rsu_raw = np.clip(rsu_raw, -5.0, 5.0)
                rsu_exp = np.exp(rsu_raw - np.max(rsu_raw))
                rsu_probs = rsu_exp / np.sum(rsu_exp)
                sim_actions['rsu_selection_probs'] = [float(x) for x in rsu_probs]
            
            # UAVé€‰æ‹©æ¦‚ç‡
            num_uavs = self.num_uavs
            uav_action = actions_dict.get('uav_agent')
            if isinstance(uav_action, (list, tuple, np.ndarray)) and num_uavs > 0:
                uav_raw = np.array(uav_action[:num_uavs], dtype=np.float32)
            else:
                uav_raw = vehicle_action_array[3 + num_rsus:3 + num_rsus + num_uavs]
            if num_uavs > 0:
                # ğŸ”§ ä¿®å¤ï¼šåŒæ ·æ”¾å¤§UAVé€‰æ‹©æƒé‡
                uav_raw = np.clip(uav_raw, -1.0, 1.0) * 5.0  # æ”¾å¤§5å€
                uav_raw = np.clip(uav_raw, -5.0, 5.0)
                uav_exp = np.exp(uav_raw - np.max(uav_raw))
                uav_probs = uav_exp / np.sum(uav_exp)
                sim_actions['uav_selection_probs'] = [float(x) for x in uav_probs]
            
            # ğŸ¤– =============== æ–°å¢è”åˆç¼“å­˜-è¿ç§»æ§åˆ¶å‚æ•° ===============
            control_start = 3 + num_rsus + num_uavs
            control_end = control_start + 10
            cache_migration_actions = vehicle_action_array[control_start:control_end]
            if cache_migration_actions.size < 10:
                padded = np.zeros(10, dtype=np.float32)
                padded[:cache_migration_actions.size] = cache_migration_actions
                cache_migration_actions = padded
            cache_migration_actions = np.clip(cache_migration_actions, -1.0, 1.0)

            cache_params, migration_params, joint_params = map_agent_actions_to_params(cache_migration_actions)

            self.adaptive_cache_controller.update_agent_params(cache_params)
            if not self.disable_migration:
                self.adaptive_migration_controller.update_agent_params(migration_params)
            if getattr(self, 'strategy_coordinator', None) is not None:
                self.strategy_coordinator.update_joint_params(joint_params)

            payload = {
                'adaptive_cache_params': cache_params,
                'cache_controller': self.adaptive_cache_controller,
                'joint_strategy_params': joint_params,
            }
            if not self.disable_migration:
                payload.update({
                    'adaptive_migration_params': migration_params,
                    'migration_controller': self.adaptive_migration_controller
                })
            sim_actions.update(payload)

            # ğŸ” è®©ç³»ç»Ÿæ¨¡æ‹Ÿå™¨æ¥æ”¶Actorå¯¼å‡ºçš„æŒ‡å¯¼ä¿¡å·ï¼ˆç»Ÿä¸€é”®åä¸ºrl_guidanceï¼‰
            guidance_payload = actions_dict.get('guidance') if isinstance(actions_dict, dict) else None
            if isinstance(guidance_payload, dict) and guidance_payload:
                sim_actions['rl_guidance'] = guidance_payload

            # ğŸ¯ =============== ä¸­å¤®èµ„æºåˆ†é…åŠ¨ä½œ (Phase 1) ===============
            if self.central_resource_enabled and self.central_resource_action_dim > 0:
                central_start = self.base_action_dim
                central_end = central_start + self.central_resource_action_dim
                central_vector = vehicle_action_array[central_start:central_end]
                allocations = self._decode_central_resource_actions(central_vector)
                if allocations:
                    try:
                        self.simulator.apply_resource_allocation(allocations)
                        sim_actions['central_resource_allocation'] = allocations
                    except Exception as exc:
                        print(f"âš ï¸ ä¸­å¤®èµ„æºåˆ†é…åº”ç”¨å¤±è´¥: {exc}")
            
            forced_mode = getattr(self, 'enforce_offload_mode', '')
            if forced_mode == 'local_only':
                sim_actions['vehicle_offload_pref'] = {'local': 1.0, 'rsu': 0.0, 'uav': 0.0}
            elif forced_mode == 'remote_only':
                if num_rsus == 0 and num_uavs == 0:
                    sim_actions['vehicle_offload_pref'] = {'local': 1.0, 'rsu': 0.0, 'uav': 0.0}
                elif num_rsus == 0:
                    sim_actions['vehicle_offload_pref'] = {'local': 0.0, 'rsu': 0.0, 'uav': 1.0}
                elif num_uavs == 0:
                    sim_actions['vehicle_offload_pref'] = {'local': 0.0, 'rsu': 1.0, 'uav': 0.0}
                else:
                    sim_actions['vehicle_offload_pref'] = {'local': 0.0, 'rsu': 0.5, 'uav': 0.5}

            # Attach distance-cache tradeoff gate for heuristic guidance (if actor exposes it)
            try:
                import numpy as _np  # safe local import
                actor_obj = getattr(self.agent_env, 'agent', None)
                if actor_obj is not None:
                    actor_obj = getattr(actor_obj, 'actor', None)
                gate = None
                if actor_obj is not None:
                    gate = getattr(actor_obj, 'last_tradeoff_gate', None)
                    if gate is None:
                        enc = getattr(actor_obj, 'encoder', None)
                        if enc is not None:
                            gate = getattr(enc, 'last_gate', None)
                if gate is not None:
                    try:
                        sim_actions['dc_tradeoff_gate'] = float(_np.clip(gate, 0.0, 1.0))
                    except Exception:
                        pass
            except Exception:
                pass

            return sim_actions
        except Exception as e:
            print(f"âš ï¸ åŠ¨ä½œæ„é€ å¼‚å¸¸: {e}")
            return None
    
    def _collect_resource_state(self) -> Optional[Dict[str, Any]]:
        if not self.central_resource_enabled:
            return None
        resource_pool = getattr(self.simulator, 'resource_pool', None)
        if resource_pool is None:
            return None
        try:
            return resource_pool.get_resource_state()
        except Exception:
            return None
    
    @staticmethod
    def _normalize_allocation(vector: np.ndarray, size: int) -> np.ndarray:
        if size <= 0:
            return np.zeros(0, dtype=np.float32)
        vec = np.array(vector, dtype=np.float32).reshape(-1)
        if vec.size < size:
            vec = np.pad(vec, (0, size - vec.size), constant_values=0.0)
        elif vec.size > size:
            vec = vec[:size]
        vec = np.clip(vec, 0.0, 1.0)
        total = float(np.sum(vec))
        if total <= 1e-6:
            return np.full(size, 1.0 / size, dtype=np.float32)
        return (vec / total).astype(np.float32)
    
    def _decode_central_resource_actions(
        self, central_vector: np.ndarray
    ) -> Optional[Dict[str, np.ndarray]]:
        if not self.central_resource_enabled or self.central_resource_action_dim <= 0:
            return None
        vector = np.array(central_vector, dtype=np.float32).reshape(-1)
        expected = self.central_resource_action_dim
        if vector.size < expected:
            padded = np.zeros(expected, dtype=np.float32)
            padded[:vector.size] = vector
            vector = padded
        elif vector.size > expected:
            vector = vector[:expected]
        vector = np.clip(vector, 0.0, 1.0)
        
        idx = 0
        bandwidth = self._normalize_allocation(
            vector[idx:idx + self.num_vehicles], self.num_vehicles
        )
        idx += self.num_vehicles
        vehicle_compute = self._normalize_allocation(
            vector[idx:idx + self.num_vehicles], self.num_vehicles
        )
        idx += self.num_vehicles
        rsu_compute = self._normalize_allocation(
            vector[idx:idx + self.num_rsus], self.num_rsus
        )
        idx += self.num_rsus
        uav_compute = self._normalize_allocation(
            vector[idx:idx + self.num_uavs], self.num_uavs
        )
        
        return {
            'bandwidth': bandwidth,
            'vehicle_compute': vehicle_compute,
            'rsu_compute': rsu_compute,
            'uav_compute': uav_compute,
        }
    
    def _encode_continuous_action(self, actions_dict) -> np.ndarray:
        """
        ğŸ¤– å°†åŠ¨ä½œå­—å…¸ç¼–ç ä¸ºè¿ç»­åŠ¨ä½œå‘é‡ - åŠ¨æ€é€‚é…åŠ¨ä½œç»´åº¦
        """
        # å¤„ç†å¯èƒ½çš„ä¸åŒè¾“å…¥ç±»å‹
        action_dim = getattr(self.agent_env, 'action_dim', 18)
        if not isinstance(actions_dict, dict):
            # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œè¿”å›é»˜è®¤åŠ¨ä½œç»´åº¦
            return np.zeros(action_dim, dtype=np.float32)

        # ğŸ¤– åªä½¿ç”¨vehicle_agentçš„å®Œæ•´åŠ¨ä½œå‘é‡
        vehicle_action = actions_dict.get('vehicle_agent')
        if isinstance(vehicle_action, (list, tuple, np.ndarray)):
            vehicle_action = np.array(vehicle_action, dtype=np.float32)
            if vehicle_action.size >= action_dim:
                return vehicle_action[:action_dim]
            action = np.zeros(action_dim, dtype=np.float32)
            action[:vehicle_action.size] = vehicle_action
            return action

        # é»˜è®¤è¿”å›å…¨é›¶åŠ¨ä½œ
        return np.zeros(action_dim, dtype=np.float32)
    
    def _build_actions_from_vector(self, action_vector: np.ndarray) -> Dict[str, np.ndarray]:
        """å°†è¿ç»­åŠ¨ä½œå‘é‡æ¢å¤ä¸ºä»¿çœŸå™¨éœ€è¦çš„åŠ¨ä½œå­—å…¸ï¼ˆåŠ¨æ€ç»´åº¦ï¼‰"""
        import numpy as np

        if not isinstance(action_vector, np.ndarray):
            action_vector = np.array(action_vector, dtype=np.float32)

        action_dim = getattr(self.agent_env, 'action_dim', action_vector.size)
        if action_vector.size < action_dim:
            padded = np.zeros(action_dim, dtype=np.float32)
            padded[:action_vector.size] = action_vector
            action_vector = padded
        else:
            action_vector = action_vector.astype(np.float32)[:action_dim]

        num_rsus = len(getattr(self.simulator, 'rsus', []))
        num_uavs = len(getattr(self.simulator, 'uavs', []))
        rsu_start = 3
        rsu_end = rsu_start + num_rsus
        uav_end = rsu_end + num_uavs

        return {
            'vehicle_agent': action_vector,
            'rsu_agent': action_vector[rsu_start:rsu_end],
            'uav_agent': action_vector[rsu_end:uav_end]
        }

    def _encode_discrete_action(self, actions_dict) -> int:
        """å°†åŠ¨ä½œå­—å…¸ç¼–ç ä¸ºç¦»æ•£åŠ¨ä½œç´¢å¼•"""
        # å¤„ç†å¯èƒ½çš„ä¸åŒè¾“å…¥ç±»å‹
        if not isinstance(actions_dict, dict):
            return 0  # é»˜è®¤åŠ¨ä½œç´¢å¼•
        
        # ç®€åŒ–å®ç°ï¼šå°†æ¯ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œç»„åˆæˆä¸€ä¸ªç´¢å¼•
        vehicle_action = actions_dict.get('vehicle_agent', 0)
        rsu_action = actions_dict.get('rsu_agent', 0)
        uav_action = actions_dict.get('uav_agent', 0)
        
        # å®‰å…¨åœ°å°†åŠ¨ä½œè½¬æ¢ä¸ºæ•´æ•°
        def safe_int_conversion(value):
            if isinstance(value, (int, np.integer)):
                return int(value)
            elif isinstance(value, np.ndarray):
                if value.size == 1:
                    return int(value.item())
                else:
                    return int(value[0])  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
            elif isinstance(value, (float, np.floating)):
                return int(value)
            else:
                return 0
        
        vehicle_action = safe_int_conversion(vehicle_action)
        rsu_action = safe_int_conversion(rsu_action)
        uav_action = safe_int_conversion(uav_action)
        
        # 5^3 = 125 ç§ç»„åˆ
        return vehicle_action * 25 + rsu_action * 5 + uav_action
    
    def _safe_int_conversion(self, value) -> int:
        """å®‰å…¨åœ°å°†ä¸åŒç±»å‹è½¬æ¢ä¸ºæ•´æ•°"""
        if isinstance(value, (int, np.integer)):
            return int(value)
        elif isinstance(value, np.ndarray):
            if value.size == 1:
                return int(value.item())
            else:
                return int(value[0])  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
        elif isinstance(value, (float, np.floating)):
            return int(round(value))
        else:
            return 0  # å®‰å…¨å›é€€å€¼


def train_single_algorithm(algorithm: str, num_episodes: Optional[int] = None, eval_interval: Optional[int] = None,
                          save_interval: Optional[int] = None, enable_realtime_vis: bool = False,
                          vis_port: int = 5000, silent_mode: bool = False, override_scenario: Optional[Dict[str, Any]] = None,
                          use_enhanced_cache: bool = False, disable_migration: bool = False,
                          enforce_offload_mode: Optional[str] = None, fixed_offload_policy: Optional[str] = None,
                          resume_from: Optional[str] = None, resume_lr_scale: Optional[float] = None,
                          joint_controller: bool = False, num_envs: int = 1) -> Dict:
    """è®­ç»ƒå•ä¸ªç®—æ³•
    
    Args:
        algorithm: ç®—æ³•åç§°
        num_episodes: è®­ç»ƒè½®æ¬¡
        eval_interval: è¯„ä¼°é—´éš”
        save_interval: ä¿å­˜é—´éš”
        enable_realtime_vis: æ˜¯å¦å¯ç”¨å®æ—¶å¯è§†åŒ–
        vis_port: å¯è§†åŒ–æœåŠ¡å™¨ç«¯å£
        silent_mode: é™é»˜æ¨¡å¼ï¼Œè·³è¿‡ç”¨æˆ·äº¤äº’ï¼ˆç”¨äºæ‰¹é‡å®éªŒï¼‰
        resume_from: å·²è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆ.pth æˆ–ç›®å½•å‰ç¼€ï¼‰ï¼Œç”¨äºwarm-startç»§ç»­è®­ç»ƒ
        resume_lr_scale: Warm-startåå¯¹å­¦ä¹ ç‡çš„ç¼©æ”¾ç³»æ•°ï¼ˆé»˜è®¤0.5ï¼ŒNoneè¡¨ç¤ºä¿æŒåŸå€¼ï¼‰
    """
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    if num_episodes is None:
        num_episodes = config.experiment.num_episodes
    
    # ğŸ”§ è‡ªåŠ¨è°ƒæ•´è¯„ä¼°é—´éš”å’Œä¿å­˜é—´éš”
    def auto_adjust_intervals(total_episodes: int):
        """æ ¹æ®æ€»è½®æ•°è‡ªåŠ¨è°ƒæ•´é—´éš”"""
        # è¯„ä¼°é—´éš”ï¼šæ€»è½®æ•°çš„5-8%ï¼ŒèŒƒå›´[10, 100]
        auto_eval = max(10, min(100, int(total_episodes * 0.06)))
        
        # ä¿å­˜é—´éš”ï¼šæ€»è½®æ•°çš„15-20%ï¼ŒèŒƒå›´[50, 500]  
        auto_save = max(50, min(500, int(total_episodes * 0.18)))
        
        return auto_eval, auto_save
    
    # åº”ç”¨è‡ªåŠ¨è°ƒæ•´ï¼ˆä»…å½“ç”¨æˆ·æœªæŒ‡å®šæ—¶ï¼‰
    if eval_interval is None or save_interval is None:
        auto_eval, auto_save = auto_adjust_intervals(num_episodes)
        if eval_interval is None:
            eval_interval = auto_eval
        if save_interval is None:
            save_interval = auto_save
    
    # æœ€ç»ˆå›é€€åˆ°é…ç½®é»˜è®¤å€¼
    if eval_interval is None:
        eval_interval = config.experiment.eval_interval
    if save_interval is None:
        save_interval = config.experiment.save_interval
    
    print(f"\n>> å¼€å§‹{algorithm}å•æ™ºèƒ½ä½“ç®—æ³•è®­ç»ƒ")
    print(f"DEBUG: config.rl.energy_target = {getattr(config.rl, 'energy_target', 'N/A')}")
    print("=" * 60)
    

    

    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒï¼ˆåº”ç”¨é¢å¤–åœºæ™¯è¦†ç›–ï¼‰
    if num_envs > 1:
        print(f"DEBUG: Entering parallel training block with num_envs={num_envs}")
        print(f"ğŸš€ å¯åŠ¨å¹¶è¡Œè®­ç»ƒ: {num_envs} ä¸ªç¯å¢ƒè¿›ç¨‹")
        from utils.vectorized_env import VectorizedSingleAgentEnvironment
        
        def make_env():
            return SingleAgentTrainingEnvironment(
                algorithm,
                override_scenario=override_scenario,
                use_enhanced_cache=use_enhanced_cache,
                disable_migration=disable_migration,
                enforce_offload_mode=enforce_offload_mode,
                fixed_offload_policy=fixed_offload_policy,
                joint_controller=joint_controller,
                simulation_only=True  # å…³é”®ï¼šå­è¿›ç¨‹åªè·‘ä»¿çœŸ
            )
        
        # ä¸»ç¯å¢ƒç”¨äºä¿å­˜æ¨¡å‹å’Œè¯„ä¼°ï¼ˆåŠ è½½å®Œæ•´Agentï¼‰
        main_env = SingleAgentTrainingEnvironment(
            algorithm,
            override_scenario=override_scenario,
            use_enhanced_cache=use_enhanced_cache,
            disable_migration=disable_migration,
            enforce_offload_mode=enforce_offload_mode,
            fixed_offload_policy=fixed_offload_policy,
            joint_controller=joint_controller,
            simulation_only=False
        )
        
        # å‘é‡åŒ–ç¯å¢ƒç”¨äºæ”¶é›†ç»éªŒ
        vec_env = VectorizedSingleAgentEnvironment([make_env for _ in range(num_envs)])
        training_env = main_env  # ä¿æŒæ¥å£å…¼å®¹ï¼Œä¸»è¦æ“ä½œmain_env
        print(f"âœ… å¹¶è¡Œç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
    else:
        training_env = SingleAgentTrainingEnvironment(
            algorithm,
            override_scenario=override_scenario,
            use_enhanced_cache=use_enhanced_cache,
            disable_migration=disable_migration,
            enforce_offload_mode=enforce_offload_mode,
            fixed_offload_policy=fixed_offload_policy,
            joint_controller=joint_controller,
        )
        vec_env = None

    canonical_algorithm = training_env.algorithm
    if canonical_algorithm != algorithm:
        print(f"âš™ï¸  è§„èŒƒåŒ–ç®—æ³•æ ‡è¯†: {canonical_algorithm}")
    algorithm = canonical_algorithm

    resume_loaded = False
    resume_target_path = None
    if resume_from:
        loader = getattr(training_env.agent_env, 'load_models', None)
        if callable(loader):
            try:
                resume_target_path = loader(resume_from) or resume_from
                resume_loaded = True
                print(f"â™»ï¸  ä»å·²æœ‰æ¨¡å‹åŠ è½½æˆåŠŸ: {resume_target_path}")
            except Exception as exc:  # pragma: no cover - å®¹é”™è·¯å¾„
                print(f"âš ï¸  åŠ è½½å·²æœ‰æ¨¡å‹å¤±è´¥ ({resume_from}): {exc}")
        else:
            print("âš ï¸  å½“å‰ç®—æ³•ç¯å¢ƒä¸æ”¯æŒåŠ è½½å·²æœ‰æ¨¡å‹ï¼Œå¿½ç•¥ --resume-from")

        if resume_loaded:
            agent_obj = getattr(training_env.agent_env, 'agent', None)
            warmup_adjusted = False
            if agent_obj and hasattr(agent_obj, 'config') and hasattr(agent_obj.config, 'warmup_steps'):
                original_warmup = int(getattr(agent_obj.config, 'warmup_steps', 0) or 0)
                new_warmup = max(500, original_warmup // 4) if original_warmup else 500
                if original_warmup and new_warmup < original_warmup:
                    agent_obj.config.warmup_steps = new_warmup
                    warmup_adjusted = True
            if warmup_adjusted:
                print(f"   â€¢ Warm-up æ­¥æ•°ç”± {original_warmup} ç¼©å‡è‡³ {new_warmup}ï¼ŒåŠ é€Ÿç»éªŒç¼“å†²é‡æ–°å¡«å……")

            lr_scale_value = resume_lr_scale if resume_lr_scale is not None else 0.5
            lr_info = None
            lr_callback = getattr(training_env.agent_env, 'apply_late_stage_lr', None)
            if callable(lr_callback) and lr_scale_value:
                try:
                    lr_info = lr_callback(factor=lr_scale_value, min_lr=5e-5)
                except Exception:
                    lr_info = None
            elif agent_obj and hasattr(agent_obj, 'apply_lr_schedule') and lr_scale_value:
                try:
                    lr_info = agent_obj.apply_lr_schedule(factor=lr_scale_value, min_lr=5e-5)
                except Exception:
                    lr_info = None
            if lr_info:
                print(f"   â€¢ å­¦ä¹ ç‡ç¼©æ”¾: actor_lr={lr_info.get('actor_lr', 0):.2e}, critic_lr={lr_info.get('critic_lr', 0):.2e}")
            elif resume_lr_scale:
                print("   â€¢ å­¦ä¹ ç‡ç¼©æ”¾è¯·æ±‚æœªæ‰§è¡Œï¼ˆå½“å‰ç®—æ³•ç¯å¢ƒæœªå®ç° apply_lr_scheduleï¼‰")

    lr_decay_episode: Optional[int] = None
    late_stage_lr_factor = 0.5
    lr_decay_applied = resume_loaded  # warm-start å·²ç»ç¼©æ”¾è¿‡ä¸€æ¬¡å­¦ä¹ ç‡
    if algorithm.upper() == 'TD3' and num_episodes >= 1200:
        lr_decay_episode = 1200

    # ğŸŒ åˆ›å»ºå®æ—¶å¯è§†åŒ–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    visualizer = None
    if enable_realtime_vis and REALTIME_AVAILABLE:
        print(f"ğŸŒ å¯åŠ¨å®æ—¶å¯è§†åŒ–æœåŠ¡å™¨ (ç«¯å£: {vis_port})")
        # å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–å¯è§†åŒ–å±•ç¤ºåï¼ˆç”¨äºä¸¤é˜¶æ®µæ ‡ç­¾ï¼‰
        display_name = os.environ.get('ALGO_DISPLAY_NAME', algorithm)
        visualizer = create_visualizer(
            algorithm=display_name,
            total_episodes=num_episodes,
            port=vis_port,
            auto_open=True
        )
        print(f"âœ… å®æ—¶å¯è§†åŒ–å·²å¯ç”¨ï¼Œè®¿é—® http://localhost:{vis_port}")
    elif enable_realtime_vis and not REALTIME_AVAILABLE:
        print("âš ï¸  å®æ—¶å¯è§†åŒ–æœªå¯ç”¨ï¼ˆç¼ºå°‘ä¾èµ–åŒ…ï¼‰")
    
    print(f"è®­ç»ƒé…ç½®:")
    print(f"  ç®—æ³•: {algorithm}")
    print(f"  æ€»è½®æ¬¡: {num_episodes}")
    print(f"  è¯„ä¼°é—´éš”: {eval_interval} (è‡ªåŠ¨è°ƒæ•´)" if eval_interval != config.experiment.eval_interval else f"  è¯„ä¼°é—´éš”: {eval_interval}")
    print(f"  ä¿å­˜é—´éš”: {save_interval} (è‡ªåŠ¨è°ƒæ•´)" if save_interval != config.experiment.save_interval else f"  ä¿å­˜é—´éš”: {save_interval}")
    print(f"  å®æ—¶å¯è§†åŒ–: {'å¯ç”¨ âœ“' if visualizer else 'ç¦ç”¨'}")
    if hasattr(config, 'rl'):
        print(
            f"  å¥–åŠ±æƒé‡: å»¶è¿Ÿ={getattr(config.rl, 'reward_weight_delay', 0.0):.2f}, "
            f"èƒ½è€—={getattr(config.rl, 'reward_weight_energy', 0.0):.2f}, "
            f"ä¸¢å¼ƒ={getattr(config.rl, 'reward_penalty_dropped', 0.0):.2f}"
        )
        print(f"  ã€é…ç½®ç›®æ ‡ã€‘")
        print(f"    - latency_target:    {getattr(config.rl, 'latency_target', 'N/A')}s")
        print(f"    - energy_target:     {getattr(config.rl, 'energy_target', 'N/A')}J")
        print(f"  ã€æƒé‡ã€‘")
        print(f"    - Ï‰_T (delay):       {_general_reward_calculator.weight_delay:.2f}")
        print(f"    - Ï‰_E (energy):      {_general_reward_calculator.weight_energy:.2f}")
        print(f"  ã€å…¶ä»–é…ç½®ã€‘")
        print(f"    - ä¸¢å¼ƒæƒ©ç½š:          {_general_reward_calculator.penalty_dropped:.2f}")
        print(f"    - å¥–åŠ±è£å‰ªèŒƒå›´:      {_general_reward_calculator.reward_clip_range}")
        print("=" * 60 + "\n")
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(f"results/single_agent/{algorithm.lower()}", exist_ok=True)
    os.makedirs(f"results/models/single_agent/{algorithm.lower()}", exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    # ğŸ”§ ä¿®å¤ï¼šper-stepå¥–åŠ±èŒƒå›´çº¦ä¸º-2.0åˆ°-0.5ï¼Œåˆå§‹å€¼åº”ç›¸åº”è°ƒæ•´
    best_avg_reward = -10.0  # per-stepå¥–åŠ±åˆå§‹é˜ˆå€¼ï¼ˆè´Ÿå€¼è¶Šå¤§è¶Šå¥½ï¼‰
    training_start_time = time.time()
    
    for episode in range(1, num_episodes + 1):
        episode_start_time = time.time()
        
        # è¿è¡Œè®­ç»ƒè½®æ¬¡
        if vec_env is not None:
            # å¹¶è¡Œè®­ç»ƒé€»è¾‘
            # 1. é‡ç½®æ‰€æœ‰ç¯å¢ƒ
            states = vec_env.reset()
            episode_rewards = np.zeros(num_envs)
            episode_steps_count = np.zeros(num_envs)
            active_envs = np.ones(num_envs, dtype=bool)
            infos = []
            
            # 2. æ­¥è¿›å¾ªç¯ (ä»¥max_stepsä¸ºå‡†)
            max_steps = config.experiment.max_steps_per_episode
            
            for step in range(max_steps):
                # æ‰¹é‡é€‰æ‹©åŠ¨ä½œ
                # æ³¨æ„ï¼šmain_env.agent_env å¿…é¡»åŠ è½½äº† Agent
                # states: (num_envs, state_dim)
                # actions: (num_envs, action_dim)
                actions = training_env.agent_env.select_action(states, training=True)
                
                # æ‰¹é‡æ‰§è¡ŒåŠ¨ä½œ
                # vec_env.step æ¥å— actions æ•°ç»„ï¼Œè¿”å› (next_states, rewards, dones, infos)
                next_states, rewards, dones, step_infos = vec_env.step(actions)
                
                # æ›´æ–°episodeç»Ÿè®¡
                episode_rewards[active_envs] += rewards[active_envs]
                episode_steps_count[active_envs] += 1
                infos.extend([info for i, info in enumerate(step_infos) if active_envs[i]])
                
                # æ ‡è®°å·²å®Œæˆçš„ç¯å¢ƒ
                active_envs = active_envs & ~dones
                
                # æ›´æ–°Agentï¼ˆä½¿ç”¨ä¸»ç¯å¢ƒçš„Agentï¼‰
                training_env.agent_env.update()
                
                # æ›´æ–°çŠ¶æ€
                states = next_states
                
                if not np.any(active_envs):
                    break
            
            # è®°å½•å¹³å‡å¥–åŠ±
            avg_ep_reward = np.mean(episode_rewards)
            
            # èšåˆå¤šç¯å¢ƒçš„system_metrics
            aggregated_metrics = {}
            if len(infos) > 0 and 'system_metrics' in infos[0]:
                keys = infos[0]['system_metrics'].keys()
                for key in keys:
                    values = [info['system_metrics'].get(key, 0) for info in infos]
                    try:
                        aggregated_metrics[key] = np.mean([float(v) for v in values])
                    except:
                        aggregated_metrics[key] = values[0]
            
            episode_result = {
                'avg_reward': avg_ep_reward,
                'steps': int(np.mean(episode_steps_count)),
                'system_metrics': aggregated_metrics,
                'step_stats': infos[0].get('step_stats', {}) if len(infos) > 0 else {}
            }

            # è®°å½• episode çº§æŒ‡æ ‡ï¼ˆå¹¶è¡Œç¯å¢ƒèšåˆåçš„å‡å€¼ï¼‰
            training_env._record_episode_metrics(aggregated_metrics, episode_steps=episode_result['steps'])
            
            # è®°å½•åˆ°ä¸»ç¯å¢ƒç”¨äºç»Ÿè®¡
            training_env.episode_rewards.append(avg_ep_reward)
            
        else:
            # åŸå§‹ä¸²è¡Œè®­ç»ƒ
            episode_result = training_env.run_episode(episode, visualizer=visualizer)
            training_env.episode_rewards.append(episode_result['avg_reward'])
        
        episode_steps = episode_result.get('steps', config.experiment.max_steps_per_episode)

        if algorithm.upper() == 'OPTIMIZED_TD3' and hasattr(training_env.agent_env, 'agent'):
            agent_ref = training_env.agent_env.agent
            if hasattr(agent_ref, 'set_episode_count'):
                try:
                    agent_ref.set_episode_count(episode, episode_result['avg_reward'])
                except Exception:
                    pass
        
        # æ›´æ–°æ€§èƒ½è¿½è¸ªå™¨
        training_env.performance_tracker['recent_rewards'].update(episode_result['avg_reward'])
        per_step_reward = episode_result['avg_reward'] / max(1, episode_steps)
        training_env.performance_tracker['recent_step_rewards'].update(per_step_reward)
        
        system_metrics = episode_result['system_metrics']
        training_env.performance_tracker['recent_delays'].update(system_metrics.get('avg_task_delay', 0))
        training_env.performance_tracker['recent_energy'].update(system_metrics.get('total_energy_consumption', 0))
        training_env.performance_tracker['recent_completion'].update(system_metrics.get('task_completion_rate', 0))
        # ğŸŒ æ›´æ–°å®æ—¶å¯è§†åŒ–
        if visualizer:
            step_stats = episode_result.get('step_stats', {}) # Assuming step_stats is part of episode_result
            vis_metrics = {
                'avg_delay': float(system_metrics.get('avg_task_delay', 0)),
                'total_energy': float(system_metrics.get('total_energy_consumption', 0)),
                'task_completion_rate': float(system_metrics.get('task_completion_rate', 0)),
                'cache_hit_rate': float(system_metrics.get('cache_hit_rate', 0)),
                'data_loss_ratio_bytes': float(system_metrics.get('data_loss_ratio_bytes', 0)),
                'migration_success_rate': float(system_metrics.get('migration_success_rate', 0)),
                'vehicle_positions': step_stats.get('vehicle_positions', []) # ğŸ”§ ä¼ é€’è½¦è¾†ä½ç½®
            }
            visualizer.update(episode, float(episode_result['avg_reward']), vis_metrics)
        
        episode_time = time.time() - episode_start_time
        
        # ğŸ”§ ä¿®å¤ï¼šæ¯ä¸ªepisodeéƒ½è¾“å‡ºç®€åŒ–æ—¥å¿—ï¼Œæ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        avg_reward_step = training_env.performance_tracker['recent_step_rewards'].get_average()
        avg_delay = training_env.performance_tracker['recent_delays'].get_average()
        avg_energy = training_env.performance_tracker['recent_energy'].get_average()
        avg_completion = training_env.performance_tracker['recent_completion'].get_average()
        
        # æ¯ä¸ªepisodeæ˜¾ç¤ºä¸€è¡Œç®€åŒ–ä¿¡æ¯
        print(f"Episode {episode:4d}/{num_episodes} | "
              f"å¥–åŠ±:{avg_reward_step:7.3f} | "
              f"å»¶è¿Ÿ:{avg_delay:6.3f}s | "
              f"èƒ½è€—:{avg_energy:7.1f}J | "
              f"å®Œæˆç‡:{avg_completion:5.1%} | "
              f"ç”¨æ—¶:{episode_time:5.2f}s")
        
        # å®šæœŸè¾“å‡ºè¯¦ç»†è¿›åº¦
        if episode % 10 == 0:
            print(f"\n{'='*70}")
            print(f"è½®æ¬¡ {episode:4d}/{num_episodes} è¯¦ç»†ç»Ÿè®¡:")
            print(f"  å¹³å‡æ¯æ­¥å¥–åŠ±: {avg_reward_step:8.3f}")
            print(f"  å¹³å‡æ—¶å»¶: {avg_delay:8.3f}s")
            print(f"  å¹³å‡èƒ½è€—: {avg_energy:8.1f}J")
            print(f"  å®Œæˆç‡:   {avg_completion:8.1%}")
            print(f"  è½®æ¬¡ç”¨æ—¶: {episode_time:6.3f}s")
            print(f"{'='*70}\n")
        
        # è¯„ä¼°æ¨¡å‹
        if episode % eval_interval == 0:
            eval_result = evaluate_single_model(algorithm, training_env, episode)
            print(f"\nğŸ“Š è½®æ¬¡ {episode} è¯„ä¼°ç»“æœ:")
            print(f"  Per-Stepå¥–åŠ±: {eval_result['avg_reward']:.3f}")
            print(f"  è¯„ä¼°æ—¶å»¶: {eval_result['avg_delay']:.3f}s")
            print(f"  è¯„ä¼°å®Œæˆç‡: {eval_result['completion_rate']:.1%}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if eval_result['avg_reward'] > best_avg_reward:
                best_avg_reward = eval_result['avg_reward']
                best_model_base = f"results/models/single_agent/{algorithm.lower()}/best_model"
                saved_target = training_env.agent_env.save_models(best_model_base)
                saved_display = saved_target or best_model_base
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ -> {saved_display} (Per-Stepå¥–åŠ±: {best_avg_reward:.3f})")
        
        # è¾¾åˆ°åæœŸé˜¶æ®µæ—¶ç¼©æ”¾TD3å­¦ä¹ ç‡ï¼ˆä¸€æ¬¡æ€§ï¼‰
        if (lr_decay_episode is not None and not lr_decay_applied and episode >= lr_decay_episode):
            lr_info = None
            lr_callback = getattr(training_env.agent_env, 'apply_late_stage_lr', None)
            if callable(lr_callback):
                lr_info = lr_callback(factor=late_stage_lr_factor, min_lr=5e-5)
                lr_decay_applied = True
            elif hasattr(training_env.agent_env, 'agent'):
                agent_obj = getattr(training_env.agent_env, 'agent')
                if hasattr(agent_obj, 'apply_lr_schedule'):
                    lr_info = agent_obj.apply_lr_schedule(factor=late_stage_lr_factor, min_lr=5e-5)
                    lr_decay_applied = True
            if lr_info:
                print(
                    f"ğŸ”§ ç¬¬{episode}è½®è§¦å‘TD3å­¦ä¹ ç‡ç¼©æ”¾ -> "
                    f"actor_lr={lr_info['actor_lr']:.2e}, critic_lr={lr_info['critic_lr']:.2e}"
                )

        # å®šæœŸä¿å­˜æ¨¡å‹
        if episode % save_interval == 0:
            checkpoint_base = f"results/models/single_agent/{algorithm.lower()}/checkpoint_{episode}"
            checkpoint_path = training_env.agent_env.save_models(checkpoint_base)
            checkpoint_display = checkpoint_path or checkpoint_base
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_display}")
    
    # è®­ç»ƒå®Œæˆ
    total_training_time = time.time() - training_start_time
    
    # ğŸŒ æ ‡è®°å®æ—¶å¯è§†åŒ–å®Œæˆ
    if visualizer:
        visualizer.complete()
        print(f"âœ… å®æ—¶å¯è§†åŒ–å·²æ ‡è®°å®Œæˆ")
    
    print("\n" + "=" * 60)
    print(f"ğŸ‰ {algorithm}è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_training_time/3600:.2f} å°æ—¶")
    print(f"ğŸ† æœ€ä½³Per-Stepå¥–åŠ±: {best_avg_reward:.3f}")
    
    # æ”¶é›†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯ç”¨äºæŠ¥å‘Š
    simulator_stats = {}
    
    # ğŸ¢ æ˜¾ç¤ºä¸­å¤®RSUè°ƒåº¦å™¨æŠ¥å‘Š
    try:
        central_report = training_env.simulator.get_central_scheduling_report()
        if central_report.get('status') != 'not_available' and central_report.get('status') != 'error':
            print(f"\nğŸ¢ ä¸­å¤®RSUéª¨å¹²è°ƒåº¦å™¨æ€»ç»“:")
            print(f"   ğŸ“Š è°ƒåº¦è°ƒç”¨æ¬¡æ•°: {central_report.get('scheduling_calls', 0)}")
            
            scheduler_status = central_report.get('central_scheduler_status', {})
            if 'global_metrics' in scheduler_status:
                metrics = scheduler_status['global_metrics']
                print(f"   âš–ï¸ è´Ÿè½½å‡è¡¡æŒ‡æ•°: {metrics.get('load_balance_index', 0.0):.3f}")
                print(f"   ğŸ’š ç³»ç»Ÿå¥åº·çŠ¶æ€: {scheduler_status.get('system_health', 'N/A')}")
                
                # æ”¶é›†è°ƒåº¦å™¨ç»Ÿè®¡ä¿¡æ¯
                simulator_stats['scheduling_calls'] = central_report.get('scheduling_calls', 0)
                simulator_stats['load_balance_index'] = metrics.get('load_balance_index', 0.0)
                simulator_stats['system_health'] = scheduler_status.get('system_health', 'N/A')
            
            # æ˜¾ç¤ºå„RSUè´Ÿè½½åˆ†å¸ƒ
            rsu_details = central_report.get('rsu_details', {})
            if rsu_details:
                print(f"   ğŸ“¡ å„RSUè´Ÿè½½çŠ¶æ€:")
                for rsu_id, details in rsu_details.items():
                    print(f"      {rsu_id}: CPUè´Ÿè½½={details['cpu_usage']:.1%}, ä»»åŠ¡é˜Ÿåˆ—={details['queue_length']}")
        else:
            print(f"ğŸ“‹ ä¸­å¤®è°ƒåº¦å™¨çŠ¶æ€: {central_report.get('message', 'æœªå¯ç”¨')}")
        
        # ğŸ”Œ æ˜¾ç¤ºæœ‰çº¿å›ä¼ ç½‘ç»œç»Ÿè®¡
        rsu_migration_delay = training_env.simulator.stats.get('rsu_migration_delay', 0.0)
        rsu_migration_energy = training_env.simulator.stats.get('rsu_migration_energy', 0.0)
        rsu_migration_data = training_env.simulator.stats.get('rsu_migration_data', 0.0)
        backhaul_collection_delay = training_env.simulator.stats.get('backhaul_collection_delay', 0.0)
        backhaul_command_delay = training_env.simulator.stats.get('backhaul_command_delay', 0.0)
        backhaul_total_energy = training_env.simulator.stats.get('backhaul_total_energy', 0.0)
        
        # ğŸš— æ˜¾ç¤ºå„ç§è¿ç§»ç»Ÿè®¡
        handover_migrations = training_env.simulator.stats.get('handover_migrations', 0)
        uav_migration_count = training_env.simulator.stats.get('uav_migration_count', 0)
        uav_migration_distance = training_env.simulator.stats.get('uav_migration_distance', 0.0)
        
        # æ”¶é›†è¿ç§»ç»Ÿè®¡ä¿¡æ¯
        simulator_stats['rsu_migration_delay'] = rsu_migration_delay
        simulator_stats['rsu_migration_energy'] = rsu_migration_energy
        simulator_stats['rsu_migration_data'] = rsu_migration_data
        simulator_stats['backhaul_total_energy'] = backhaul_total_energy
        simulator_stats['handover_migrations'] = handover_migrations
        simulator_stats['uav_migration_count'] = uav_migration_count
        
        if rsu_migration_data > 0 or backhaul_total_energy > 0 or handover_migrations > 0 or uav_migration_count > 0:
            print(f"\nğŸ”Œ æœ‰çº¿å›ä¼ ç½‘ç»œä¸è¿ç§»ç»Ÿè®¡:")
            print(f"   ğŸ“¡ RSUè¿ç§»æ•°æ®: {rsu_migration_data:.1f}MB")
            print(f"   â±ï¸ RSUè¿ç§»å»¶è¿Ÿ: {rsu_migration_delay*1000:.1f}ms")
            print(f"   âš¡ RSUè¿ç§»èƒ½è€—: {rsu_migration_energy:.2f}J")
            print(f"   ğŸ“Š ä¿¡æ¯æ”¶é›†å»¶è¿Ÿ: {backhaul_collection_delay*1000:.1f}ms")
            print(f"   ğŸ“¤ æŒ‡ä»¤åˆ†å‘å»¶è¿Ÿ: {backhaul_command_delay*1000:.1f}ms")
            print(f"   ğŸ”‹ å›ä¼ ç½‘ç»œæ€»èƒ½è€—: {backhaul_total_energy:.2f}J")
            if handover_migrations > 0:
                print(f"   ğŸš— è½¦è¾†è·Ÿéšè¿ç§»: {handover_migrations} æ¬¡")
            if uav_migration_count > 0:
                avg_distance = uav_migration_distance / uav_migration_count if uav_migration_count > 0 else 0
                print(f"   ğŸš UAVè¿ç§»: {uav_migration_count} æ¬¡, å¹³å‡è·ç¦»{avg_distance:.1f}m")
    except Exception as e:
        print(f"âš ï¸ ä¸­å¤®è°ƒåº¦æŠ¥å‘Šè·å–å¤±è´¥: {e}")
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    results = save_single_training_results(algorithm, training_env, total_training_time, override_scenario=override_scenario)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_single_training_curves(algorithm, training_env)
    
    # ç”ŸæˆHTMLè®­ç»ƒæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
    
    try:
        report_generator = HTMLReportGenerator()
        html_content = report_generator.generate_full_report(
            algorithm=algorithm,
            training_env=training_env,
            training_time=total_training_time,
            results=results,
            simulator_stats=simulator_stats
        )
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
        timestamp = generate_timestamp()
        report_filename = f"training_report_{timestamp}.html" if timestamp else "training_report.html"
        report_path = f"results/single_agent/{algorithm.lower()}/{report_filename}"
        
        print(f"âœ… è®­ç»ƒæŠ¥å‘Šå·²ç”Ÿæˆ")
        print(f"ğŸ“„ æŠ¥å‘ŠåŒ…å«:")
        print(f"   - æ‰§è¡Œæ‘˜è¦ä¸å…³é”®æŒ‡æ ‡")
        print(f"   - è®­ç»ƒé…ç½®è¯¦æƒ…")
        print(f"   - æ€§èƒ½æŒ‡æ ‡å¯è§†åŒ–å›¾è¡¨")
        print(f"   - è¯¦ç»†çš„ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯")
        print(f"   - è‡ªé€‚åº”æ§åˆ¶å™¨åˆ†æ")
        print(f"   - ä¼˜åŒ–å»ºè®®ä¸ç»“è®º")
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ä¿å­˜æŠ¥å‘Šï¼ˆé™é»˜æ¨¡å¼ä¸‹è‡ªåŠ¨ä¿å­˜ï¼‰
        # ğŸ”§ å¼ºåˆ¶è‡ªåŠ¨ä¿å­˜ï¼Œä¸è¯¢é—®ç”¨æˆ·
        if report_generator.save_report(html_content, report_path):
            print(f"âœ… æŠ¥å‘Šå·²è‡ªåŠ¨ä¿å­˜åˆ°: {report_path}")
        else:
            print("âŒ æŠ¥å‘Šä¿å­˜å¤±è´¥")
    
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        print("è®­ç»ƒæ•°æ®å·²æ­£å¸¸ä¿å­˜ï¼Œå¯ç¨åæ‰‹åŠ¨ç”ŸæˆæŠ¥å‘Š")
    
    return results


def evaluate_single_model(algorithm: str, training_env: SingleAgentTrainingEnvironment, 
                         episode: int, num_eval_episodes: int = 5) -> Dict:
    """è¯„ä¼°å•æ™ºèƒ½ä½“æ¨¡å‹æ€§èƒ½ - ä¿®å¤ç‰ˆï¼Œé˜²æ­¢infå’Œnan"""
    import numpy as np
    
    eval_rewards = []
    eval_delays = []
    eval_completions = []
    
    def safe_value(value: float, default: float = 0.0, max_val: float = 1e6) -> float:
        """å®‰å…¨å¤„ç†æ•°å€¼ï¼Œé˜²æ­¢infå’Œnan"""
        if np.isnan(value) or np.isinf(value):
            return default
        return np.clip(value, -max_val, max_val)
    
    eval_max_steps = getattr(config.experiment, 'max_steps_per_episode', 200)
    eval_max_steps = max(50, int(eval_max_steps))
    
    for _ in range(num_eval_episodes):
        state = training_env.reset_environment()
        episode_reward = 0.0
        episode_delay = 0.0
        episode_completion = 0.0
        steps = 0
        
        for step in range(eval_max_steps):
            if algorithm == "DQN":
                actions_result = training_env.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                action = training_env._encode_discrete_action(actions_dict)
            else:
                actions_result = training_env.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, tuple):  # PPOè¿”å›å…ƒç»„
                    actions_dict = actions_result[0]
                elif isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    actions_dict = {}
                action = training_env._encode_continuous_action(actions_dict)
            
            # è¯„ä¼°æ—¶ä¹Ÿä¼ å…¥åŠ¨ä½œå­—å…¸ï¼Œç¡®ä¿åå¥½ç”Ÿæ•ˆ
            next_state, reward, done, info = training_env.step(action, state, actions_dict)
            
            # å®‰å…¨å¤„ç†å¥–åŠ±å’ŒæŒ‡æ ‡
            safe_reward = safe_value(reward, -10.0, 120.0)
            episode_reward += safe_reward
            
            system_metrics = info['system_metrics']
            safe_delay = safe_value(system_metrics.get('avg_task_delay', 0), 0.0, 10.0)
            safe_completion = safe_value(system_metrics.get('task_completion_rate', 0), 0.0, 1.0)
            
            episode_delay += safe_delay
            episode_completion += safe_completion
            steps += 1
            
            state = next_state
            
            if done:
                break
        
        # å®‰å…¨è®¡ç®—å¹³å‡å€¼
        steps = max(1, steps)  # é˜²æ­¢é™¤é›¶
        eval_rewards.append(safe_value(episode_reward / steps, -20.0, 80.0))
        eval_delays.append(safe_value(episode_delay / steps, 0.0, 10.0))
        eval_completions.append(safe_value(episode_completion / steps, 0.0, 1.0))
    
    # å®‰å…¨è®¡ç®—æœ€ç»ˆç»“æœ
    if len(eval_rewards) == 0:
        return {'avg_reward': -1.0, 'avg_delay': 1.0, 'completion_rate': 0.0}
    
    avg_reward = safe_value(float(np.mean(eval_rewards)), -20.0, 80.0)
    avg_delay = safe_value(float(np.mean(eval_delays)), 0.0, 10.0)
    avg_completion = safe_value(float(np.mean(eval_completions)), 0.0, 1.0)
    
    return {
        'avg_reward': avg_reward,
        'avg_delay': avg_delay,
        'completion_rate': avg_completion
    }


def _calculate_stable_delay_average(training_env: SingleAgentTrainingEnvironment) -> float:
    """
    è®¡ç®—ç¨³å®šçš„æ—¶å»¶å¹³å‡å€¼ï¼Œé¿å…MovingAverage(100)çš„è®­ç»ƒæ³¢åŠ¨å½±å“
    
    ç­–ç•¥ï¼š
    1. ä¼˜å…ˆä½¿ç”¨episode_metricsä¸­çš„å®Œæ•´æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    2. ä½¿ç”¨å50%çš„æ•°æ®ï¼ˆæ’é™¤å‰æœŸå­¦ä¹ é˜¶æ®µï¼‰
    3. å¦‚æœæ•°æ®ä¸è¶³ï¼Œå›é€€åˆ°MovingAverage(100)
    
    Returns:
        float: ç¨³å®šçš„å¹³å‡æ—¶å»¶
    """
    # å°è¯•ä»episode_metricsè·å–å®Œæ•´æ—¶å»¶æ•°æ®
    if hasattr(training_env, 'episode_metrics') and 'avg_delay' in training_env.episode_metrics:
        delay_history = training_env.episode_metrics['avg_delay']
        
        if len(delay_history) >= 100:
            # ä½¿ç”¨å50%çš„æ•°æ®ï¼ˆæ›´æˆç†Ÿçš„ç­–ç•¥ï¼‰
            half_point = len(delay_history) // 2
            converged_delays = delay_history[half_point:]
            return float(np.mean(converged_delays))
        elif len(delay_history) >= 50:
            # å¦‚æœä¸è¶³100è½®ï¼Œä½¿ç”¨å30è½®
            return float(np.mean(delay_history[-30:]))
        elif len(delay_history) > 0:
            # æ•°æ®å¾ˆå°‘ï¼Œä½¿ç”¨å…¨éƒ¨
            return float(np.mean(delay_history))
    
    # å›é€€ï¼šä½¿ç”¨MovingAverage
    return training_env.performance_tracker['recent_delays'].get_average()


def _calculate_stable_completion_average(training_env: SingleAgentTrainingEnvironment) -> float:
    """
    è®¡ç®—ç¨³å®šçš„å®Œæˆç‡å¹³å‡å€¼
    
    Returns:
        float: ç¨³å®šçš„å¹³å‡å®Œæˆç‡
    """
    # å°è¯•ä»episode_metricsè·å–å®Œæ•´å®Œæˆç‡æ•°æ®
    if hasattr(training_env, 'episode_metrics') and 'task_completion_rate' in training_env.episode_metrics:
        completion_history = training_env.episode_metrics['task_completion_rate']
        
        if len(completion_history) >= 100:
            # ä½¿ç”¨å50%çš„æ•°æ®
            half_point = len(completion_history) // 2
            converged_completions = completion_history[half_point:]
            return float(np.mean(converged_completions))
        elif len(completion_history) >= 50:
            # å¦‚æœä¸è¶³100è½®ï¼Œä½¿ç”¨å30è½®
            return float(np.mean(completion_history[-30:]))
        elif len(completion_history) > 0:
            # æ•°æ®å¾ˆå°‘ï¼Œä½¿ç”¨å…¨éƒ¨
            return float(np.mean(completion_history))
    
    # å›é€€ï¼šä½¿ç”¨MovingAverage
    return training_env.performance_tracker['recent_completion'].get_average()


def save_single_training_results(algorithm: str, training_env: SingleAgentTrainingEnvironment, 
                                training_time: float,
                                override_scenario: Optional[Dict[str, Any]] = None) -> Dict:
    """ä¿å­˜è®­ç»ƒç»“æœ"""
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = generate_timestamp()
    
    # ğŸ”§ åŒæ—¶æä¾›Episodeæ€»å¥–åŠ±å’ŒPer-Stepå¹³å‡å¥–åŠ±
    recent_episode_reward = training_env.performance_tracker['recent_rewards'].get_average()
    
    # ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨å®é™…å¹³å‡æ­¥æ•°è®¡ç®— avg_step_reward
    if 'episode_steps' in training_env.episode_metrics and training_env.episode_metrics['episode_steps']:
        # ä½¿ç”¨æœ€è¿‘100ä¸ªepisodeçš„å¹³å‡æ­¥æ•°
        recent_steps = training_env.episode_metrics['episode_steps'][-100:]
        avg_steps_per_episode = sum(recent_steps) / len(recent_steps)
    else:
        # å›é€€åˆ°é…ç½®çš„é»˜è®¤å€¼
        avg_steps_per_episode = config.experiment.max_steps_per_episode
    
    avg_step_reward = recent_episode_reward / avg_steps_per_episode
    
    # è·å–ç½‘ç»œæ‹“æ‰‘ä¿¡æ¯
    num_vehicles = len(training_env.simulator.vehicles)
    num_rsus = len(training_env.simulator.rsus)
    num_uavs = len(training_env.simulator.uavs)
    state_dim = getattr(training_env.agent_env, 'state_dim', 'N/A')
    
    # ğŸ†• ä¿®å¤ï¼šæ”¶é›†å®Œæ•´çš„ç³»ç»Ÿé…ç½®å‚æ•°ï¼ˆç”¨äºHTMLæŠ¥å‘Šæ˜¾ç¤ºï¼‰
    # ç›´æ¥ä½¿ç”¨å·²å¯¼å…¥çš„configå¯¹è±¡
    
    results = {
        'algorithm': algorithm,
        'agent_type': 'single_agent',
        'timestamp': timestamp,
        'training_start_time': datetime.now().isoformat(),
        'network_topology': {
            'num_vehicles': num_vehicles,
            'num_rsus': num_rsus,
            'num_uavs': num_uavs,
        },
        'state_dim': state_dim,
        'override_scenario': override_scenario,
        'training_config': {
            'num_episodes': len(training_env.episode_rewards),
            'training_time_hours': training_time / 3600,
            'max_steps_per_episode': config.experiment.max_steps_per_episode
        },
        # ğŸ†• æ·»åŠ ç³»ç»Ÿé…ç½®å‚æ•°ï¼ˆHTMLæŠ¥å‘Šéœ€è¦ï¼‰
        'system_config': {
            'num_vehicles': num_vehicles,
            'num_rsus': num_rsus,
            'num_uavs': num_uavs,
            'simulation_time': config.simulation_time,
            'time_slot': config.time_slot,
            'device': str(config.device),
            'random_seed': config.random_seed,
        },
        # ğŸ†• æ·»åŠ ç½‘ç»œé…ç½®å‚æ•°
        'network_config': {
            'bandwidth': config.network.bandwidth,
            'carrier_frequency': config.communication.carrier_frequency,
            'coverage_radius': config.network.coverage_radius,
        },
        # ğŸ†• æ·»åŠ é€šä¿¡é…ç½®å‚æ•°
        'communication_config': {
            'vehicle_tx_power': config.communication.vehicle_tx_power,
            'rsu_tx_power': config.communication.rsu_tx_power,
            'uav_tx_power': config.communication.uav_tx_power,
            'antenna_gain_vehicle': config.communication.antenna_gain_vehicle,
            'antenna_gain_rsu': config.communication.antenna_gain_rsu,
            'antenna_gain_uav': config.communication.antenna_gain_uav,
        },
        # ğŸ†• æ·»åŠ è®¡ç®—èƒ½åŠ›å‚æ•°
        'compute_config': {
            'vehicle_cpu_freq': config.compute.vehicle_cpu_freq,
            'rsu_cpu_freq': config.compute.rsu_cpu_freq,
            'uav_cpu_freq': config.compute.uav_cpu_freq,
            'vehicle_memory': getattr(config.compute, 'vehicle_memory', 4e9),
            'rsu_memory': getattr(config.compute, 'rsu_memory', 32e9),
            'uav_memory': getattr(config.compute, 'uav_memory', 16e9),
            'vehicle_static_power': config.compute.vehicle_static_power,
            'rsu_static_power': config.compute.rsu_static_power,
            'uav_static_power': getattr(config.compute, 'uav_static_power', 20.0),
        },
        # ğŸ†• æ·»åŠ ä»»åŠ¡å’Œè¿ç§»å‚æ•°
        'task_migration_config': {
            'task_arrival_rate': config.task.arrival_rate,
            'task_size_mean': sum(config.task.data_size_range) / 2,
            'task_size_std': (config.task.data_size_range[1] - config.task.data_size_range[0]) / 4,
            'task_cpu_cycles_mean': sum(config.task.compute_cycles_range) / 2,
            'task_cpu_cycles_std': (config.task.compute_cycles_range[1] - config.task.compute_cycles_range[0]) / 4,
            'task_deadline_mean': sum(config.task.deadline_range) / 2,
            'cache_capacity_rsu': config.cache.rsu_cache_capacity,
            'cache_capacity_uav': config.cache.uav_cache_capacity,
            'migration_threshold': getattr(config.migration, 'threshold', 0.8),
        },
        'episode_rewards': training_env.episode_rewards,
        'episode_metrics': training_env.episode_metrics,
        'final_performance': {
            # æä¾›ä¸¤ç§å¥–åŠ±æŒ‡æ ‡ï¼Œç”¨é€”ä¸åŒ
            'avg_episode_reward': recent_episode_reward,  # Episodeæ€»å¥–åŠ±ï¼ˆè®­ç»ƒç›®æ ‡ï¼‰
            'avg_step_reward': avg_step_reward,           # æ¯æ­¥å¹³å‡å¥–åŠ±ï¼ˆå¯¹æ¯”è¯„ä¼°ï¼‰
            'avg_reward': avg_step_reward,  # å‘åå…¼å®¹ï¼šé»˜è®¤ä½¿ç”¨per-stepï¼ˆä¸å¯è§†åŒ–ä¸€è‡´ï¼‰
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ›´ç¨³å®šçš„å¹³å‡æ–¹æ³•ï¼Œé¿å…MovingAverage(100)çš„æ³¢åŠ¨å½±å“
            'avg_delay': _calculate_stable_delay_average(training_env),
            'avg_completion': _calculate_stable_completion_average(training_env)
        }
    }
    
    print(f"ğŸ“Š æ”¶é›†çš„é…ç½®å‚æ•°:")
    print(f"   ç³»ç»Ÿæ‹“æ‰‘: {num_vehicles}è½¦è¾†, {num_rsus}RSU, {num_uavs}UAV")
    print(f"   ç½‘ç»œé…ç½®: å¸¦å®½{config.network.bandwidth/1e6:.0f}MHz, é¢‘ç‡{config.communication.carrier_frequency/1e9:.1f}GHz")
    print(f"   ä»»åŠ¡å‚æ•°: åˆ°è¾¾ç‡{config.task.arrival_rate:.1f}, æ•°æ®é‡{sum(config.task.data_size_range)/2/1e6:.1f}MB")
    
    # ä½¿ç”¨æ—¶é—´æˆ³æ–‡ä»¶å
    filename = get_timestamped_filename("training_results")
    filepath = f"results/single_agent/{algorithm.lower()}/{filename}"
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ {algorithm}è®­ç»ƒç»“æœå·²ä¿å­˜åˆ° {filepath}")
    
    return results


def plot_single_training_curves(algorithm: str, training_env: SingleAgentTrainingEnvironment):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿ - ç®€æ´ä¼˜ç¾ç‰ˆ"""
    
    # ğŸ¨ ä½¿ç”¨æ–°çš„ç®€æ´å¯è§†åŒ–ç³»ç»Ÿ
    from visualization.clean_charts import create_training_chart, cleanup_old_charts, plot_objective_function_breakdown
    
    # åˆ›å»ºç®—æ³•ç›®å½•
    algorithm_dir = f"results/single_agent/{algorithm.lower()}"
    
    # æ¸…ç†æ—§çš„å†—ä½™å›¾è¡¨
    cleanup_old_charts(algorithm_dir)
    
    # ç”Ÿæˆæ ¸å¿ƒå›¾è¡¨
    chart_path = f"{algorithm_dir}/training_overview.png"
    create_training_chart(training_env, algorithm, chart_path)
    
    # ğŸ¯ ç”Ÿæˆç›®æ ‡å‡½æ•°åˆ†è§£å›¾ï¼ˆæ˜¾ç¤ºæ—¶å»¶ã€èƒ½è€—ä¸¤é¡¹æ ¸å¿ƒç›®æ ‡çš„æƒé‡è´¡çŒ®ï¼‰
    objective_path = f"{algorithm_dir}/objective_analysis.png"
    plot_objective_function_breakdown(training_env, algorithm, objective_path)
    
    print(f"ğŸ“ˆ {algorithm} è®­ç»ƒå¯è§†åŒ–å·²å®Œæˆ")
    print(f"   è®­ç»ƒæ€»è§ˆ: {chart_path}")
    print(f"   ç›®æ ‡åˆ†æ: {objective_path}")
    
    # ç”Ÿæˆè®­ç»ƒæ€»ç»“
    from visualization.clean_charts import get_summary_text
    summary = get_summary_text(training_env, algorithm)
    print(f"\n{summary}")


def compare_single_algorithms(algorithms: List[str], num_episodes: Optional[int] = None) -> Dict:
    """æ¯”è¾ƒå¤šä¸ªå•æ™ºèƒ½ä½“ç®—æ³•çš„æ€§èƒ½"""
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    if num_episodes is None:
        num_episodes = config.experiment.num_episodes
    
    print("\nğŸ”¥ å¼€å§‹å•æ™ºèƒ½ä½“ç®—æ³•æ€§èƒ½æ¯”è¾ƒ")
    print("=" * 60)
    
    results = {}
    
    # è®­ç»ƒæ‰€æœ‰ç®—æ³•
    for algorithm in algorithms:
        print(f"\nå¼€å§‹è®­ç»ƒ {algorithm}...")
        results[algorithm] = train_single_algorithm(algorithm, num_episodes)
    
    # ğŸ¨ ç”Ÿæˆç®€æ´çš„å¯¹æ¯”å›¾è¡¨
    from visualization.clean_charts import create_comparison_chart
    timestamp = generate_timestamp()
    comparison_chart_path = f"results/single_agent_comparison_{timestamp}.png" if timestamp else "results/single_agent_comparison.png"
    create_comparison_chart(results, comparison_chart_path)
    
    # ä¿å­˜æ¯”è¾ƒç»“æœ
    timestamp = generate_timestamp()
    comparison_results = {
        'algorithms': algorithms,
        'agent_type': 'single_agent',
        'num_episodes': num_episodes,
        'timestamp': timestamp,
        'comparison_time': datetime.now().isoformat(),
        'results': results,
        'summary': {}
    }
    
    # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
    for algorithm, result in results.items():
        final_perf = result['final_performance']
        comparison_results['summary'][algorithm] = {
            'final_avg_reward': final_perf['avg_reward'],
            'final_avg_delay': final_perf['avg_delay'],
            'final_completion_rate': final_perf['avg_completion'],
            'training_time_hours': result['training_config']['training_time_hours']
        }
    
    # ä½¿ç”¨æ—¶é—´æˆ³æ–‡ä»¶å
    comparison_filename = get_timestamped_filename("single_agent_comparison")
    with open(f"results/{comparison_filename}", "w", encoding="utf-8") as f:
        json.dump(comparison_results, f, indent=2, ensure_ascii=False)
    
    print("\nğŸ¯ å•æ™ºèƒ½ä½“ç®—æ³•æ¯”è¾ƒå®Œæˆï¼")
    print(f"ğŸ“„ æ¯”è¾ƒç»“æœå·²ä¿å­˜åˆ° results/{comparison_filename}")
    print(f"ğŸ“Š å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ° {comparison_chart_path}")
    
    return comparison_results




def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å•æ™ºèƒ½ä½“ç®—æ³•è®­ç»ƒè„šæœ¬')
    parser.add_argument('--algorithm', type=str, choices=['DDPG', 'TD3', 'TD3-LE', 'TD3_LE', 'TD3_LATENCY_ENERGY', 'DQN', 'PPO', 'SAC', 'CAM_TD3', 'OPTIMIZED_TD3'],
                       help='é€‰æ‹©è®­ç»ƒç®—æ³•')
    parser.add_argument('--episodes', type=int, default=None, help=f'è®­ç»ƒè½®æ¬¡ (é»˜è®¤: {config.experiment.num_episodes})')
    parser.add_argument('--eval_interval', type=int, default=None, help=f'è¯„ä¼°é—´éš” (é»˜è®¤: {config.experiment.eval_interval})')
    parser.add_argument('--save_interval', type=int, default=None, help=f'ä¿å­˜é—´éš” (é»˜è®¤: {config.experiment.save_interval})')
    parser.add_argument('--compare', action='store_true', help='æ¯”è¾ƒæ‰€æœ‰ç®—æ³•')
    parser.add_argument('--quick-test', action='store_true', help='å¿«é€ŸåŸºå‡†æµ‹è¯•ï¼Œä»…è¿è¡Œå°‘é‡ episodes')
    parser.add_argument('--seed', type=int, default=None, help='è¦†ç›–éšæœºç§å­ (é»˜è®¤è¯»å–configæˆ–ç¯å¢ƒå˜é‡)')
    parser.add_argument('--num-vehicles', type=int, default=None, help='è¦†ç›–è½¦è¾†æ•°é‡ç”¨äºå®éªŒ')
    parser.add_argument('--force-offload', type=str, choices=['local', 'remote', 'local_only', 'remote_only'],
                        help='å¼ºåˆ¶å¸è½½æ¨¡å¼ï¼šlocal/local_only æˆ– remote/remote_only')
    parser.add_argument('--fixed-offload-policy', type=str, 
                        choices=['random', 'greedy', 'local_only', 'rsu_only', 'round_robin', 'weighted'],
                        help='å›ºå®šå¸è½½ç­–ç•¥ï¼ˆä¸ä½¿ç”¨æ™ºèƒ½ä½“å­¦ä¹ ï¼‰ï¼šrandom/greedy/local_only/rsu_only/round_robin/weighted')
    # ğŸŒ å®æ—¶å¯è§†åŒ–å‚æ•°ï¼ˆé»˜è®¤å¼€å¯ï¼Œå¯é€šè¿‡ --no-realtime-vis å…³é—­ï¼‰
    parser.add_argument(
        '--realtime-vis',
        action='store_true',
        dest='realtime_vis',
        default=True,
        help='å¯ç”¨å®æ—¶å¯è§†åŒ–ï¼ˆé»˜è®¤å·²å¼€å¯ï¼‰'
    )
    parser.add_argument(
        '--no-realtime-vis',
        action='store_false',
        dest='realtime_vis',
        help='ç¦ç”¨å®æ—¶å¯è§†åŒ–ï¼ˆè¦†ç›–é»˜è®¤å¼€å¯è¡Œä¸ºï¼‰'
    )
    parser.add_argument('--vis-port', type=int, default=5000, help='å®æ—¶å¯è§†åŒ–æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 5000)')
    # ğŸš€ å¢å¼ºç¼“å­˜å‚æ•°ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    parser.add_argument('--no-enhanced-cache', action='store_true', 
                       help='ç¦ç”¨å¢å¼ºç¼“å­˜ç³»ç»Ÿï¼ˆé»˜è®¤å¯ç”¨åˆ†å±‚L1/L2 + çƒ­åº¦ç­–ç•¥ + RSUåä½œï¼‰')
    # ğŸ§­ ä¸¤é˜¶æ®µç®¡çº¿å¼€å…³ï¼ˆStage-1 é¢„åˆ†é… + Stage-2 ç²¾ç»†è°ƒåº¦ï¼‰
    parser.add_argument('--two-stage', action='store_true', help='å¯ç”¨ä¸¤é˜¶æ®µæ±‚è§£ï¼ˆé¢„åˆ†é…+ç²¾ç»†è°ƒåº¦ï¼‰')
    # ğŸ§  æŒ‡å®šä¸¤ä¸ªé˜¶æ®µçš„ç®—æ³•
    parser.add_argument('--stage1-alg', type=str, default=None,
                        help='é˜¶æ®µä¸€ç®—æ³•ï¼ˆoffloading å¤´ï¼‰ï¼šheuristic|greedy|cache_first|distance_first')
    parser.add_argument('--stage2-alg', type=str, default=None,
                        help='é˜¶æ®µäºŒç®—æ³•ï¼ˆç¼“å­˜/è¿ç§»æ§åˆ¶çš„RLï¼‰ï¼šTD3|SAC|DDPG|PPO|DQN|TD3-LE|OPTIMIZED_TD3')
    # ğŸ¯ ä¸­å¤®èµ„æºåˆ†é…æ¶æ„ï¼ˆPhase 1 + Phase 2ï¼‰- é»˜è®¤å¯ç”¨
    parser.add_argument('--central-resource', action='store_true', default=True,
                        help='å¯ç”¨ä¸­å¤®èµ„æºåˆ†é…æ¶æ„ï¼ˆPhase 1å†³ç­– + Phase 2æ‰§è¡Œï¼‰ï¼Œæ‰©å±•çŠ¶æ€/åŠ¨ä½œç©ºé—´ [é»˜è®¤å¯ç”¨]')
    parser.add_argument('--no-central-resource', action='store_false', dest='central_resource',
                        help='ç¦ç”¨ä¸­å¤®èµ„æºåˆ†é…æ¶æ„ï¼Œä½¿ç”¨æ ‡å‡†å‡åŒ€èµ„æºåˆ†é…')
    parser.add_argument('--silent-mode', action='store_true',
                        help='å¯ç”¨é™é»˜æ¨¡å¼ï¼Œè·³è¿‡è®­ç»ƒç»“æŸåçš„äº¤äº’æç¤º')
    parser.add_argument('--resume-from', type=str,
                        help='ä»å·²æœ‰æ¨¡å‹ (.pth æˆ–ç›®å½•å‰ç¼€) ç»§ç»­è®­ç»ƒï¼Œå¤ç”¨å·²å­¦ç­–ç•¥')
    parser.add_argument('--resume-lr-scale', type=float, default=None,
                        help='Warm-start åçš„å­¦ä¹ ç‡ç¼©æ”¾ç³»æ•° (é»˜è®¤0.5ï¼Œè®¾ä¸º1å¯ä¿ç•™åŸå€¼)')
    parser.add_argument('--num-envs', type=int, default=4,
                        help='å¹¶è¡Œè®­ç»ƒç¯å¢ƒæ•°é‡ (é»˜è®¤: 4)')
    
    # ğŸ†• é€šä¿¡æ¨¡å‹ä¼˜åŒ–å‚æ•°ï¼ˆ3GPPæ ‡å‡†å¢å¼ºï¼‰
    parser.add_argument('--comm-enhancements', action='store_true',
                        help='å¯ç”¨æ‰€æœ‰é€šä¿¡æ¨¡å‹ä¼˜åŒ–ï¼ˆå¿«è¡°è½+ç³»ç»Ÿçº§å¹²æ‰°+åŠ¨æ€å¸¦å®½ï¼‰Enable all communication model enhancements')
    parser.add_argument('--fast-fading', action='store_true',
                        help='å¯ç”¨éšæœºå¿«è¡°è½ï¼ˆRayleigh/Ricianï¼‰Enable fast fading')
    parser.add_argument('--system-interference', action='store_true',
                        help='å¯ç”¨ç³»ç»Ÿçº§å¹²æ‰°è®¡ç®— Enable system-level interference calculation')
    parser.add_argument('--dynamic-bandwidth', action='store_true',
                        help='å¯ç”¨åŠ¨æ€å¸¦å®½åˆ†é… Enable dynamic bandwidth allocation')
    
    args = parser.parse_args()

    if args.seed is not None:
        os.environ['RANDOM_SEED'] = str(args.seed)
        _apply_global_seed_from_env()

    # è®¾ç½®é»˜è®¤è¶…å‚æ•°ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
    os.environ.setdefault('TD3_ACTOR_LR', '5e-5')
    os.environ.setdefault('TD3_CRITIC_LR', '8e-5')
    os.environ.setdefault('TD3_BATCH_SIZE', '512')
    os.environ.setdefault('RL_SMOOTH_DELAY', '0.6')
    os.environ.setdefault('RL_SMOOTH_ENERGY', '0.6')
    os.environ.setdefault('RL_SMOOTH_ALPHA', '0.25')

    # å¿«é€ŸåŸºå‡†æµ‹è¯•æ¨¡å¼
    if args.quick_test:
        print("=== QUICK TEST (Baseline Fixed Policy) ===")
        # åˆ›å»ºç¯å¢ƒå¹¶å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ç­–ç•¥
        env = SingleAgentTrainingEnvironment('OPTIMIZED_TD3', enforce_offload_mode='local_only')
        for ep in range(5):
            state = env.reset_environment()
            total_reward = 0.0
            for step in range(100):
                # è·å–åŠ¨ä½œï¼ˆè™½ç„¶è¢«å¼ºåˆ¶æœ¬åœ°ç­–ç•¥è¦†ç›–ï¼Œä½†ä»éœ€ä¼ å…¥ï¼‰
                actions_result = env.agent_env.get_actions(state, training=False)
                if isinstance(actions_result, dict):
                    actions_dict = actions_result
                else:
                    actions_dict = actions_result[0] if isinstance(actions_result, tuple) else actions_result
                
                # ç¼–ç åŠ¨ä½œ
                if hasattr(env, '_encode_continuous_action'):
                    action = env._encode_continuous_action(actions_dict)
                else:
                    # Fallback for simple envs
                    action = np.zeros(env.agent_env.action_dim)

                next_state, reward, done, info = env.step(action, state, actions_dict)
                total_reward += reward
                state = next_state
                if done:
                    break
            print(f"Baseline Episode {ep}: Reward = {total_reward:.4f}")
        print("=== QUICK TEST DONE ===")
        return
    # ğŸ¯ ä¸­å¤®èµ„æºåˆ†é…æ¶æ„ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    if args.central_resource:
        os.environ['CENTRAL_RESOURCE'] = '1'
        print("ğŸ¯ å¯ç”¨ä¸­å¤®èµ„æºåˆ†é…æ¶æ„ï¼ˆPhase 1 + Phase 2ï¼‰[é»˜è®¤æ¨¡å¼]")
    else:
        os.environ.pop('CENTRAL_RESOURCE', None)
        print("âš ï¸  ä½¿ç”¨æ ‡å‡†å‡åŒ€èµ„æºåˆ†é…æ¨¡å¼ï¼ˆå·²é€šè¿‡ --no-central-resource ç¦ç”¨ä¸­å¤®èµ„æºï¼‰")
    
    # ğŸ†• é€šä¿¡æ¨¡å‹ä¼˜åŒ–é…ç½®
    if args.comm_enhancements or args.fast_fading or args.system_interference or args.dynamic_bandwidth:
        print("\n" + "="*70)
        print("ğŸŒ é€šä¿¡æ¨¡å‹ä¼˜åŒ–é…ç½®ï¼ˆ3GPPæ ‡å‡†å¢å¼ºï¼‰")
        print("="*70)
        
        # å¦‚æœå¯ç”¨äº†--comm-enhancementsï¼Œåˆ™å¯ç”¨æ‰€æœ‰ä¼˜åŒ–
        if args.comm_enhancements:
            config.communication.enable_fast_fading = True
            config.communication.use_system_interference = True
            config.communication.use_bandwidth_allocator = True
            config.communication.use_communication_enhancements = True
            print("âœ… å¯ç”¨æ‰€æœ‰é€šä¿¡æ¨¡å‹ä¼˜åŒ–ï¼ˆå®Œæ•´3GPPæ ‡å‡†æ¨¡å¼ï¼‰")
        else:
            # å•ç‹¬é…ç½®å„é¡¹ä¼˜åŒ–
            if args.fast_fading:
                config.communication.enable_fast_fading = True
                print("âœ… å¯ç”¨éšæœºå¿«è¡°è½ï¼ˆRayleigh/Ricianåˆ†å¸ƒï¼‰")
            
            if args.system_interference:
                config.communication.use_system_interference = True
                print("âœ… å¯ç”¨ç³»ç»Ÿçº§å¹²æ‰°è®¡ç®—")
            
            if args.dynamic_bandwidth:
                config.communication.use_bandwidth_allocator = True
                print("âœ… å¯ç”¨åŠ¨æ€å¸¦å®½åˆ†é…è°ƒåº¦å™¨")
        
        # æ˜¾ç¤ºé…ç½®è¯¦æƒ…
        print("\né…ç½®è¯¦æƒ…ï¼š")
        print(f"  - å¿«è¡°è½: {'å¯ç”¨' if config.communication.enable_fast_fading else 'ç¦ç”¨'}")
        print(f"  - ç³»ç»Ÿçº§å¹²æ‰°: {'å¯ç”¨' if config.communication.use_system_interference else 'ç¦ç”¨'}")
        print(f"  - åŠ¨æ€å¸¦å®½åˆ†é…: {'å¯ç”¨' if config.communication.use_bandwidth_allocator else 'ç¦ç”¨'}")
        print(f"  - è½½æ³¢é¢‘ç‡: {config.communication.carrier_frequency/1e9:.1f} GHz")
        print(f"  - ç¼–ç æ•ˆç‡: {config.communication.coding_efficiency}")
        if config.communication.enable_fast_fading:
            print(f"  - å¿«è¡°è½å‚æ•°: Ïƒ={config.communication.fast_fading_std}, K={config.communication.rician_k_factor}dB")
        print("="*70 + "\n")
    
    # Toggle two-stage pipeline via environment for the simulator
    if args.two_stage:
        os.environ['TWO_STAGE_MODE'] = '1'
    # Stage1/Stage2 algorithm selections (env-based for env init)
    if args.stage1_alg:
        os.environ['STAGE1_ALG'] = args.stage1_alg
    if args.stage2_alg:
        # å…è®¸è¦†ç›–ä¸»ç®—æ³•é€‰æ‹©
        if not args.algorithm:
            args.algorithm = args.stage2_alg
        else:
            # è¦†å†™ä¸ºé˜¶æ®µäºŒé€‰æ‹©
            args.algorithm = args.stage2_alg

    # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®æ„å»ºoverride_scenarioå‚æ•°
    override_scenario = None
    if args.num_vehicles is not None:
        override_scenario = {
            "num_vehicles": args.num_vehicles,
        }
        # åŒæ—¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå‘åå…¼å®¹ï¼‰
        os.environ['TRAINING_SCENARIO_OVERRIDES'] = json.dumps(override_scenario)
        print(f"ğŸ“‹ è¦†ç›–å‚æ•°: è½¦è¾†æ•° = {args.num_vehicles}")
   
    enforce_mode = None
    if getattr(args, 'force_offload', None):
        if args.force_offload in ('local', 'local_only'):
            enforce_mode = 'local_only'
        elif args.force_offload in ('remote', 'remote_only'):
            enforce_mode = 'remote_only'
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs("results/single_agent", exist_ok=True)
    
    # ğŸ¯ æ˜¾ç¤ºCAMTD3ç³»ç»Ÿä¿¡æ¯
    if args.algorithm and not args.compare:
        print("\n" + "="*80)
        print("ğŸš€ CAMTD3 è®­ç»ƒç³»ç»Ÿå¯åŠ¨")
        print("="*80)
        print(f"ç³»ç»Ÿåç§°: CAMTD3 (Cache-Aware Migration with Twin Delayed DDPG)")
        print(f"ä½¿ç”¨ç®—æ³•: {args.algorithm}")
        print(f"ç³»ç»Ÿæ¶æ„: Phase 1 (ä¸­å¤®èµ„æºåˆ†é…) + Phase 2 (ä»»åŠ¡æ‰§è¡Œ)")
        print(f"è®­ç»ƒè½®æ•°: {args.episodes}")
        if args.seed:
            print(f"éšæœºç§å­: {args.seed}")
        print(f"å®Œæ•´åç§°: CAMTD3-{args.algorithm}")
        print("="*80 + "\n")
    
    if args.compare:
        # æ¯”è¾ƒæ‰€æœ‰ç®—æ³•
        algorithms = ['DDPG', 'TD3', 'TD3-LE', 'DQN', 'PPO', 'SAC']
        compare_single_algorithms(algorithms, args.episodes)
    elif args.algorithm:
        # è®­ç»ƒå•ä¸ªç®—æ³• - ğŸ”§ ä¼ é€’override_scenarioå‚æ•°
        train_single_algorithm(
            args.algorithm, 
            args.episodes, 
            args.eval_interval, 
            args.save_interval,
            enable_realtime_vis=args.realtime_vis,
            vis_port=args.vis_port,
            override_scenario=override_scenario,  # ğŸ”§ æ–°å¢ï¼šä¼ é€’è¦†ç›–å‚æ•°
            use_enhanced_cache=not args.no_enhanced_cache,  # ğŸš€ é»˜è®¤å¯ç”¨å¢å¼ºç¼“å­˜
            enforce_offload_mode=enforce_mode,
            fixed_offload_policy=getattr(args, 'fixed_offload_policy', None),  # ğŸ¯ å›ºå®šå¸è½½ç­–ç•¥
            silent_mode=args.silent_mode,
            resume_from=args.resume_from,
            resume_lr_scale=args.resume_lr_scale,
            num_envs=args.num_envs
        )
    else:
        print("è¯·æŒ‡å®š --algorithm æˆ–ä½¿ç”¨ --compare æ ‡å¿—")
        print("ä½¿ç”¨ python train_single_agent.py --help æŸ¥çœ‹å¸®åŠ©")


if __name__ == "__main__":
    main()
    
"""

ğŸ”„ å®Œæ•´æ‰§è¡Œæµç¨‹ï¼ˆåˆ†5ä¸ªé˜¶æ®µï¼‰
ğŸ“Œ é˜¶æ®µ1: ç³»ç»Ÿåˆå§‹åŒ– (train_single_agent.py: mainå‡½æ•°)
1.1 å‚æ•°è§£æä¸é…ç½®
â”œâ”€ è§£æå‘½ä»¤è¡Œå‚æ•°
â”‚  â”œâ”€ algorithm = "TD3"
â”‚  â”œâ”€ episodes = 800  
â”‚  â”œâ”€ num_vehicles = 12
â”‚  â””â”€ enhanced_cache = True (é»˜è®¤)
â”‚
â”œâ”€ è®¾ç½®éšæœºç§å­
â”‚  â””â”€ ä»configæˆ–ç¯å¢ƒå˜é‡è¯»å–ç§å­
â”‚
â””â”€ æ„å»ºåœºæ™¯é…ç½® override_scenario
   â””â”€ {'num_vehicles': 12, 'override_topology': True}
   
1.2 åˆ›å»ºè®­ç»ƒç¯å¢ƒ (SingleAgentTrainingEnvironment)
ç¯å¢ƒåˆå§‹åŒ–æµç¨‹:
â”œâ”€ 1) é€‰æ‹©ä»¿çœŸå™¨ç±»å‹
â”‚  â”œâ”€ use_enhanced_cache=True
â”‚  â””â”€ simulator = EnhancedSystemSimulator(scenario_config)
â”‚
â”œâ”€ 2) åˆå§‹åŒ–ä»¿çœŸå™¨ç»„ä»¶ (system_simulator.py)
â”‚  â”œâ”€ è½¦è¾†åˆå§‹åŒ–: 12è¾†è½¦
â”‚  â”‚  â”œâ”€ ä½ç½®: éšæœºåˆ†å¸ƒåœ¨é“è·¯ä¸Š
â”‚  â”‚  â”œâ”€ é€Ÿåº¦: 30-50 km/h
â”‚  â”‚  â””â”€ ç¼“å­˜: L1(200MB) + L2(300MB)
â”‚  â”‚
â”‚  â”œâ”€ RSUéƒ¨ç½²: 4ä¸ªè·¯ä¾§å•å…ƒ (å›ºå®šæ‹“æ‰‘)
â”‚  â”‚  â”œâ”€ ä½ç½®: ç­‰é—´è·åˆ†å¸ƒ
â”‚  â”‚  â”œâ”€ è¦†ç›–åŠå¾„: 150m
â”‚  â”‚  â”œâ”€ ç¼“å­˜å®¹é‡: 1000MB
â”‚  â”‚  â””â”€ è®¡ç®—èƒ½åŠ›: 50 GHz
â”‚  â”‚
â”‚  â””â”€ UAVéƒ¨ç½²: 2ä¸ªæ— äººæœº
â”‚     â”œâ”€ ä½ç½®: åŠ¨æ€å·¡èˆª
â”‚     â”œâ”€ é«˜åº¦: 100m
â”‚     â”œâ”€ ç¼“å­˜å®¹é‡: 200MB
â”‚     â””â”€ è®¡ç®—èƒ½åŠ›: 20 GHz
â”‚
â”œâ”€ 3) åˆå§‹åŒ–è‡ªé€‚åº”æ§åˆ¶å™¨
â”‚  â”œâ”€ AdaptiveCacheController (æ™ºèƒ½ç¼“å­˜æ§åˆ¶)
â”‚  â”‚  â”œâ”€ åˆ†å±‚L1/L2ç¼“å­˜ç­–ç•¥
â”‚  â”‚  â”œâ”€ çƒ­åº¦è¿½è¸ª (HeatBasedStrategy)
â”‚  â”‚  â””â”€ RSUåä½œç¼“å­˜
â”‚  â”‚
â”‚  â””â”€ AdaptiveMigrationController (è¿ç§»å†³ç­–æ§åˆ¶)
â”‚     â”œâ”€ è´Ÿè½½å†å²è¿½è¸ª
â”‚     â”œâ”€ å¤šç»´è§¦å‘æ¡ä»¶
â”‚     â””â”€ æˆæœ¬æ•ˆç›Šåˆ†æ
â”‚
â””â”€ 4) æ‹“æ‰‘ä¼˜åŒ– (FixedTopologyOptimizer)
   â”œâ”€ æ ¹æ®è½¦è¾†æ•°ä¼˜åŒ–è¶…å‚æ•°
   â”œâ”€ num_vehicles=12 â†’ hidden_dim=512
   â”œâ”€ actor_lr=1e-4, critic_lr=8e-5
   â””â”€ batch_size=256
   
1.3 åˆ›å»ºTD3æ™ºèƒ½ä½“ (TD3Environment)
TD3ç®—æ³•åˆå§‹åŒ–:
â”œâ”€ ç½‘ç»œç»“æ„
â”‚  â”œâ”€ Actorç½‘ç»œ (ç­–ç•¥ç½‘ç»œ)
â”‚  â”‚  â”œâ”€ è¾“å…¥: state_dim = è½¦è¾†(12Ã—5) + RSU(4Ã—5) + UAV(2Ã—5) + å…¨å±€(16) = 106ç»´
â”‚  â”‚  â”œâ”€ éšè—å±‚: 512 â†’ 512 â†’ 256
â”‚  â”‚  â””â”€ è¾“å‡º: action_dim = 3(ä»»åŠ¡åˆ†é…) + 4(RSUé€‰æ‹©) + 2(UAVé€‰æ‹©) + 8(æ§åˆ¶å‚æ•°) = 17ç»´
â”‚  â”‚
â”‚  â”œâ”€ Twin Criticç½‘ç»œ (ä»·å€¼ç½‘ç»œÃ—2)
â”‚  â”‚  â”œâ”€ Critic1: è¯„ä¼°çŠ¶æ€-åŠ¨ä½œä»·å€¼
â”‚  â”‚  â”œâ”€ Critic2: å‡å°‘è¿‡ä¼°è®¡åå·®
â”‚  â”‚  â””â”€ è¾“å…¥: state(106ç»´) + action(17ç»´) â†’ è¾“å‡º: Qå€¼
â”‚  â”‚
â”‚  â””â”€ Targetç½‘ç»œ (ç›®æ ‡ç½‘ç»œ)
â”‚     â”œâ”€ Target Actor: ç”Ÿæˆç›®æ ‡åŠ¨ä½œ
â”‚     â”œâ”€ Target Critic1 & Critic2: è®¡ç®—ç›®æ ‡Qå€¼
â”‚     â””â”€ è½¯æ›´æ–°å‚æ•°: Ï„=0.005
â”‚
â”œâ”€ ç»éªŒå›æ”¾ç¼“å†²åŒº
â”‚  â”œâ”€ å®¹é‡: 100,000æ¡ç»éªŒ
â”‚  â”œâ”€ æ‰¹æ¬¡å¤§å°: 256
â”‚  â””â”€ ä¼˜å…ˆçº§ç»éªŒå›æ”¾ (PER)
â”‚     â”œâ”€ Î±=0.6 (ä¼˜å…ˆçº§æŒ‡æ•°)
â”‚     â””â”€ Î²=0.4â†’1.0 (é‡è¦æ€§é‡‡æ ·)
â”‚
â””â”€ TD3ç‰¹æœ‰æœºåˆ¶
   â”œâ”€ ç­–ç•¥å»¶è¿Ÿæ›´æ–°: policy_delay=2 (æ¯2æ­¥æ›´æ–°Actor)
   â”œâ”€ ç›®æ ‡ç­–ç•¥å¹³æ»‘: target_noise=0.05
   â”œâ”€ æ¢ç´¢å™ªå£°: exploration_noise=0.2 (æŒ‡æ•°è¡°å‡)
   â””â”€ æ¢¯åº¦è£å‰ª: gradient_clip=0.7
   
ğŸ“Œ é˜¶æ®µ2: Episodeå¾ªç¯ (è®­ç»ƒ800ä¸ªepisode)
2.1 Episodeé‡ç½®
æ¯ä¸ªEpisodeå¼€å§‹æ—¶:
â”œâ”€ 1) é‡ç½®ä»¿çœŸå™¨ (system_simulator.py: initialize_components)
â”‚  â”œâ”€ æ¸…ç©ºæ‰€æœ‰é˜Ÿåˆ—
â”‚  â”œâ”€ é‡ç½®è½¦è¾†ä½ç½®å’Œé€Ÿåº¦
â”‚  â”œâ”€ æ¸…ç©ºç¼“å­˜å†…å®¹
â”‚  â”œâ”€ é‡ç½®ç»Ÿè®¡æ•°æ®
â”‚  â””â”€ é‡æ–°ç”Ÿæˆå†…å®¹åº“ (1000ä¸ªå†…å®¹)
â”‚
â”œâ”€ 2) æ„å»ºåˆå§‹çŠ¶æ€
â”‚  â”œâ”€ è½¦è¾†çŠ¶æ€ (12Ã—5ç»´)
â”‚  â”‚  â”œâ”€ ä½ç½®(x,y): å½’ä¸€åŒ–åˆ°[0,1]
â”‚  â”‚  â”œâ”€ é€Ÿåº¦: å½’ä¸€åŒ–åˆ°[0,1]
â”‚  â”‚  â”œâ”€ ä»»åŠ¡é˜Ÿåˆ—é•¿åº¦: å½’ä¸€åŒ–
â”‚  â”‚  â””â”€ èƒ½è€—: å½’ä¸€åŒ–
â”‚  â”‚
â”‚  â”œâ”€ RSUçŠ¶æ€ (4Ã—5ç»´)
â”‚  â”‚  â”œâ”€ ä½ç½®(x,y)
â”‚  â”‚  â”œâ”€ ç¼“å­˜åˆ©ç”¨ç‡
â”‚  â”‚  â”œâ”€ é˜Ÿåˆ—è´Ÿè½½
â”‚  â”‚  â””â”€ èƒ½è€—
â”‚  â”‚
â”‚  â”œâ”€ UAVçŠ¶æ€ (2Ã—5ç»´)
â”‚  â”‚  â”œâ”€ ä½ç½®(x,y,z)
â”‚  â”‚  â”œâ”€ ç¼“å­˜åˆ©ç”¨ç‡
â”‚  â”‚  â””â”€ èƒ½è€—
â”‚  â”‚
â”‚  â””â”€ å…¨å±€çŠ¶æ€ (16ç»´)
â”‚     â”œâ”€ å¹³å‡é˜Ÿåˆ—é•¿åº¦
â”‚     â”œâ”€ å¹³å‡ç¼“å­˜åˆ©ç”¨ç‡
â”‚     â”œâ”€ ç³»ç»Ÿè´Ÿè½½
â”‚     â”œâ”€ ä»»åŠ¡ç±»å‹åˆ†å¸ƒ (4ç»´)
â”‚     â”œâ”€ ä»»åŠ¡ç±»å‹é˜Ÿåˆ—å æ¯” (4ç»´)
â”‚     â””â”€ ä»»åŠ¡ç±»å‹æˆªæ­¢æœŸ (4ç»´)
â”‚
â””â”€ 3) é‡ç½®æ§åˆ¶å™¨çŠ¶æ€
   â”œâ”€ ç¼“å­˜æ§åˆ¶å™¨: æ¸…ç©ºçƒ­åº¦è¿½è¸ª
   â””â”€ è¿ç§»æ§åˆ¶å™¨: æ¸…ç©ºè´Ÿè½½å†å²

2.2 æ—¶é—´æ­¥å¾ªç¯ (æ¯ä¸ªEpisodeçº¦200-300æ­¥)
æ¯ä¸ªæ—¶é—´æ­¥çš„æ‰§è¡Œæµç¨‹:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ­¥éª¤1: TD3é€‰æ‹©åŠ¨ä½œ (td3.py: select_action)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è¾“å…¥: state (106ç»´å‘é‡)                            â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ å‰å‘ä¼ æ’­é€šè¿‡Actorç½‘ç»œ                          â”‚
â”‚  â”‚  â””â”€ è¾“å‡ºåŸå§‹åŠ¨ä½œ: action_raw (17ç»´)             â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ æ·»åŠ æ¢ç´¢å™ªå£° (é«˜æ–¯å™ªå£°)                        â”‚
â”‚  â”‚  â”œâ”€ noise = N(0, exploration_noise)              â”‚
â”‚  â”‚  â””â”€ action = action_raw + noise                  â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ åŠ¨ä½œè£å‰ªåˆ°[-1, 1]                              â”‚
â”‚  â”‚                                                   â”‚
â”‚  â””â”€ åŠ¨ä½œåˆ†è§£ (decompose_action)                    â”‚
â”‚     â”œâ”€ ä»»åŠ¡åˆ†é…åå¥½ [0:3]                          â”‚
â”‚     â”‚  â””â”€ softmax([local, rsu, uav])               â”‚
â”‚     â”œâ”€ RSUé€‰æ‹©æƒé‡ [3:7]                           â”‚
â”‚     â”‚  â””â”€ softmax(4ä¸ªRSUçš„æƒé‡)                    â”‚
â”‚     â”œâ”€ UAVé€‰æ‹©æƒé‡ [7:9]                           â”‚
â”‚     â”‚  â””â”€ softmax(2ä¸ªUAVçš„æƒé‡)                    â”‚
â”‚     â””â”€ æ§åˆ¶å‚æ•° [9:17]                             â”‚
â”‚        â”œâ”€ ç¼“å­˜æ§åˆ¶ (4ç»´)                           â”‚
â”‚        â”‚  â”œâ”€ çƒ­åº¦é˜ˆå€¼è°ƒæ•´                          â”‚
â”‚        â”‚  â”œâ”€ æ·˜æ±°ç­–ç•¥æƒé‡                          â”‚
â”‚        â”‚  â”œâ”€ åä½œå¼ºåº¦                              â”‚
â”‚        â”‚  â””â”€ L1/L2æ¯”ä¾‹                             â”‚
â”‚        â””â”€ è¿ç§»æ§åˆ¶ (4ç»´)                           â”‚
â”‚           â”œâ”€ è´Ÿè½½é˜ˆå€¼                              â”‚
â”‚           â”œâ”€ æˆæœ¬æ•æ„Ÿåº¦                            â”‚
â”‚           â”œâ”€ å»¶è¿Ÿæƒé‡                              â”‚
â”‚           â””â”€ èƒ½è€—æƒé‡                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ­¥éª¤2: æ˜ å°„åŠ¨ä½œåˆ°è‡ªé€‚åº”æ§åˆ¶å™¨                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  (train_single_agent.py: _build_simulator_actions)  â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ è§£ææ§åˆ¶å‚æ•° (å8ç»´åŠ¨ä½œ)                       â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ è°ƒç”¨ map_agent_actions_to_params()             â”‚
â”‚  â”‚  â”œâ”€ å°†[-1,1]èŒƒå›´æ˜ å°„åˆ°å…·ä½“å‚æ•°èŒƒå›´             â”‚
â”‚  â”‚  â””â”€ åˆ†ç¦»ç¼“å­˜å‚æ•°å’Œè¿ç§»å‚æ•°                     â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ æ›´æ–° AdaptiveCacheController                   â”‚
â”‚  â”‚  â”œâ”€ heat_threshold = action[0] * 50 + 50        â”‚
â”‚  â”‚  â”œâ”€ eviction_strategy_weight = sigmoid(action[1])â”‚
â”‚  â”‚  â”œâ”€ collaboration_strength = action[2] * 0.5 + 0.5â”‚
â”‚  â”‚  â””â”€ l1_l2_ratio = action[3] * 0.3 + 0.4         â”‚
â”‚  â”‚                                                   â”‚
â”‚  â””â”€ æ›´æ–° AdaptiveMigrationController               â”‚
â”‚     â”œâ”€ load_threshold = action[4] * 0.3 + 0.6      â”‚
â”‚     â”œâ”€ cost_sensitivity = action[5] * 0.5 + 0.5    â”‚
â”‚     â”œâ”€ delay_weight = action[6] * 0.4 + 0.4        â”‚
â”‚     â””â”€ energy_weight = action[7] * 0.4 + 0.4       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ­¥éª¤3: ä»¿çœŸå™¨æ‰§è¡Œä¸€æ­¥                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  (system_simulator.py: run_simulation_step)         â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 3.1 æ›´æ–°è½¦è¾†ä½ç½®                               â”‚
â”‚  â”‚  â”œâ”€ æ ¹æ®é€Ÿåº¦å’Œæ–¹å‘ç§»åŠ¨                         â”‚
â”‚  â”‚  â”œâ”€ å¤„ç†è·¯å£è½¬å‘                               â”‚
â”‚  â”‚  â””â”€ æ·»åŠ éšæœºæ‰°åŠ¨                               â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 3.2 ç”Ÿæˆä»»åŠ¡                                   â”‚
â”‚  â”‚  â”œâ”€ æ³Šæ¾è¿‡ç¨‹é‡‡æ · (Î»=è½¦è¾†æ•°Ã—ä»»åŠ¡ç‡)            â”‚
â”‚  â”‚  â”œâ”€ ä¸ºæ¯è¾†è½¦ç”Ÿæˆä»»åŠ¡                           â”‚
â”‚  â”‚  â”‚  â”œâ”€ ä»»åŠ¡ç±»å‹ (1-4): æ ¹æ®åœºæ™¯åˆ†å¸ƒ           â”‚
â”‚  â”‚  â”‚  â”œâ”€ æ•°æ®å¤§å°: 0.5-2.0 MB                    â”‚
â”‚  â”‚  â”‚  â”œâ”€ è®¡ç®—éœ€æ±‚: 500-3000 CPUå‘¨æœŸ              â”‚
â”‚  â”‚  â”‚  â””â”€ æˆªæ­¢æœŸ: 0.5-3.0ç§’                       â”‚
â”‚  â”‚  â””â”€ æ·»åŠ åˆ°è½¦è¾†ä»»åŠ¡é˜Ÿåˆ—                         â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 3.3 ä»»åŠ¡åˆ†é…ä¸è°ƒåº¦                             â”‚
â”‚  â”‚  â”œâ”€ å¯¹æ¯ä¸ªä»»åŠ¡å†³ç­–å¸è½½ç›®æ ‡                     â”‚
â”‚  â”‚  â”‚  â”œâ”€ æœ¬åœ°å¤„ç† (æ¦‚ç‡: local_pref)             â”‚
â”‚  â”‚  â”‚  â”œâ”€ RSUå¸è½½ (æ¦‚ç‡: rsu_pref)                â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€ æ ¹æ®RSUé€‰æ‹©æƒé‡é€‰æ‹©å…·ä½“RSU          â”‚
â”‚  â”‚  â”‚  â””â”€ UAVå¸è½½ (æ¦‚ç‡: uav_pref)                â”‚
â”‚  â”‚  â”‚     â””â”€ æ ¹æ®UAVé€‰æ‹©æƒé‡é€‰æ‹©å…·ä½“UAV          â”‚
â”‚  â”‚  â”‚                                              â”‚
â”‚  â”‚  â”œâ”€ ç¼“å­˜å‘½ä¸­æ£€æŸ¥                               â”‚
â”‚  â”‚  â”‚  â””â”€ check_cache_hit_adaptive()              â”‚
â”‚  â”‚  â”‚     â”œâ”€ æ£€æŸ¥å†…å®¹æ˜¯å¦åœ¨èŠ‚ç‚¹ç¼“å­˜ä¸­            â”‚
â”‚  â”‚  â”‚     â”œâ”€ å‘½ä¸­: å‡å°‘ä¼ è¾“æ—¶å»¶                  â”‚
â”‚  â”‚  â”‚     â””â”€ æœªå‘½ä¸­: æ™ºèƒ½ç¼“å­˜å†³ç­–                â”‚
â”‚  â”‚  â”‚        â”œâ”€ è°ƒç”¨ç¼“å­˜æ§åˆ¶å™¨.should_cache_contentâ”‚
â”‚  â”‚  â”‚        â”œâ”€ åŸºäºçƒ­åº¦å†³å®šæ˜¯å¦ç¼“å­˜             â”‚
â”‚  â”‚  â”‚        â””â”€ æ‰§è¡Œæ·˜æ±°å’Œåä½œç¼“å­˜               â”‚
â”‚  â”‚  â”‚                                              â”‚
â”‚  â”‚  â””â”€ ä»»åŠ¡ä¼ è¾“ä¸å…¥é˜Ÿ                             â”‚
â”‚  â”‚     â”œâ”€ è®¡ç®—ä¸Šè¡Œä¼ è¾“æ—¶å»¶å’Œèƒ½è€—                 â”‚
â”‚  â”‚     â”œâ”€ å°†ä»»åŠ¡åŠ å…¥èŠ‚ç‚¹è®¡ç®—é˜Ÿåˆ—                 â”‚
â”‚  â”‚     â””â”€ è®°å½•ä»»åŠ¡å…ƒæ•°æ®                         â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 3.4 å¤„ç†è®¡ç®—é˜Ÿåˆ—                               â”‚
â”‚  â”‚  â””â”€ _process_node_queues()                      â”‚
â”‚  â”‚     â”œâ”€ éå†æ‰€æœ‰RSUå’ŒUAV                        â”‚
â”‚  â”‚     â”œâ”€ å¯¹æ¯ä¸ªèŠ‚ç‚¹:                             â”‚
â”‚  â”‚     â”‚  â”œâ”€ è·å–é˜Ÿåˆ—é•¿åº¦                         â”‚
â”‚  â”‚     â”‚  â”œâ”€ åŠ¨æ€è°ƒæ•´å¤„ç†èƒ½åŠ›                     â”‚
â”‚  â”‚     â”‚  â”‚  â””â”€ capacity = base + boost(é˜Ÿåˆ—é•¿åº¦) â”‚
â”‚  â”‚     â”‚  â”œâ”€ å¤„ç†ä»»åŠ¡å·¥ä½œé‡                       â”‚
â”‚  â”‚     â”‚  â”‚  â””â”€ work_remaining -= capacity        â”‚
â”‚  â”‚     â”‚  â”œâ”€ å®Œæˆçš„ä»»åŠ¡:                          â”‚
â”‚  â”‚     â”‚  â”‚  â”œâ”€ è®¡ç®—ä¸‹è¡Œä¼ è¾“                      â”‚
â”‚  â”‚     â”‚  â”‚  â”œâ”€ æ›´æ–°ç»Ÿè®¡(å»¶è¿Ÿã€èƒ½è€—)             â”‚
â”‚  â”‚     â”‚  â”‚  â””â”€ æ ‡è®°å®Œæˆ                          â”‚
â”‚  â”‚     â”‚  â””â”€ å¤„ç†è¶…æœŸä»»åŠ¡                         â”‚
â”‚  â”‚     â””â”€ æ›´æ–°èŠ‚ç‚¹çŠ¶æ€                             â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 3.5 è‡ªé€‚åº”è¿ç§»æ£€æŸ¥                             â”‚
â”‚  â”‚  â””â”€ check_adaptive_migration()                  â”‚
â”‚  â”‚     â”œâ”€ è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹è´Ÿè½½å› å­                    â”‚
â”‚  â”‚     â”‚  â””â”€ load = 0.8Ã—é˜Ÿåˆ—è´Ÿè½½ + 0.2Ã—ç¼“å­˜åˆ©ç”¨ç‡â”‚
â”‚  â”‚     â”œâ”€ æ›´æ–°è¿ç§»æ§åˆ¶å™¨è´Ÿè½½å†å²                  â”‚
â”‚  â”‚     â”œâ”€ åˆ¤æ–­æ˜¯å¦è§¦å‘è¿ç§»                        â”‚
â”‚  â”‚     â”‚  â”œâ”€ è´Ÿè½½è¶…é˜ˆå€¼                           â”‚
â”‚  â”‚     â”‚  â”œâ”€ æŒç»­æ—¶é—´è¶³å¤Ÿ                         â”‚
â”‚  â”‚     â”‚  â””â”€ æˆæœ¬æ•ˆç›Šåˆ†æé€šè¿‡                     â”‚
â”‚  â”‚     â””â”€ æ‰§è¡Œè¿ç§»                                â”‚
â”‚  â”‚        â”œâ”€ RSUâ†’RSU (æœ‰çº¿è¿ç§»)                  â”‚
â”‚  â”‚        â”‚  â”œâ”€ é€‰æ‹©ç›®æ ‡RSU (è´Ÿè½½æœ€è½»)           â”‚
â”‚  â”‚        â”‚  â”œâ”€ è®¡ç®—è¿ç§»æˆæœ¬                      â”‚
â”‚  â”‚        â”‚  â”œâ”€ ä¼ è¾“ä»»åŠ¡                          â”‚
â”‚  â”‚        â”‚  â””â”€ æ›´æ–°ç»Ÿè®¡                          â”‚
â”‚  â”‚        â””â”€ UAVâ†’RSU (æ— çº¿è¿ç§»)                  â”‚
â”‚  â”‚           â””â”€ ç±»ä¼¼æµç¨‹                          â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 3.6 æ›´æ–°ç»Ÿè®¡æŒ‡æ ‡                               â”‚
â”‚  â”‚  â”œâ”€ ç´¯è®¡å®Œæˆä»»åŠ¡æ•°                             â”‚
â”‚  â”‚  â”œâ”€ ç´¯è®¡å»¶è¿Ÿ                                   â”‚
â”‚  â”‚  â”œâ”€ ç´¯è®¡èƒ½è€—                                   â”‚
â”‚  â”‚  â”œâ”€ ç¼“å­˜å‘½ä¸­ç‡                                 â”‚
â”‚  â”‚  â”œâ”€ è¿ç§»æˆåŠŸç‡                                 â”‚
â”‚  â”‚  â””â”€ ä»»åŠ¡ç±»å‹åˆ†å¸ƒç»Ÿè®¡                           â”‚
â”‚  â”‚                                                   â”‚
â”‚  â””â”€ è¿”å› step_stats (æœ¬æ­¥ç»Ÿè®¡æ•°æ®)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ­¥éª¤4: è®¡ç®—å¥–åŠ±å’Œä¸‹ä¸€çŠ¶æ€                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  (train_single_agent.py: step æ–¹æ³•)                 â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 4.1 æå–ç³»ç»ŸæŒ‡æ ‡                               â”‚
â”‚  â”‚  â”œâ”€ å¹³å‡å»¶è¿Ÿ: avg_delay (ç§’)                   â”‚
â”‚  â”‚  â”œâ”€ æ€»èƒ½è€—: total_energy (ç„¦è€³)                â”‚
â”‚  â”‚  â”œâ”€ ä»»åŠ¡å®Œæˆç‡: completion_rate                â”‚
â”‚  â”‚  â”œâ”€ ç¼“å­˜å‘½ä¸­ç‡: cache_hit_rate                 â”‚
â”‚  â”‚  â”œâ”€ æ•°æ®ä¸¢å¤±ç‡: data_loss_ratio                â”‚
â”‚  â”‚  â””â”€ è¿ç§»æˆåŠŸç‡: migration_success_rate         â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 4.2 è°ƒç”¨ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨                         â”‚
â”‚  â”‚  â””â”€ unified_reward_calculator.calculate_reward()â”‚
â”‚  â”‚     â”‚                                            â”‚
â”‚  â”‚     â”œâ”€ å»¶è¿Ÿæƒ©ç½š: -Î± Ã— log(avg_delay + Îµ)       â”‚
â”‚  â”‚     â”‚  â””â”€ Î±=15.0, å¼ºè°ƒä½å»¶è¿Ÿ                  â”‚
â”‚  â”‚     â”‚                                            â”‚
â”‚  â”‚     â”œâ”€ èƒ½è€—æƒ©ç½š: -Î² Ã— log(total_energy + Îµ)    â”‚
â”‚  â”‚     â”‚  â””â”€ Î²=0.01, å¹³è¡¡èƒ½æ•ˆ                    â”‚
â”‚  â”‚     â”‚                                            â”‚
â”‚  â”‚     â”œâ”€ å®Œæˆç‡å¥–åŠ±: +Î³ Ã— completion_rate        â”‚
â”‚  â”‚     â”‚  â””â”€ Î³=200.0, é¼“åŠ±ä»»åŠ¡å®Œæˆ               â”‚
â”‚  â”‚     â”‚                                            â”‚
â”‚  â”‚     â”œâ”€ ç¼“å­˜å‘½ä¸­å¥–åŠ±: +Î´ Ã— cache_hit_rate       â”‚
â”‚  â”‚     â”‚  â””â”€ Î´=10.0, é¼“åŠ±é«˜å‘½ä¸­ç‡                â”‚
â”‚  â”‚     â”‚                                            â”‚
â”‚  â”‚     â”œâ”€ æ•°æ®ä¸¢å¤±æƒ©ç½š: -Îµ Ã— data_loss_ratio      â”‚
â”‚  â”‚     â”‚  â””â”€ Îµ=50.0, é¿å…ä¸¢åŒ…                    â”‚
â”‚  â”‚     â”‚                                            â”‚
â”‚  â”‚     â””â”€ è¿ç§»æˆåŠŸå¥–åŠ±: +Î¶ Ã— migration_success    â”‚
â”‚  â”‚        â””â”€ Î¶=5.0, é¼“åŠ±æœ‰æ•ˆè¿ç§»                 â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”‚     æœ€ç»ˆå¥–åŠ± = Î£(å„é¡¹å¥–åŠ±/æƒ©ç½š)                â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 4.3 æ„å»ºä¸‹ä¸€çŠ¶æ€å‘é‡ (106ç»´)                   â”‚
â”‚  â”‚  â””â”€ ä¸åˆå§‹çŠ¶æ€ç›¸åŒçš„ç»“æ„                       â”‚
â”‚  â”‚                                                   â”‚
â”‚  â””â”€ 4.4 åˆ¤æ–­Episodeæ˜¯å¦ç»“æŸ                        â”‚
â”‚     â”œâ”€ è¾¾åˆ°æœ€å¤§æ­¥æ•° (200-300æ­¥)                   â”‚
â”‚     â”œâ”€ ç³»ç»Ÿå´©æºƒ (æ‰€æœ‰èŠ‚ç‚¹è¿‡è½½)                     â”‚
â”‚     â””â”€ å®Œæˆç‡è¿‡ä½ (<20%)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ­¥éª¤5: TD3å­¦ä¹ æ›´æ–° (td3.py: update)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 5.1 å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº                       â”‚
â”‚  â”‚  â””â”€ buffer.add(state, action, reward, next_state, done)â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 5.2 é‡‡æ ·æ‰¹æ¬¡æ•°æ® (batch_size=256)              â”‚
â”‚  â”‚  â””â”€ ä½¿ç”¨PERä¼˜å…ˆçº§é‡‡æ ·                          â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 5.3 è®¡ç®—CriticæŸå¤±                             â”‚
â”‚  â”‚  â”œâ”€ ç”Ÿæˆç›®æ ‡åŠ¨ä½œ (Target Actor)                â”‚
â”‚  â”‚  â”‚  â””â”€ target_action = target_actor(next_state) â”‚
â”‚  â”‚  â”‚     + clipped_noise  # ç›®æ ‡ç­–ç•¥å¹³æ»‘        â”‚
â”‚  â”‚  â”‚                                              â”‚
â”‚  â”‚  â”œâ”€ è®¡ç®—ç›®æ ‡Qå€¼ (Twin Target Critics)          â”‚
â”‚  â”‚  â”‚  â”œâ”€ q1_target = target_critic1(next_state, target_action)â”‚
â”‚  â”‚  â”‚  â”œâ”€ q2_target = target_critic2(next_state, target_action)â”‚
â”‚  â”‚  â”‚  â””â”€ target_q = min(q1, q2)  # å‡å°‘è¿‡ä¼°è®¡    â”‚
â”‚  â”‚  â”‚                                              â”‚
â”‚  â”‚  â”œâ”€ è®¡ç®—TDç›®æ ‡                                 â”‚
â”‚  â”‚  â”‚  â””â”€ y = reward + Î³ Ã— (1-done) Ã— target_q   â”‚
â”‚  â”‚  â”‚                                              â”‚
â”‚  â”‚  â”œâ”€ è®¡ç®—å½“å‰Qå€¼                                â”‚
â”‚  â”‚  â”‚  â”œâ”€ current_q1 = critic1(state, action)     â”‚
â”‚  â”‚  â”‚  â””â”€ current_q2 = critic2(state, action)     â”‚
â”‚  â”‚  â”‚                                              â”‚
â”‚  â”‚  â”œâ”€ CriticæŸå¤±                                 â”‚
â”‚  â”‚  â”‚  â””â”€ loss = MSE(current_q1, y) + MSE(current_q2, y)â”‚
â”‚  â”‚  â”‚                                              â”‚
â”‚  â”‚  â””â”€ åå‘ä¼ æ’­æ›´æ–°Critic                         â”‚
â”‚  â”‚     â”œâ”€ critic_optimizer.zero_grad()             â”‚
â”‚  â”‚     â”œâ”€ loss.backward()                          â”‚
â”‚  â”‚     â”œâ”€ æ¢¯åº¦è£å‰ª (norm=0.7)                     â”‚
â”‚  â”‚     â””â”€ critic_optimizer.step()                  â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ 5.4 å»¶è¿ŸActoræ›´æ–° (æ¯policy_delay=2æ­¥)        â”‚
â”‚  â”‚  â”œâ”€ è®¡ç®—ActoræŸå¤±                              â”‚
â”‚  â”‚  â”‚  â”œâ”€ new_action = actor(state)                â”‚
â”‚  â”‚  â”‚  â””â”€ actor_loss = -critic1(state, new_action).mean()â”‚
â”‚  â”‚  â”‚                                              â”‚
â”‚  â”‚  â”œâ”€ åå‘ä¼ æ’­æ›´æ–°Actor                          â”‚
â”‚  â”‚  â”‚  â”œâ”€ actor_optimizer.zero_grad()              â”‚
â”‚  â”‚  â”‚  â”œâ”€ actor_loss.backward()                    â”‚
â”‚  â”‚  â”‚  â”œâ”€ æ¢¯åº¦è£å‰ª                                â”‚
â”‚  â”‚  â”‚  â””â”€ actor_optimizer.step()                   â”‚
â”‚  â”‚  â”‚                                              â”‚
â”‚  â”‚  â””â”€ è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ                             â”‚
â”‚  â”‚     â”œâ”€ target_actor = Ï„Ã—actor + (1-Ï„)Ã—target_actorâ”‚
â”‚  â”‚     â””â”€ target_critics = Ï„Ã—critics + (1-Ï„)Ã—target_criticsâ”‚
â”‚  â”‚                                                   â”‚
â”‚  â””â”€ 5.5 æ›´æ–°PERä¼˜å…ˆçº§                             â”‚
â”‚     â””â”€ æ ¹æ®TDè¯¯å·®æ›´æ–°æ ·æœ¬ä¼˜å…ˆçº§                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Œ é˜¶æ®µ3: Episodeç»“æŸä¸ç»Ÿè®¡
Episodeç»“æŸå:
â”œâ”€ è®°å½•Episodeç»Ÿè®¡
â”‚  â”œâ”€ æ€»å¥–åŠ±
â”‚  â”œâ”€ å¹³å‡å»¶è¿Ÿ
â”‚  â”œâ”€ æ€»èƒ½è€—
â”‚  â”œâ”€ å®Œæˆç‡
â”‚  â”œâ”€ ç¼“å­˜å‘½ä¸­ç‡
â”‚  â””â”€ è¿ç§»ç»Ÿè®¡
â”‚
â”œâ”€ è¡°å‡æ¢ç´¢å™ªå£°
â”‚  â””â”€ exploration_noise *= noise_decay (0.9997)
â”‚
â””â”€ æ‰“å°è¿›åº¦ä¿¡æ¯
   â””â”€ æ¯50ä¸ªEpisodeæ‰“å°ä¸€æ¬¡è¯¦ç»†ç»Ÿè®¡

ğŸ“Œ é˜¶æ®µ4: å‘¨æœŸæ€§è¯„ä¼° (æ¯eval_interval=50ä¸ªepisode)
è¯„ä¼°æµç¨‹:
â”œâ”€ å…³é—­æ¢ç´¢å™ªå£°
â”œâ”€ è¿è¡Œ10ä¸ªæµ‹è¯•Episode
â”œâ”€ è®¡ç®—å¹³å‡æ€§èƒ½æŒ‡æ ‡
â”‚  â”œâ”€ å¹³å‡å¥–åŠ±
â”‚  â”œâ”€ å¹³å‡å»¶è¿Ÿ
â”‚  â”œâ”€ å¹³å‡èƒ½è€—
â”‚  â””â”€ å¹³å‡å®Œæˆç‡
â””â”€ ä¿å­˜æ€§èƒ½æ›²çº¿

ğŸ“Œ é˜¶æ®µ5: è®­ç»ƒç»“æŸä¸ä¿å­˜ (800ä¸ªepisodeå®Œæˆå)
ä¿å­˜ç»“æœ:
â”œâ”€ 1) æ¨¡å‹æƒé‡
â”‚  â””â”€ results/models/single_agent/td3/
â”‚     â”œâ”€ actor_final.pth
â”‚     â”œâ”€ critic1_final.pth
â”‚     â”œâ”€ critic2_final.pth
â”‚     â””â”€ target_networks_final.pth
â”‚
â”œâ”€ 2) è®­ç»ƒæ•°æ®
â”‚  â””â”€ results/single_agent/td3/training_results_YYYYMMDD_HHMMSS.json
â”‚     â”œâ”€ rewards: [...]
â”‚     â”œâ”€ delays: [...]
â”‚     â”œâ”€ energies: [...]
â”‚     â”œâ”€ completion_rates: [...]
â”‚     â””â”€ cache_metrics: {...}
â”‚
â””â”€ 3) å¯è§†åŒ–å›¾è¡¨
   â””â”€ results/single_agent/td3/training_chart_YYYYMMDD_HHMMSS.png
      â”œâ”€ å¥–åŠ±æ›²çº¿
      â”œâ”€ å»¶è¿Ÿæ›²çº¿
      â”œâ”€ èƒ½è€—æ›²çº¿
      â””â”€ å®Œæˆç‡æ›²çº¿
      
ğŸ”‘ æ ¸å¿ƒæŠ€æœ¯äº®ç‚¹
1. Twin Delayed DDPG (TD3)
    åŒCriticç½‘ç»œå‡å°‘Qå€¼è¿‡ä¼°è®¡
    å»¶è¿Ÿç­–ç•¥æ›´æ–°æé«˜ç¨³å®šæ€§
    ç›®æ ‡ç­–ç•¥å¹³æ»‘åŒ–å‡å°‘æ–¹å·®
2. è‡ªé€‚åº”æ§åˆ¶æœºåˆ¶
    æ™ºèƒ½ç¼“å­˜æ§åˆ¶ï¼šçƒ­åº¦è¿½è¸ª + åˆ†å±‚ç¼“å­˜
    æ™ºèƒ½è¿ç§»æ§åˆ¶ï¼šå¤šç»´è§¦å‘ + æˆæœ¬æ•ˆç›Š
3. ç»Ÿä¸€å¥–åŠ±å‡½æ•°
    å¤šç›®æ ‡ä¼˜åŒ–ï¼šå»¶è¿Ÿã€èƒ½è€—ã€å®Œæˆç‡
    å¯¹æ•°æƒ©ç½šï¼šé¿å…æç«¯å€¼å½±å“
    å¹³è¡¡æƒé‡ï¼šç¡®ä¿å„é¡¹æŒ‡æ ‡åè°ƒ
4. åŠ¨æ€ç½‘ç»œæ‹“æ‰‘
    è½¦è¾†ç§»åŠ¨æ¨¡å‹ï¼šçœŸå®é“è·¯åœºæ™¯
    å›ºå®šRSU/UAVï¼šéªŒè¯ç®—æ³•æœ‰æ•ˆæ€§
    è‡ªé€‚åº”è®¡ç®—èµ„æºåˆ†é…

"""

