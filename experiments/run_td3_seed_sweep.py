#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TD3 å¤šéšæœºç§å­å®éªŒè„šæœ¬

ç¤ºä¾‹ç”¨æ³•ï¼š
    python experiments/run_td3_seed_sweep.py --seeds 42 2025 3407 --episodes 200
    python experiments/run_td3_seed_sweep.py --seed-start 0 --seed-count 5 --episodes 100

è„šæœ¬ä¼šå¾ªç¯è°ƒç”¨ `train_single_agent.train_single_algorithm`ï¼Œåˆ†åˆ«è®¾ç½®éšæœºç§å­ï¼Œ
å¹¶ä¿å­˜æ¯æ¬¡è¿è¡Œçš„å…³é”®æŒ‡æ ‡åˆ° JSON å’Œ Markdown æŠ¥å‘Šä¸­ï¼Œæ–¹ä¾¿è®ºæ–‡å¤ç°å®éªŒã€‚

ã€ç”¨é€”ã€‘
- æ‰¹é‡è¿è¡ŒTD3åœ¨å¤šä¸ªéšæœºç§å­ä¸‹çš„è®­ç»ƒï¼Œæ±‡æ€»å‡å€¼ä¸ç½®ä¿¡åŒºé—´ï¼Œè¾“å‡ºJSONä¸Markdownç®€æŠ¥ã€‚

ã€è¿è¡Œå‘½ä»¤ã€‘
- æŒ‡å®šç§å­åˆ—è¡¨ï¼špython experiments/run_td3_seed_sweep.py --seeds 42 2025 3407 --episodes 200
- è¿ç»­ç”Ÿæˆç§å­ï¼špython experiments/run_td3_seed_sweep.py --seed-start 0 --seed-count 5 --episodes 100
"""

from __future__ import annotations

import argparse
import json
import os
import sys
from datetime import datetime
from math import sqrt
from pathlib import Path
from statistics import NormalDist
from typing import Dict, List, Optional

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
parent_dir = Path(__file__).parent.parent
if str(parent_dir) not in sys.path:
    sys.path.insert(0, str(parent_dir))

from train_single_agent import (
    _apply_global_seed_from_env,  # type: ignore F401 - å†…éƒ¨å‡½æ•°ç”¨äºé‡è®¾éšæœºç§å­
    train_single_algorithm,
)

try:
    from scipy.stats import t as student_t  # type: ignore
except ImportError:  # pragma: no cover - optionalä¾èµ–
    student_t = None

CONFIDENCE_LEVEL = 0.95


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="è¿è¡ŒTD3å¤šéšæœºç§å­å®éªŒ")
    parser.add_argument(
        "--seeds",
        type=int,
        nargs="*",
        help="æ˜¾å¼æŒ‡å®šéšæœºç§å­åˆ—è¡¨ (ä¼˜å…ˆçº§é«˜äº --seed-start/--seed-count)",
    )
    parser.add_argument(
        "--seed-start",
        type=int,
        default=0,
        help="å½“æœªæ˜¾å¼æŒ‡å®š --seeds æ—¶ï¼Œèµ·å§‹éšæœºç§å­ (é»˜è®¤: 0)",
    )
    parser.add_argument(
        "--seed-count",
        type=int,
        default=3,
        help="å½“æœªæ˜¾å¼æŒ‡å®š --seeds æ—¶ï¼Œéœ€è¦è¿è¡Œçš„ç§å­æ•°é‡ (é»˜è®¤: 3)",
    )
    parser.add_argument(
        "--episodes",
        type=int,
        default=200,
        help="æ¯ä¸ªç§å­çš„è®­ç»ƒè½®æ•° (é»˜è®¤: 200)",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("results/experiments/td3_seed_sweep"),
        help="å®éªŒç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: results/experiments/td3_seed_sweep)",
    )
    parser.add_argument(
        "--eval-interval",
        type=int,
        default=None,
        help="è®­ç»ƒè¯„ä¼°é—´éš”ï¼Œé€ä¼ ç»™ train_single_algorithm (é»˜è®¤è‡ªåŠ¨)",
    )
    parser.add_argument(
        "--save-interval",
        type=int,
        default=None,
        help="æ¨¡å‹ä¿å­˜é—´éš”ï¼Œé€ä¼ ç»™ train_single_algorithm (é»˜è®¤è‡ªåŠ¨)",
    )
    return parser.parse_args()


def _build_seed_list(args: argparse.Namespace) -> List[int]:
    if args.seeds:
        return args.seeds
    return [args.seed_start + idx for idx in range(args.seed_count)]


def _run_single_seed(seed: int, episodes: int, eval_interval: int | None, save_interval: int | None) -> Dict:
    previous_seed = os.environ.get("RANDOM_SEED")
    os.environ["RANDOM_SEED"] = str(seed)
    _apply_global_seed_from_env()
    try:
        return train_single_algorithm(
            "TD3",
            num_episodes=episodes,
            eval_interval=eval_interval,
            save_interval=save_interval,
            silent_mode=True  # ğŸ”§ å¯ç”¨é™é»˜æ¨¡å¼ï¼Œé¿å…ç”¨æˆ·äº¤äº’é˜»å¡æ‰¹é‡å®éªŒ
        )
    finally:
        if previous_seed is not None:
            os.environ["RANDOM_SEED"] = previous_seed
            _apply_global_seed_from_env()
        else:
            os.environ.pop("RANDOM_SEED", None)


def _extract_summary(seed: int, run_result: Dict) -> Dict:
    final_perf = run_result.get("final_performance", {})
    training_cfg = run_result.get("training_config", {})
    return {
        "seed": seed,
        "episodes": training_cfg.get("num_episodes", 0),
        "training_time_hours": training_cfg.get("training_time_hours", 0.0),
        "avg_step_reward": final_perf.get("avg_step_reward", 0.0),
        "avg_delay": final_perf.get("avg_delay", 0.0),
        "avg_completion": final_perf.get("avg_completion", 0.0),
    }


def _compute_confidence_interval(values: List[float], confidence: float = CONFIDENCE_LEVEL) -> Dict[str, float]:
    cleaned = [float(v) for v in values if v is not None]
    n = len(cleaned)
    if n == 0:
        return {"mean": 0.0, "half_width": 0.0}
    mean_val = sum(cleaned) / n
    if n == 1:
        return {"mean": mean_val, "half_width": 0.0}
    variance = sum((v - mean_val) ** 2 for v in cleaned) / (n - 1)
    std_dev = sqrt(variance)
    standard_error = std_dev / sqrt(n)
    if student_t is not None:
        critical = float(student_t.ppf((1 + confidence) / 2.0, n - 1))
    else:
        critical = float(NormalDist().inv_cdf((1 + confidence) / 2.0))
    return {"mean": mean_val, "half_width": critical * standard_error}


def _aggregate_metrics(summaries: List[Dict], confidence: float) -> Dict[str, Dict[str, float]]:
    metric_keys = {
        "avg_step_reward": "avg_step_reward",
        "avg_delay": "avg_delay",
        "avg_completion": "avg_completion",
    }
    aggregates: Dict[str, Dict[str, float]] = {}
    for metric, source_key in metric_keys.items():
        metric_values = [summary.get(source_key, 0.0) for summary in summaries]
        aggregates[metric] = _compute_confidence_interval(metric_values, confidence=confidence)
    return aggregates


def _save_results(output_dir: Path, summaries: List[Dict],
                  aggregates: Optional[Dict[str, Dict[str, float]]],
                  confidence: float) -> None:
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    summary_path = output_dir / f"td3_seed_sweep_summary_{timestamp}.json"
    with summary_path.open("w", encoding="utf-8") as fp:
        payload = {"runs": summaries}
        if aggregates is not None:
            payload["aggregated"] = {
                "confidence": confidence,
                "metrics": aggregates,
            }
        json.dump(payload, fp, indent=2, ensure_ascii=False)

    # åŒæ­¥è¾“å‡ºMarkdownç®€æŠ¥ï¼Œæ–¹ä¾¿è®ºæ–‡ä½¿ç”¨
    md_path = output_dir / f"td3_seed_sweep_summary_{timestamp}.md"
    with md_path.open("w", encoding="utf-8") as fp:
        fp.write("# TD3 å¤šéšæœºç§å­å®éªŒç»“æœ\n\n")
        fp.write("| Seed | Episodes | Training Hours | Avg Step Reward | Avg Delay (s) | Completion Rate |\n")
        fp.write("| ---- | -------- | --------------- | ---------------- | ------------- | ---------------- |\n")
        for item in summaries:
            fp.write(
                f"| {item['seed']} | {item['episodes']} | {item['training_time_hours']:.3f} |"
                f" {item['avg_step_reward']:.4f} | {item['avg_delay']:.4f} | {item['avg_completion']:.2%} |\n"
            )
        if aggregates:
            fp.write("\n## ç»Ÿè®¡æ±‡æ€»\n\n")
            fp.write(f"ç½®ä¿¡æ°´å¹³ï¼š{confidence:.0%}\n\n")
            fp.write("| æŒ‡æ ‡ | å¹³å‡å€¼ | ç½®ä¿¡åŒºé—´åŠå®½ |\n")
            fp.write("| ---- | ------ | -------------- |\n")
            for metric, stats in aggregates.items():
                fp.write(f"| {metric} | {stats['mean']:.4f} | Â±{stats['half_width']:.4f} |\n")

    print(f"âœ… ç»“æœå·²ä¿å­˜: {summary_path}")
    print(f"âœ… Markdown ç®€æŠ¥: {md_path}")


def main() -> None:
    args = parse_args()
    seeds = _build_seed_list(args)
    print("=" * 80)
    print("ğŸš€ TD3 å¤šéšæœºç§å­å®éªŒå¯åŠ¨")
    print(f"è¿è¡Œç§å­: {seeds}")
    print(f"æ¯ä¸ªå®éªŒè®­ç»ƒè½®æ•°: {args.episodes}")
    print("=" * 80)

    summaries: List[Dict] = []
    for seed in seeds:
        print("-" * 60)
        print(f"â–¶ï¸ å¼€å§‹è¿è¡Œ Seed = {seed}")
        result = _run_single_seed(seed, args.episodes, args.eval_interval, args.save_interval)
        summary = _extract_summary(seed, result)
        summaries.append(summary)
        print(f"âœ… Seed {seed} è¿è¡Œå®Œæˆ: Avg Delay={summary['avg_delay']:.4f}s, Completion={summary['avg_completion']:.2%}")

    aggregates = _aggregate_metrics(summaries, confidence=CONFIDENCE_LEVEL)
    print("\nç»Ÿè®¡æ±‡æ€»ï¼ˆå‡å€¼ Â± ç½®ä¿¡åŒºé—´åŠå®½ï¼‰")
    for metric, stats in aggregates.items():
        print(f"- {metric}: {stats['mean']:.4f} Â± {stats['half_width']:.4f} (@ {CONFIDENCE_LEVEL:.0%})")

    _save_results(args.output_dir, summaries, aggregates, CONFIDENCE_LEVEL)
    print("=" * 80)
    print("ğŸ‰ å¤šéšæœºç§å­å®éªŒå…¨éƒ¨å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    main()


