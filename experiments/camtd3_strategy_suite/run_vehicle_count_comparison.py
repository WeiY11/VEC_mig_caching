#!/usr/bin/env python3
"""
CAMTD3 è½¦è¾†æ•°é‡å¯¹æ¯”å®éªŒï¼ˆå…­ç­–ç•¥ç‰ˆæœ¬ï¼‰
==========================================

ã€åŠŸèƒ½ã€‘
è¯„ä¼°ä¸åŒè½¦è¾†æ•°é‡å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“ï¼Œå¯¹æ¯”å…­ç§ç­–ç•¥çš„å¯æ‰©å±•æ€§ã€‚
é€šè¿‡æ‰«æä¸åŒçš„è½¦è¾†æ•°é‡é…ç½®ï¼Œåˆ†æï¼š
- ç³»ç»Ÿè§„æ¨¡å¦‚ä½•å½±å“å†³ç­–æ€§èƒ½
- å„ç­–ç•¥åœ¨ä¸åŒè§„æ¨¡ä¸‹çš„é€‚åº”èƒ½åŠ›
- ç³»ç»Ÿå¯æ‰©å±•æ€§å’Œå®¹é‡è§„åˆ’

ã€è®ºæ–‡å¯¹åº”ã€‘
- å‚æ•°æ•æ„Ÿæ€§åˆ†æï¼ˆParameter Sensitivity Analysisï¼‰
- ç³»ç»Ÿå¯æ‰©å±•æ€§è¯„ä¼°ï¼ˆScalabilityï¼‰
- éªŒè¯CAMTD3åœ¨ä¸åŒç½‘ç»œè§„æ¨¡ä¸‹çš„æ€§èƒ½

ã€å®éªŒè®¾è®¡ã€‘
æ‰«æå‚æ•°: num_vehicles (è½¦è¾†æ•°é‡)
- å°è§„æ¨¡: 6 è¾†ï¼ˆåŸºç¡€åœºæ™¯ï¼‰
- ä¸­å°è§„æ¨¡: 9 è¾†
- æ ‡å‡†è§„æ¨¡: 12 è¾†ï¼ˆé»˜è®¤é…ç½®ï¼‰
- ä¸­å¤§è§„æ¨¡: 15 è¾†
- å¤§è§„æ¨¡: 18 è¾†ï¼ˆé«˜å¯†åº¦åœºæ™¯ï¼‰

å›ºå®šå‚æ•°:
- RSUæ•°: 4
- UAVæ•°: 2
- è®­ç»ƒè½®æ•°: å¯é…ç½®ï¼ˆé»˜è®¤500ï¼‰

ã€æ ¸å¿ƒæŒ‡æ ‡ã€‘
- å¹³å‡æ€»æˆæœ¬ï¼ˆæ—¶å»¶+èƒ½è€—ï¼‰
- å¹³å‡æ—¶å»¶ï¼ˆè½¦è¾†è¶Šå¤šç«äº‰è¶Šæ¿€çƒˆï¼‰
- å¹³å‡èƒ½è€—ï¼ˆå—è´Ÿè½½å½±å“ï¼‰
- å½’ä¸€åŒ–æˆæœ¬

ã€ä½¿ç”¨ç¤ºä¾‹ã€‘
```bash
# âœ… é»˜è®¤é™é»˜è¿è¡Œï¼ˆæ— éœ€æ‰‹åŠ¨äº¤äº’ï¼Œæ¨èï¼‰
# å¿«é€Ÿæµ‹è¯•ï¼ˆ100è½®ï¼‰
python experiments/camtd3_strategy_suite/run_vehicle_count_comparison.py \\
    --episodes 100 --suite-id vehicle_quick

# å®Œæ•´å®éªŒï¼ˆ500è½®ï¼‰- è‡ªåŠ¨ä¿å­˜æŠ¥å‘Šï¼Œæ— äººå€¼å®ˆè¿è¡Œ
python experiments/camtd3_strategy_suite/run_vehicle_count_comparison.py \\
    --episodes 500 --seed 42 --suite-id vehicle_paper

# è‡ªå®šä¹‰è½¦è¾†æ•°é‡é…ç½®
python experiments/camtd3_strategy_suite/run_vehicle_count_comparison.py \\
    --vehicle-counts "6,12,18,24" --episodes 300

# ğŸ’¡ å¦‚éœ€äº¤äº’å¼ç¡®è®¤ä¿å­˜æŠ¥å‘Šï¼Œæ·»åŠ  --interactive å‚æ•°
python experiments/camtd3_strategy_suite/run_vehicle_count_comparison.py \\
    --episodes 500 --interactive
```

ã€é¢„è®¡è¿è¡Œæ—¶é—´ã€‘
- å¿«é€Ÿæµ‹è¯•ï¼ˆ100è½® Ã— 5é…ç½® Ã— 6ç­–ç•¥ï¼‰ï¼šçº¦1.5-2.5å°æ—¶
- å®Œæ•´å®éªŒï¼ˆ500è½® Ã— 5é…ç½® Ã— 6ç­–ç•¥ï¼‰ï¼šçº¦6-9å°æ—¶

ã€è¾“å‡ºå›¾è¡¨ã€‘
- vehicle_count_vs_cost.png: è½¦è¾†æ•° vs å¹³å‡æˆæœ¬
- vehicle_count_vs_delay.png: è½¦è¾†æ•° vs å¹³å‡æ—¶å»¶
- vehicle_count_vs_energy.png: è½¦è¾†æ•° vs å¹³å‡èƒ½è€—
- vehicle_count_vs_normalized_cost.png: è½¦è¾†æ•° vs å½’ä¸€åŒ–æˆæœ¬

ã€è®ºæ–‡è´¡çŒ®ã€‘
å±•ç¤ºCAMTD3åœ¨ä¸åŒç½‘ç»œè§„æ¨¡ä¸‹çš„ä¼˜åŠ¿ï¼Œè¯æ˜å…¶è‰¯å¥½çš„å¯æ‰©å±•æ€§
"""

from __future__ import annotations

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List

import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parents[2]
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from experiments.camtd3_strategy_suite.strategy_runner import (
    STRATEGY_KEYS,
    evaluate_configs,
    strategy_label,
)

DEFAULT_EPISODES = 500
DEFAULT_SEED = 42
DEFAULT_VEHICLE_COUNTS = [6, 9, 12, 15, 18]


def parse_vehicle_counts(value: str) -> List[int]:
    if not value or value.strip().lower() == "default":
        return list(DEFAULT_VEHICLE_COUNTS)
    return [int(x.strip()) for x in value.split(",") if x.strip()]


def plot_results(results: List[Dict[str, object]], suite_path: Path) -> None:
    vehicle_counts = [int(r["num_vehicles"]) for r in results]

    def make_chart(metric: str, ylabel: str, filename: str) -> None:
        plt.figure(figsize=(10, 6))
        for strat_key in STRATEGY_KEYS:
            values = [r["strategies"][strat_key][metric] for r in results]
            plt.plot(vehicle_counts, values, marker="o", linewidth=2, label=strategy_label(strat_key))
        plt.xlabel("Number of Vehicles")
        plt.ylabel(ylabel)
        plt.title(f"Impact of Vehicle Count on {ylabel}")
        plt.grid(alpha=0.3)
        plt.legend()
        plt.tight_layout()
        plt.savefig(suite_path / filename, dpi=300, bbox_inches="tight")
        plt.close()

    make_chart("raw_cost", "Average Cost", "vehicle_count_vs_cost.png")
    make_chart("avg_delay", "Average Delay (s)", "vehicle_count_vs_delay.png")
    make_chart("avg_energy", "Average Energy (J)", "vehicle_count_vs_energy.png")
    make_chart("normalized_cost", "Normalized Cost", "vehicle_count_vs_normalized_cost.png")

    print("\nCharts saved:")
    for name in [
        "vehicle_count_vs_cost.png",
        "vehicle_count_vs_delay.png",
        "vehicle_count_vs_energy.png",
        "vehicle_count_vs_normalized_cost.png",
    ]:
        print(f"  - {suite_path / name}")


def main() -> None:
    parser = argparse.ArgumentParser(description="Evaluate strategy performance across different vehicle counts.")
    parser.add_argument("--vehicle-counts", type=str, default="default", help="Comma-separated vehicle counts.")
    parser.add_argument("--episodes", type=int, help="Training episodes per configuration (default 500).")
    parser.add_argument("--seed", type=int, help="Random seed (default 42).")
    parser.add_argument(
        "--suite-id",
        type=str,
        default=f"vehicle_count_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        help="Suite identifier.",
    )
    parser.add_argument("--output-root", type=str, default="results/parameter_sensitivity", help="Output root directory.")
    parser.add_argument("--silent", action="store_true", default=True, help="Run training in silent mode (default: True for batch experiments).")
    parser.add_argument("--interactive", action="store_true", help="Enable interactive mode (overrides silent).")
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº† --interactiveï¼Œåˆ™ç¦ç”¨é™é»˜æ¨¡å¼
    if args.interactive:
        args.silent = False

    vehicle_counts = parse_vehicle_counts(args.vehicle_counts)
    episodes = args.episodes or DEFAULT_EPISODES
    seed = args.seed if args.seed is not None else DEFAULT_SEED

    configs: List[Dict[str, object]] = []
    for count in vehicle_counts:
        overrides = {
            "num_vehicles": count,
            "num_rsus": 4,
            "num_uavs": 2,
            "override_topology": True,
        }
        configs.append(
            {
                "key": f"{count}veh",
                "label": f"{count} Vehicles",
                "overrides": overrides,
                "num_vehicles": count,
            }
        )

    suite_path = Path(args.output_root) / args.suite_id
    results = evaluate_configs(
        configs=configs,
        episodes=episodes,
        seed=seed,
        silent=args.silent,
        suite_path=suite_path,
    )

    summary = {
        "experiment_type": "vehicle_count_sensitivity",
        "suite_id": args.suite_id,
        "created_at": datetime.now().isoformat(),
        "num_configs": len(results),
        "episodes_per_config": episodes,
        "seed": seed,
        "results": results,
    }
    summary_path = suite_path / "summary.json"
    summary_path.write_text(json.dumps(summary, indent=2, ensure_ascii=False), encoding="utf-8")

    plot_results(results, suite_path)

    print("\nVehicle Count Sensitivity Analysis Completed")
    print(f"Suite ID: {args.suite_id}")
    print(f"Configurations tested: {len(results)}")
    print(f"{'Vehicles':<12}", end="")
    for strat_key in STRATEGY_KEYS:
        print(f"{strategy_label(strat_key):>18}", end="")
    print()
    print("-" * (12 + 18 * len(STRATEGY_KEYS)))
    for record in results:
        print(f"{record['num_vehicles']:<12}", end="")
        for strat_key in STRATEGY_KEYS:
            print(f"{record['strategies'][strat_key]['raw_cost']:<18.4f}", end="")
        print()
    print(f"\nSummary saved to: {summary_path}")


if __name__ == "__main__":
    main()
