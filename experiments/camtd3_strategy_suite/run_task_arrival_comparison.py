#!/usr/bin/env python3
"""
CAMTD3 ä»»åŠ¡åˆ°è¾¾ç‡å¯¹æ¯”å®éªŒï¼ˆå…­ç­–ç•¥ç‰ˆæœ¬ï¼‰
==========================================

ã€åŠŸèƒ½ã€‘
è¯„ä¼°ä¸åŒä»»åŠ¡åˆ°è¾¾ç‡å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“ï¼Œå¯¹æ¯”å…­ç§ç­–ç•¥åœ¨ä¸åŒè´Ÿè½½å¼ºåº¦ä¸‹çš„è¡¨ç°ã€‚
é€šè¿‡æ‰«æä¸åŒçš„ä»»åŠ¡åˆ°è¾¾ç‡é…ç½®ï¼Œåˆ†æï¼š
- ç³»ç»Ÿè´Ÿè½½å¦‚ä½•å½±å“æ€»æˆæœ¬å’Œæ—¶å»¶
- å„ç­–ç•¥åœ¨é«˜è´Ÿè½½åœºæ™¯ä¸‹çš„é²æ£’æ€§
- ç³»ç»Ÿå®¹é‡çš„ä¸Šé™å’Œç“¶é¢ˆ

ã€è®ºæ–‡å¯¹åº”ã€‘
- å‚æ•°æ•æ„Ÿæ€§åˆ†æï¼ˆParameter Sensitivity Analysisï¼‰
- ç³»ç»Ÿå¯æ‰©å±•æ€§è¯„ä¼°
- é«˜è´Ÿè½½åœºæ™¯ä¸‹çš„æ€§èƒ½å¯¹æ¯”
Experiment design:
Sweep parameter: task_arrival_rate (tasks/s)
- Light load: 0.8 tasks/s
- Balanced load: 1.0 tasks/s
- Standard load: 1.2 tasks/s (default)
- High load: 1.4 tasks/s
- Stress load: 1.6 tasks/s



å›ºå®šå‚æ•°:
- è½¦è¾†æ•°: 12
- RSUæ•°: 4
- UAVæ•°: 2
- è®­ç»ƒè½®æ•°: å¯é…ç½®ï¼ˆé»˜è®¤500ï¼‰

ã€æ ¸å¿ƒæŒ‡æ ‡ã€‘
- å¹³å‡æ€»æˆæœ¬ï¼ˆæ—¶å»¶+èƒ½è€—ï¼‰
- å¹³å‡æ—¶å»¶ï¼ˆè´Ÿè½½è¶Šé«˜è¶Šå¤§ï¼‰
- ä»»åŠ¡å®Œæˆç‡ï¼ˆé«˜è´Ÿè½½ä¸‹çš„å…³é”®æŒ‡æ ‡ï¼‰
- å½’ä¸€åŒ–æˆæœ¬

ã€ä½¿ç”¨ç¤ºä¾‹ã€‘
```bash
# âœ… é»˜è®¤é™é»˜è¿è¡Œï¼ˆæ— éœ€æ‰‹åŠ¨äº¤äº’ï¼Œæ¨èï¼‰
# å¿«é€Ÿæµ‹è¯•ï¼ˆ100è½®ï¼‰
python experiments/camtd3_strategy_suite/run_task_arrival_comparison.py \\
    --episodes 100 --suite-id arrival_quick

# å®Œæ•´å®éªŒï¼ˆ500è½®ï¼‰- è‡ªåŠ¨ä¿å­˜æŠ¥å‘Šï¼Œæ— äººå€¼å®ˆè¿è¡Œ
python experiments/camtd3_strategy_suite/run_task_arrival_comparison.py \\
    --episodes 500 --seed 42 --suite-id arrival_paper

# ????????????tasks/s?

python experiments/camtd3_strategy_suite/run_task_arrival_comparison.py \\
    --arrival-rates "0.5,1.0,1.5,2.0" --episodes 300

# ğŸ’¡ å¦‚éœ€äº¤äº’å¼ç¡®è®¤ä¿å­˜æŠ¥å‘Šï¼Œæ·»åŠ  --interactive å‚æ•°
python experiments/camtd3_strategy_suite/run_task_arrival_comparison.py \\
    --episodes 500 --interactive
```

ã€é¢„è®¡è¿è¡Œæ—¶é—´ã€‘
- å¿«é€Ÿæµ‹è¯•ï¼ˆ100è½® Ã— 6é…ç½® Ã— 6ç­–ç•¥ï¼‰ï¼šçº¦1.5-3å°æ—¶
- å®Œæ•´å®éªŒï¼ˆ500è½® Ã— 6é…ç½® Ã— 6ç­–ç•¥ï¼‰ï¼šçº¦6-10å°æ—¶

ã€è¾“å‡ºå›¾è¡¨ã€‘
- arrival_rate_vs_cost.png: åˆ°è¾¾ç‡ vs å¹³å‡æˆæœ¬
- arrival_rate_vs_delay.png: åˆ°è¾¾ç‡ vs å¹³å‡æ—¶å»¶
- arrival_rate_vs_completion.png: åˆ°è¾¾ç‡ vs ä»»åŠ¡å®Œæˆç‡
- arrival_rate_vs_normalized_cost.png: åˆ°è¾¾ç‡ vs å½’ä¸€åŒ–æˆæœ¬
"""

from __future__ import annotations

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List

import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parents[2]
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from experiments.camtd3_strategy_suite.strategy_runner import (
    STRATEGY_KEYS,
    evaluate_configs,
    strategy_label,
)

DEFAULT_EPISODES = 500
DEFAULT_SEED = 42
DEFAULT_ARRIVAL_RATES = [0.8, 1.0, 1.2, 1.4, 1.6]


def parse_arrival_rates(value: str) -> List[float]:
    if not value or value.strip().lower() == "default":
        return list(DEFAULT_ARRIVAL_RATES)
    return [float(item.strip()) for item in value.split(",") if item.strip()]


def plot_results(results: List[Dict[str, object]], suite_path: Path) -> None:
    arrival_rates = [float(r["arrival_rate"]) for r in results]

    def make_chart(metric: str, ylabel: str, filename: str) -> None:
        plt.figure(figsize=(10, 6))
        for strat_key in STRATEGY_KEYS:
            values = [r["strategies"][strat_key][metric] for r in results]
            plt.plot(arrival_rates, values, marker="o", linewidth=2, label=strategy_label(strat_key))
        plt.xlabel("Task Arrival Rate (tasks/s)")
        plt.ylabel(ylabel)
        plt.title(f"Impact of Arrival Rate on {ylabel}")
        plt.grid(alpha=0.3)
        plt.legend()
        plt.tight_layout()
        plt.savefig(suite_path / filename, dpi=300, bbox_inches="tight")
        plt.close()

    make_chart("raw_cost", "Average Cost", "arrival_rate_vs_cost.png")
    make_chart("avg_delay", "Average Delay (s)", "arrival_rate_vs_delay.png")
    make_chart("completion_rate", "Completion Rate", "arrival_rate_vs_completion.png")
    make_chart("normalized_cost", "Normalized Cost", "arrival_rate_vs_normalized_cost.png")

    print("\nCharts saved:")
    for name in [
        "arrival_rate_vs_cost.png",
        "arrival_rate_vs_delay.png",
        "arrival_rate_vs_completion.png",
        "arrival_rate_vs_normalized_cost.png",
    ]:
        print(f"  - {suite_path / name}")


def main() -> None:
    parser = argparse.ArgumentParser(description="Evaluate strategy performance across task arrival rates.")
    parser.add_argument("--arrival-rates", type=str, default="default", help="Comma-separated arrival rates.")
    parser.add_argument("--episodes", type=int, help="Training episodes per configuration (default 500).")
    parser.add_argument("--seed", type=int, help="Random seed (default 42).")
    parser.add_argument(
        "--suite-id",
        type=str,
        default=f"arrival_rate_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        help="Suite identifier.",
    )
    parser.add_argument("--output-root", type=str, default="results/parameter_sensitivity", help="Output root directory.")
    parser.add_argument("--silent", action="store_true", default=True, help="Run training in silent mode (default: True for batch experiments).")
    parser.add_argument("--interactive", action="store_true", help="Enable interactive mode (overrides silent).")
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº† --interactiveï¼Œåˆ™ç¦ç”¨é™é»˜æ¨¡å¼
    if args.interactive:
        args.silent = False

    arrival_rates = parse_arrival_rates(args.arrival_rates)
    episodes = args.episodes or DEFAULT_EPISODES
    seed = args.seed if args.seed is not None else DEFAULT_SEED

    configs: List[Dict[str, object]] = []
    for rate in arrival_rates:
        overrides = {
            "num_vehicles": 12,
            "num_rsus": 4,
            "num_uavs": 2,
            "task_arrival_rate": float(rate),
            "override_topology": True,
        }
        configs.append(
            {
                "key": f"{rate:.2f}",
                "label": f"{rate:.2f} tasks/s",
                "overrides": overrides,
                "arrival_rate": rate,
            }
        )

    suite_path = Path(args.output_root) / args.suite_id
    results = evaluate_configs(
        configs=configs,
        episodes=episodes,
        seed=seed,
        silent=args.silent,
        suite_path=suite_path,
    )

    summary = {
        "experiment_type": "task_arrival_sensitivity",
        "suite_id": args.suite_id,
        "created_at": datetime.now().isoformat(),
        "num_configs": len(results),
        "episodes_per_config": episodes,
        "seed": seed,
        "results": results,
    }
    summary_path = suite_path / "summary.json"
    summary_path.write_text(json.dumps(summary, indent=2, ensure_ascii=False), encoding="utf-8")

    plot_results(results, suite_path)

    print("\nArrival Rate Sensitivity Analysis Completed")
    print(f"Suite ID: {args.suite_id}")
    print(f"Configurations tested: {len(results)}")
    print(f"{'Arrival Rate':<18}", end="")
    for strat_key in STRATEGY_KEYS:
        print(f"{strategy_label(strat_key):>18}", end="")
    print()
    print("-" * (18 + 18 * len(STRATEGY_KEYS)))
    for record in results:
        print(f"{record['arrival_rate']:<18.2f}", end="")
        for strat_key in STRATEGY_KEYS:
            print(f"{record['strategies'][strat_key]['raw_cost']:<18.4f}", end="")
        print()
    print(f"\nSummary saved to: {summary_path}")


if __name__ == "__main__":
    main()
