#!/usr/bin/env python3
"""
Shared utilities for evaluating the six strategy presets across arbitrary scenarios.
"""

from __future__ import annotations

import copy
import json
import os
import sys
from pathlib import Path
from typing import Callable, Dict, Iterable, List, Optional

import numpy as np

project_root = Path(__file__).resolve().parents[2]
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from config import config  # noqa: E402
from train_single_agent import _apply_global_seed_from_env, train_single_algorithm  # noqa: E402
from experiments.camtd3_strategy_suite.run_strategy_training import STRATEGY_PRESETS  # noqa: E402
from utils.unified_reward_calculator import UnifiedRewardCalculator  # noqa: E402
from experiments.camtd3_strategy_suite.strategy_model_cache import get_global_cache  # noqa: E402

STRATEGY_KEYS: List[str] = list(STRATEGY_PRESETS.keys())

# ========== 初始化统一奖励计算器 ==========
# 使用统一奖励计算器确保与训练时的奖励函数一致
_reward_calculator = None


def _get_reward_calculator() -> UnifiedRewardCalculator:
    """获取全局奖励计算器实例（延迟初始化）"""
    global _reward_calculator
    if _reward_calculator is None:
        _reward_calculator = UnifiedRewardCalculator(algorithm="general")
    return _reward_calculator


def tail_mean(values: Iterable[float]) -> float:
    seq = list(map(float, values or []))
    if not seq:
        return 0.0
    if len(seq) >= 100:
        seq = seq[len(seq) // 2 :]
    return float(np.mean(seq))


def compute_cost(avg_delay: float, avg_energy: float) -> float:
    """
    计算统一代价函数值（与训练时的奖励函数一致）
    
    【功能】
    使用统一奖励计算器计算归一化的加权代价，确保与训练时使用的
    奖励函数完全一致。该函数用于策略对比实验的性能评估。
    
    【参数】
    avg_delay: float - 平均任务时延（秒）
    avg_energy: float - 平均总能耗（焦耳）
    
    【返回值】
    float - 归一化的加权代价（越小越好）
    
    【计算公式】
    Cost = ω_T · (T / T_target) + ω_E · (E / E_target)
    其中：
    - ω_T = 2.0（时延权重）
    - ω_E = 1.2（能耗权重）
    - T_target = 0.4s（时延目标值，用于归一化）
    - E_target = 1200J（能耗目标值，用于归一化）
    
    【修复说明】
    ✅ 修复后：使用latency_target和energy_target，与训练时的奖励计算完全一致
    ✅ 修复前：错误使用了delay_normalizer(0.2)和energy_normalizer(1000)
    ✅ 确保评估指标与训练指标可比
    """
    weight_delay = float(config.rl.reward_weight_delay)
    weight_energy = float(config.rl.reward_weight_energy)
    
    # ✅ 修复：使用与训练时完全一致的归一化因子
    calc = _get_reward_calculator()
    delay_normalizer = calc.latency_target  # 0.4（与训练一致）
    energy_normalizer = calc.energy_target  # 1200.0（与训练一致）
    
    return (
        weight_delay * (avg_delay / max(delay_normalizer, 1e-6))
        + weight_energy * (avg_energy / max(energy_normalizer, 1e-6))
    )


def normalize_costs(cost_map: Dict[str, float]) -> Dict[str, float]:
    if not cost_map:
        return {}
    min_cost = min(cost_map.values())
    max_cost = max(cost_map.values())
    span = max(max_cost - min_cost, 1e-12)
    return {k: (v - min_cost) / span for k, v in cost_map.items()}


def enrich_with_normalized_costs(results: Dict[str, Dict[str, float]]) -> Dict[str, Dict[str, float]]:
    normalized = normalize_costs({k: v["raw_cost"] for k, v in results.items()})
    enriched: Dict[str, Dict[str, float]] = {}
    for key, metrics in results.items():
        enriched[key] = dict(metrics)
        enriched[key]["normalized_cost"] = normalized[key]
        enriched[key]["strategy_label"] = strategy_label(key)
    return enriched


def run_strategy_suite(
    override_scenario: Dict[str, object],
    episodes: int,
    seed: int,
    silent: bool,
    strategies: Optional[Iterable[str]] = None,
) -> Dict[str, Dict[str, float]]:
    return _run_strategy_suite_internal(
        override_scenario=override_scenario,
        episodes=episodes,
        seed=seed,
        silent=silent,
        strategies=strategies,
        include_episode_metrics=False,
    )


def run_strategy_suite_with_details(
    override_scenario: Dict[str, object],
    episodes: int,
    seed: int,
    silent: bool,
    strategies: Optional[Iterable[str]] = None,
) -> Dict[str, Dict[str, object]]:
    return _run_strategy_suite_internal(
        override_scenario=override_scenario,
        episodes=episodes,
        seed=seed,
        silent=silent,
        strategies=strategies,
        include_episode_metrics=True,
    )


def _run_strategy_suite_internal(
    override_scenario: Dict[str, object],
    episodes: int,
    seed: int,
    silent: bool,
    strategies: Optional[Iterable[str]],
    include_episode_metrics: bool,
) -> Dict[str, Dict[str, float]]:
    keys = list(strategies) if strategies is not None else STRATEGY_KEYS
    results: Dict[str, Dict[str, float]] = {}
    
    # 获取全局缓存实例
    cache = get_global_cache()
    
    for key in keys:
        preset = STRATEGY_PRESETS[key]
        merged_override = copy.deepcopy(preset.get("override_scenario", {}))
        merged_override.update(override_scenario or {})
        merged_override.setdefault("override_topology", True)

        # ========== 检查缓存 ==========
        cached_data = cache.get_cached_model(key, episodes, seed, merged_override)
        
        if cached_data is not None:
            # 使用缓存的训练结果
            cached_metrics = cached_data["metrics"]
            episode_metrics = cached_metrics.get("episode_metrics", {})
            
            avg_delay = tail_mean(episode_metrics.get("avg_delay", []))
            avg_energy = tail_mean(episode_metrics.get("total_energy", []))
            completion_rate = tail_mean(episode_metrics.get("task_completion_rate", []))
            raw_cost = compute_cost(avg_delay, avg_energy)

            results[key] = {
                "avg_delay": avg_delay,
                "avg_energy": avg_energy,
                "completion_rate": completion_rate,
                "raw_cost": raw_cost,
                "episodes": episodes,
                "seed": seed,
                "from_cache": True,  # 标记为缓存数据
            }
            if include_episode_metrics:
                results[key]["episode_metrics"] = episode_metrics
            
            continue  # 跳过训练
        
        # ========== 没有缓存，执行训练 ==========
        os.environ["RANDOM_SEED"] = str(seed)
        _apply_global_seed_from_env()

        outcome = train_single_algorithm(
            preset["algorithm"],
            num_episodes=episodes,
            silent_mode=silent,
            override_scenario=merged_override,
            use_enhanced_cache=preset["use_enhanced_cache"],
            disable_migration=preset["disable_migration"],
            enforce_offload_mode=preset["enforce_offload_mode"],
        )

        episode_metrics = outcome.get("episode_metrics", {})
        avg_delay = tail_mean(episode_metrics.get("avg_delay", []))
        avg_energy = tail_mean(episode_metrics.get("total_energy", []))
        completion_rate = tail_mean(episode_metrics.get("task_completion_rate", []))
        raw_cost = compute_cost(avg_delay, avg_energy)

        results[key] = {
            "avg_delay": avg_delay,
            "avg_energy": avg_energy,
            "completion_rate": completion_rate,
            "raw_cost": raw_cost,
            "episodes": episodes,
            "seed": seed,
            "from_cache": False,  # 标记为新训练数据
        }
        if include_episode_metrics:
            results[key]["episode_metrics"] = episode_metrics
        
        # ========== 保存到缓存 ==========
        try:
            cache.save_model(
                strategy_key=key,
                episodes=episodes,
                seed=seed,
                overrides=merged_override,
                outcome=outcome,
                model_state=None,  # 暂不保存模型参数（仅保存metrics）
            )
        except Exception as e:
            # 缓存保存失败不影响训练结果
            if not silent:
                print(f"⚠️ 缓存保存失败: {e}")
    
    return results


def attach_normalized_costs(result_list: List[Dict[str, object]]) -> None:
    for item in result_list:
        strategies = item.get("strategies", {})
        costs = {k: v.get("raw_cost", 0.0) for k, v in strategies.items()}
        normalized = normalize_costs(costs)
        for key, value in normalized.items():
            strategies[key]["normalized_cost"] = value


def strategy_label(strategy_key: str) -> str:
    return STRATEGY_PRESETS[strategy_key]["description"]


def dump_json(path: os.PathLike[str] | str, data: Dict[str, object]) -> None:
    path_obj = Path(path)
    path_obj.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")


def evaluate_configs(
    configs: List[Dict[str, object]],
    episodes: int,
    seed: int,
    silent: bool,
    suite_path: Path,
    strategies: Optional[Iterable[str]] = None,
    per_strategy_hook: Optional[Callable[[str, Dict[str, float], Dict[str, object], Dict[str, List[float]]], None]] = None,
) -> List[Dict[str, object]]:
    suite_path.mkdir(parents=True, exist_ok=True)
    evaluated: List[Dict[str, object]] = []
    keys = list(strategies) if strategies is not None else STRATEGY_KEYS

    for index, cfg in enumerate(configs, 1):
        cfg_copy = dict(cfg)
        cfg_key = str(cfg_copy.get("key") or f"config_{index}")
        label = str(cfg_copy.get("label", cfg_key))
        overrides = dict(cfg_copy.get("overrides", {}))
        config_dir = suite_path / cfg_key
        config_dir.mkdir(parents=True, exist_ok=True)

        print(f"\n{'='*72}")
        print(f"Configuration: {label} ({cfg_key})")
        print(f"Overrides: {json.dumps(overrides, ensure_ascii=False)}")
        print('=' * 72)

        raw = run_strategy_suite_with_details(
            override_scenario=overrides,
            episodes=episodes,
            seed=seed,
            silent=silent,
            strategies=keys,
        )
        enriched = enrich_with_normalized_costs(raw)

        for strat_key in keys:
            metrics = enriched[strat_key]
            episode_metrics = metrics.pop("episode_metrics", None)
            if per_strategy_hook:
                per_strategy_hook(strat_key, metrics, cfg_copy, episode_metrics or {})
            detail_path = config_dir / f"{strat_key}.json"
            metrics_to_save = dict(cfg_copy)
            metrics_to_save.update({"strategy": strat_key, **metrics})
            detail_path.write_text(json.dumps(metrics_to_save, indent=2, ensure_ascii=False), encoding="utf-8")
            print(
                f"  - {strategy_label(strat_key)}: "
                f"Cost={metrics['raw_cost']:.4f} Delay={metrics['avg_delay']:.4f}s "
                f"Energy={metrics['avg_energy']:.2f}J"
            )

        cfg_entry = {
            **cfg_copy,
            "key": cfg_key,
            "label": label,
            "strategies": enriched,
            "episodes": episodes,
            "seed": seed,
        }
        evaluated.append(cfg_entry)

    return evaluated
