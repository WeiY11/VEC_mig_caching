#!/usr/bin/env python3
"""
TD3 Strategy Training Runner
--------------------------------

This module orchestrates the TD3 ablation / comparison suites. It activates or disables
unloading, resource allocation, and migration so that each component's contribution can be
quantified. The comparison now focuses on the six strategies requested for the paper:
  1. local-only (pure on-board execution)
  2. remote-only (single RSU enforced, no local execution)
  3. offloading-only (layered policy where RSU decides the destination)
  4. resource-only (multi-RSU resource allocation without local processing)
  5. comprehensive-no-migration (full TD3 stack with migration disabled)
  6. comprehensive-migration (your original TD3 pipeline; identical to running
     `python train_single_agent.py --algorithm TD3 --episodes 2000 --num-vehicles 12`)

Example usage:
```bash
python experiments/td3_strategy_suite/run_strategy_training.py \\
    --strategy local-only --episodes 800 --seed 42

for strategy in local-only remote-only offloading-only resource-only comprehensive-no-migration comprehensive-migration; do
    python experiments/td3_strategy_suite/run_strategy_training.py \\
        --strategy $strategy --suite-id ablation_20231029 --episodes 800
done
```
"""

from __future__ import annotations

import argparse
import json
import os
import shutil
import sys
from collections import OrderedDict
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence

import numpy as np

# å¨£è¯²å§æ¤¤åœ­æ´°éåœ­æ´°è¤°æ›åŸŒPythonç’ºç·
project_root = Path(__file__).resolve().parents[2]
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from config import config
from train_single_agent import (
    _apply_global_seed_from_env,
    _build_scenario_config,
    SingleAgentTrainingEnvironment,
    train_single_algorithm,
)
from utils.unified_reward_calculator import UnifiedRewardCalculator
from experiments.fallback_baselines import (
    HeuristicPolicy,
    LocalOnlyPolicy,
    RSUOnlyPolicy,
    GreedyPolicy,
    create_baseline_algorithm,
)

StrategyPreset = Dict[str, Any]  # ç»›æ «æšæ£°å‹®é–°å¶‡ç–†ç»«è¯²ç€·

# ========== åˆå§‹åŒ–ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ ==========
# ä½¿ç”¨ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ç¡®ä¿ä¸è®­ç»ƒæ—¶çš„å¥–åŠ±å‡½æ•°ä¸€è‡´
_reward_calculator: Optional[UnifiedRewardCalculator] = None


def _get_reward_calculator() -> UnifiedRewardCalculator:
    """é‘¾å³°å½‡éã„¥çœ¬æ¿‚æ §å§³ç’ï¼„ç•»é£ã„¥ç–„æ¸šå¬¶ç´™å¯¤æƒ°ç¹œé’æ¿†é–æ µç´š"""
    global _reward_calculator
    if _reward_calculator is None:
        _reward_calculator = UnifiedRewardCalculator(algorithm="general")
    return _reward_calculator

# ========== é»˜è®¤å®éªŒå‚æ•° ==========
DEFAULT_EPISODES = 1500  # é»˜è®¤è®­ç»ƒè½®æ•°ï¼ˆå»ºè®®â‰¥1500ç¡®ä¿TD3å……åˆ†æ”¶æ•›ï¼‰
DEFAULT_SEED = 42        # é»˜è®¤éšæœºç§å­ï¼ˆä¿è¯å®éªŒå¯é‡å¤æ€§ï¼‰

# ========== ç»›æ «æšéµÑ†æ¤¤å“„ç°­ ==========
# é¸å¤Œåæ¾¶å¶†æ½…æ´ï¹‚â‚¬æ‘éºæ‘åªé”›æ°«ç² é—æ›šç«´é”ç†»å…˜é’æ¿ç•¬éå¯¸éƒ´ç¼?
# æ©æ¬é‡œæ¤¤å“„ç°­æ¶”ç†ºæ•¤æµœåº£æ•“é´æ„¬å§£æ–¿æµ˜ç›ã„¦æ¤‚é¨å‹«çç»€æ´ªã€æ´?
STRATEGY_ORDER = [
    "local-only",
    "remote-only",
    "offloading-only",
    "resource-only",
    "comprehensive-no-migration",
    "comprehensive-migration",
]



def _build_override(
    num_rsus: Optional[int],
    num_uavs: Optional[int],
    allow_local: Optional[bool] = None,
) -> Dict[str, Any]:
    """
    æ„å»ºç­–ç•¥ä¸“ç”¨çš„åœºæ™¯è¦†ç›–é…ç½®ï¼ˆåœ¨é»˜è®¤é…ç½®åŸºç¡€ä¸Šåšæœ€å°ä¿®æ”¹ï¼‰

    ã€åŠŸèƒ½ã€‘é¿å…ä¸é»˜è®¤å‘½ä»¤ `python train_single_agent.py --algorithm TD3`
    å‡ºç°é…ç½®æ¼‚ç§»ï¼Œä»…è°ƒæ•´ä¸ç­–ç•¥ä¸¥æ ¼ç›¸å…³çš„å‚æ•°ï¼Œç¡®ä¿å¯¹æ¯”å…¬å¹³ã€‚

    ã€å‚æ•°ã€‘
    num_rsus: Optional[int] - RSU æ•°é‡ï¼ˆNone è¡¨ç¤ºæ²¿ç”¨é»˜è®¤é…ç½®ï¼‰
    num_uavs: Optional[int] - UAV æ•°é‡ï¼ˆNone è¡¨ç¤ºæ²¿ç”¨é»˜è®¤é…ç½®ï¼‰
    allow_local: Optional[bool] - æ˜¯å¦å…è®¸æœ¬åœ°å¤„ç†ï¼ˆNone è¡¨ç¤ºæ²¿ç”¨é»˜è®¤å€¼ï¼‰

    ã€è¿”å›å€¼ã€‘Dict[str, Any] - è¦†ç›–å­—å…¸ï¼Œä»…åŒ…å«è¢«ä¿®æ”¹çš„é”®

    ã€è®¾è®¡åŸåˆ™ã€‘
    - ç»§æ‰¿é»˜è®¤çš„è½¦è¾†è§„æ¨¡ã€è¦†ç›–åŠå¾„ç­‰åŸºç¡€å‚æ•°
    - ä»…è°ƒæ•´ RSU/UAV æ•°é‡æˆ–æœ¬åœ°æ‰§è¡Œå¼€å…³
    - å›ºå®šæ‹“æ‰‘ï¼Œå‡å°‘è·¨ç­–ç•¥æ¯”è¾ƒçš„éšæœºæ€§
    """
    _ = _build_scenario_config()  # è°ƒç”¨ä»¥ç¡®ä¿é…ç½®åŠ è½½ï¼Œä¸é»˜è®¤è®­ç»ƒä¿æŒåŒæ­¥
    override: Dict[str, Any] = {}

    if num_rsus is not None:
        override["num_rsus"] = num_rsus
    if num_uavs is not None:
        override["num_uavs"] = num_uavs
    if allow_local is not None:
        override["allow_local_processing"] = allow_local

    override["override_topology"] = True
    return override

@dataclass(frozen=True)
class ScenarioProfile:
    """Descriptor for the scenario tweaks applied to a strategy."""

    key: str
    label: str
    num_rsus: Optional[int]
    num_uavs: Optional[int]
    allow_local: Optional[bool]
    extra_overrides: Optional[Dict[str, Any]] = None
    env_options: Optional[Dict[str, Any]] = None


SCENARIO_PROFILES: Dict[str, ScenarioProfile] = {
    "shared_edge": ScenarioProfile(
        key="shared_edge",
        label="Shared scenario: 4 RSU + 2 UAV (local allowed)",
        num_rsus=4,
        num_uavs=2,
        allow_local=True,
    ),
    "baseline_single_rsu": ScenarioProfile(
        key="baseline_single_rsu",
        label="Single RSU baseline (local allowed, no UAV)",
        num_rsus=1,
        num_uavs=0,
        allow_local=True,
    ),
    "baseline_single_rsu_remote": ScenarioProfile(
        key="baseline_single_rsu_remote",
        label="Single RSU baseline (remote enforced, no UAV)",
        num_rsus=1,
        num_uavs=0,
        allow_local=False,
    ),
    "layered_multi_edge": ScenarioProfile(
        key="layered_multi_edge",
        label="Layered multi-edge (4 RSU + 2 UAV, local allowed)",
        num_rsus=4,
        num_uavs=2,
        allow_local=True,
    ),
    "layered_multi_edge_remote": ScenarioProfile(
        key="layered_multi_edge_remote",
        label="Layered multi-edge (remote enforced, no local execution)",
        num_rsus=4,
        num_uavs=2,
        allow_local=False,
    ),
}


def _scenario_override(profile_key: str) -> Optional[Dict[str, Any]]:
    """Convert a scenario profile into the override dict consumed by training."""
    profile = SCENARIO_PROFILES[profile_key]
    if (
        profile.num_rsus is None
        and profile.num_uavs is None
        and profile.allow_local is None
        and not profile.extra_overrides
    ):
        return None
    override = _build_override(
        num_rsus=profile.num_rsus,
        num_uavs=profile.num_uavs,
        allow_local=profile.allow_local,
    )
    if profile.extra_overrides:
        override.update(profile.extra_overrides)
    return override


def _make_preset(
    *,
    description: str,
    scenario_key: str,
    use_enhanced_cache: bool,
    disable_migration: bool,
    enforce_offload_mode: Optional[str],
    algorithm: str = "TD3",
    flags: Optional[Sequence[str]] = None,
    heuristic_name: Optional[str] = None,
    group: str = "baseline",
    central_resource: bool = False,
    env_options: Optional[Dict[str, Any]] = None,
) -> StrategyPreset:
    """Factory keeping strategy definitions concise and consistent."""
    scenario = SCENARIO_PROFILES[scenario_key]
    merged_env_options: Dict[str, Any] = {}
    if scenario.env_options:
        merged_env_options.update(scenario.env_options)
    if env_options:
        merged_env_options.update(env_options)
    preset: StrategyPreset = {
        "description": description,
        "algorithm": algorithm,
        "episodes": DEFAULT_EPISODES,
        "use_enhanced_cache": use_enhanced_cache,
        "disable_migration": disable_migration,
        "enforce_offload_mode": enforce_offload_mode,
        "override_scenario": _scenario_override(scenario_key),
        "scenario_key": scenario.key,
        "scenario_label": scenario.label,
        "flags": list(flags or ()),
        "heuristic_name": heuristic_name,
        "group": group,
        "central_resource": bool(central_resource),
        "env_options": merged_env_options or None,
    }
    return preset


STRATEGY_PRESETS: "OrderedDict[str, StrategyPreset]" = OrderedDict(
    [
        (
            "random",
            _make_preset(
                description="Random baseline",
                scenario_key="layered_multi_edge",
                use_enhanced_cache=False,
                disable_migration=True,
                enforce_offload_mode=None,
                algorithm="heuristic",
                heuristic_name="random",
                flags=("cache_off", "migration_off", "random"),
                group="heuristic",
            ),
        ),
        (
            "round-robin",
            _make_preset(
                description="Round-robin baseline",
                scenario_key="layered_multi_edge",
                use_enhanced_cache=False,
                disable_migration=True,
                enforce_offload_mode=None,
                algorithm="heuristic",
                heuristic_name="round_robin",
                flags=("cache_off", "migration_off", "round_robin"),
                group="heuristic",
            ),
        ),
        (
            "local-only",
            _make_preset(
                description="Local-only baseline",
                scenario_key="layered_multi_edge",  # ä¿æŒç›¸åŒåœºæ™¯ä»¥ä¿è¯å¯¹æ¯”å…¬å¹³
                use_enhanced_cache=False,
                disable_migration=True,
                enforce_offload_mode=None,  # ğŸ”§ ç§»é™¤å¼ºåˆ¶æ¨¡å¼ï¼Œçº¯ç­–ç•¥å†³ç­–
                algorithm="heuristic",
                heuristic_name="local_only",
                flags=("cache_off", "migration_off", "local_only"),
                group="baseline",
            ),
        ),
        (
            "remote-only",
            _make_preset(
                description="Remote-only baseline",
                scenario_key="layered_multi_edge",  # ğŸ”§ æ”¹ä¸ºé€šç”¨åœºæ™¯
                use_enhanced_cache=False,
                disable_migration=True,
                enforce_offload_mode=None,  # ğŸ”§ ç§»é™¤å¼ºåˆ¶æ¨¡å¼ï¼Œç”±RSUOnlyPolicyå®ç°
                algorithm="heuristic",
                heuristic_name="rsu_only",  # ä½¿ç”¨é‡æ„åRSUOnlyPolicy
                flags=("cache_off", "migration_off", "edge_only"),
                group="baseline",
            ),
        ),
        (
            "offloading-only",
            _make_preset(
                description="Offloading-only",
                scenario_key="layered_multi_edge",
                use_enhanced_cache=False,
                disable_migration=True,
                enforce_offload_mode=None,
                algorithm="heuristic",
                heuristic_name="greedy",  # ä½¿ç”¨é‡æ„åGreedyPolicy
                flags=("cache_off", "migration_off", "smart_offload"),
                group="layered",
            ),
        ),
        (
            "resource-only",
            _make_preset(
                description="Resource-only",
                scenario_key="layered_multi_edge",  # ğŸ”§ æ”¹ä¸ºé€šç”¨åœºæ™¯
                use_enhanced_cache=True,
                disable_migration=True,
                enforce_offload_mode=None,  # ğŸ”§ ç§»é™¤å¼ºåˆ¶æ¨¡å¼ï¼ŒRemoteGreedyPolicyä¼šæ‹’ç»æœ¬åœ°
                algorithm="heuristic",
                heuristic_name="remote_greedy",  # ä½¿ç”¨é‡æ„åRemoteGreedyPolicy
                flags=("cache_on", "migration_off", "resource_alloc"),
                group="layered",
            ),
        ),
        (
            "comprehensive-no-migration",
            _make_preset(
                description="TD3noMIG",
                scenario_key="layered_multi_edge",
                use_enhanced_cache=True,
                disable_migration=True,
                enforce_offload_mode=None,
                algorithm="OPTIMIZED_TD3",  # ğŸ¯ ä½¿ç”¨OPTIMIZED_TD3ä¿æŒä¸CAMTD3ä¸€è‡´
                flags=("cache_on", "migration_off", "multi_edge"),
                group="layered",
            ),
        ),
        (
            "comprehensive-migration",
            _make_preset(
                description="CAMTD3",
                scenario_key="layered_multi_edge",
                use_enhanced_cache=True,
                disable_migration=False,
                enforce_offload_mode=None,
                algorithm="OPTIMIZED_TD3",  # ğŸ¯ ä¿®å¤ï¼šä½¿ç”¨OPTIMIZED_TD3ä»£æ›¿TD3
                flags=("cache_on", "migration_on", "multi_edge"),
                group="layered",
            ),
        ),
    ]
)


class RemoteGreedyPolicy(HeuristicPolicy):
    """Intelligent resource allocation policy for edge nodes.
    
    ğŸ¯ è®¾è®¡ç›®æ ‡ï¼šæä¾›çœŸæ­£çš„èµ„æºåˆ†é…åŸºçº¿ï¼ŒéªŒè¯CAMTD3çš„ç¼“å­˜å’Œè¿ç§»ä¼˜åŠ¿
    
    ğŸ“Š å¯¹æ¯”ä»·å€¼ï¼š
    - æ—¶å»¶ï¼šä¸­ä½ï¼ˆè¾¹ç¼˜è®¡ç®—+è´Ÿè½½å‡è¡¡ï¼‰
    - èƒ½è€—ï¼šä¸­ç­‰ï¼ˆä¼˜åŒ–é€šä¿¡+è®¡ç®—ï¼‰
    - å®Œæˆç‡ï¼šä¸­é«˜ï¼ˆæ™ºèƒ½èµ„æºåŒ¹é…ï¼‰
    
    ğŸ”§ é‡æ„è¦ç‚¹ï¼š
    - çœŸæ­£çš„å¤šç»´èµ„æºè¯„ä¼°ï¼šè®¡ç®—ã€ç¼“å­˜ã€å¸¦å®½ã€é˜Ÿåˆ—
    - æ”¯æŒRSUèµ„æºå˜åŒ–é€‚åº”ï¼ˆé€šè¿‡çŠ¶æ€è´Ÿè½½æ„ŸçŸ¥ï¼‰
    - å……åˆ†åˆ©ç”¨ç¼“å­˜çŠ¶æ€ï¼ˆuse_enhanced_cache=Trueï¼‰
    """

    def __init__(self) -> None:
        super().__init__("RemoteGreedy")
        # ğŸ”§ å¤šç»´èµ„æºæƒé‡ï¼ˆä½“ç°â€œèµ„æºåˆ†é…â€æ ¸å¿ƒï¼‰
        self.queue_weight = 1.8      # é˜Ÿåˆ—è´Ÿè½½æƒé‡
        self.cache_weight = 1.2      # ç¼“å­˜å‘½ä¸­æƒé‡ï¼ˆè´Ÿåˆ©ç›Šï¼‰
        self.comm_weight = 1.0       # é€šä¿¡æˆæœ¬æƒé‡
        self.energy_weight = 0.7     # èƒ½è€—æƒé‡

    def select_action(self, state) -> np.ndarray:
        veh, rsu, uav = self._structured_state(state)
        
        # è®¡ç®—è½¦è¾†è´¨å¿ƒä½ç½®
        anchor = np.mean(veh[:, :2], axis=0) if veh.size > 0 else np.zeros(2, dtype=np.float32)
        
        candidates = []
        
        # ğŸ”§ é‡æ„ï¼šè¯„ä¼°æ‰€æœ‰RSUï¼ˆèµ„æºæ„ŸçŸ¥ï¼‰
        if rsu.size > 0 and rsu.ndim == 2:
            for i in range(rsu.shape[0]):
                score = self._evaluate_rsu_resource(rsu[i], anchor)
                candidates.append(('rsu', i, score))
        
        # ğŸ”§ é‡æ„ï¼šè¯„ä¼°æ‰€æœ‰UAVï¼ˆèµ„æºæ„ŸçŸ¥ï¼‰
        if uav.size > 0 and uav.ndim == 2:
            for i in range(uav.shape[0]):
                score = self._evaluate_uav_resource(uav[i], anchor)
                candidates.append(('uav', i, score))
        
        if not candidates:
            # æ— è¾¹ç¼˜èŠ‚ç‚¹ï¼Œæå¼ºæ‹’ç»æœ¬åœ°ï¼ˆä¸remote-onlyè¯­ä¹‰ä¸€è‡´ï¼‰
            return self._action_from_preference(
                local_score=-5.0, 
                rsu_score=0.0, 
                uav_score=0.0
            )
        
        # é€‰æ‹©èµ„æºæˆæœ¬æœ€ä½çš„è¾¹ç¼˜èŠ‚ç‚¹
        kind, idx, _ = min(candidates, key=lambda x: x[2])
        
        if kind == 'rsu':
            return self._action_from_preference(
                local_score=-5.0,
                rsu_score=5.0,
                uav_score=-3.0,
                rsu_index=idx,
            )
        else:  # UAV
            return self._action_from_preference(
                local_score=-5.0,
                rsu_score=-3.0,
                uav_score=5.0,
                uav_index=idx,
            )
    
    def _evaluate_rsu_resource(self, rsu_state: np.ndarray, veh_pos: np.ndarray) -> float:
        """ğŸ”§ å¤šç»´åº¦RSUèµ„æºè¯„ä¼°ï¼šé˜Ÿåˆ— + ç¼“å­˜ + é€šä¿¡ + èƒ½è€—"""
        # é˜Ÿåˆ—è´Ÿè½½ï¼ˆåˆ—3ï¼‰
        queue_load = float(rsu_state[3]) if rsu_state.size > 3 else 0.6
        
        # ç¼“å­˜åˆ©ç”¨ç‡ï¼ˆåˆ—2ï¼‰- ç¼“å­˜å‘½ä¸­ä¸ºè´Ÿæˆæœ¬
        cache_util = float(rsu_state[2]) if rsu_state.size > 2 else 0.5
        cache_benefit = -(1.0 - cache_util)  # å‘½ä¸­è¶Šé«˜ï¼Œæˆæœ¬è¶Šä½
        
        # é€šä¿¡æˆæœ¬ï¼ˆåŸºäºè·ç¦»ï¼‰
        rsu_pos = rsu_state[:2] if rsu_state.size >= 2 else veh_pos
        distance = float(np.linalg.norm(rsu_pos - veh_pos))
        comm_cost = distance / 1000.0
        
        # èƒ½è€—çŠ¶æ€ï¼ˆåˆ—4ï¼‰
        energy = float(rsu_state[4]) if rsu_state.size > 4 else 0.5
        
        # ğŸ¯ ç»¼åˆèµ„æºæˆæœ¬
        total_cost = (
            self.queue_weight * queue_load +
            self.cache_weight * cache_benefit +  # ç¼“å­˜æ˜¯è´Ÿæˆæœ¬
            self.comm_weight * comm_cost +
            self.energy_weight * energy * 0.5
        )
        
        return float(total_cost)
    
    def _evaluate_uav_resource(self, uav_state: np.ndarray, veh_pos: np.ndarray) -> float:
        """ğŸ”§ å¤šç»´åº¦UAVèµ„æºè¯„ä¼°ï¼šé˜Ÿåˆ— + é€šä¿¡ + æ‚¬åœèƒ½è€—"""
        # é˜Ÿåˆ—è´Ÿè½½
        queue_load = float(uav_state[3]) if uav_state.size > 3 else 0.7
        
        # é€šä¿¡æˆæœ¬ï¼ˆUAVç©ºä¸­ä¿¡é“è¡°å‡æ›´å¿«ï¼‰
        uav_pos = uav_state[:2] if uav_state.size >= 2 else veh_pos
        distance = float(np.linalg.norm(uav_pos - veh_pos))
        comm_cost = distance / 800.0  # UAVé€šä¿¡èŒƒå›´è¾ƒå°
        
        # æ‚¬åœèƒ½è€—ï¼ˆåˆ—4ï¼‰
        energy = float(uav_state[4]) if uav_state.size > 4 else 0.8
        
        # UAVæ— ç¼“å­˜ï¼Œèƒ½è€—æƒé‡æ›´é«˜
        total_cost = (
            self.queue_weight * queue_load +
            self.comm_weight * comm_cost * 1.3 +  # ç©ºä¸­é€šä¿¡æƒ©ç½š
            self.energy_weight * energy * 1.2  # UAVèƒ½è€—æƒ©ç½šæ›´é«˜
        )
        
        return float(total_cost)


def _resolve_heuristic_policy(name: Optional[str], seed: int) -> HeuristicPolicy:
    key = (name or "").strip().lower()
    if key in {"random"}:
        from experiments.fallback_baselines import RandomPolicy
        return RandomPolicy(seed=seed)
    if key in {"round_robin", "roundrobin", "round-robin"}:
        from experiments.fallback_baselines import RoundRobinPolicy
        return RoundRobinPolicy()
    if key in {"local_only", "localonly"}:
        return LocalOnlyPolicy()
    if key in {"rsu_only", "remote_only"}:
        return RSUOnlyPolicy()
    if key in {"remote_greedy"}:
        return RemoteGreedyPolicy()
    if key in {"greedy"}:
        return GreedyPolicy()

    policy = create_baseline_algorithm(key or "greedy", seed=seed)
    if not isinstance(policy, HeuristicPolicy):
        raise TypeError(f"Heuristic factory for '{name}' did not return a HeuristicPolicy.")
    return policy


def _run_heuristic_strategy(
    preset: StrategyPreset,
    episodes: int,
    seed: int,
    extra_override: Optional[Dict[str, Any]] = None,
    env_options: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """Execute deterministic heuristic policies under the shared scenario."""

    controller = _resolve_heuristic_policy(preset.get("heuristic_name"), seed)
    override = dict(preset.get("override_scenario") or {})
    if extra_override:
        override.update(extra_override)
    env_kwargs = dict(env_options or {})
    env = SingleAgentTrainingEnvironment(
        "TD3",
        override_scenario=override,
        use_enhanced_cache=preset["use_enhanced_cache"],
        disable_migration=preset["disable_migration"],
        enforce_offload_mode=preset["enforce_offload_mode"],
        joint_controller=env_kwargs.get("joint_controller", False),
    )
    if hasattr(controller, "update_environment"):
        controller.update_environment(env)

    max_steps = int(config.experiment.max_steps_per_episode)
    delay_records: List[float] = []
    energy_records: List[float] = []
    completion_records: List[float] = []
    cache_records: List[float] = []
    migration_records: List[float] = []
    reward_records: List[float] = []  # ğŸ¯ æ–°å¢ï¼šæ”¶é›†å¥–åŠ±

    for _ in range(episodes):
        state = env.reset_environment()
        if hasattr(controller, "reset"):
            controller.reset()

        last_info: Dict[str, Any] = {}
        episode_reward = 0.0  # ğŸ¯ æ–°å¢ï¼šç´¯ç§¯episodeå¥–åŠ±
        for _ in range(max_steps):
            action_vec = controller.select_action(state)
            actions_dict = env._build_actions_from_vector(action_vec)
            next_state, reward, done, info = env.step(action_vec, state, actions_dict)
            episode_reward += reward  # ğŸ¯ æ–°å¢ï¼šç´¯ç§¯å¥–åŠ±
            state = next_state
            last_info = info
            if done:
                break

        metrics = last_info.get("system_metrics", {})
        delay_records.append(float(metrics.get("avg_task_delay", 0.0)))
        energy_records.append(float(metrics.get("total_energy_consumption", 0.0)))
        completion_records.append(float(metrics.get("task_completion_rate", 0.0)))
        cache_records.append(float(metrics.get("cache_hit_rate", 0.0)))
        migration_records.append(float(metrics.get("migration_success_rate", 0.0)))
        reward_records.append(episode_reward)  # ğŸ¯ æ–°å¢ï¼šè®°å½•episodeå¥–åŠ±

    episode_metrics = {
        "avg_delay": delay_records,
        "total_energy": energy_records,
        "task_completion_rate": completion_records,
        "cache_hit_rate": cache_records,
        "migration_success_rate": migration_records,
    }
    if hasattr(env, "episode_metrics"):
        env_metrics: Dict[str, Any] = getattr(env, "episode_metrics", {}) or {}

        def _coerce_numeric_series(series: Any) -> List[float]:
            if series is None:
                return []
            if not isinstance(series, list):
                series = [series]
            cleaned: List[float] = []
            for item in series:
                if isinstance(item, (list, tuple)):
                    for sub_item in item:
                        try:
                            cleaned.append(float(sub_item))
                        except (TypeError, ValueError):
                            continue
                    continue
                try:
                    cleaned.append(float(item))
                except (TypeError, ValueError):
                    if isinstance(item, np.ndarray) and item.size == 1:
                        cleaned.append(float(item.item()))
            return cleaned

        for key, values in env_metrics.items():
            if key in episode_metrics:
                continue
            numeric_values = _coerce_numeric_series(values)
            if numeric_values:
                episode_metrics[key] = numeric_values

    return {
        "algorithm": "heuristic",
        "timestamp": datetime.now().isoformat(),
        "episode_metrics": episode_metrics,
        "episode_rewards": reward_records,  # ğŸ¯ æ–°å¢ï¼šè¿”å›å¥–åŠ±åˆ—è¡¨
        "artifacts": {},
    }


def tail_mean(values: Any) -> float:
    """
    ç’ï¼„ç•»æ´å¿“åªéšåº¡å´é–®ã„¥åé¨å‹­Ç”ç€¹æ°¬æ½éŠ?    
    éŠ†æ„¬å§›é‘³å§â‚¬?    æµ£è·¨æ•¤ç’ç²Œéšåº¢æ¹¡éç‰ˆåµç’ï¼„ç•»é¬Ñ†å…˜é¸å›¨çˆ£é¨å‹­Ç”ç€¹æ°¬æ½éŠç¡·ç´é–¬å®å¤é“å¶†æ¹¡éºãˆ¢å‚¨é—ƒèˆµé¨å‹¯ç®é‚ç‘°æ¨Šéªå‰å£ˆéŠ†?    æ©æ¬æ§¸ç’‡å‹ªåŠé€èˆµæšƒéšåº¢â‚¬Ñ†å…˜é¨å‹¬çˆ£é‘å—˜æŸŸå¨‰æ›˜â‚¬?    
    éŠ†æ„¬å¼¬éèˆ¬â‚¬?    values: Any - é¬Ñ†å…˜é¸å›¨çˆ£æ´å¿“åªé”›å å§£å¿šç–†é¨å‹¬æ¤‚å¯¤èº²â‚¬ä½½å…˜é‘°æ¥ƒç“‘é”›?    
    éŠ†æ„¯ç¹‘é¥ç‚²â‚¬ç¬ºâ‚¬?    float - éšåº¢æ¹¡ç»‹å†²ç•¾é—ƒèˆµé¨å‹«æ½éŠ?    
    éŠ†æ„¯ç» æ¥ƒç“¥é£ãƒ£â‚¬?    - æ´å¿“åªé—€å®å®³ >= 100: æµ£è·¨æ•¤éš?0%éç‰ˆåµé”›å å–é’å—˜æ•¹éæ¶³ç´š
    - æ´å¿“åªé—€å®å®³ >= 50: æµ£è·¨æ•¤éˆâ‚¬éš?0ææšŸé¹?    - æ´å¿“åªé—€å®å®³ < 50: æµ£è·¨æ•¤éã„©å„´éç‰ˆåµé”›å æ©é–«ç†¸ç¥´ç’‡æ›ŸÄå¯®å¿¥ç´š
    
    éŠ†æ„¯é‚å›§æ´æ–»â‚¬?    ç’‡å‹ªåŠé€èˆµæšƒé¬Ñ†å…˜éƒè®¹ç´é–«æ°¬çˆ¶æµ£è·¨æ•¤ç’ç²Œéšåº¢æ¹¡é¨å‹«é’©é§å›§â‚¬é—´ç¶”æ¶“çƒ˜æ¸¶ç¼å Ÿâ‚¬Ñ†å…˜é¸å›¨çˆ£
    """
    if not values:
        return 0.0
    seq = list(map(float, values))
    length = len(seq)
    if length >= 100:
        subset = seq[length // 2 :]
    elif length >= 50:
        subset = seq[-30:]
    else:
        subset = seq
    return float(sum(subset) / max(1, len(subset)))


# âš ï¸ å·²åºŸå¼ƒï¼šè¯·ä½¿ç”¨ strategy_runner.py::compute_cost ä»£æ›¿
# è¯¥å‡½æ•°ä»…åšæ‰‹åŠ¨è®¡ç®—ï¼Œä¸ä½¿ç”¨avg_rewardï¼Œå·²ç»Ÿä¸€åˆ°compute_costï¼ˆä¼˜å…ˆä½¿ç”¨-rewardï¼‰
def compute_raw_cost(delay_mean: float, energy_mean: float, completion_rate: Optional[float] = None) -> float:
    """
    ç’ï¼„ç•»ç¼ç†¶ç«´æµ ï½„ç¯é‘èŠ¥æšŸé¨å‹«å¸«æ¿®å¬ªâ‚¬?    
    éŠ†æ„¬å§›é‘³å§â‚¬?    æµ£è·¨æ•¤ç¼ç†¶ç«´æ¿‚æ §å§³ç’ï¼„ç•»é£ã„¨ç» æ¤¾å”¬æµ å‡¤ç´çº­ç¹šæ¶“åº¤ç¼å†©æ¤‚æµ£è·¨æ•¤é¨å‹«é”ååš±éæ¿ç•¬éã„¤ç«´é‘·æ·¬â‚¬?    ç’‡ãƒ¥åš±éæ‰®æ•¤æµœåº£ç“¥é£ãƒ©æ£¿é¨å‹«å•éªå†²å§£æ–»â‚¬?    
    éŠ†æ„¬å¼¬éèˆ¬â‚¬?    delay_mean: float - éªå†²æ½éƒè·ºæ¬¢é”›å ¢é”›?    energy_mean: float - éªå†²æ½é‘³å€Ÿâ‚¬æ¥‹ç´™é’ï¹â‚¬ç­¹ç´š
    
    éŠ†æ„¯ç¹‘é¥ç‚²â‚¬ç¬ºâ‚¬?    float - è¤°æç«´é–æ §æ‚—é¨å‹«å§é‰å†§å”¬æµ ?    
    éŠ†æ„¯ç» æ¥€å•å¯®å¿‹â‚¬?    Raw Cost = è …_T è·¯ (T / T_target) + è …_E è·¯ (E / E_target)
    éæœµè…‘é”›?    - è …_T = 2.0é”›å Ÿæ¤‚å¯¤èˆµæ½ˆé–²å¶ç´š
    - è …_E = 1.2é”›å £å…˜é‘°æ¥æ½ˆé–²å¶ç´š
    - T_target = 0.4sé”›å Ÿæ¤‚å¯¤å‰æ´°éå›§â‚¬ç¡·ç´é¢ã„¤ç°¬è¤°æç«´é–æ µç´š
    - E_target = 1200Jé”›å £å…˜é‘°æ¥ƒæ´°éå›§â‚¬ç¡·ç´é¢ã„¤ç°¬è¤°æç«´é–æ µç´š
    
    éŠ†æ„¯é‚å›§æ´æ–»â‚¬?    æµ¼æ¨ºå¯²é©çˆ£é”›æ­®inimize è …_Tè·¯éƒè·ºæ¬¢ + è …_Eè·¯é‘³å€Ÿâ‚¬?    ç’‡ãƒ¦å¯šéå›ªç§ºçå¿¥ç´ç»¯è¤ç²ºé¬Ñ†å…˜ç“’å©‚ã‚½
    
    éŠ†æ„ªæ…¨æ¾¶å¶ˆé„åº›â‚¬?    é‰?æ·‡éšåº¯ç´°æµ£è·¨æ•¤latency_targetéœå®”nergy_targeté”›å±¼ç¬Œç’ç²Œéƒå‰æ®‘æ¿‚æ §å§³ç’ï¼„ç•»ç€¹å±½åæ¶“â‚¬é‘·?    é‰?æ·‡é“å¶ç´°é–¿æ¬’æµ£è·¨æ•¤æµœå“¾elay_normalizer(0.2)éœå®”nergy_normalizer(1000)
    é‰?æ¾¶å¶‡æ•¤ç¼ç†¶ç«´å¦¯â€³æ½¡é”›å²„ä¼’å¯°ç‹£RYé˜ç†·å¯
    """
    weight_delay = float(config.rl.reward_weight_delay)      # è …_T = 2.0
    weight_energy = float(config.rl.reward_weight_energy)    # è …_E = 1.2
    
    # é‰?æ·‡é”›æ°«å¨‡é¢ã„¤ç¬Œç’ç²Œéƒè·ºç•¬éã„¤ç«´é‘·å¯¸æ®‘è¤°æç«´é–æ §æ´œç€›?
    reward_calc = _get_reward_calculator()
    delay_normalizer = reward_calc.latency_target  # 0.4é”›å œç¬Œç’ç²Œæ¶“â‚¬é‘·è¾¾ç´š
    energy_normalizer = reward_calc.energy_target  # 1200.0é”›å œç¬Œç’ç²Œæ¶“â‚¬é‘·è¾¾ç´š
    
    
    base_cost = (
        weight_delay * (delay_mean / max(delay_normalizer, 1e-6))
        + weight_energy * (energy_mean / max(energy_normalizer, 1e-6))
    )
    
    if completion_rate is not None and completion_rate > 0:
        import math
        completion_penalty = 1.0 + 0.5 * math.log(1.0 / max(completion_rate, 0.5))
        return base_cost * completion_penalty
    
    return base_cost


def update_summary(
    suite_path: Path,
    strategy: str,
    preset: StrategyPreset,
    result: Dict[str, Any],
    metrics: Dict[str, float],
    artifacts: Dict[str, str],
    episodes: int,
    seed: int,
) -> None:
    """
    é‡å­˜æŸŠç»›æ «æšç€¹ç‚ºç™é½æ¨¿JSONé‚å›¦æ¬¢
    
    éŠ†æ„¬å§›é‘³å§â‚¬?    çå——å´Ÿæ¶“ç“¥é£ãƒ§æ®‘ç’ç²Œç¼æ’´ç‰æ©è—‰å§é’çšŠuiteç»¾ÑƒåŸ†é¨å‰†ummary.jsonæ¶“â‚¬?    ç’‡ãƒ¦æƒæµ èˆµçœ¹é¬ç»˜å¢éˆå¤Œç“¥é£ãƒ§æ®‘é¬Ñ†å…˜é¸å›¨çˆ£é”›å²€æ•¤æµœåº¡æ‚—ç¼æ®‘ç€µè§„ç˜®é’å—˜ç€½éœå±½å½²ç‘™å——å¯²éŠ†?    
    éŠ†æ„¬å¼¬éèˆ¬â‚¬?    suite_path: Path - Suiteéåœ­æ´°è¤°æ›¡çŸ¾å¯°?    strategy: str - ç»›æ «æšéšå¶‡Ğé”›å "local-only"é”›?    preset: StrategyPreset - ç»›æ «æšæ£°å‹®é–°å¶‡ç–†
    result: Dict[str, Any] - ç’ç²Œæ©æ–¿æ´–é¨å‹«ç•¬éå¯¸ç²¨é‹?    metrics: Dict[str, float] - ç’ï¼„ç•»éšåº£æ®‘é¬Ñ†å…˜é¸å›¨çˆ£
    artifacts: Dict[str, str] - é¢ç†¸åšé¨å‹¬æƒæµ æƒ°çŸ¾å¯°?    episodes: int - ç€¹ç‚ºæª¯ç’ç²ŒææšŸ
    seed: int - æµ£è·¨æ•¤é¨å‹¯æ®¢éˆè™¹ç€›?    
    éŠ†æ„¯ç¹‘é¥ç‚²â‚¬ç¬ºâ‚¬?    Noneé”›å ¢æ´¿éºãƒ¥å•“éãƒ¦æƒæµ è®¹ç´š
    
    éŠ†æ ummary.jsonç¼æ’´ç€¯éŠ†?    {
      "suite_id": "20231029_123456",
      "created_at": "2023-10-29T12:34:56",
      "updated_at": "2023-10-29T13:45:00",
      "strategies": {
        "local-only": {
          "description": "...",
          "metrics": {"delay_mean": 0.15, ...},
          "controls": {...},
          "artifacts": {...}
        },
        ...
      }
    }
    
    éŠ†æ„ªå¨‡é¢ã„¥æº€é…â‚¬?    - å§£å¿é‡œç»›æ «æšç’ç²Œç€¹å±¾åšéšåº¤çšŸé¢ã„¤ç«´å¨†?    - é€å¯”æ¾§ç‚ºå™ºé‡å­˜æŸŠé”›å å½²æ¾¶æ°­æ©æ„¯æ¶“å¶…æ‚“ç»›æ «æšé”›?    - éšåº£ç”»é™æ•¤æµœåº£æ•“é´æ„¬å§£æ–¿æµ˜ç›?    """
    summary_path = suite_path / "summary.json"
    
    # ========== é”çŠºæµ‡é´æ §å±å¯¤ç°Šummary ==========
    if summary_path.exists():
        summary = json.loads(summary_path.read_text(encoding="utf-8"))
    else:
        summary = {
            "suite_id": suite_path.name,
            "created_at": datetime.now().isoformat(),
            "strategies": {},
        }
    
    # ========== é‡å­˜æŸŠç»›æ «æšæ·‡â„ƒä¼… ==========
    summary["updated_at"] = datetime.now().isoformat()
    summary["strategies"][strategy] = {
        "description": preset["description"],
        "timestamp": result.get("timestamp"),
        "algorithm": result.get("algorithm"),
        "episodes": episodes,
        "seed": seed,
        "controls": {
            "use_enhanced_cache": preset["use_enhanced_cache"],
            "disable_migration": preset["disable_migration"],
            "enforce_offload_mode": preset["enforce_offload_mode"],
            "scenario_key": preset.get("scenario_key"),
            "scenario_label": preset.get("scenario_label"),
            "flags": preset.get("flags", []),
        },
        "metrics": metrics,
        "artifacts": artifacts,
    }
    
    # ========== é¸ä½·ç®™é–æ ¦ç¹šç€›?==========
    summary_path.write_text(json.dumps(summary, indent=2, ensure_ascii=False), encoding="utf-8")


def copy_artifacts(
    result: Dict[str, Any],
    strategy_dir: Path,
) -> Dict[str, str]:
    """
    æ¾¶å¶…åŸ—ç’ç²ŒæµœÑ…æ•“é¨å‹¬ç‰³è¹‡å†©æƒæµ è·ºåŸŒç»›æ «æšæ¶“æ’³ç˜é©ç¶
    
    éŠ†æ„¬å§›é‘³å§â‚¬?    çå”—rain_single_agent.pyé¢ç†¸åšé¨å‹­ç²¨é‹æ»„æƒæµ è®¹ç´™JSON/é¥æã€ƒ/é¶ãƒ¥æ†¡é”›å¤Šé’è·ºåŸŒ
    ç»›æ «æšæ¶“æ’³ç˜é‚å›¦æ¬¢æ¾¶ç™¸ç´æ¸šå¤¸ç°¬éšåº£ç”»é’å—˜ç€½éœå±½ç¶Šå¦—ï½ƒâ‚¬?    
    éŠ†æ„¬å¼¬éèˆ¬â‚¬?    result: Dict[str, Any] - ç’ç²Œç¼æ’´ç‰ç€›æ¥€å€é”›å å¯˜éšçœlgorithméŠ†ä¹¼imestampç»›å¤›ç´š
    strategy_dir: Path - ç»›æ «æšæ¶“æ’³ç˜é©ç¶é”›å results/td3_strategy_suite/suite_id/local-only/é”›?    
    éŠ†æ„¯ç¹‘é¥ç‚²â‚¬ç¬ºâ‚¬?    Dict[str, str] - æ¾¶å¶…åŸ—éšåº£æ®‘é‚å›¦æ¬¢ç’ºç·ç€›æ¥€å€
        {
          "training_json": "path/to/training_results.json",
          "training_chart": "path/to/training_overview.png",
          "training_report": "path/to/training_report.html"
        }
    
    éŠ†æ„¬é’å‰æ®‘é‚å›¦æ¬¢éŠ†?    1. training_results_{timestamp}.json - ç€¹å±¾æš£ç’ç²Œéç‰ˆåµ
    2. training_overview.png - ç’ç²Œé‡èŒ¬åšé¥æã€ƒ
    3. training_report_{timestamp}.html - ç’ç²Œé¶ãƒ¥æ†¡
    
    éŠ†æ„­ç°®é‚å›¦æ¬¢æµ£å¶‡ç–†éŠ†?    results/single_agent/{algorithm}/
    
    éŠ†æ„®æ´°éå›¦ç¶…ç¼ƒâ‚¬?    results/td3_strategy_suite/{suite_id}/{strategy}/
    """
    algorithm = str(result.get("algorithm", "")).lower()
    timestamp = result.get("timestamp")
    artifacts: Dict[str, str] = {}

    # ========== çº­ç•¾å©§æ„­æƒæµ æƒ°çŸ¾å¯°?==========
    src_root = Path("results") / "single_agent" / algorithm
    if timestamp:
        json_name = f"training_results_{timestamp}.json"
        report_name = f"training_report_{timestamp}.html"
    else:
        json_name = "training_results.json"
        report_name = "training_report.html"
    chart_name = "training_overview.png"

    # ========== ç€¹æ°«ç®Ÿæ¾¶å¶…åŸ—å¨“å‘­å´Ÿ ==========
    copies = [
        ("training_json", src_root / json_name),
        ("training_chart", src_root / chart_name),
        ("training_report", src_root / report_name),
    ]
    
    # ========== éµÑ†æ¾¶å¶…åŸ— ==========
    strategy_dir.mkdir(parents=True, exist_ok=True)
    for key, src in copies:
        if src.exists():
            dst = strategy_dir / src.name
            shutil.copy2(src, dst)
            artifacts[key] = str(dst)
    
    return artifacts


def run_strategy(strategy: str, args: argparse.Namespace) -> None:
    """
    éµÑ†é—æ›šé‡œç»›æ «æšé¨å‹«ç•¬éç£‹ç¼å†©ç¥¦ç»‹?    
    éŠ†æ„¬å§›é‘³å§â‚¬?    æ©æ¬æ§¸æ¶“ç»˜å¢½ç›å±½åš±éå¸®ç´ç€¹å±¾åšæµ ãƒ¤ç¬…æµ è¯²å§Ÿé”›?    1. é”çŠºæµ‡ç»›æ «æšé–°å¶‡ç–†
    2. ç’å‰§ç–†é—…å¿”æº€ç»‰å¶…ç“™
    3. ç’‹å†ªæ•¤train_single_algorithmæ©æ¶œç’ç²Œ
    4. ç’ï¼„ç•»ç»‹å†²ç•¾é¬Ñ†å…˜é¸å›¨çˆ£
    5. æ¾¶å¶…åŸ—ç¼æ’´ç‰é‚å›¦æ¬¢
    6. é‡å­˜æŸŠsummary.json
    7. éµæ’³åµƒç¼æ’´ç‰é½æ¨¿
    
    éŠ†æ„¬å¼¬éèˆ¬â‚¬?    strategy: str - ç»›æ «æšéšå¶‡Ğé”›å ç¹€æ¤¤è¯²æ¹ªSTRATEGY_PRESETSæ¶“ç•¾æ¶”å¤›ç´š
    args: argparse.Namespace - é›æˆ’æŠ¤ç›å±½å¼¬é?    
    éŠ†æ„¬ä¼æµ£æ»„ç¥¦ç»‹å¬¨â‚¬?    å§ãƒ©1: æ¥ å²ƒç˜‰ç»›æ «æšéšå¶‡Ğ
    å§ãƒ©2: ç’å‰§ç–†é—…å¿”æº€ç»‰å¶…ç“™é”›å œç¹šç’‡ä½¸å½²é–²å¶…é¬Ñç´š
    å§ãƒ©3: ç’‹å†ªæ•¤ç’ç²Œé‘èŠ¥æšŸé”›å œå¨‡é¢ã„§ç“¥é£ãƒ¤ç¬“çç‚ºå¤ç¼ƒç´š
    å§ãƒ©4: æµ åº¤ç¼å†ªç²¨é‹æ»€è…‘é»æ„¬å½‡é¬Ñ†å…˜é¸å›¨çˆ£
    å§ãƒ©5: ç’ï¼„ç•»ç»‹å†²ç•¾é§å›§â‚¬ç¡·ç´™æµ£è·¨æ•¤tail_meané”›?    å§ãƒ©6: æ¾¶å¶…åŸ—é¢ç†¸åšé¨å‹¬æƒæµ è·ºåŸŒç»›æ «æšé©ç¶
    å§ãƒ©7: é‡å­˜æŸŠå§¹å›¨â‚¬ç±SON
    å§ãƒ©8: éµæ’³åµƒç¼æ’´ç‰
    
    éŠ†æ„¯ç·­é‘çƒ˜æƒæµ å‰ç²¨é‹å‹©â‚¬?    results/td3_strategy_suite/{suite_id}/
    éˆ¹æº¾æ”¢éˆ¹â‚¬ summary.json                    # å§¹å›¨â‚¬ç»˜æƒæµ è®¹ç´™éµâ‚¬éˆå¤Œç“¥é£ãƒ¯ç´š
    éˆ¹æº¾æ”¢éˆ¹â‚¬ local-only/
    éˆ¹?  éˆ¹æº¾æ”¢éˆ¹â‚¬ training_results_*.json
    éˆ¹?  éˆ¹æº¾æ”¢éˆ¹â‚¬ training_overview.png
    éˆ¹?  éˆ¹æ–ºæ”¢éˆ¹â‚¬ training_report_*.html
    éˆ¹æº¾æ”¢éˆ¹â‚¬ remote-only/
    éˆ¹?  éˆ¹æ–ºæ”¢éˆ¹â‚¬ ...
    éˆ¹æ–ºæ”¢éˆ¹â‚¬ ...
    
    éŠ†æ„­â‚¬Ñ†å…˜é¸å›¨çˆ£éŠ†?    - delay_mean: éªå†²æ½æµ è¯²å§Ÿéƒè·ºæ¬¢é”›å ¢é”›?    - energy_mean: éªå†²æ½é¬æ˜å…˜é‘°æ¥‹ç´™é’ï¹â‚¬ç­¹ç´š
    - completion_mean: æµ è¯²å§Ÿç€¹å±¾åšéœå›·ç´™0-1é”›?    - raw_cost: ç¼ç†¶ç«´æµ ï½„ç¯é‘èŠ¥æšŸé”›å £ç§ºçå¿šç§ºæ¿‚æ–¤ç´š
    """
    # ========== å§ãƒ©1: é”çŠºæµ‡ç»›æ «æšé–°å¶‡ç–† ==========
    if strategy not in STRATEGY_PRESETS:
        raise ValueError(f"Unknown strategy: {strategy}")
    preset = STRATEGY_PRESETS[strategy]
    scenario_label = preset.get("scenario_label", "Simulator defaults")
    control_flags = ", ".join(preset.get("flags", [])) or "none"

    # ========== å§ãƒ©2: çº­ç•¾ç’ç²Œé™å‚›æšŸ ==========
    episodes = args.episodes or preset["episodes"]
    seed = args.seed if args.seed is not None else DEFAULT_SEED

    # ========== å§ãƒ©3: ç’å‰§ç–†é—…å¿”æº€ç»‰å¶…ç“™ ==========
    os.environ["RANDOM_SEED"] = str(seed)
    _apply_global_seed_from_env()

    # ========== æ­¥éª¤4: æ‰§è¡Œç­–ç•¥ ==========
    # TD3 ç»§ç»­è°ƒç”¨è®­ç»ƒæ¥å£ï¼Œå¯å‘å¼ç­–ç•¥èµ°è½»é‡è¯„ä¼°
    env_options = dict(preset.get("env_options") or {})
    if preset.get("central_resource"):
        os.environ['CENTRAL_RESOURCE'] = '1'
    else:
        os.environ.pop('CENTRAL_RESOURCE', None)

    algorithm_kind = str(preset["algorithm"]).lower()
    if algorithm_kind == "heuristic":
        silent = True
        results = _run_heuristic_strategy(preset, episodes, seed, env_options=env_options)
    else:
        silent = getattr(args, "silent", True)
        results = train_single_algorithm(
            preset["algorithm"],
            num_episodes=episodes,
            silent_mode=silent,
            override_scenario=preset["override_scenario"],
            use_enhanced_cache=preset["use_enhanced_cache"],
            disable_migration=preset["disable_migration"],
            enforce_offload_mode=preset["enforce_offload_mode"],
            joint_controller=env_options.get("joint_controller", False),
        )

    # ========== æ­¥éª¤5: æå–æ€§èƒ½æŒ‡æ ‡ ==========
    episode_metrics: Dict[str, Any] = results.get("episode_metrics", {})
    delay_mean = tail_mean(episode_metrics.get("avg_delay", []))
    energy_mean = tail_mean(episode_metrics.get("total_energy", []))
    completion_mean = tail_mean(episode_metrics.get("task_completion_rate", []))
    
    # ğŸ¯ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨å¥–åŠ±è®¡ç®—æˆæœ¬ï¼ˆä¸strategy_runner.pyä¸€è‡´ï¼‰
    episode_rewards = results.get("episode_rewards", [])
    avg_reward: Optional[float] = None
    if episode_rewards and len(episode_rewards) > 0:
        # ä½¿ç”¨å50%æ•°æ®ï¼ˆæ”¶æ•›åï¼‰
        if len(episode_rewards) >= 100:
            half_point = len(episode_rewards) // 2
            avg_reward = float(np.mean(episode_rewards[half_point:]))
        elif len(episode_rewards) >= 50:
            avg_reward = float(np.mean(episode_rewards[-30:]))
        else:
            avg_reward = float(np.mean(episode_rewards))
    
    # å¯¼å…¥ç»Ÿä¸€çš„compute_costå‡½æ•°ï¼ˆè‡ªåŠ¨å¤„ç†rewardä¼˜å…ˆé€»è¾‘ï¼‰
    from experiments.td3_strategy_suite.strategy_runner import compute_cost
    raw_cost = compute_cost(delay_mean, energy_mean, avg_reward, completion_mean)

    # ========== å§ãƒ©6: é‘å——æˆæ’³åš­é©ç¶ ==========
    suite_id = args.suite_id or datetime.now().strftime("%Y%m%d_%H%M%S")
    suite_path = Path(args.output_root) / suite_id
    strategy_dir = suite_path / strategy
    suite_path.mkdir(parents=True, exist_ok=True)

    # ========== å§ãƒ©7: æ¾¶å¶…åŸ—ç¼æ’´ç‰é‚å›¦æ¬¢ ==========
    artifacts = copy_artifacts(results, strategy_dir)

    # ========== å§ãƒ©8: å§¹å›¨â‚¬ç»˜â‚¬Ñ†å…˜é¸å›¨çˆ£ ==========
    metrics = {
        "delay_mean": delay_mean,
        "energy_mean": energy_mean,
        "completion_mean": completion_mean,
        "raw_cost": raw_cost,
    }
    
    # ========== å§ãƒ©9: é‡å­˜æŸŠsummary.json ==========
    update_summary(suite_path, strategy, preset, results, metrics, artifacts, episodes, seed)

    # ========== å§ãƒ©10: éµæ’³åµƒç¼æ’´ç‰é½æ¨¿ ==========
    print("\n=== Strategy Run Completed ===")
    print(f"Suite ID        : {suite_id}")
    print(f"Strategy        : {strategy}")
    print(f"Episodes        : {episodes}")
    print(f"Seed            : {seed}")
    print(f"Scenario Profile: {scenario_label}")
    print(f"Toggles         : {control_flags}")
    print(f"Average Delay   : {delay_mean:.4f} s")
    print(f"Average Energy  : {energy_mean:.2f} J")
    print(f"Completion Rate : {completion_mean:.3f}")
    print(f"Raw Cost        : {raw_cost:.4f}")
    if artifacts:
        print("Artifacts:")
        for key, path in artifacts.items():
            print(f"  - {key}: {path}")
    summary_path = suite_path / "summary.json"
    print(f"Summary updated : {summary_path}")


def build_argument_parser() -> argparse.ArgumentParser:
    """
    é‹å‹«ç¼“é›æˆ’æŠ¤ç›å±½å¼¬éæ‹ŒĞ’é‹æ„¬æ«’
    
    éŠ†æ„¬å§›é‘³å§â‚¬?    ç€¹æ°«ç®Ÿé‘´æ°­æ¹°é¨å‹«æ‡¡æµ ã‚ˆéºãƒ¥å½›é”›å±¾æ•®é¸ä½ºä¼’å¨²å©šå¤ç¼ƒç¼å†¨å¼¬éèˆ¬â‚¬?    
    éŠ†æ„¯ç¹‘é¥ç‚²â‚¬ç¬ºâ‚¬?    argparse.ArgumentParser - é–°å¶‡ç–†æ¿‚ç•Œæ®‘é™å‚›æšŸç‘™ï½†ç€½é£?    
    éŠ†æ„¬æ‡¡æµ ã‚ˆé™å‚›æšŸéŠ†?    --strategy: str (è¹‡å‘´æ¸¶)
        - ç»›æ «æšéšå¶‡Ğé”›å±½å½²é–«å¤Šâ‚¬? local-only, remote-only, offloading-only, 
          resource-only, comprehensive-no-migration, comprehensive-migration
    
    --episodes: int (é™â‚¬?
        - ç’ç²ŒææšŸé”›å²„ç²¯ç’?00
        - è¹‡â‚¬ç†¸ç¥´ç’‡æ›å½²é¢?0-100é”›å±½ç•¬éæ‘ç–„æ¥ å±½ç¼“ç’?00-1000
    
    --seed: int (é™â‚¬?
        - é—…å¿”æº€ç»‰å¶…ç“™é”›å²„ç²¯ç’?2
        - é¢ã„¤ç°¬æ·‡æ¿Šç˜‰ç€¹ç‚ºç™é™å™¸æ¾¶å¶†â‚¬?    
    --suite-id: str (é™â‚¬?
        - Suiteéå›ªç˜‘ç»—ï¸¼ç´é¢ã„¤ç°¬çå——æ¶“ç“¥é£ãƒ¥ç¶Šæ¶“å“„æ‚“æ¶“â‚¬ç¼å‹«ç–„æ¥ ?        - éˆå¯šç€¹æ°­æ¤‚é‘·å§©é¢ç†¸åšéƒå •æ£¿é´ç­¹ç´™YYYYMMDD_HHMMSSé”›?    
    --output-root: str (é™â‚¬?
        - æˆæ’³åš­éåœ­æ´°è¤°æ›ªç´æ¦›æ¨¿"results/td3_strategy_suite"
    
    --silent: bool (é™â‚¬?
        - é—ˆæ¬“ç²¯å¦¯â€³ç´¡é”›å±½å™ºçæˆ£ç¼å†­ç¹ƒç»‹å¬¬æ®‘æˆæ’³åš­
        - é‰?å¨‰ã„¦å‰°é”›æ°­å£’é–²å¿“ç–„æ¥ å²ƒå‰¼éˆç²¯ç’ã‚…å‡¡éšæ•¤é—ˆæ¬“ç²¯å¦¯â€³ç´¡é”›å±¾æ£¤é—‡â‚¬éµå¬ªå§©æµœã‚„ç°°
    
    éŠ†æ„ªå¨‡é¢ã„§ãšæ¸šå¬¨â‚¬?    # é‰?æ¦›æ¨¿é—ˆæ¬“ç²¯æ©æ„¯é”›å Ÿæ£¤é—‡â‚¬éµå¬ªå§©æµœã‚„ç°°é”›å±¾å¸¹é‘½æ„¶ç´š
    # é©çƒ˜æ¹°é¢ã„¦ç¡¶
    python run_strategy_training.py --strategy local-only
    
    # é¸å›§ç•¾é™å‚›æšŸ - é‘·å§©æ·‡æ¿†ç“¨é¶ãƒ¥æ†¡é”›å±¾æ£¤æµœå“„â‚¬ç…ç•§æ©æ„¯
    python run_strategy_training.py --strategy comprehensive-migration \\
        --episodes 1000 --seed 123 --suite-id exp_ablation_v1
    
    # è¹‡â‚¬ç†¸ç¥´ç’‡æ›ªç´™å®¸æŸ¥ç²¯ç’ã‚‰æ½¤æ¦›æ©ˆç´š
    python run_strategy_training.py --strategy offloading-only \\
        --episodes 50
    
    # é¦ƒæŒ• æ¿¡å‚æ¸¶æµœã‚„ç°°å¯®å¿•'ç’ã‚„ç¹šç€›æ¨»å§¤é›å©ç´å¨£è¯²å§ --interactive é™å‚›æšŸ
    python run_strategy_training.py --strategy td3-full \\
        --episodes 500 --interactive
    """
    parser = argparse.ArgumentParser(
        description="Run TD3 under a specific strategy baseline and collect results."
    )
    parser.add_argument(
        "--strategy",
        type=str,
        required=True,
        choices=list(STRATEGY_PRESETS.keys()),
        help="Select which strategy preset to train.",
    )
    parser.add_argument(
        "--episodes", 
        type=int, 
        help="Override number of training episodes (default 800)."
    )
    parser.add_argument(
        "--seed", 
        type=int, 
        help="Random seed (default 42)."
    )
    parser.add_argument(
        "--suite-id", 
        type=str, 
        help="Suite identifier to group multiple runs."
    )
    parser.add_argument(
        "--output-root",
        type=str,
        default="results/td3_strategy_suite",
        help="Root folder where per-strategy results will be stored.",
    )
    parser.add_argument(
        "--silent", 
        action="store_true", 
        help="Run training in silent mode."
    )
    return parser


def main() -> None:
    """
    é‘´æ°­æ¹°æ¶“è¯²å†é™ï½…åš±é?    
    éŠ†æ„¬å§›é‘³å§â‚¬?    ç‘™ï½†ç€½é›æˆ’æŠ¤ç›å±½å¼¬éæ¿è‹Ÿéšå§©ç»›æ «æšç’ç²Œå¨´ä½ºâ–¼éŠ†?    
    éŠ†æ„­å¢½ç›å±¾ç¥¦ç»‹å¬¨â‚¬?    1. é‹å‹«ç¼“é™å‚›æšŸç‘™ï½†ç€½é£?    2. ç‘™ï½†ç€½é›æˆ’æŠ¤ç›å±½å¼¬é?    3. ç’‹å†ªæ•¤run_strategyéµÑ†ç’ç²Œ
    
    éŠ†æ„°æ•Šç’‡éå—â‚¬?    - éˆç…¡ç»›æ «æšéšå¶‡Ğé”›æ­alueError
    - é™å‚›æšŸç¼‚å“„ã‘é”›æ­›rgparseé‘·å§©é»æ„®ãš
    - ç’ç²Œæ©å›©â–¼é–¿æ¬’é”›æ°±æ•±train_single_algorithmæ¾¶å‹­æ‚Š
    """
    parser = build_argument_parser()
    args = parser.parse_args()
    run_strategy(args.strategy, args)


if __name__ == "__main__":
    main()
