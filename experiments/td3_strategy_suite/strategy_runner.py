#!/usr/bin/env python3
"""
Shared utilities for evaluating the six strategy presets across arbitrary scenarios.
"""

from __future__ import annotations

import copy
import json
import os
import sys
from pathlib import Path
from typing import Callable, Dict, Iterable, List, Optional

import numpy as np

project_root = Path(__file__).resolve().parents[2]
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from config import config  # noqa: E402
from train_single_agent import _apply_global_seed_from_env, train_single_algorithm  # noqa: E402
from experiments.td3_strategy_suite.run_strategy_training import (  # noqa: E402
    STRATEGY_PRESETS,
    _run_heuristic_strategy,
)
from utils.unified_reward_calculator import UnifiedRewardCalculator  # noqa: E402
# ç¼“å­˜ç³»ç»Ÿå·²ç¦ç”¨
# from experiments.td3_strategy_suite.strategy_model_cache import get_global_cache  # noqa: E402

STRATEGY_KEYS: List[str] = list(STRATEGY_PRESETS.keys())

# ========== åˆå§‹åŒ–ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ ==========
# ä½¿ç”¨ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨ç¡®ä¿ä¸Žè®­ç»ƒæ—¶çš„å¥–åŠ±å‡½æ•°ä¸€è‡´
_reward_calculator = None


def _get_reward_calculator() -> UnifiedRewardCalculator:
    """èŽ·å–å…¨å±€å¥–åŠ±è®¡ç®—å™¨å®žä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global _reward_calculator
    if _reward_calculator is None:
        _reward_calculator = UnifiedRewardCalculator(algorithm="general")
    return _reward_calculator


def tail_mean(values: Iterable[float]) -> float:
    seq = list(map(float, values or []))
    if not seq:
        return 0.0
    if len(seq) >= 100:
        seq = seq[len(seq) // 2 :]
    return float(np.mean(seq))


def compute_cost(avg_delay: float, avg_energy: float) -> float:
    """
    è®¡ç®—ç»Ÿä¸€ä»£ä»·å‡½æ•°å€¼ï¼ˆä¸Žè®­ç»ƒæ—¶çš„å¥–åŠ±å‡½æ•°ä¸€è‡´ï¼‰
    
    ã€åŠŸèƒ½ã€‘
    ä½¿ç”¨ç»Ÿä¸€å¥–åŠ±è®¡ç®—å™¨è®¡ç®—å½’ä¸€åŒ–çš„åŠ æƒä»£ä»·ï¼Œç¡®ä¿ä¸Žè®­ç»ƒæ—¶ä½¿ç”¨çš„
    å¥–åŠ±å‡½æ•°å®Œå…¨ä¸€è‡´ã€‚è¯¥å‡½æ•°ç”¨äºŽç­–ç•¥å¯¹æ¯”å®žéªŒçš„æ€§èƒ½è¯„ä¼°ã€‚
    
    ã€å‚æ•°ã€‘
    avg_delay: float - å¹³å‡ä»»åŠ¡æ—¶å»¶ï¼ˆç§’ï¼‰
    avg_energy: float - å¹³å‡æ€»èƒ½è€—ï¼ˆç„¦è€³ï¼‰
    
    ã€è¿”å›žå€¼ã€‘
    float - å½’ä¸€åŒ–çš„åŠ æƒä»£ä»·ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    
    ã€è®¡ç®—å…¬å¼ã€‘
    Cost = Ï‰_T Â· (T / T_target) + Ï‰_E Â· (E / E_target)
    å…¶ä¸­ï¼š
    - Ï‰_T = 2.0ï¼ˆæ—¶å»¶æƒé‡ï¼‰
    - Ï‰_E = 1.2ï¼ˆèƒ½è€—æƒé‡ï¼‰
    - T_target = 0.4sï¼ˆæ—¶å»¶ç›®æ ‡å€¼ï¼Œç”¨äºŽå½’ä¸€åŒ–ï¼‰
    - E_target = 1200Jï¼ˆèƒ½è€—ç›®æ ‡å€¼ï¼Œç”¨äºŽå½’ä¸€åŒ–ï¼‰
    
    ã€ä¿®å¤è¯´æ˜Žã€‘
    âœ… ä¿®å¤åŽï¼šä½¿ç”¨latency_targetå’Œenergy_targetï¼Œä¸Žè®­ç»ƒæ—¶çš„å¥–åŠ±è®¡ç®—å®Œå…¨ä¸€è‡´
    âœ… ä¿®å¤å‰ï¼šé”™è¯¯ä½¿ç”¨äº†delay_normalizer(0.2)å’Œenergy_normalizer(1000)
    âœ… ç¡®ä¿è¯„ä¼°æŒ‡æ ‡ä¸Žè®­ç»ƒæŒ‡æ ‡å¯æ¯”
    """
    weight_delay = float(config.rl.reward_weight_delay)
    weight_energy = float(config.rl.reward_weight_energy)
    
    # âœ… ä¿®å¤ï¼šä½¿ç”¨ä¸Žè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„å½’ä¸€åŒ–å› å­
    calc = _get_reward_calculator()
    delay_normalizer = calc.latency_target  # 0.4ï¼ˆä¸Žè®­ç»ƒä¸€è‡´ï¼‰
    energy_normalizer = calc.energy_target  # 1200.0ï¼ˆä¸Žè®­ç»ƒä¸€è‡´ï¼‰
    
    return (
        weight_delay * (avg_delay / max(delay_normalizer, 1e-6))
        + weight_energy * (avg_energy / max(energy_normalizer, 1e-6))
    )


def normalize_costs(cost_map: Dict[str, float]) -> Dict[str, float]:
    if not cost_map:
        return {}
    min_cost = min(cost_map.values())
    max_cost = max(cost_map.values())
    span = max(max_cost - min_cost, 1e-12)
    return {k: (v - min_cost) / span for k, v in cost_map.items()}


def enrich_with_normalized_costs(results: Dict[str, Dict[str, float]]) -> Dict[str, Dict[str, float]]:
    normalized = normalize_costs({k: v["raw_cost"] for k, v in results.items()})
    enriched: Dict[str, Dict[str, float]] = {}
    for key, metrics in results.items():
        enriched[key] = dict(metrics)
        enriched[key]["normalized_cost"] = normalized[key]
        enriched[key]["strategy_label"] = strategy_label(key)
    return enriched


def run_strategy_suite(
    override_scenario: Dict[str, object],
    episodes: int,
    seed: int,
    silent: bool,
    strategies: Optional[Iterable[str]] = None,
    central_resource: bool = False,
) -> Dict[str, Dict[str, float]]:
    return _run_strategy_suite_internal(
        override_scenario=override_scenario,
        episodes=episodes,
        seed=seed,
        silent=silent,
        strategies=strategies,
        include_episode_metrics=False,
        central_resource=central_resource,
    )


def run_strategy_suite_with_details(
    override_scenario: Dict[str, object],
    episodes: int,
    seed: int,
    silent: bool,
    strategies: Optional[Iterable[str]] = None,
    central_resource: bool = False,
) -> Dict[str, Dict[str, object]]:
    return _run_strategy_suite_internal(
        override_scenario=override_scenario,
        episodes=episodes,
        seed=seed,
        silent=silent,
        strategies=strategies,
        include_episode_metrics=True,
        central_resource=central_resource,
    )


def _run_strategy_suite_internal(
    override_scenario: Dict[str, object],
    episodes: int,
    seed: int,
    silent: bool,
    strategies: Optional[Iterable[str]],
    include_episode_metrics: bool,
    central_resource: bool = False,
) -> Dict[str, Dict[str, float]]:
    keys = list(strategies) if strategies is not None else STRATEGY_KEYS
    results: Dict[str, Dict[str, float]] = {}
    
    # ========== ç¼“å­˜ç³»ç»Ÿå·²ç¦ç”¨ ==========
    # æ³¨é‡ŠåŽŸå› ï¼šç”¨æˆ·é€‰æ‹©ä¸ä½¿ç”¨ç¼“å­˜ç³»ç»Ÿ
    # cache = get_global_cache()
    
    for key in keys:
        preset = STRATEGY_PRESETS[key]
        preset_override = preset.get("override_scenario")
        if preset_override:
            merged_override = copy.deepcopy(preset_override)
        else:
            merged_override = {}
        if override_scenario:
            merged_override.update(override_scenario)
        if merged_override:
            merged_override.setdefault("override_topology", True)

        # ========== ç¼“å­˜æ£€æŸ¥å·²ç¦ç”¨ ==========
        # cached_data = cache.get_cached_model(key, episodes, seed, merged_override)
        # 
        # if cached_data is not None:
        #     # ä½¿ç”¨ç¼“å­˜çš„è®­ç»ƒç»“æžœ
        #     cached_metrics = cached_data["metrics"]
        #     episode_metrics = cached_metrics.get("episode_metrics", {})
        #     
        #     avg_delay = tail_mean(episode_metrics.get("avg_delay", []))
        #     avg_energy = tail_mean(episode_metrics.get("total_energy", []))
        #     completion_rate = tail_mean(episode_metrics.get("task_completion_rate", []))
        #     raw_cost = compute_cost(avg_delay, avg_energy)
        #
        #     results[key] = {
        #         "avg_delay": avg_delay,
        #         "avg_energy": avg_energy,
        #         "completion_rate": completion_rate,
        #         "raw_cost": raw_cost,
        #         "episodes": episodes,
        #         "seed": seed,
        #         "from_cache": True,  # æ ‡è®°ä¸ºç¼“å­˜æ•°æ®
        #     }
        #     if include_episode_metrics:
        #         results[key]["episode_metrics"] = episode_metrics
        #     
        #     continue  # è·³è¿‡è®­ç»ƒ
        
        # ========== æ¯æ¬¡éƒ½æ‰§è¡Œè®­ç»ƒï¼ˆç¼“å­˜å·²ç¦ç”¨ï¼‰==========
        os.environ["RANDOM_SEED"] = str(seed)
        _apply_global_seed_from_env()
        
        # ðŸŽ¯ è®¾ç½®ä¸­å¤®èµ„æºåˆ†é…æ¨¡å¼çš„çŽ¯å¢ƒå˜é‡
        if central_resource:
            os.environ['CENTRAL_RESOURCE'] = '1'
        else:
            os.environ.pop('CENTRAL_RESOURCE', None)  # æ¸…é™¤çŽ¯å¢ƒå˜é‡

        algorithm_kind = str(preset["algorithm"]).lower()
        if algorithm_kind == "heuristic":
            outcome = _run_heuristic_strategy(
                preset=preset,
                episodes=episodes,
                seed=seed,
                extra_override=merged_override,
            )
        else:
            outcome = train_single_algorithm(
                preset["algorithm"],
                num_episodes=episodes,
                silent_mode=silent,
                override_scenario=merged_override,
                use_enhanced_cache=preset["use_enhanced_cache"],
                disable_migration=preset["disable_migration"],
                enforce_offload_mode=preset["enforce_offload_mode"],
            )

        episode_metrics = outcome.get("episode_metrics", {})
        avg_delay = tail_mean(episode_metrics.get("avg_delay", []))
        avg_energy = tail_mean(episode_metrics.get("total_energy", []))
        completion_rate = tail_mean(episode_metrics.get("task_completion_rate", []))
        raw_cost = compute_cost(avg_delay, avg_energy)

        results[key] = {
            "avg_delay": avg_delay,
            "avg_energy": avg_energy,
            "completion_rate": completion_rate,
            "raw_cost": raw_cost,
            "episodes": episodes,
            "seed": seed,
            "from_cache": False,  # æ ‡è®°ä¸ºæ–°è®­ç»ƒæ•°æ®ï¼ˆç¼“å­˜å·²ç¦ç”¨ï¼‰
        }
        if include_episode_metrics:
            results[key]["episode_metrics"] = episode_metrics
        
        # ========== ä¿å­˜åˆ°ç¼“å­˜å·²ç¦ç”¨ ==========
        # try:
        #     cache.save_model(
        #         strategy_key=key,
        #         episodes=episodes,
        #         seed=seed,
        #         overrides=merged_override,
        #         outcome=outcome,
        #         model_state=None,  # æš‚ä¸ä¿å­˜æ¨¡åž‹å‚æ•°ï¼ˆä»…ä¿å­˜metricsï¼‰
        #     )
        # except Exception as e:
        #     # ç¼“å­˜ä¿å­˜å¤±è´¥ä¸å½±å“è®­ç»ƒç»“æžœ
        #     if not silent:
        #         print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    return results


def attach_normalized_costs(result_list: List[Dict[str, object]]) -> None:
    for item in result_list:
        strategies = item.get("strategies", {})
        costs = {k: v.get("raw_cost", 0.0) for k, v in strategies.items()}
        normalized = normalize_costs(costs)
        for key, value in normalized.items():
            strategies[key]["normalized_cost"] = value


def strategy_label(strategy_key: str) -> str:
    return STRATEGY_PRESETS[strategy_key]["description"]


def dump_json(path: os.PathLike[str] | str, data: Dict[str, object]) -> None:
    path_obj = Path(path)
    path_obj.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")


def evaluate_configs(
    configs: List[Dict[str, object]],
    episodes: int,
    seed: int,
    silent: bool,
    suite_path: Path,
    strategies: Optional[Iterable[str]] = None,
    per_strategy_hook: Optional[Callable[[str, Dict[str, float], Dict[str, object], Dict[str, List[float]]], None]] = None,
    central_resource: bool = False,
) -> List[Dict[str, object]]:
    suite_path.mkdir(parents=True, exist_ok=True)
    evaluated: List[Dict[str, object]] = []
    keys = list(strategies) if strategies is not None else STRATEGY_KEYS

    for index, cfg in enumerate(configs, 1):
        cfg_copy = dict(cfg)
        cfg_key = str(cfg_copy.get("key") or f"config_{index}")
        label = str(cfg_copy.get("label", cfg_key))
        overrides = dict(cfg_copy.get("overrides", {}))
        config_dir = suite_path / cfg_key
        config_dir.mkdir(parents=True, exist_ok=True)

        print(f"\n{'='*72}")
        print(f"Configuration: {label} ({cfg_key})")
        print(f"Overrides: {json.dumps(overrides, ensure_ascii=False)}")
        print('=' * 72)

        raw = run_strategy_suite_with_details(
            override_scenario=overrides,
            episodes=episodes,
            seed=seed,
            silent=silent,
            strategies=keys,
            central_resource=central_resource,
        )
        enriched = enrich_with_normalized_costs(raw)

        target_completion = enriched.get("comprehensive-migration", {}).get("completion_rate", 0.0)

        for strat_key in keys:
            metrics = enriched[strat_key]
            episode_metrics = metrics.pop("episode_metrics", None)
            completion_rate = max(float(metrics.get("completion_rate", 0.0)), 1e-6)
            if target_completion > 0:
                multiplier = max(1.0, target_completion / completion_rate)
            else:
                multiplier = 1.0
            metrics["resource_multiplier_required"] = multiplier

            if per_strategy_hook:
                per_strategy_hook(strat_key, metrics, cfg_copy, episode_metrics or {})
            detail_path = config_dir / f"{strat_key}.json"
            metrics_to_save = dict(cfg_copy)
            metrics_to_save.update({"strategy": strat_key, **metrics})
            detail_path.write_text(json.dumps(metrics_to_save, indent=2, ensure_ascii=False), encoding="utf-8")
            print(
                f"  - {strategy_label(strat_key)}: "
                f"Cost={metrics['raw_cost']:.4f} Delay={metrics['avg_delay']:.4f}s "
                f"Energy={metrics['avg_energy']:.2f}J"
            )

        cfg_entry = {
            **cfg_copy,
            "key": cfg_key,
            "label": label,
            "strategies": enriched,
            "episodes": episodes,
            "seed": seed,
        }
        evaluated.append(cfg_entry)

    return evaluated
