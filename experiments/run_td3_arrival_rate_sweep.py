#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TD3ä»»åŠ¡åˆ°è¾¾ç‡æ•æ„Ÿæ€§åˆ†æå®éªŒ

ã€åŠŸèƒ½ã€‘
æµ‹è¯•ä¸åŒä»»åŠ¡åˆ°è¾¾ç‡(arrival_rate)å¯¹TD3ç®—æ³•æ€§èƒ½çš„å½±å“

ã€å®éªŒè®¾è®¡ã€‘
- ç®—æ³•: TD3
- æµ‹è¯•åˆ°è¾¾ç‡: 1.0, 1.5, 2.0, 2.5, 3.0, 3.5 tasks/s
- è½¦è¾†æ•°: 12è¾†ï¼ˆå›ºå®šï¼‰
- è¯„ä¼°æŒ‡æ ‡: ave_reward_per_stepï¼ˆå¹³å‡æ­¥å¥–åŠ±ï¼‰

ã€ä½¿ç”¨æ–¹æ³•ã€‘
# å¿«é€Ÿæµ‹è¯•ï¼ˆ50è½®ï¼‰
python experiments/run_td3_arrival_rate_sweep.py --episodes 50

# å®Œæ•´å®éªŒï¼ˆ800è½®ï¼‰
python experiments/run_td3_arrival_rate_sweep.py --episodes 800

# è‡ªå®šä¹‰åˆ°è¾¾ç‡èŒƒå›´
python experiments/run_td3_arrival_rate_sweep.py --rates 1.0 2.0 3.0 --episodes 200

ã€è¾“å‡ºã€‘
- ç»“æœä¿å­˜: results/parameter_sensitivity/arrival_rate/
- å¯¹æ¯”å›¾è¡¨: arrival_rate_comparison_[timestamp].png
- æ±‡æ€»æ•°æ®: arrival_rate_summary_[timestamp].json
"""

import os
import sys
import json
import argparse
import subprocess
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import config


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='TD3ä»»åŠ¡åˆ°è¾¾ç‡æ•æ„Ÿæ€§åˆ†æ')
    
    parser.add_argument('--rates', type=float, nargs='+',
                        default=[1.0, 1.5, 2.0, 2.5, 3.0, 3.5],
                        help='ä»»åŠ¡åˆ°è¾¾ç‡åˆ—è¡¨ (tasks/s)ï¼Œé»˜è®¤: 1.0-3.5')
    
    parser.add_argument('--episodes', type=int, default=200,
                        help='æ¯ä¸ªåˆ°è¾¾ç‡çš„è®­ç»ƒè½®æ¬¡ï¼Œé»˜è®¤: 200')
    
    parser.add_argument('--num-vehicles', type=int, default=12,
                        help='è½¦è¾†æ•°é‡ï¼Œé»˜è®¤: 12')
    
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­ï¼Œé»˜è®¤: 42')
    
    parser.add_argument('--skip-training', action='store_true',
                        help='è·³è¿‡è®­ç»ƒï¼Œä»…ä»ç°æœ‰ç»“æœç”Ÿæˆå›¾è¡¨')
    
    parser.add_argument('--output-dir', type=str,
                        default='results/parameter_sensitivity/arrival_rate',
                        help='ç»“æœè¾“å‡ºç›®å½•')
    
    return parser.parse_args()


def run_training(arrival_rate: float, episodes: int, num_vehicles: int, 
                 seed: int, output_dir: str) -> Dict[str, Any]:
    """
    è¿è¡Œå•ä¸ªåˆ°è¾¾ç‡çš„TD3è®­ç»ƒ
    
    ã€å‚æ•°ã€‘
    - arrival_rate: ä»»åŠ¡åˆ°è¾¾ç‡ (tasks/s)
    - episodes: è®­ç»ƒè½®æ¬¡
    - num_vehicles: è½¦è¾†æ•°é‡
    - seed: éšæœºç§å­
    - output_dir: è¾“å‡ºç›®å½•
    
    ã€è¿”å›ã€‘
    è®­ç»ƒç»“æœå­—å…¸ï¼ŒåŒ…å«æŒ‡æ ‡å’Œè·¯å¾„ä¿¡æ¯
    """
    print(f"\n{'='*80}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: arrival_rate={arrival_rate} tasks/s")
    print(f"{'='*80}")
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤
    cmd = [
        sys.executable,  # pythonè§£é‡Šå™¨
        'train_single_agent.py',
        '--algorithm', 'TD3',
        '--episodes', str(episodes),
        '--num-vehicles', str(num_vehicles),
        '--seed', str(seed),
        '--silent-mode'  # é™é»˜æ¨¡å¼ï¼Œé¿å…è¿‡å¤šè¾“å‡º
    ]
    
    # è®¾ç½®ç¯å¢ƒå˜é‡æ¥è¦†ç›–arrival_rate
    env = os.environ.copy()
    env['TASK_ARRIVAL_RATE'] = str(arrival_rate)
    
    # ä¸´æ—¶ä¿®æ”¹configï¼ˆè¿è¡Œæ—¶ï¼‰
    original_rate = config.task.arrival_rate
    config.task.arrival_rate = arrival_rate
    
    try:
        # è¿è¡Œè®­ç»ƒ
        print(f"ğŸ“ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print(f"ğŸ“Š ä»»åŠ¡åˆ°è¾¾ç‡: {arrival_rate} tasks/s")
        
        result = subprocess.run(
            cmd,
            env=env,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='replace'
        )
        
        if result.returncode != 0:
            print(f"âŒ è®­ç»ƒå¤±è´¥! è¿”å›ç : {result.returncode}")
            print(f"é”™è¯¯è¾“å‡º:\n{result.stderr}")
            return None
        
        print(f"âœ… è®­ç»ƒå®Œæˆ: arrival_rate={arrival_rate}")
        
        # æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒç»“æœæ–‡ä»¶
        results_dir = Path('results/single_agent/td3')
        if not results_dir.exists():
            print(f"âš ï¸  ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
            return None
        
        # æŸ¥æ‰¾æœ€æ–°çš„training_resultsæ–‡ä»¶
        result_files = list(results_dir.glob('training_results_*.json'))
        if not result_files:
            print(f"âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒç»“æœæ–‡ä»¶")
            return None
        
        latest_file = max(result_files, key=lambda p: p.stat().st_mtime)
        
        # è¯»å–ç»“æœ
        with open(latest_file, 'r', encoding='utf-8') as f:
            training_results = json.load(f)
        
        # æå–å…³é”®æŒ‡æ ‡
        metrics = extract_metrics(training_results, arrival_rate)
        
        # ä¿å­˜åˆ°æŒ‡å®šè¾“å‡ºç›®å½•
        save_result(metrics, arrival_rate, output_dir)
        
        return metrics
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    finally:
        # æ¢å¤åŸå§‹arrival_rate
        config.task.arrival_rate = original_rate


def extract_metrics(training_results: Dict[str, Any], arrival_rate: float) -> Dict[str, Any]:
    """
    ä»è®­ç»ƒç»“æœä¸­æå–å…³é”®æŒ‡æ ‡
    
    ã€å‚æ•°ã€‘
    - training_results: è®­ç»ƒç»“æœå­—å…¸
    - arrival_rate: ä»»åŠ¡åˆ°è¾¾ç‡
    
    ã€è¿”å›ã€‘
    åŒ…å«å…³é”®æŒ‡æ ‡çš„å­—å…¸
    """
    metrics = {
        'arrival_rate': arrival_rate,
        'timestamp': datetime.now().isoformat()
    }
    
    # æå–episodeå†å²æ•°æ®
    if 'episode_history' in training_results:
        history = training_results['episode_history']
        
        # è®¡ç®—æœ€å50è½®çš„å¹³å‡å€¼ï¼ˆç¨³å®šæœŸæ€§èƒ½ï¼‰
        last_n = 50
        
        if 'ave_reward_per_step' in history and len(history['ave_reward_per_step']) > 0:
            rewards = history['ave_reward_per_step']
            metrics['ave_reward_per_step_final'] = np.mean(rewards[-last_n:])
            metrics['ave_reward_per_step_std'] = np.std(rewards[-last_n:])
            metrics['ave_reward_per_step_all'] = rewards
        
        if 'avg_delay' in history and len(history['avg_delay']) > 0:
            delays = history['avg_delay']
            metrics['avg_delay_final'] = np.mean(delays[-last_n:])
            metrics['avg_delay_std'] = np.std(delays[-last_n:])
            metrics['avg_delay_all'] = delays
        
        if 'avg_energy' in history and len(history['avg_energy']) > 0:
            energies = history['avg_energy']
            metrics['avg_energy_final'] = np.mean(energies[-last_n:])
            metrics['avg_energy_std'] = np.std(energies[-last_n:])
            metrics['avg_energy_all'] = energies
        
        if 'total_dropped_tasks' in history and len(history['total_dropped_tasks']) > 0:
            dropped = history['total_dropped_tasks']
            metrics['dropped_tasks_final'] = np.mean(dropped[-last_n:])
            metrics['dropped_tasks_std'] = np.std(dropped[-last_n:])
            metrics['dropped_tasks_all'] = dropped
    
    # æå–æœ€ç»ˆè¯„ä¼°ç»“æœ
    if 'final_evaluation' in training_results:
        final_eval = training_results['final_evaluation']
        metrics['final_evaluation'] = final_eval
    
    return metrics


def save_result(metrics: Dict[str, Any], arrival_rate: float, output_dir: str):
    """ä¿å­˜å•ä¸ªå®éªŒç»“æœ"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    filename = f"arrival_rate_{arrival_rate:.1f}_results.json"
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {filepath}")


def load_existing_results(output_dir: str) -> List[Dict[str, Any]]:
    """ä»è¾“å‡ºç›®å½•åŠ è½½å·²æœ‰çš„å®éªŒç»“æœ"""
    results = []
    
    if not os.path.exists(output_dir):
        return results
    
    result_files = Path(output_dir).glob('arrival_rate_*_results.json')
    
    for filepath in result_files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                metrics = json.load(f)
                results.append(metrics)
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç»“æœæ–‡ä»¶å¤±è´¥: {filepath}, é”™è¯¯: {e}")
    
    return results


def generate_comparison_plots(all_results: List[Dict[str, Any]], output_dir: str):
    """
    ç”Ÿæˆåˆ°è¾¾ç‡å¯¹æ¯”å›¾è¡¨
    
    ã€å‚æ•°ã€‘
    - all_results: æ‰€æœ‰å®éªŒç»“æœåˆ—è¡¨
    - output_dir: è¾“å‡ºç›®å½•
    """
    if not all_results:
        print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„å®éªŒç»“æœ")
        return
    
    # æŒ‰arrival_rateæ’åº
    all_results = sorted(all_results, key=lambda x: x['arrival_rate'])
    
    arrival_rates = [r['arrival_rate'] for r in all_results]
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('TD3ç®—æ³• - ä»»åŠ¡åˆ°è¾¾ç‡æ•æ„Ÿæ€§åˆ†æ', fontsize=16, fontweight='bold')
    
    # ========== å›¾1: å¹³å‡æ­¥å¥–åŠ± vs åˆ°è¾¾ç‡ ==========
    ax1 = axes[0, 0]
    
    rewards_mean = [r.get('ave_reward_per_step_final', 0) for r in all_results]
    rewards_std = [r.get('ave_reward_per_step_std', 0) for r in all_results]
    
    ax1.plot(arrival_rates, rewards_mean, 'o-', linewidth=2, markersize=8, 
             color='#2ecc71', label='å¹³å‡æ­¥å¥–åŠ±')
    ax1.fill_between(arrival_rates, 
                     np.array(rewards_mean) - np.array(rewards_std),
                     np.array(rewards_mean) + np.array(rewards_std),
                     alpha=0.2, color='#2ecc71')
    ax1.set_xlabel('ä»»åŠ¡åˆ°è¾¾ç‡ (tasks/s)', fontsize=12)
    ax1.set_ylabel('å¹³å‡æ­¥å¥–åŠ±', fontsize=12)
    ax1.set_title('(a) å¹³å‡æ­¥å¥–åŠ± vs ä»»åŠ¡åˆ°è¾¾ç‡', fontsize=13, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # ========== å›¾2: å¹³å‡æ—¶å»¶ vs åˆ°è¾¾ç‡ ==========
    ax2 = axes[0, 1]
    
    delays_mean = [r.get('avg_delay_final', 0) for r in all_results]
    delays_std = [r.get('avg_delay_std', 0) for r in all_results]
    
    ax2.plot(arrival_rates, delays_mean, 's-', linewidth=2, markersize=8,
             color='#e74c3c', label='å¹³å‡æ—¶å»¶')
    ax2.fill_between(arrival_rates,
                     np.array(delays_mean) - np.array(delays_std),
                     np.array(delays_mean) + np.array(delays_std),
                     alpha=0.2, color='#e74c3c')
    ax2.set_xlabel('ä»»åŠ¡åˆ°è¾¾ç‡ (tasks/s)', fontsize=12)
    ax2.set_ylabel('å¹³å‡æ—¶å»¶ (s)', fontsize=12)
    ax2.set_title('(b) å¹³å‡æ—¶å»¶ vs ä»»åŠ¡åˆ°è¾¾ç‡', fontsize=13, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    # ========== å›¾3: å¹³å‡èƒ½è€— vs åˆ°è¾¾ç‡ ==========
    ax3 = axes[1, 0]
    
    energies_mean = [r.get('avg_energy_final', 0) for r in all_results]
    energies_std = [r.get('avg_energy_std', 0) for r in all_results]
    
    ax3.plot(arrival_rates, energies_mean, '^-', linewidth=2, markersize=8,
             color='#3498db', label='å¹³å‡èƒ½è€—')
    ax3.fill_between(arrival_rates,
                     np.array(energies_mean) - np.array(energies_std),
                     np.array(energies_mean) + np.array(energies_std),
                     alpha=0.2, color='#3498db')
    ax3.set_xlabel('ä»»åŠ¡åˆ°è¾¾ç‡ (tasks/s)', fontsize=12)
    ax3.set_ylabel('å¹³å‡èƒ½è€— (J)', fontsize=12)
    ax3.set_title('(c) å¹³å‡èƒ½è€— vs ä»»åŠ¡åˆ°è¾¾ç‡', fontsize=13, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    ax3.legend()
    
    # ========== å›¾4: ä¸¢å¼ƒä»»åŠ¡æ•° vs åˆ°è¾¾ç‡ ==========
    ax4 = axes[1, 1]
    
    dropped_mean = [r.get('dropped_tasks_final', 0) for r in all_results]
    dropped_std = [r.get('dropped_tasks_std', 0) for r in all_results]
    
    ax4.plot(arrival_rates, dropped_mean, 'd-', linewidth=2, markersize=8,
             color='#f39c12', label='ä¸¢å¼ƒä»»åŠ¡æ•°')
    ax4.fill_between(arrival_rates,
                     np.array(dropped_mean) - np.array(dropped_std),
                     np.array(dropped_mean) + np.array(dropped_std),
                     alpha=0.2, color='#f39c12')
    ax4.set_xlabel('ä»»åŠ¡åˆ°è¾¾ç‡ (tasks/s)', fontsize=12)
    ax4.set_ylabel('ä¸¢å¼ƒä»»åŠ¡æ•°', fontsize=12)
    ax4.set_title('(d) ä¸¢å¼ƒä»»åŠ¡æ•° vs ä»»åŠ¡åˆ°è¾¾ç‡', fontsize=13, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    ax4.legend()
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    plot_filename = f"arrival_rate_comparison_{timestamp}.png"
    plot_path = os.path.join(output_dir, plot_filename)
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {plot_path}")
    
    plt.close()


def generate_summary_report(all_results: List[Dict[str, Any]], output_dir: str):
    """
    ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    
    ã€å‚æ•°ã€‘
    - all_results: æ‰€æœ‰å®éªŒç»“æœåˆ—è¡¨
    - output_dir: è¾“å‡ºç›®å½•
    """
    if not all_results:
        return
    
    # æŒ‰arrival_rateæ’åº
    all_results = sorted(all_results, key=lambda x: x['arrival_rate'])
    
    # åˆ›å»ºæ±‡æ€»æ•°æ®
    summary = {
        'experiment_info': {
            'algorithm': 'TD3',
            'parameter': 'arrival_rate',
            'unit': 'tasks/s',
            'num_experiments': len(all_results),
            'timestamp': datetime.now().isoformat()
        },
        'results': []
    }
    
    print(f"\n{'='*80}")
    print("ğŸ“Š å®éªŒç»“æœæ±‡æ€»")
    print(f"{'='*80}")
    print(f"{'åˆ°è¾¾ç‡':>10} | {'å¹³å‡æ­¥å¥–åŠ±':>12} | {'å¹³å‡æ—¶å»¶':>10} | {'å¹³å‡èƒ½è€—':>10} | {'ä¸¢å¼ƒä»»åŠ¡':>10}")
    print(f"{'-'*80}")
    
    for result in all_results:
        rate = result['arrival_rate']
        reward = result.get('ave_reward_per_step_final', 0)
        delay = result.get('avg_delay_final', 0)
        energy = result.get('avg_energy_final', 0)
        dropped = result.get('dropped_tasks_final', 0)
        
        print(f"{rate:>10.1f} | {reward:>12.4f} | {delay:>10.4f} | {energy:>10.4f} | {dropped:>10.2f}")
        
        summary['results'].append({
            'arrival_rate': rate,
            'ave_reward_per_step': reward,
            'avg_delay': delay,
            'avg_energy': energy,
            'dropped_tasks': dropped
        })
    
    print(f"{'='*80}\n")
    
    # ä¿å­˜æ±‡æ€»JSON
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    summary_filename = f"arrival_rate_summary_{timestamp}.json"
    summary_path = os.path.join(output_dir, summary_filename)
    
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print("="*80)
    print("ğŸ”¬ TD3ä»»åŠ¡åˆ°è¾¾ç‡æ•æ„Ÿæ€§åˆ†æå®éªŒ")
    print("="*80)
    print(f"ğŸ“‹ å®éªŒé…ç½®:")
    print(f"   - ç®—æ³•: TD3")
    print(f"   - åˆ°è¾¾ç‡èŒƒå›´: {args.rates} tasks/s")
    print(f"   - è®­ç»ƒè½®æ¬¡: {args.episodes}")
    print(f"   - è½¦è¾†æ•°: {args.num_vehicles}")
    print(f"   - éšæœºç§å­: {args.seed}")
    print(f"   - è¾“å‡ºç›®å½•: {args.output_dir}")
    print("="*80)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    all_results = []
    
    if not args.skip_training:
        # è¿è¡Œæ‰€æœ‰å®éªŒ
        for rate in args.rates:
            result = run_training(
                arrival_rate=rate,
                episodes=args.episodes,
                num_vehicles=args.num_vehicles,
                seed=args.seed,
                output_dir=args.output_dir
            )
            
            if result:
                all_results.append(result)
    else:
        print("â­ï¸  è·³è¿‡è®­ç»ƒï¼ŒåŠ è½½å·²æœ‰ç»“æœ...")
        all_results = load_existing_results(args.output_dir)
    
    if not all_results:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„å®éªŒç»“æœ!")
        return
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    print(f"\n{'='*80}")
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    print(f"{'='*80}")
    generate_comparison_plots(all_results, args.output_dir)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    generate_summary_report(all_results, args.output_dir)
    
    print(f"\n{'='*80}")
    print("âœ… å®éªŒå®Œæˆ!")
    print(f"{'='*80}")
    print(f"ğŸ“ ç»“æœç›®å½•: {args.output_dir}")
    print(f"ğŸ“Š å…±å®Œæˆ {len(all_results)} ä¸ªå®éªŒ")
    print(f"{'='*80}\n")


if __name__ == '__main__':
    main()

