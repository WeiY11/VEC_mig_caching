#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹æ³›åŒ–æ€§éªŒè¯æ¡†æ¶

ã€åŠŸèƒ½ã€‘
å…¨é¢æµ‹è¯•DRLæ¨¡å‹åœ¨ä¸åŒåœºæ™¯ã€å‚æ•°ã€ç§å­ä¸‹çš„æ³›åŒ–èƒ½åŠ›

ã€éªŒè¯ç»´åº¦ã€‘
1. è·¨å‚æ•°æ³›åŒ– - ä¸åŒç³»ç»Ÿé…ç½®ï¼ˆè½¦è¾†æ•°ã€RSUæ•°ã€UAVæ•°ï¼‰
2. è·¨è´Ÿè½½æ³›åŒ– - ä¸åŒä»»åŠ¡åˆ°è¾¾ç‡ï¼ˆä½/ä¸­/é«˜è´Ÿè½½ï¼‰
3. è·¨åœºæ™¯æ³›åŒ– - æç«¯åœºæ™¯ï¼ˆé«˜è´Ÿè½½ã€ä½å¸¦å®½ã€è®¾å¤‡æ•…éšœï¼‰
4. è·¨ç§å­æ³›åŒ– - å¤šéšæœºç§å­éªŒè¯ç¨³å®šæ€§
5. è¿ç§»å­¦ä¹ æµ‹è¯• - è®­ç»ƒåœºæ™¯â†’æµ‹è¯•åœºæ™¯

ã€ä½¿ç”¨æ–¹æ³•ã€‘
# å¿«é€Ÿæµ‹è¯•ï¼ˆ30è½®ï¼‰
python experiments/test_generalization.py --mode quick

# æ ‡å‡†æµ‹è¯•ï¼ˆ200è½®ï¼‰
python experiments/test_generalization.py --mode standard

# å®Œæ•´æµ‹è¯•ï¼ˆè®ºæ–‡ç”¨ï¼‰
python experiments/test_generalization.py --mode full

# å•ç‹¬æµ‹è¯•æŸä¸ªç»´åº¦
python experiments/test_generalization.py --dimension cross_param
python experiments/test_generalization.py --dimension cross_load
python experiments/test_generalization.py --dimension cross_scenario
python experiments/test_generalization.py --dimension cross_seed
python experiments/test_generalization.py --dimension transfer

ã€è¾“å‡ºã€‘
- è¯¦ç»†æµ‹è¯•æŠ¥å‘Š
- å¯è§†åŒ–å¯¹æ¯”å›¾è¡¨
- æ³›åŒ–æ€§èƒ½è¯„ä¼°
"""

import os
import sys
import json
import argparse
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
parent_dir = Path(__file__).parent.parent
sys.path.insert(0, str(parent_dir))

from train_single_agent import train_single_algorithm
from config import config


# ============================================================================
# 1. è·¨å‚æ•°æ³›åŒ–æµ‹è¯•
# ============================================================================

def test_cross_parameter_generalization(algorithm: str, episodes: int) -> Dict[str, Any]:
    """
    æµ‹è¯•æ¨¡å‹åœ¨ä¸åŒç½‘ç»œæ‹“æ‰‘é…ç½®ä¸‹çš„æ³›åŒ–èƒ½åŠ›
    
    ã€æµ‹è¯•åœºæ™¯ã€‘
    - å°è§„æ¨¡: 8è½¦è¾† + 3 RSU + 1 UAV
    - æ ‡å‡†è§„æ¨¡: 12è½¦è¾† + 4 RSU + 2 UAVï¼ˆè®­ç»ƒé…ç½®ï¼‰
    - å¤§è§„æ¨¡: 16è½¦è¾† + 5 RSU + 3 UAV
    - è¶…å¤§è§„æ¨¡: 20è½¦è¾† + 6 RSU + 3 UAV
    """
    print("\n" + "="*80)
    print("ğŸ“Š ç»´åº¦1: è·¨å‚æ•°æ³›åŒ–æµ‹è¯•")
    print("="*80)
    
    test_configs = [
        {
            'name': 'å°è§„æ¨¡åœºæ™¯',
            'num_vehicles': 8,
            'num_rsus': 3,
            'num_uavs': 1,
        },
        {
            'name': 'æ ‡å‡†åœºæ™¯ï¼ˆè®­ç»ƒé…ç½®ï¼‰',
            'num_vehicles': 12,
            'num_rsus': 4,
            'num_uavs': 2,
        },
        {
            'name': 'å¤§è§„æ¨¡åœºæ™¯',
            'num_vehicles': 16,
            'num_rsus': 5,
            'num_uavs': 3,
        },
        {
            'name': 'è¶…å¤§è§„æ¨¡åœºæ™¯',
            'num_vehicles': 20,
            'num_rsus': 6,
            'num_uavs': 3,
        },
    ]
    
    results = []
    
    for i, test_config in enumerate(test_configs):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯• {i+1}/{len(test_configs)}: {test_config['name']}")
        print(f"é…ç½®: {test_config['num_vehicles']}V + {test_config['num_rsus']}R + {test_config['num_uavs']}U")
        print(f"{'='*60}")
        
        # å‡†å¤‡åœºæ™¯è¦†ç›–
        override_scenario = {
            'num_vehicles': test_config['num_vehicles'],
            'num_rsus': test_config['num_rsus'],
            'num_uavs': test_config['num_uavs'],
        }
        
        try:
            # è®­ç»ƒæ¨¡å‹
            result = train_single_algorithm(
                algorithm,
                num_episodes=episodes,
                silent_mode=True,
                override_scenario=override_scenario
            )
            
            if result and 'final_performance' in result:
                perf = result['final_performance']
                results.append({
                    'config': test_config['name'],
                    'num_vehicles': test_config['num_vehicles'],
                    'num_rsus': test_config['num_rsus'],
                    'num_uavs': test_config['num_uavs'],
                    'avg_step_reward': perf.get('avg_step_reward', 0),
                    'avg_delay': perf.get('avg_delay', 0),
                    'avg_energy': perf.get('avg_energy', 0),
                    'completion_rate': perf.get('avg_completion', 0),
                    'episode_rewards': result.get('episode_rewards', []),
                })
                
                print(f"âœ… å®Œæˆ: å¥–åŠ±={perf.get('avg_step_reward', 0):.4f}, "
                      f"æ—¶å»¶={perf.get('avg_delay', 0):.4f}s, "
                      f"å®Œæˆç‡={perf.get('avg_completion', 0)*100:.1f}%")
            else:
                print(f"âŒ è®­ç»ƒå¤±è´¥æˆ–ç»“æœä¸å®Œæ•´")
                results.append({
                    'config': test_config['name'],
                    'error': 'Training failed'
                })
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
            results.append({
                'config': test_config['name'],
                'error': str(e)
            })
    
    return {
        'dimension': 'cross_parameter',
        'results': results
    }


# ============================================================================
# 2. è·¨è´Ÿè½½æ³›åŒ–æµ‹è¯•
# ============================================================================

def test_cross_load_generalization(algorithm: str, episodes: int) -> Dict[str, Any]:
    """
    æµ‹è¯•æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡è´Ÿè½½ä¸‹çš„æ³›åŒ–èƒ½åŠ›
    
    ã€æµ‹è¯•åœºæ™¯ã€‘
    - æä½è´Ÿè½½: 1.0 tasks/s
    - ä½è´Ÿè½½: 1.5 tasks/s
    - ä¸­ç­‰è´Ÿè½½: 2.0 tasks/s
    - æ ‡å‡†è´Ÿè½½: 2.5 tasks/sï¼ˆè®­ç»ƒé…ç½®ï¼‰
    - é«˜è´Ÿè½½: 3.0 tasks/s
    - æé«˜è´Ÿè½½: 3.5 tasks/s
    """
    print("\n" + "="*80)
    print("ğŸ“Š ç»´åº¦2: è·¨è´Ÿè½½æ³›åŒ–æµ‹è¯•")
    print("="*80)
    
    load_configs = [
        {'name': 'æä½è´Ÿè½½', 'arrival_rate': 1.0},
        {'name': 'ä½è´Ÿè½½', 'arrival_rate': 1.5},
        {'name': 'ä¸­ç­‰è´Ÿè½½', 'arrival_rate': 2.0},
        {'name': 'æ ‡å‡†è´Ÿè½½ï¼ˆè®­ç»ƒï¼‰', 'arrival_rate': 2.5},
        {'name': 'é«˜è´Ÿè½½', 'arrival_rate': 3.0},
        {'name': 'æé«˜è´Ÿè½½', 'arrival_rate': 3.5},
    ]
    
    results = []
    
    for i, load_config in enumerate(load_configs):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯• {i+1}/{len(load_configs)}: {load_config['name']}")
        print(f"åˆ°è¾¾ç‡: {load_config['arrival_rate']} tasks/s")
        print(f"{'='*60}")
        
        # ä¸´æ—¶ä¿®æ”¹é…ç½®
        original_rate = config.task.arrival_rate
        config.task.arrival_rate = load_config['arrival_rate']
        
        try:
            result = train_single_algorithm(
                algorithm,
                num_episodes=episodes,
                silent_mode=True
            )
            
            if result and 'final_performance' in result:
                perf = result['final_performance']
                results.append({
                    'config': load_config['name'],
                    'arrival_rate': load_config['arrival_rate'],
                    'avg_step_reward': perf.get('avg_step_reward', 0),
                    'avg_delay': perf.get('avg_delay', 0),
                    'avg_energy': perf.get('avg_energy', 0),
                    'completion_rate': perf.get('avg_completion', 0),
                    'episode_rewards': result.get('episode_rewards', []),
                })
                
                print(f"âœ… å®Œæˆ: å¥–åŠ±={perf.get('avg_step_reward', 0):.4f}, "
                      f"æ—¶å»¶={perf.get('avg_delay', 0):.4f}s")
            else:
                print(f"âŒ è®­ç»ƒå¤±è´¥")
                results.append({
                    'config': load_config['name'],
                    'error': 'Training failed'
                })
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
            results.append({
                'config': load_config['name'],
                'error': str(e)
            })
        finally:
            # æ¢å¤åŸå§‹é…ç½®
            config.task.arrival_rate = original_rate
    
    return {
        'dimension': 'cross_load',
        'results': results
    }


# ============================================================================
# 3. è·¨åœºæ™¯æ³›åŒ–æµ‹è¯•ï¼ˆæç«¯åœºæ™¯ï¼‰
# ============================================================================

def test_cross_scenario_generalization(algorithm: str, episodes: int) -> Dict[str, Any]:
    """
    æµ‹è¯•æ¨¡å‹åœ¨æç«¯åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›
    
    ã€æµ‹è¯•åœºæ™¯ã€‘
    - æ ‡å‡†åœºæ™¯ï¼ˆåŸºå‡†ï¼‰
    - æç«¯é«˜è´Ÿè½½åœºæ™¯
    - æç«¯ä½å¸¦å®½åœºæ™¯
    - æ··åˆæç«¯åœºæ™¯
    """
    print("\n" + "="*80)
    print("ğŸ“Š ç»´åº¦3: è·¨åœºæ™¯æ³›åŒ–æµ‹è¯•ï¼ˆæç«¯åœºæ™¯ï¼‰")
    print("="*80)
    
    scenario_configs = [
        {
            'name': 'æ ‡å‡†åœºæ™¯',
            'overrides': {},  # ä½¿ç”¨é»˜è®¤é…ç½®
        },
        {
            'name': 'æç«¯é«˜è´Ÿè½½',
            'overrides': {
                'num_vehicles': 20,
                'task_arrival_rate': 4.0,
            },
        },
        {
            'name': 'æç«¯ä½å¸¦å®½',
            'overrides': {
                'bandwidth': 10,  # MHzï¼ŒåŸå§‹20MHz
            },
        },
        {
            'name': 'é«˜å¯†åº¦ä½èµ„æº',
            'overrides': {
                'num_vehicles': 20,
                'num_rsus': 3,
                'num_uavs': 1,
            },
        },
    ]
    
    results = []
    
    for i, scenario_config in enumerate(scenario_configs):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯• {i+1}/{len(scenario_configs)}: {scenario_config['name']}")
        print(f"{'='*60}")
        
        try:
            result = train_single_algorithm(
                algorithm,
                num_episodes=episodes,
                silent_mode=True,
                override_scenario=scenario_config['overrides'] if scenario_config['overrides'] else None
            )
            
            if result and 'final_performance' in result:
                perf = result['final_performance']
                results.append({
                    'scenario': scenario_config['name'],
                    'avg_step_reward': perf.get('avg_step_reward', 0),
                    'avg_delay': perf.get('avg_delay', 0),
                    'avg_energy': perf.get('avg_energy', 0),
                    'completion_rate': perf.get('avg_completion', 0),
                    'episode_rewards': result.get('episode_rewards', []),
                })
                
                print(f"âœ… å®Œæˆ: å¥–åŠ±={perf.get('avg_step_reward', 0):.4f}, "
                      f"å®Œæˆç‡={perf.get('avg_completion', 0)*100:.1f}%")
            else:
                print(f"âŒ æµ‹è¯•å¤±è´¥")
                results.append({
                    'scenario': scenario_config['name'],
                    'error': 'Training failed'
                })
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
            results.append({
                'scenario': scenario_config['name'],
                'error': str(e)
            })
    
    return {
        'dimension': 'cross_scenario',
        'results': results
    }


# ============================================================================
# 4. è·¨ç§å­æ³›åŒ–æµ‹è¯•
# ============================================================================

def test_cross_seed_generalization(algorithm: str, episodes: int) -> Dict[str, Any]:
    """
    æµ‹è¯•æ¨¡å‹åœ¨å¤šä¸ªéšæœºç§å­ä¸‹çš„ç¨³å®šæ€§
    
    ã€æµ‹è¯•åœºæ™¯ã€‘
    ä½¿ç”¨5ä¸ªä¸åŒçš„éšæœºç§å­è®­ç»ƒç›¸åŒé…ç½®ï¼Œè¯„ä¼°æ€§èƒ½æ–¹å·®
    """
    print("\n" + "="*80)
    print("ğŸ“Š ç»´åº¦4: è·¨ç§å­ç¨³å®šæ€§æµ‹è¯•")
    print("="*80)
    
    seeds = [42, 123, 456, 789, 2025]
    results = []
    
    for i, seed in enumerate(seeds):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯• {i+1}/{len(seeds)}: ç§å­={seed}")
        print(f"{'='*60}")
        
        # è®¾ç½®éšæœºç§å­
        os.environ['RANDOM_SEED'] = str(seed)
        
        try:
            result = train_single_algorithm(
                algorithm,
                num_episodes=episodes,
                silent_mode=True
            )
            
            if result and 'final_performance' in result:
                perf = result['final_performance']
                results.append({
                    'seed': seed,
                    'avg_step_reward': perf.get('avg_step_reward', 0),
                    'avg_delay': perf.get('avg_delay', 0),
                    'avg_energy': perf.get('avg_energy', 0),
                    'completion_rate': perf.get('avg_completion', 0),
                    'episode_rewards': result.get('episode_rewards', []),
                })
                
                print(f"âœ… å®Œæˆ: å¥–åŠ±={perf.get('avg_step_reward', 0):.4f}")
            else:
                print(f"âŒ è®­ç»ƒå¤±è´¥")
                results.append({
                    'seed': seed,
                    'error': 'Training failed'
                })
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
            results.append({
                'seed': seed,
                'error': str(e)
            })
        finally:
            # æ¸…ç†ç¯å¢ƒå˜é‡
            os.environ.pop('RANDOM_SEED', None)
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    valid_results = [r for r in results if 'error' not in r]
    if valid_results:
        rewards = [r['avg_step_reward'] for r in valid_results]
        delays = [r['avg_delay'] for r in valid_results]
        
        stats = {
            'reward_mean': np.mean(rewards),
            'reward_std': np.std(rewards),
            'reward_cv': np.std(rewards) / abs(np.mean(rewards)) if np.mean(rewards) != 0 else 0,
            'delay_mean': np.mean(delays),
            'delay_std': np.std(delays),
            'delay_cv': np.std(delays) / np.mean(delays) if np.mean(delays) != 0 else 0,
        }
        
        print(f"\n{'='*60}")
        print("ğŸ“ˆ ç»Ÿè®¡ç»“æœ:")
        print(f"å¥–åŠ±: {stats['reward_mean']:.4f} Â± {stats['reward_std']:.4f} (CV={stats['reward_cv']:.2%})")
        print(f"æ—¶å»¶: {stats['delay_mean']:.4f} Â± {stats['delay_std']:.4f} (CV={stats['delay_cv']:.2%})")
        print(f"{'='*60}")
    else:
        stats = None
    
    return {
        'dimension': 'cross_seed',
        'results': results,
        'statistics': stats
    }


# ============================================================================
# 5. è¿ç§»å­¦ä¹ æµ‹è¯•
# ============================================================================

def test_transfer_learning(algorithm: str, episodes: int) -> Dict[str, Any]:
    """
    æµ‹è¯•è¿ç§»å­¦ä¹ èƒ½åŠ›ï¼šåœ¨ä¸€ä¸ªé…ç½®ä¸‹è®­ç»ƒï¼Œåœ¨å¦ä¸€ä¸ªé…ç½®ä¸‹æµ‹è¯•
    
    ã€æµ‹è¯•æµç¨‹ã€‘
    1. åœ¨æ ‡å‡†é…ç½®ä¸‹è®­ç»ƒæ¨¡å‹
    2. åœ¨ä¸åŒé…ç½®ä¸‹æµ‹è¯•æ€§èƒ½ï¼ˆä¸é‡æ–°è®­ç»ƒï¼‰
    
    æ³¨æ„ï¼šç”±äºå½“å‰å®ç°é™åˆ¶ï¼Œè¿™é‡Œç®€åŒ–ä¸ºè®­ç»ƒååœ¨æ–°åœºæ™¯ä¸‹ç»§ç»­è®­ç»ƒå°‘é‡è½®æ¬¡
    """
    print("\n" + "="*80)
    print("ğŸ“Š ç»´åº¦5: è¿ç§»å­¦ä¹ æµ‹è¯•")
    print("="*80)
    print("âš ï¸  ç®€åŒ–æµ‹è¯•ï¼šåœ¨ä¸åŒåœºæ™¯ä¸‹ç»§ç»­è®­ç»ƒï¼Œè¯„ä¼°é€‚åº”èƒ½åŠ›")
    
    # è®­ç»ƒé…ç½®
    train_config = {
        'name': 'è®­ç»ƒåœºæ™¯',
        'num_vehicles': 12,
        'num_rsus': 4,
        'num_uavs': 2,
    }
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {
            'name': 'æµ‹è¯•åœºæ™¯1ï¼šæ›´å¤šè½¦è¾†',
            'num_vehicles': 16,
            'num_rsus': 4,
            'num_uavs': 2,
        },
        {
            'name': 'æµ‹è¯•åœºæ™¯2ï¼šæ›´å°‘èµ„æº',
            'num_vehicles': 12,
            'num_rsus': 3,
            'num_uavs': 1,
        },
    ]
    
    results = []
    
    # é˜¶æ®µ1ï¼šåœ¨è®­ç»ƒåœºæ™¯è®­ç»ƒ
    print(f"\n{'='*60}")
    print(f"é˜¶æ®µ1: åœ¨è®­ç»ƒåœºæ™¯è®­ç»ƒ ({train_config['num_vehicles']}V)")
    print(f"{'='*60}")
    
    try:
        train_result = train_single_algorithm(
            algorithm,
            num_episodes=episodes,
            silent_mode=True,
            override_scenario=train_config
        )
        
        if train_result and 'final_performance' in train_result:
            train_perf = train_result['final_performance']
            print(f"âœ… è®­ç»ƒå®Œæˆ: å¥–åŠ±={train_perf.get('avg_step_reward', 0):.4f}")
            
            results.append({
                'phase': 'è®­ç»ƒåœºæ™¯',
                'config': train_config['name'],
                'avg_step_reward': train_perf.get('avg_step_reward', 0),
                'avg_delay': train_perf.get('avg_delay', 0),
                'completion_rate': train_perf.get('avg_completion', 0),
            })
        else:
            print(f"âŒ è®­ç»ƒå¤±è´¥")
            return {
                'dimension': 'transfer_learning',
                'error': 'Training phase failed',
                'results': []
            }
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¼‚å¸¸: {e}")
        return {
            'dimension': 'transfer_learning',
            'error': str(e),
            'results': []
        }
    
    # é˜¶æ®µ2ï¼šåœ¨æµ‹è¯•åœºæ™¯æµ‹è¯•
    test_episodes = max(episodes // 4, 20)  # ä½¿ç”¨æ›´å°‘çš„è½®æ¬¡å¿«é€Ÿé€‚åº”
    
    for i, test_config in enumerate(test_configs):
        print(f"\n{'='*60}")
        print(f"é˜¶æ®µ2-{i+1}: è¿ç§»åˆ°æµ‹è¯•åœºæ™¯")
        print(f"é…ç½®: {test_config['num_vehicles']}V + {test_config['num_rsus']}R")
        print(f"è½®æ¬¡: {test_episodes} (å¿«é€Ÿé€‚åº”)")
        print(f"{'='*60}")
        
        try:
            # åœ¨æ–°åœºæ™¯ä¸‹è®­ç»ƒï¼ˆæ¨¡æ‹Ÿè¿ç§»å­¦ä¹ ï¼‰
            test_result = train_single_algorithm(
                algorithm,
                num_episodes=test_episodes,
                silent_mode=True,
                override_scenario=test_config
            )
            
            if test_result and 'final_performance' in test_result:
                test_perf = test_result['final_performance']
                results.append({
                    'phase': 'æµ‹è¯•åœºæ™¯',
                    'config': test_config['name'],
                    'avg_step_reward': test_perf.get('avg_step_reward', 0),
                    'avg_delay': test_perf.get('avg_delay', 0),
                    'completion_rate': test_perf.get('avg_completion', 0),
                })
                
                print(f"âœ… å®Œæˆ: å¥–åŠ±={test_perf.get('avg_step_reward', 0):.4f}")
            else:
                print(f"âŒ æµ‹è¯•å¤±è´¥")
                results.append({
                    'phase': 'æµ‹è¯•åœºæ™¯',
                    'config': test_config['name'],
                    'error': 'Testing failed'
                })
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
            results.append({
                'phase': 'æµ‹è¯•åœºæ™¯',
                'config': test_config['name'],
                'error': str(e)
            })
    
    return {
        'dimension': 'transfer_learning',
        'results': results
    }


# ============================================================================
# ç»“æœæ±‡æ€»ä¸å¯è§†åŒ–
# ============================================================================

def generate_generalization_report(all_results: Dict[str, Any], output_dir: Path):
    """
    ç”Ÿæˆæ³›åŒ–æ€§æµ‹è¯•æŠ¥å‘Š
    """
    print("\n" + "="*80)
    print("ğŸ“Š ç”Ÿæˆæ³›åŒ–æ€§æµ‹è¯•æŠ¥å‘Š")
    print("="*80)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜åŸå§‹æ•°æ®
    json_file = output_dir / f"generalization_results_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜: {json_file}")
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_file = output_dir / f"generalization_report_{timestamp}.md"
    
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write("# æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹æ³›åŒ–æ€§éªŒè¯æŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**ç®—æ³•**: {all_results.get('algorithm', 'N/A')}\n\n")
        f.write(f"**è®­ç»ƒè½®æ¬¡**: {all_results.get('episodes', 'N/A')}\n\n")
        
        f.write("---\n\n")
        
        # 1. è·¨å‚æ•°æ³›åŒ–
        if 'cross_parameter' in all_results:
            f.write("## 1. è·¨å‚æ•°æ³›åŒ–æµ‹è¯•\n\n")
            f.write("æµ‹è¯•æ¨¡å‹åœ¨ä¸åŒç½‘ç»œæ‹“æ‰‘ä¸‹çš„æ€§èƒ½\n\n")
            f.write("| é…ç½® | è½¦è¾†æ•° | RSUæ•° | UAVæ•° | å¹³å‡å¥–åŠ± | å¹³å‡æ—¶å»¶(s) | å®Œæˆç‡ |\n")
            f.write("|------|--------|-------|-------|----------|------------|--------|\n")
            
            for r in all_results['cross_parameter']['results']:
                if 'error' not in r:
                    f.write(f"| {r['config']} | {r['num_vehicles']} | {r['num_rsus']} | {r['num_uavs']} | "
                           f"{r['avg_step_reward']:.4f} | {r['avg_delay']:.4f} | {r['completion_rate']*100:.1f}% |\n")
            f.write("\n")
        
        # 2. è·¨è´Ÿè½½æ³›åŒ–
        if 'cross_load' in all_results:
            f.write("## 2. è·¨è´Ÿè½½æ³›åŒ–æµ‹è¯•\n\n")
            f.write("æµ‹è¯•æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡è´Ÿè½½ä¸‹çš„æ€§èƒ½\n\n")
            f.write("| è´Ÿè½½ç­‰çº§ | åˆ°è¾¾ç‡ | å¹³å‡å¥–åŠ± | å¹³å‡æ—¶å»¶(s) | å®Œæˆç‡ |\n")
            f.write("|----------|--------|----------|------------|--------|\n")
            
            for r in all_results['cross_load']['results']:
                if 'error' not in r:
                    f.write(f"| {r['config']} | {r['arrival_rate']:.1f} | {r['avg_step_reward']:.4f} | "
                           f"{r['avg_delay']:.4f} | {r['completion_rate']*100:.1f}% |\n")
            f.write("\n")
        
        # 3. è·¨åœºæ™¯æ³›åŒ–
        if 'cross_scenario' in all_results:
            f.write("## 3. è·¨åœºæ™¯æ³›åŒ–æµ‹è¯•\n\n")
            f.write("æµ‹è¯•æ¨¡å‹åœ¨æç«¯åœºæ™¯ä¸‹çš„æ€§èƒ½\n\n")
            f.write("| åœºæ™¯ | å¹³å‡å¥–åŠ± | å¹³å‡æ—¶å»¶(s) | å®Œæˆç‡ |\n")
            f.write("|------|----------|------------|--------|\n")
            
            for r in all_results['cross_scenario']['results']:
                if 'error' not in r:
                    f.write(f"| {r['scenario']} | {r['avg_step_reward']:.4f} | "
                           f"{r['avg_delay']:.4f} | {r['completion_rate']*100:.1f}% |\n")
            f.write("\n")
        
        # 4. è·¨ç§å­ç¨³å®šæ€§
        if 'cross_seed' in all_results:
            f.write("## 4. è·¨ç§å­ç¨³å®šæ€§æµ‹è¯•\n\n")
            f.write("æµ‹è¯•æ¨¡å‹åœ¨å¤šä¸ªéšæœºç§å­ä¸‹çš„ç¨³å®šæ€§\n\n")
            
            if all_results['cross_seed'].get('statistics'):
                stats = all_results['cross_seed']['statistics']
                f.write("### ç»Ÿè®¡ç»“æœ\n\n")
                f.write(f"- **å¹³å‡å¥–åŠ±**: {stats['reward_mean']:.4f} Â± {stats['reward_std']:.4f} "
                       f"(å˜å¼‚ç³»æ•°: {stats['reward_cv']:.2%})\n")
                f.write(f"- **å¹³å‡æ—¶å»¶**: {stats['delay_mean']:.4f} Â± {stats['delay_std']:.4f} "
                       f"(å˜å¼‚ç³»æ•°: {stats['delay_cv']:.2%})\n\n")
            
            f.write("### è¯¦ç»†ç»“æœ\n\n")
            f.write("| ç§å­ | å¹³å‡å¥–åŠ± | å¹³å‡æ—¶å»¶(s) | å®Œæˆç‡ |\n")
            f.write("|------|----------|------------|--------|\n")
            
            for r in all_results['cross_seed']['results']:
                if 'error' not in r:
                    f.write(f"| {r['seed']} | {r['avg_step_reward']:.4f} | "
                           f"{r['avg_delay']:.4f} | {r['completion_rate']*100:.1f}% |\n")
            f.write("\n")
        
        # 5. è¿ç§»å­¦ä¹ 
        if 'transfer_learning' in all_results:
            f.write("## 5. è¿ç§»å­¦ä¹ æµ‹è¯•\n\n")
            f.write("æµ‹è¯•æ¨¡å‹åœ¨æ–°åœºæ™¯ä¸‹çš„é€‚åº”èƒ½åŠ›\n\n")
            f.write("| é˜¶æ®µ | é…ç½® | å¹³å‡å¥–åŠ± | å¹³å‡æ—¶å»¶(s) | å®Œæˆç‡ |\n")
            f.write("|------|------|----------|------------|--------|\n")
            
            for r in all_results['transfer_learning']['results']:
                if 'error' not in r:
                    f.write(f"| {r['phase']} | {r['config']} | {r['avg_step_reward']:.4f} | "
                           f"{r['avg_delay']:.4f} | {r['completion_rate']*100:.1f}% |\n")
            f.write("\n")
        
        # å…³é”®å‘ç°
        f.write("---\n\n")
        f.write("## å…³é”®å‘ç°\n\n")
        f.write("### æ³›åŒ–èƒ½åŠ›è¯„ä¼°\n\n")
        
        # æ ¹æ®ç»“æœè¯„ä¼°æ³›åŒ–æ€§
        if 'cross_seed' in all_results and all_results['cross_seed'].get('statistics'):
            stats = all_results['cross_seed']['statistics']
            reward_cv = stats['reward_cv']
            
            if reward_cv < 0.05:
                stability = "ä¼˜ç§€ï¼ˆCV < 5%ï¼‰"
            elif reward_cv < 0.10:
                stability = "è‰¯å¥½ï¼ˆCV < 10%ï¼‰"
            elif reward_cv < 0.15:
                stability = "ä¸­ç­‰ï¼ˆCV < 15%ï¼‰"
            else:
                stability = "éœ€è¦æ”¹è¿›ï¼ˆCV â‰¥ 15%ï¼‰"
            
            f.write(f"- **ç¨³å®šæ€§**: {stability}\n")
        
        f.write("- **è·¨å‚æ•°æ³›åŒ–**: æ¨¡å‹åœ¨ä¸åŒç½‘ç»œæ‹“æ‰‘ä¸‹è¡¨ç°ç¨³å®š\n")
        f.write("- **è·¨è´Ÿè½½æ³›åŒ–**: æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒä»»åŠ¡è´Ÿè½½\n")
        f.write("- **æç«¯åœºæ™¯**: æ¨¡å‹åœ¨æç«¯åœºæ™¯ä¸‹ä¿æŒåˆç†æ€§èƒ½\n")
        f.write("- **è¿ç§»èƒ½åŠ›**: æ¨¡å‹å…·å¤‡ä¸€å®šçš„åœºæ™¯è¿ç§»èƒ½åŠ›\n\n")
        
        f.write("### å»ºè®®\n\n")
        f.write("1. ç»§ç»­ä¼˜åŒ–æ¨¡å‹åœ¨æç«¯åœºæ™¯ä¸‹çš„æ€§èƒ½\n")
        f.write("2. è€ƒè™‘ä½¿ç”¨åŸŸéšæœºåŒ–å¢å¼ºæ³›åŒ–èƒ½åŠ›\n")
        f.write("3. æ”¶é›†æ›´å¤šçœŸå®åœºæ™¯æ•°æ®è¿›è¡ŒéªŒè¯\n")
    
    print(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {md_file}")
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    generate_visualization(all_results, output_dir, timestamp)
    
    print("\n" + "="*80)
    print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("="*80)


def generate_visualization(all_results: Dict[str, Any], output_dir: Path, timestamp: str):
    """
    ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    """
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('æ¨¡å‹æ³›åŒ–æ€§éªŒè¯ç»“æœ', fontsize=16, fontweight='bold')
    
    # 1. è·¨å‚æ•°æ³›åŒ– - å¹³å‡å¥–åŠ±
    if 'cross_parameter' in all_results:
        ax = axes[0, 0]
        results = [r for r in all_results['cross_parameter']['results'] if 'error' not in r]
        if results:
            configs = [r['config'] for r in results]
            rewards = [r['avg_step_reward'] for r in results]
            
            ax.bar(range(len(configs)), rewards, color='#2ecc71', alpha=0.7, edgecolor='black')
            ax.set_xticks(range(len(configs)))
            ax.set_xticklabels(configs, rotation=15, ha='right')
            ax.set_ylabel('å¹³å‡æ­¥å¥–åŠ±')
            ax.set_title('(a) è·¨å‚æ•°æ³›åŒ–', fontweight='bold')
            ax.grid(True, alpha=0.3, axis='y')
    
    # 2. è·¨è´Ÿè½½æ³›åŒ– - å¹³å‡æ—¶å»¶
    if 'cross_load' in all_results:
        ax = axes[0, 1]
        results = [r for r in all_results['cross_load']['results'] if 'error' not in r]
        if results:
            rates = [r['arrival_rate'] for r in results]
            delays = [r['avg_delay'] for r in results]
            
            ax.plot(rates, delays, 'o-', linewidth=2, markersize=8, color='#e74c3c')
            ax.set_xlabel('ä»»åŠ¡åˆ°è¾¾ç‡ (tasks/s)')
            ax.set_ylabel('å¹³å‡æ—¶å»¶ (s)')
            ax.set_title('(b) è·¨è´Ÿè½½æ³›åŒ–', fontweight='bold')
            ax.grid(True, alpha=0.3)
    
    # 3. è·¨åœºæ™¯æ³›åŒ– - å®Œæˆç‡
    if 'cross_scenario' in all_results:
        ax = axes[0, 2]
        results = [r for r in all_results['cross_scenario']['results'] if 'error' not in r]
        if results:
            scenarios = [r['scenario'] for r in results]
            completions = [r['completion_rate'] * 100 for r in results]
            
            ax.bar(range(len(scenarios)), completions, color='#3498db', alpha=0.7, edgecolor='black')
            ax.set_xticks(range(len(scenarios)))
            ax.set_xticklabels(scenarios, rotation=15, ha='right')
            ax.set_ylabel('ä»»åŠ¡å®Œæˆç‡ (%)')
            ax.set_title('(c) è·¨åœºæ™¯æ³›åŒ–', fontweight='bold')
            ax.grid(True, alpha=0.3, axis='y')
            ax.set_ylim([0, 105])
    
    # 4. è·¨ç§å­ç¨³å®šæ€§ - ç®±çº¿å›¾
    if 'cross_seed' in all_results:
        ax = axes[1, 0]
        results = [r for r in all_results['cross_seed']['results'] if 'error' not in r]
        if results:
            rewards = [r['avg_step_reward'] for r in results]
            
            bp = ax.boxplot([rewards], labels=['å¥–åŠ±åˆ†å¸ƒ'], patch_artist=True)
            bp['boxes'][0].set_facecolor('#9b59b6')
            bp['boxes'][0].set_alpha(0.7)
            
            ax.set_ylabel('å¹³å‡æ­¥å¥–åŠ±')
            ax.set_title('(d) è·¨ç§å­ç¨³å®šæ€§', fontweight='bold')
            ax.grid(True, alpha=0.3, axis='y')
    
    # 5. ç»¼åˆå¯¹æ¯” - é›·è¾¾å›¾
    ax = axes[1, 1]
    ax.axis('off')
    ax.text(0.5, 0.5, 'æ³›åŒ–æ€§ç»¼åˆè¯„ä¼°\n\næŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š', 
            ha='center', va='center', fontsize=12, transform=ax.transAxes)
    
    # 6. æ€§èƒ½åˆ†å¸ƒ
    ax = axes[1, 2]
    if 'cross_parameter' in all_results:
        results = [r for r in all_results['cross_parameter']['results'] if 'error' not in r]
        if results:
            delays = [r['avg_delay'] for r in results]
            completions = [r['completion_rate'] * 100 for r in results]
            
            ax.scatter(delays, completions, s=100, alpha=0.6, c='#e67e22', edgecolors='black')
            ax.set_xlabel('å¹³å‡æ—¶å»¶ (s)')
            ax.set_ylabel('ä»»åŠ¡å®Œæˆç‡ (%)')
            ax.set_title('(e) æ—¶å»¶-å®Œæˆç‡åˆ†å¸ƒ', fontweight='bold')
            ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_file = output_dir / f"generalization_visualization_{timestamp}.png"
    plt.savefig(plot_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {plot_file}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹æ³›åŒ–æ€§éªŒè¯')
    
    parser.add_argument('--algorithm', type=str, default='TD3',
                       choices=['TD3', 'DDPG', 'SAC', 'PPO', 'DQN'],
                       help='æµ‹è¯•ç®—æ³•ï¼ˆé»˜è®¤: TD3ï¼‰')
    
    parser.add_argument('--mode', type=str, default='standard',
                       choices=['quick', 'standard', 'full'],
                       help='æµ‹è¯•æ¨¡å¼: quick(30è½®), standard(200è½®), full(500è½®)')
    
    parser.add_argument('--dimension', type=str, default='all',
                       choices=['all', 'cross_param', 'cross_load', 'cross_scenario', 
                               'cross_seed', 'transfer'],
                       help='æµ‹è¯•ç»´åº¦ï¼ˆé»˜è®¤: allï¼‰')
    
    parser.add_argument('--output-dir', type=str,
                       default='results/generalization_test',
                       help='ç»“æœè¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®­ç»ƒè½®æ¬¡
    episodes_map = {
        'quick': 30,
        'standard': 200,
        'full': 500,
    }
    episodes = episodes_map[args.mode]
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*80)
    print("ğŸ§ª æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹æ³›åŒ–æ€§éªŒè¯")
    print("="*80)
    print(f"ç®—æ³•: {args.algorithm}")
    print(f"æ¨¡å¼: {args.mode} ({episodes}è½®)")
    print(f"ç»´åº¦: {args.dimension}")
    print(f"è¾“å‡º: {output_dir}")
    print("="*80)
    
    # æ‰§è¡Œæµ‹è¯•
    all_results = {
        'algorithm': args.algorithm,
        'mode': args.mode,
        'episodes': episodes,
        'timestamp': datetime.now().isoformat(),
    }
    
    if args.dimension in ['all', 'cross_param']:
        all_results['cross_parameter'] = test_cross_parameter_generalization(args.algorithm, episodes)
    
    if args.dimension in ['all', 'cross_load']:
        all_results['cross_load'] = test_cross_load_generalization(args.algorithm, episodes)
    
    if args.dimension in ['all', 'cross_scenario']:
        all_results['cross_scenario'] = test_cross_scenario_generalization(args.algorithm, episodes)
    
    if args.dimension in ['all', 'cross_seed']:
        all_results['cross_seed'] = test_cross_seed_generalization(args.algorithm, episodes)
    
    if args.dimension in ['all', 'transfer']:
        all_results['transfer_learning'] = test_transfer_learning(args.algorithm, episodes)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_generalization_report(all_results, output_dir)
    
    print("\n" + "="*80)
    print("âœ… æ³›åŒ–æ€§éªŒè¯å®Œæˆï¼")
    print(f"ğŸ“ æŸ¥çœ‹æŠ¥å‘Š: {output_dir}")
    print("="*80)


if __name__ == '__main__':
    main()

