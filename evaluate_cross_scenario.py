#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ è·¨åœºæ™¯è¯„ä¼°å·¥å…· - Cross-Scenario Evaluation Tool

ã€åŠŸèƒ½ã€‘
ä½¿ç”¨è®­ç»ƒå¥½çš„agentåœ¨ä¸åŒåœºæ™¯ä¸‹è¿›è¡Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
1. è½¦è¾†å¯†åº¦å˜åŒ–ï¼ˆ8/12/16/20/24è¾†ï¼‰
2. ä»»åŠ¡åˆ°è¾¾ç‡å˜åŒ–ï¼ˆä½/ä¸­/é«˜è´Ÿè½½ï¼‰
3. ä»»åŠ¡ç±»å‹åˆ†å¸ƒå˜åŒ–ï¼ˆç´§æ€¥ä»»åŠ¡æ¯”ä¾‹ï¼‰
4. ç½‘ç»œæ¡ä»¶å˜åŒ–ï¼ˆä¿¡é“è´¨é‡ï¼‰
5. RSU/UAVæ•°é‡å˜åŒ–

ã€ä½¿ç”¨æ–¹æ³•ã€‘
# 1. è¯„ä¼°å•ä¸ªæ¨¡å‹åœ¨å¤šåœºæ™¯ä¸‹
python evaluate_cross_scenario.py --model results/models/single_agent/td3/best_model_td3.pth --algorithm TD3

# 2. è¯„ä¼°å¤šä¸ªç®—æ³•å¯¹æ¯”
python evaluate_cross_scenario.py --compare --algorithms TD3 SAC DDPG --scenario-set all

# 3. è‡ªå®šä¹‰åœºæ™¯
python evaluate_cross_scenario.py --model results/models/single_agent/td3/best_model_td3.pth \
    --algorithm TD3 --num-vehicles 20 --arrival-rate 3.5 --eval-episodes 20

# 4. æ³›åŒ–èƒ½åŠ›åˆ†æï¼ˆè®­ç»ƒåœºæ™¯ vs æµ‹è¯•åœºæ™¯ï¼‰
python evaluate_cross_scenario.py --model results/models/single_agent/td3/best_model_td3.pth \
    --algorithm TD3 --generalization-test

ã€è¾“å‡ºã€‘
- å„åœºæ™¯ä¸‹çš„æ€§èƒ½æŒ‡æ ‡ï¼ˆæ—¶å»¶ã€èƒ½è€—ã€å®Œæˆç‡ï¼‰
- æ³›åŒ–èƒ½åŠ›åˆ†æå›¾è¡¨
- æ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾
- è¯¦ç»†çš„JSONç»“æœæ–‡ä»¶
"""

import os
import sys
import json
import time
import argparse
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
from config.system_config import config
from evaluation.system_simulator import CompleteSystemSimulator


# ========== åœºæ™¯å®šä¹‰ ==========

class ScenarioConfig:
    """åœºæ™¯é…ç½®ç±»"""
    
    def __init__(self, name: str, description: str, params: Dict[str, Any]):
        self.name = name
        self.description = description
        self.params = params
    
    def apply_to_config(self):
        """åº”ç”¨åœºæ™¯å‚æ•°åˆ°å…¨å±€é…ç½®"""
        for key, value in self.params.items():
            if '.' in key:
                # å¤„ç†åµŒå¥—å±æ€§ï¼Œå¦‚ 'rl.reward_weight_delay'
                parts = key.split('.')
                obj = config
                for part in parts[:-1]:
                    obj = getattr(obj, part)
                setattr(obj, parts[-1], value)
            else:
                setattr(config, key, value)


# é¢„å®šä¹‰åœºæ™¯é›†
SCENARIO_SETS = {
    # ========== 1. è½¦è¾†å¯†åº¦å˜åŒ– ==========
    "vehicle_density": [
        ScenarioConfig(
            name="low_density",
            description="ä½å¯†åº¦åœºæ™¯ï¼ˆ8è¾†è½¦ï¼‰",
            params={'num_vehicles': 8, 'task_arrival_rate': 2.0}
        ),
        ScenarioConfig(
            name="medium_density", 
            description="ä¸­å¯†åº¦åœºæ™¯ï¼ˆ12è¾†è½¦ï¼Œè®­ç»ƒåœºæ™¯ï¼‰",
            params={'num_vehicles': 12, 'task_arrival_rate': 2.5}
        ),
        ScenarioConfig(
            name="high_density",
            description="é«˜å¯†åº¦åœºæ™¯ï¼ˆ16è¾†è½¦ï¼‰",
            params={'num_vehicles': 16, 'task_arrival_rate': 3.0}
        ),
        ScenarioConfig(
            name="extreme_density",
            description="æé«˜å¯†åº¦åœºæ™¯ï¼ˆ24è¾†è½¦ï¼‰",
            params={'num_vehicles': 24, 'task_arrival_rate': 4.0}
        ),
    ],
    
    # ========== 2. ä»»åŠ¡è´Ÿè½½å˜åŒ– ==========
    "task_load": [
        ScenarioConfig(
            name="light_load",
            description="è½»è´Ÿè½½ï¼ˆåˆ°è¾¾ç‡1.5 tasks/sï¼‰",
            params={'task_arrival_rate': 1.5}
        ),
        ScenarioConfig(
            name="normal_load",
            description="æ­£å¸¸è´Ÿè½½ï¼ˆåˆ°è¾¾ç‡2.5 tasks/sï¼Œè®­ç»ƒåœºæ™¯ï¼‰",
            params={'task_arrival_rate': 2.5}
        ),
        ScenarioConfig(
            name="heavy_load",
            description="é‡è´Ÿè½½ï¼ˆåˆ°è¾¾ç‡3.5 tasks/sï¼‰",
            params={'task_arrival_rate': 3.5}
        ),
        ScenarioConfig(
            name="extreme_load",
            description="æç«¯è´Ÿè½½ï¼ˆåˆ°è¾¾ç‡5.0 tasks/sï¼‰",
            params={'task_arrival_rate': 5.0}
        ),
    ],
    
    # ========== 3. åŸºç¡€è®¾æ–½å˜åŒ– ==========
    "infrastructure": [
        ScenarioConfig(
            name="limited_rsu",
            description="æœ‰é™RSUï¼ˆ4ä¸ªï¼‰",
            params={'num_rsus': 4}
        ),
        ScenarioConfig(
            name="standard_rsu",
            description="æ ‡å‡†RSUï¼ˆ6ä¸ªï¼Œè®­ç»ƒåœºæ™¯ï¼‰",
            params={'num_rsus': 6}
        ),
        ScenarioConfig(
            name="abundant_rsu",
            description="å……è¶³RSUï¼ˆ8ä¸ªï¼‰",
            params={'num_rsus': 8}
        ),
        ScenarioConfig(
            name="with_uav",
            description="å¢åŠ UAVæ”¯æŒï¼ˆ2ä¸ªUAVï¼‰",
            params={'num_uavs': 2}
        ),
    ],
    
    # ========== 4. ç½‘ç»œæ¡ä»¶å˜åŒ– ==========
    "network_condition": [
        ScenarioConfig(
            name="poor_channel",
            description="å·®ä¿¡é“æ¡ä»¶ï¼ˆé«˜å™ªå£°ï¼‰",
            params={'noise_power_dbm': -164}  # æé«˜10dBå™ªå£°
        ),
        ScenarioConfig(
            name="normal_channel",
            description="æ­£å¸¸ä¿¡é“æ¡ä»¶ï¼ˆè®­ç»ƒåœºæ™¯ï¼‰",
            params={'noise_power_dbm': -174}
        ),
        ScenarioConfig(
            name="good_channel",
            description="å¥½ä¿¡é“æ¡ä»¶ï¼ˆä½å™ªå£°ï¼‰",
            params={'noise_power_dbm': -184}  # é™ä½10dBå™ªå£°
        ),
    ],
    
    # ========== 5. ä»»åŠ¡ç±»å‹åˆ†å¸ƒå˜åŒ– ==========
    "task_distribution": [
        ScenarioConfig(
            name="high_urgency",
            description="é«˜ç´§æ€¥ä»»åŠ¡æ¯”ä¾‹ï¼ˆ40%ç±»å‹1ï¼‰",
            params={'emergency_task_ratio': 0.40}
        ),
        ScenarioConfig(
            name="normal_mix",
            description="æ­£å¸¸æ··åˆåˆ†å¸ƒï¼ˆè®­ç»ƒåœºæ™¯ï¼‰",
            params={'emergency_task_ratio': 0.15}
        ),
        ScenarioConfig(
            name="low_urgency",
            description="ä½ç´§æ€¥ä»»åŠ¡æ¯”ä¾‹ï¼ˆ5%ç±»å‹1ï¼‰",
            params={'emergency_task_ratio': 0.05}
        ),
    ],
}


# ========== AgentåŠ è½½å™¨ ==========

def load_trained_agent(algorithm: str, model_path: str, state_dim: int, action_dim: int):
    """
    åŠ è½½è®­ç»ƒå¥½çš„agent
    
    ã€å‚æ•°ã€‘
    - algorithm: ç®—æ³•åç§°ï¼ˆTD3/SAC/DDPG/PPO/DQNï¼‰
    - model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    - state_dim: çŠ¶æ€ç©ºé—´ç»´åº¦
    - action_dim: åŠ¨ä½œç©ºé—´ç»´åº¦
    
    ã€è¿”å›ã€‘
    - agent: åŠ è½½å¥½çš„æ™ºèƒ½ä½“
    """
    algorithm = algorithm.upper()
    
    # å¯¼å…¥å¯¹åº”çš„agentç±»
    if algorithm == 'TD3':
        from single_agent.td3 import TD3Agent
        agent = TD3Agent(state_dim, action_dim, config.rl)
    elif algorithm == 'SAC':
        from single_agent.sac import SACAgent
        agent = SACAgent(state_dim, action_dim, config.rl)
    elif algorithm == 'DDPG':
        from single_agent.ddpg import DDPGAgent
        agent = DDPGAgent(state_dim, action_dim, config.rl)
    elif algorithm == 'PPO':
        from single_agent.ppo import PPOAgent
        agent = PPOAgent(state_dim, action_dim, config.rl)
    elif algorithm == 'DQN':
        from single_agent.dqn import DQNAgent
        # DQNçš„action_dimæ˜¯ç¦»æ•£åŠ¨ä½œæ•°é‡
        agent = DQNAgent(state_dim, action_dim, config.rl)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
    
    # åŠ è½½æ¨¡å‹å‚æ•°
    if os.path.exists(model_path):
        # ç§»é™¤æ–‡ä»¶åç¼€ï¼ˆå¦‚_td3.pthï¼‰ï¼Œå› ä¸ºload_modelä¼šè‡ªåŠ¨æ·»åŠ 
        base_path = model_path.replace(f'_{algorithm.lower()}.pth', '')
        agent.load_model(base_path)
        print(f"âœ“ æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
    else:
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆä¸æ¢ç´¢ï¼‰
    agent.actor.eval()
    
    return agent


def evaluate_agent_in_scenario(agent, algorithm: str, scenario: ScenarioConfig, 
                               num_episodes: int = 20) -> Dict[str, Any]:
    """
    åœ¨æŒ‡å®šåœºæ™¯ä¸‹è¯„ä¼°agentæ€§èƒ½
    
    ã€å‚æ•°ã€‘
    - agent: è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    - algorithm: ç®—æ³•åç§°
    - scenario: åœºæ™¯é…ç½®
    - num_episodes: è¯„ä¼°è½®æ¬¡
    
    ã€è¿”å›ã€‘
    - results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š è¯„ä¼°åœºæ™¯: {scenario.name}")
    print(f"   æè¿°: {scenario.description}")
    print(f"   å‚æ•°: {scenario.params}")
    print(f"{'='*60}\n")
    
    # åº”ç”¨åœºæ™¯é…ç½®
    original_config = {}
    for key, value in scenario.params.items():
        if '.' not in key and hasattr(config, key):
            original_config[key] = getattr(config, key)
    
    scenario.apply_to_config()
    
    # åˆ›å»ºç¯å¢ƒï¼ˆä½¿ç”¨å¯¹åº”ç®—æ³•çš„Environmentç±»ï¼‰
    if algorithm.upper() == 'TD3':
        from single_agent.td3 import TD3Environment
        env = TD3Environment()
    elif algorithm.upper() == 'SAC':
        from single_agent.sac import SACEnvironment
        env = SACEnvironment()
    elif algorithm.upper() == 'DDPG':
        from single_agent.ddpg import DDPGEnvironment
        env = DDPGEnvironment()
    elif algorithm.upper() == 'PPO':
        from single_agent.ppo import PPOEnvironment
        env = PPOEnvironment()
    elif algorithm.upper() == 'DQN':
        from single_agent.dqn import DQNEnvironment
        env = DQNEnvironment()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
    
    # æ”¶é›†è¯„ä¼°æŒ‡æ ‡
    episode_rewards = []
    episode_delays = []
    episode_energies = []
    episode_completion_rates = []
    episode_cache_hit_rates = []
    episode_migration_success_rates = []
    
    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0.0
        done = False
        step_count = 0
        
        while not done and step_count < config.experiment.max_steps_per_episode:
            # Agenté€‰æ‹©åŠ¨ä½œï¼ˆä¸æ·»åŠ å™ªå£°ï¼‰
            with torch.no_grad():
                if algorithm.upper() in ['TD3', 'SAC', 'DDPG']:
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
                    action = agent.actor(state_tensor).cpu().numpy()[0]
                elif algorithm.upper() == 'PPO':
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
                    action, _, _ = agent.actor(state_tensor)
                    action = action.cpu().numpy()[0]
                elif algorithm.upper() == 'DQN':
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
                    q_values = agent.q_network(state_tensor)
                    action = q_values.argmax(dim=1).item()
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)
            
            episode_reward += reward
            state = next_state
            step_count += 1
        
        # æ”¶é›†episodeç»Ÿè®¡
        episode_rewards.append(episode_reward)
        
        # ä»ä»¿çœŸå™¨è·å–æ€§èƒ½æŒ‡æ ‡
        metrics = env.simulator.get_metrics()
        episode_delays.append(metrics.get('avg_task_delay', 0.0))
        episode_energies.append(metrics.get('total_energy_consumption', 0.0))
        episode_completion_rates.append(metrics.get('task_completion_rate', 0.0))
        episode_cache_hit_rates.append(metrics.get('cache_hit_rate', 0.0))
        episode_migration_success_rates.append(metrics.get('migration_success_rate', 0.0))
        
        if (episode + 1) % 5 == 0:
            print(f"  Episode {episode+1}/{num_episodes}: "
                  f"Reward={episode_reward:.2f}, "
                  f"Delay={episode_delays[-1]:.3f}s, "
                  f"Completion={episode_completion_rates[-1]:.2%}")
    
    # è®¡ç®—å¹³å‡ç»“æœ
    results = {
        'scenario_name': scenario.name,
        'scenario_description': scenario.description,
        'scenario_params': scenario.params,
        'num_episodes': num_episodes,
        'avg_reward': float(np.mean(episode_rewards)),
        'std_reward': float(np.std(episode_rewards)),
        'avg_delay': float(np.mean(episode_delays)),
        'std_delay': float(np.std(episode_delays)),
        'avg_energy': float(np.mean(episode_energies)),
        'std_energy': float(np.std(episode_energies)),
        'avg_completion_rate': float(np.mean(episode_completion_rates)),
        'std_completion_rate': float(np.std(episode_completion_rates)),
        'avg_cache_hit_rate': float(np.mean(episode_cache_hit_rates)),
        'avg_migration_success_rate': float(np.mean(episode_migration_success_rates)),
    }
    
    # æ¢å¤åŸå§‹é…ç½®
    for key, value in original_config.items():
        setattr(config, key, value)
    
    print(f"\nâœ“ åœºæ™¯è¯„ä¼°å®Œæˆ:")
    print(f"  å¹³å‡å¥–åŠ±: {results['avg_reward']:.2f} Â± {results['std_reward']:.2f}")
    print(f"  å¹³å‡æ—¶å»¶: {results['avg_delay']:.3f}s Â± {results['std_delay']:.3f}s")
    print(f"  å¹³å‡èƒ½è€—: {results['avg_energy']:.2f} Â± {results['std_energy']:.2f}")
    print(f"  å®Œæˆç‡: {results['avg_completion_rate']:.2%} Â± {results['std_completion_rate']:.2%}")
    
    return results


# ========== æ³›åŒ–èƒ½åŠ›æµ‹è¯• ==========

def run_generalization_test(agent, algorithm: str, num_episodes: int = 20) -> Dict:
    """
    è¿è¡Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•ï¼ˆåœ¨æ‰€æœ‰é¢„å®šä¹‰åœºæ™¯ä¸‹è¯„ä¼°ï¼‰
    
    ã€åŠŸèƒ½ã€‘
    æµ‹è¯•æ¨¡å‹åœ¨è®­ç»ƒåœºæ™¯ä¹‹å¤–çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬ï¼š
    1. è½¦è¾†å¯†åº¦æ³›åŒ–
    2. ä»»åŠ¡è´Ÿè½½æ³›åŒ–
    3. åŸºç¡€è®¾æ–½å˜åŒ–é€‚åº”
    4. ç½‘ç»œæ¡ä»¶é²æ£’æ€§
    5. ä»»åŠ¡åˆ†å¸ƒå˜åŒ–é€‚åº”
    """
    print(f"\n{'='*80}")
    print(f"ğŸ¯ å¼€å§‹æ³›åŒ–èƒ½åŠ›æµ‹è¯•")
    print(f"{'='*80}\n")
    
    all_results = {}
    
    for set_name, scenarios in SCENARIO_SETS.items():
        print(f"\nğŸ“¦ æµ‹è¯•åœºæ™¯é›†: {set_name}")
        print(f"{'='*60}")
        
        set_results = []
        for scenario in scenarios:
            result = evaluate_agent_in_scenario(agent, algorithm, scenario, num_episodes)
            set_results.append(result)
        
        all_results[set_name] = set_results
    
    return all_results


# ========== å¤šç®—æ³•å¯¹æ¯” ==========

def compare_algorithms_cross_scenario(algorithms: List[str], scenario_set: str,
                                      num_episodes: int = 20) -> Dict:
    """
    å¯¹æ¯”å¤šä¸ªç®—æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½
    
    ã€å‚æ•°ã€‘
    - algorithms: ç®—æ³•åˆ—è¡¨ï¼Œå¦‚['TD3', 'SAC', 'DDPG']
    - scenario_set: åœºæ™¯é›†åç§°ï¼Œå¦‚'vehicle_density', 'task_load', 'all'
    - num_episodes: æ¯ä¸ªåœºæ™¯çš„è¯„ä¼°è½®æ¬¡
    """
    print(f"\n{'='*80}")
    print(f"ğŸ”¬ å¤šç®—æ³•è·¨åœºæ™¯å¯¹æ¯”")
    print(f"ç®—æ³•: {', '.join(algorithms)}")
    print(f"åœºæ™¯é›†: {scenario_set}")
    print(f"{'='*80}\n")
    
    # ç¡®å®šè¦æµ‹è¯•çš„åœºæ™¯
    if scenario_set == 'all':
        test_scenarios = []
        for scenarios in SCENARIO_SETS.values():
            test_scenarios.extend(scenarios)
    elif scenario_set in SCENARIO_SETS:
        test_scenarios = SCENARIO_SETS[scenario_set]
    else:
        raise ValueError(f"æœªçŸ¥åœºæ™¯é›†: {scenario_set}")
    
    # ä¸ºæ¯ä¸ªç®—æ³•åŠ è½½æ¨¡å‹å¹¶è¯„ä¼°
    comparison_results = {}
    
    for algorithm in algorithms:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¯„ä¼°ç®—æ³•: {algorithm}")
        print(f"{'='*60}")
        
        # æŸ¥æ‰¾æœ€ä½³æ¨¡å‹
        model_path = f"results/models/single_agent/{algorithm.lower()}/best_model_{algorithm.lower()}.pth"
        
        if not os.path.exists(model_path):
            print(f"âš ï¸ æœªæ‰¾åˆ°{algorithm}çš„æœ€ä½³æ¨¡å‹ï¼Œè·³è¿‡")
            continue
        
        # åŠ è½½agent
        # éœ€è¦è·å–state_dimå’Œaction_dimï¼ˆè¿™é‡Œä½¿ç”¨é»˜è®¤å€¼ï¼‰
        from single_agent.td3 import TD3Environment
        temp_env = TD3Environment()
        state_dim = temp_env.get_state_dim()
        action_dim = temp_env.get_action_dim()
        
        try:
            agent = load_trained_agent(algorithm, model_path, state_dim, action_dim)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½{algorithm}æ¨¡å‹å¤±è´¥: {e}")
            continue
        
        # åœ¨æ‰€æœ‰åœºæ™¯ä¸‹è¯„ä¼°
        algorithm_results = []
        for scenario in test_scenarios:
            result = evaluate_agent_in_scenario(agent, algorithm, scenario, num_episodes)
            algorithm_results.append(result)
        
        comparison_results[algorithm] = algorithm_results
    
    return comparison_results


# ========== ç»“æœå¯è§†åŒ– ==========

def visualize_cross_scenario_results(results: Dict, save_dir: str):
    """
    å¯è§†åŒ–è·¨åœºæ™¯è¯„ä¼°ç»“æœ
    
    ã€ç”Ÿæˆå›¾è¡¨ã€‘
    1. å„åœºæ™¯æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
    2. æ³›åŒ–èƒ½åŠ›é›·è¾¾å›¾
    3. æ€§èƒ½åˆ†å¸ƒç®±çº¿å›¾
    4. åœºæ™¯æ•æ„Ÿæ€§åˆ†æ
    """
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    matplotlib.rcParams['axes.unicode_minus'] = False
    
    os.makedirs(save_dir, exist_ok=True)
    
    # å¦‚æœæ˜¯æ³›åŒ–æµ‹è¯•ç»“æœï¼ˆåŒ…å«å¤šä¸ªåœºæ™¯é›†ï¼‰
    if isinstance(next(iter(results.values())), list) and \
       isinstance(next(iter(results.values()))[0], dict) and \
       'scenario_name' in next(iter(results.values()))[0]:
        
        # 1. å„åœºæ™¯é›†çš„æ€§èƒ½å¯¹æ¯”
        for set_name, set_results in results.items():
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'åœºæ™¯é›†: {set_name}', fontsize=16, fontweight='bold')
            
            scenario_names = [r['scenario_name'] for r in set_results]
            
            # æ—¶å»¶å¯¹æ¯”
            ax = axes[0, 0]
            delays = [r['avg_delay'] for r in set_results]
            delay_stds = [r['std_delay'] for r in set_results]
            ax.bar(scenario_names, delays, yerr=delay_stds, capsize=5, alpha=0.7, color='steelblue')
            ax.set_ylabel('å¹³å‡æ—¶å»¶ (s)')
            ax.set_title('æ—¶å»¶å¯¹æ¯”')
            ax.tick_params(axis='x', rotation=45)
            ax.grid(axis='y', alpha=0.3)
            
            # èƒ½è€—å¯¹æ¯”
            ax = axes[0, 1]
            energies = [r['avg_energy'] for r in set_results]
            energy_stds = [r['std_energy'] for r in set_results]
            ax.bar(scenario_names, energies, yerr=energy_stds, capsize=5, alpha=0.7, color='coral')
            ax.set_ylabel('æ€»èƒ½è€— (J)')
            ax.set_title('èƒ½è€—å¯¹æ¯”')
            ax.tick_params(axis='x', rotation=45)
            ax.grid(axis='y', alpha=0.3)
            
            # å®Œæˆç‡å¯¹æ¯”
            ax = axes[1, 0]
            completion = [r['avg_completion_rate'] * 100 for r in set_results]
            completion_stds = [r['std_completion_rate'] * 100 for r in set_results]
            ax.bar(scenario_names, completion, yerr=completion_stds, capsize=5, alpha=0.7, color='seagreen')
            ax.set_ylabel('å®Œæˆç‡ (%)')
            ax.set_title('ä»»åŠ¡å®Œæˆç‡å¯¹æ¯”')
            ax.tick_params(axis='x', rotation=45)
            ax.grid(axis='y', alpha=0.3)
            ax.set_ylim([0, 105])
            
            # å¥–åŠ±å¯¹æ¯”
            ax = axes[1, 1]
            rewards = [r['avg_reward'] for r in set_results]
            reward_stds = [r['std_reward'] for r in set_results]
            ax.bar(scenario_names, rewards, yerr=reward_stds, capsize=5, alpha=0.7, color='mediumpurple')
            ax.set_ylabel('å¹³å‡å¥–åŠ±')
            ax.set_title('å¥–åŠ±å¯¹æ¯”')
            ax.tick_params(axis='x', rotation=45)
            ax.grid(axis='y', alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(os.path.join(save_dir, f'scenario_comparison_{set_name}.png'), dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ“ ä¿å­˜å›¾è¡¨: scenario_comparison_{set_name}.png")


def save_results_to_file(results: Dict, filepath: str):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {filepath}")


# ========== å‘½ä»¤è¡Œæ¥å£ ==========

def parse_args():
    parser = argparse.ArgumentParser(description='è·¨åœºæ™¯è¯„ä¼°å·¥å…·')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', type=str, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--algorithm', type=str, default='TD3',
                       choices=['TD3', 'SAC', 'DDPG', 'PPO', 'DQN'],
                       help='ç®—æ³•åç§°')
    
    # è¯„ä¼°æ¨¡å¼
    parser.add_argument('--generalization-test', action='store_true',
                       help='è¿è¡Œå®Œæ•´æ³›åŒ–èƒ½åŠ›æµ‹è¯•')
    parser.add_argument('--compare', action='store_true',
                       help='å¤šç®—æ³•å¯¹æ¯”æ¨¡å¼')
    parser.add_argument('--algorithms', type=str, nargs='+',
                       default=['TD3', 'SAC', 'DDPG'],
                       help='å¯¹æ¯”çš„ç®—æ³•åˆ—è¡¨')
    
    # åœºæ™¯å‚æ•°
    parser.add_argument('--scenario-set', type=str, default='vehicle_density',
                       choices=list(SCENARIO_SETS.keys()) + ['all'],
                       help='åœºæ™¯é›†åç§°')
    parser.add_argument('--num-vehicles', type=int, help='è‡ªå®šä¹‰è½¦è¾†æ•°')
    parser.add_argument('--arrival-rate', type=float, help='è‡ªå®šä¹‰ä»»åŠ¡åˆ°è¾¾ç‡')
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument('--eval-episodes', type=int, default=20,
                       help='æ¯ä¸ªåœºæ™¯çš„è¯„ä¼°è½®æ¬¡')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output-dir', type=str, default='results/cross_scenario',
                       help='ç»“æœä¿å­˜ç›®å½•')
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(args.output_dir, timestamp)
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\n{'='*80}")
    print(f"ğŸš€ è·¨åœºæ™¯è¯„ä¼°å·¥å…·")
    print(f"{'='*80}\n")
    
    # ========== æ¨¡å¼1: å¤šç®—æ³•å¯¹æ¯” ==========
    if args.compare:
        results = compare_algorithms_cross_scenario(
            algorithms=args.algorithms,
            scenario_set=args.scenario_set,
            num_episodes=args.eval_episodes
        )
        
        # ä¿å­˜ç»“æœ
        result_file = os.path.join(output_dir, 'algorithm_comparison.json')
        save_results_to_file(results, result_file)
        
        # å¯è§†åŒ–
        visualize_cross_scenario_results(results, output_dir)
    
    # ========== æ¨¡å¼2: æ³›åŒ–èƒ½åŠ›æµ‹è¯• ==========
    elif args.generalization_test:
        if not args.model:
            print("âŒ é”™è¯¯: æ³›åŒ–æµ‹è¯•éœ€è¦æŒ‡å®š--modelå‚æ•°")
            return
        
        # åŠ è½½agent
        from single_agent.td3 import TD3Environment
        temp_env = TD3Environment()
        state_dim = temp_env.get_state_dim()
        action_dim = temp_env.get_action_dim()
        
        agent = load_trained_agent(args.algorithm, args.model, state_dim, action_dim)
        
        # è¿è¡Œæ³›åŒ–æµ‹è¯•
        results = run_generalization_test(agent, args.algorithm, args.eval_episodes)
        
        # ä¿å­˜ç»“æœ
        result_file = os.path.join(output_dir, f'generalization_test_{args.algorithm.lower()}.json')
        save_results_to_file(results, result_file)
        
        # å¯è§†åŒ–
        visualize_cross_scenario_results(results, output_dir)
    
    # ========== æ¨¡å¼3: å•åœºæ™¯è¯„ä¼° ==========
    else:
        if not args.model:
            print("âŒ é”™è¯¯: éœ€è¦æŒ‡å®š--modelå‚æ•°")
            return
        
        # åŠ è½½agent
        from single_agent.td3 import TD3Environment
        temp_env = TD3Environment()
        state_dim = temp_env.get_state_dim()
        action_dim = temp_env.get_action_dim()
        
        agent = load_trained_agent(args.algorithm, args.model, state_dim, action_dim)
        
        # è‡ªå®šä¹‰åœºæ™¯
        if args.num_vehicles or args.arrival_rate:
            params = {}
            if args.num_vehicles:
                params['num_vehicles'] = args.num_vehicles
            if args.arrival_rate:
                params['task_arrival_rate'] = args.arrival_rate
            
            scenario = ScenarioConfig(
                name="custom_scenario",
                description="è‡ªå®šä¹‰åœºæ™¯",
                params=params
            )
            
            result = evaluate_agent_in_scenario(agent, args.algorithm, scenario, args.eval_episodes)
            
            # ä¿å­˜ç»“æœ
            result_file = os.path.join(output_dir, 'custom_scenario_result.json')
            save_results_to_file(result, result_file)
        
        # ä½¿ç”¨é¢„å®šä¹‰åœºæ™¯é›†
        else:
            scenarios = SCENARIO_SETS[args.scenario_set]
            results = []
            
            for scenario in scenarios:
                result = evaluate_agent_in_scenario(agent, args.algorithm, scenario, args.eval_episodes)
                results.append(result)
            
            # ä¿å­˜ç»“æœ
            result_file = os.path.join(output_dir, f'{args.scenario_set}_results.json')
            save_results_to_file({'scenarios': results}, result_file)
            
            # å¯è§†åŒ–
            visualize_cross_scenario_results({args.scenario_set: results}, output_dir)
    
    print(f"\n{'='*80}")
    print(f"âœ… è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()




