#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ è·¨åœºæ™¯è¯„ä¼°å·¥å…· - Cross-Scenario Evaluation Tool

ã€åŠŸèƒ½ã€‘
ä½¿ç”¨è®­ç»ƒå¥½çš„agentåœ¨ä¸åŒåœºæ™¯ä¸‹è¿›è¡Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
1. è½¦è¾†å¯†åº¦å˜åŒ–ï¼ˆ8/12/16/20/24è¾†ï¼‰
2. ä»»åŠ¡åˆ°è¾¾ç‡å˜åŒ–ï¼ˆä½/ä¸­/é«˜è´Ÿè½½ï¼‰
3. ä»»åŠ¡ç±»å‹åˆ†å¸ƒå˜åŒ–ï¼ˆç´§æ€¥ä»»åŠ¡æ¯”ä¾‹ï¼‰
4. ç½‘ç»œæ¡ä»¶å˜åŒ–ï¼ˆä¿¡é“è´¨é‡ï¼‰
5. RSU/UAVæ•°é‡å˜åŒ–ï¼ˆæ–°å¢1~5æ¶UAVæ¢¯åº¦åœºæ™¯ï¼‰

 ğŸ› ï¸ TD3ä½¿ç”¨ç¤ºä¾‹ï¼ˆä»…å…³æ³¨å•æ™ºèƒ½ä½“TD3æ—¶ï¼‰ï¼š
1) è½¦è¾†å¯†åº¦ï¼š
  python evaluate_cross_scenario.py --algorithm TD3 --model results/models/single_agent/td3/best_model_td3.pth --scenario-set vehicle_density --eval-episodes 20
2) ä»»åŠ¡è´Ÿè½½ï¼š
  python evaluate_cross_scenario.py --algorithm TD3 --model results/models/single_agent/td3/best_model_td3.pth --scenario-set task_load --eval-episodes 20
3) ä»»åŠ¡ç±»å‹åˆ†å¸ƒï¼š
  python evaluate_cross_scenario.py --algorithm TD3 --model results/models/single_agent/td3/best_model_td3.pth --scenario-set task_distribution --eval-episodes 20
4) ç½‘ç»œæ¡ä»¶ï¼š
  python evaluate_cross_scenario.py --algorithm TD3 --model results/models/single_agent/td3/best_model_td3.pth --scenario-set network_condition --eval-episodes 20
  

ã€è¾“å‡ºã€‘
- å„åœºæ™¯ä¸‹çš„æ€§èƒ½æŒ‡æ ‡ï¼ˆæ—¶å»¶ã€èƒ½è€—ã€å®Œæˆç‡ï¼‰
- æ³›åŒ–èƒ½åŠ›åˆ†æå›¾è¡¨
- æ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾
- è¯¦ç»†çš„JSONç»“æœæ–‡ä»¶
"""

import os
import sys
import json
import time
import copy
import argparse
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
from config.system_config import config
from evaluation.system_simulator import CompleteSystemSimulator


# ========== åœºæ™¯å®šä¹‰ ==========

_MISSING = object()

SCENARIO_PARAM_ALIASES = {
    'task_arrival_rate': 'task.arrival_rate',
    'noise_power_dbm': 'communication.thermal_noise_density',
}

TOPOLOGY_KEYS = {'num_vehicles', 'num_rsus', 'num_uavs'}


def _resolve_config_target(path: str) -> Tuple[Any, str]:
    """è¿”å›(path)æ‰€åœ¨å¯¹è±¡ä¸å±æ€§å"""
    parts = path.split('.')
    target = config
    for part in parts[:-1]:
        if not hasattr(target, part):
            raise AttributeError(f"é…ç½®ä¸­ä¸å­˜åœ¨å±æ€§ '{part}' (è·¯å¾„: {path})")
        target = getattr(target, part)
    return target, parts[-1]


def _snapshot_value(value: Any) -> Any:
    """åœ¨éœ€è¦æ—¶æ·±æ‹·è´å€¼ï¼Œä¾¿äºæ¢å¤"""
    if value is _MISSING:
        return _MISSING
    if isinstance(value, (dict, list)):
        return copy.deepcopy(value)
    return value


def _set_config_value(path: str, value: Any) -> Dict[str, Any]:
    """è®¾ç½®é…ç½®å€¼å¹¶è¿”å›åŸå€¼"""
    target, attr = _resolve_config_target(path)
    original = _snapshot_value(getattr(target, attr, _MISSING))
    setattr(target, attr, value)
    return {path: original}


def _apply_emergency_ratio(ratio: float) -> Dict[str, Any]:
    """æ ¹æ®ç´§æ€¥ä»»åŠ¡æ¯”ä¾‹è°ƒæ•´ä»»åŠ¡ç±»å‹æƒé‡"""
    overrides: Dict[str, Any] = {}
    task_cfg = config.task
    original_weights = copy.deepcopy(task_cfg.type_priority_weights)
    overrides['task.type_priority_weights'] = original_weights
    
    ratio = float(np.clip(ratio, 0.05, 0.9))
    other_keys = [k for k in original_weights.keys() if k != 1]
    other_sum = sum(original_weights[k] for k in other_keys) or float(len(other_keys))
    scaled = (1.0 - ratio) / other_sum
    
    updated_weights = original_weights.copy()
    updated_weights[1] = ratio
    for key in other_keys:
        updated_weights[key] = original_weights[key] * scaled
    
    task_cfg.type_priority_weights = updated_weights
    previous_ratio = getattr(task_cfg, 'emergency_task_ratio', _MISSING)
    setattr(task_cfg, 'emergency_task_ratio', ratio)
    overrides['task.emergency_task_ratio'] = previous_ratio
    return overrides


def restore_config_overrides(overrides: Dict[str, Any]) -> None:
    """æ ¹æ®è®°å½•æ¢å¤é…ç½®"""
    for path, original in overrides.items():
        target, attr = _resolve_config_target(path)
        if original is _MISSING:
            if hasattr(target, attr):
                delattr(target, attr)
        else:
            value = copy.deepcopy(original) if isinstance(original, (dict, list)) else original
            setattr(target, attr, value)


class ScenarioConfig:
    """åœºæ™¯é…ç½®ç±»"""
    
    def __init__(self, name: str, description: str, params: Dict[str, Any]):
        self.name = name
        self.description = description
        self.params = params
    
    def apply_to_config(self) -> Dict[str, Any]:
        """åº”ç”¨åœºæ™¯å‚æ•°åˆ°å…¨å±€é…ç½®ï¼Œå¹¶è¿”å›æ¢å¤è®°å½•"""
        overrides: Dict[str, Any] = {}
        for key, value in self.params.items():
            normalized_key = SCENARIO_PARAM_ALIASES.get(key, key)
            
            if key == 'emergency_task_ratio':
                overrides.update(_apply_emergency_ratio(value))
                continue
            
            if normalized_key in TOPOLOGY_KEYS:
                overrides.update(_set_config_value(normalized_key, value))
                network_path = f"network.{normalized_key}"
                overrides.update(_set_config_value(network_path, value))
                continue
            
            overrides.update(_set_config_value(normalized_key, value))
            
            if key == 'noise_power_dbm':
                overrides.update(_set_config_value('communication.noise_power_dbm', value))
        
        return overrides


# é¢„å®šä¹‰åœºæ™¯é›†
SCENARIO_SETS = {
    # ========== 1. è½¦è¾†å¯†åº¦å˜åŒ– ==========
    "vehicle_density": [
        ScenarioConfig(
            name="low_density",
            description="ä½å¯†åº¦åœºæ™¯ï¼ˆ8è¾†è½¦ï¼‰",
            params={'num_vehicles': 8, 'task_arrival_rate': 2.0}
        ),
        ScenarioConfig(
            name="medium_density", 
            description="ä¸­å¯†åº¦åœºæ™¯ï¼ˆ12è¾†è½¦ï¼Œè®­ç»ƒåœºæ™¯ï¼‰",
            params={'num_vehicles': 12, 'task_arrival_rate': 2.5}
        ),
        ScenarioConfig(
            name="high_density",
            description="é«˜å¯†åº¦åœºæ™¯ï¼ˆ16è¾†è½¦ï¼‰",
            params={'num_vehicles': 16, 'task_arrival_rate': 3.0}
        ),
        ScenarioConfig(
            name="extreme_density",
            description="æé«˜å¯†åº¦åœºæ™¯ï¼ˆ24è¾†è½¦ï¼‰",
            params={'num_vehicles': 24, 'task_arrival_rate': 4.0}
        ),
    ],
    
    # ========== 2. ä»»åŠ¡è´Ÿè½½å˜åŒ– ==========
    "task_load": [
        ScenarioConfig(
            name="light_load",
            description="è½»è´Ÿè½½ï¼ˆåˆ°è¾¾ç‡1.5 tasks/sï¼‰",
            params={'task_arrival_rate': 1.5}
        ),
        ScenarioConfig(
            name="normal_load",
            description="æ­£å¸¸è´Ÿè½½ï¼ˆåˆ°è¾¾ç‡2.5 tasks/sï¼Œè®­ç»ƒåœºæ™¯ï¼‰",
            params={'task_arrival_rate': 2.5}
        ),
        ScenarioConfig(
            name="heavy_load",
            description="é‡è´Ÿè½½ï¼ˆåˆ°è¾¾ç‡3.5 tasks/sï¼‰",
            params={'task_arrival_rate': 3.5}
        ),
        ScenarioConfig(
            name="extreme_load",
            description="æç«¯è´Ÿè½½ï¼ˆåˆ°è¾¾ç‡5.0 tasks/sï¼‰",
            params={'task_arrival_rate': 5.0}
        ),
    ],
    
    # ========== 3. åŸºç¡€è®¾æ–½å˜åŒ– ==========
    "infrastructure": [
        ScenarioConfig(
            name="limited_rsu",
            description="æœ‰é™RSUï¼ˆ4ä¸ªï¼‰",
            params={'num_rsus': 4}
        ),
        ScenarioConfig(
            name="standard_rsu",
            description="æ ‡å‡†RSUï¼ˆ6ä¸ªï¼Œè®­ç»ƒåœºæ™¯ï¼‰",
            params={'num_rsus': 6}
        ),
        ScenarioConfig(
            name="abundant_rsu",
            description="å……è¶³RSUï¼ˆ8ä¸ªï¼‰",
            params={'num_rsus': 8}
        ),
        ScenarioConfig(
            name="with_uav",
            description="å¢åŠ UAVæ”¯æŒï¼ˆ2ä¸ªUAVï¼‰",
            params={'num_uavs': 2}
        ),
    ],
    
    # ========== 4. ç½‘ç»œæ¡ä»¶å˜åŒ– ==========
    "network_condition": [
        ScenarioConfig(
            name="poor_channel",
            description="å·®ä¿¡é“æ¡ä»¶ï¼ˆé«˜å™ªå£°ï¼‰",
            params={'noise_power_dbm': -164}  # æé«˜10dBå™ªå£°
        ),
        ScenarioConfig(
            name="normal_channel",
            description="æ­£å¸¸ä¿¡é“æ¡ä»¶ï¼ˆè®­ç»ƒåœºæ™¯ï¼‰",
            params={'noise_power_dbm': -174}
        ),
        ScenarioConfig(
            name="good_channel",
            description="å¥½ä¿¡é“æ¡ä»¶ï¼ˆä½å™ªå£°ï¼‰",
            params={'noise_power_dbm': -184}  # é™ä½10dBå™ªå£°
        ),
    ],
    
    # ========== 5. ä»»åŠ¡ç±»å‹åˆ†å¸ƒå˜åŒ– ==========
    "task_distribution": [
        ScenarioConfig(
            name="high_urgency",
            description="é«˜ç´§æ€¥ä»»åŠ¡æ¯”ä¾‹ï¼ˆ40%ç±»å‹1ï¼‰",
            params={'emergency_task_ratio': 0.40}
        ),
        ScenarioConfig(
            name="normal_mix",
            description="æ­£å¸¸æ··åˆåˆ†å¸ƒï¼ˆè®­ç»ƒåœºæ™¯ï¼‰",
            params={'emergency_task_ratio': 0.15}
        ),
        ScenarioConfig(
            name="low_urgency",
            description="ä½ç´§æ€¥ä»»åŠ¡æ¯”ä¾‹ï¼ˆ5%ç±»å‹1ï¼‰",
            params={'emergency_task_ratio': 0.05}
        ),
    ],
    
    # ========== 6. UAVæ•°é‡æ¢¯åº¦ï¼ˆ1~5æ¶ï¼‰ ==========
    "uav_scale": [
        ScenarioConfig(
            name="uav_1",
            description="ä»…éƒ¨ç½²1æ¶UAVï¼ˆæé™åœºæ™¯ï¼‰",
            params={'num_uavs': 1}
        ),
        ScenarioConfig(
            name="uav_2",
            description="éƒ¨ç½²2æ¶UAVï¼ˆé»˜è®¤é…ç½®ï¼‰",
            params={'num_uavs': 2}
        ),
        ScenarioConfig(
            name="uav_3",
            description="éƒ¨ç½²3æ¶UAVï¼ˆå¢å¼ºç©ºä¸­è¦†ç›–ï¼‰",
            params={'num_uavs': 3}
        ),
        ScenarioConfig(
            name="uav_4",
            description="éƒ¨ç½²4æ¶UAVï¼ˆé«˜ç©ºååŒï¼‰",
            params={'num_uavs': 4}
        ),
        ScenarioConfig(
            name="uav_5",
            description="éƒ¨ç½²5æ¶UAVï¼ˆæé™é«˜ç©ºæ”¯æŒï¼‰",
            params={'num_uavs': 5}
        ),
    ],
}


# ========== AgentåŠ è½½å™¨ ==========

def get_state_action_signature(algorithm: str) -> Tuple[int, int]:
    """åŸºäºç®—æ³•åˆ›å»ºç¯å¢ƒä»¥è·å–çŠ¶æ€/åŠ¨ä½œç»´åº¦"""
    algorithm = algorithm.upper()
    if algorithm == 'TD3':
        from single_agent.td3 import TD3Environment
        env = TD3Environment()
    elif algorithm == 'SAC':
        from single_agent.sac import SACEnvironment
        env = SACEnvironment()
    elif algorithm == 'DDPG':
        from single_agent.ddpg import DDPGEnvironment
        env = DDPGEnvironment()
    elif algorithm == 'PPO':
        from single_agent.ppo import PPOEnvironment
        env = PPOEnvironment()
    elif algorithm == 'DQN':
        from single_agent.dqn import DQNEnvironment
        env = DQNEnvironment()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
    
    state_dim, action_dim = env.state_dim, env.action_dim
    del env
    return state_dim, action_dim


def load_trained_agent(algorithm: str, model_path: str,
                       state_dim: Optional[int] = None,
                       action_dim: Optional[int] = None):
    """
    åŠ è½½è®­ç»ƒå¥½çš„agent
    
    ã€å‚æ•°ã€‘
    - algorithm: ç®—æ³•åç§°ï¼ˆTD3/SAC/DDPG/PPO/DQNï¼‰
    - model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    - state_dim: çŠ¶æ€ç©ºé—´ç»´åº¦
    - action_dim: åŠ¨ä½œç©ºé—´ç»´åº¦
    
    ã€è¿”å›ã€‘
    - agent: åŠ è½½å¥½çš„æ™ºèƒ½ä½“
    """
    algorithm = algorithm.upper()
    if state_dim is None or action_dim is None:
        state_dim, action_dim = get_state_action_signature(algorithm)
    
    if algorithm == 'TD3':
        from single_agent.td3 import TD3Agent, TD3Config
        agent = TD3Agent(state_dim, action_dim, TD3Config())
    elif algorithm == 'SAC':
        from single_agent.sac import SACAgent, SACConfig
        agent = SACAgent(state_dim, action_dim, SACConfig())
    elif algorithm == 'DDPG':
        from single_agent.ddpg import DDPGAgent, DDPGConfig
        agent = DDPGAgent(state_dim, action_dim, DDPGConfig())
    elif algorithm == 'PPO':
        from single_agent.ppo import PPOAgent, PPOConfig
        agent = PPOAgent(state_dim, action_dim, PPOConfig())
    elif algorithm == 'DQN':
        from single_agent.dqn import DQNAgent, DQNConfig
        agent = DQNAgent(state_dim, action_dim, DQNConfig())
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
    
    if not os.path.exists(model_path):
        candidate = f"{model_path}_{algorithm.lower()}.pth"
        if os.path.exists(candidate):
            model_path = candidate
        else:
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    suffix = f"_{algorithm.lower()}.pth"
    if model_path.lower().endswith(suffix):
        base_path = model_path[:-len(suffix)]
    else:
        base_path = model_path
    
    agent.load_model(base_path)
    print(f"âœ“ æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
    
    if hasattr(agent, 'actor') and hasattr(agent.actor, 'eval'):
        agent.actor.eval()
    if hasattr(agent, 'critic') and hasattr(agent.critic, 'eval'):
        agent.critic.eval()
    if hasattr(agent, 'q_network') and hasattr(agent.q_network, 'eval'):
        agent.q_network.eval()
    
    return agent


def evaluate_agent_in_scenario(agent, algorithm: str, scenario: ScenarioConfig, 
                               num_episodes: int = 20) -> Dict[str, Any]:
    """
    åœ¨æŒ‡å®šåœºæ™¯ä¸‹è¯„ä¼°agentæ€§èƒ½
    
    ã€å‚æ•°ã€‘
    - agent: è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    - algorithm: ç®—æ³•åç§°
    - scenario: åœºæ™¯é…ç½®
    - num_episodes: è¯„ä¼°è½®æ¬¡
    
    ã€è¿”å›ã€‘
    - results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š è¯„ä¼°åœºæ™¯: {scenario.name}")
    print(f"   æè¿°: {scenario.description}")
    print(f"   å‚æ•°: {scenario.params}")
    print(f"{'='*60}\n")
    
    overrides = scenario.apply_to_config()
    env_kwargs = {
        'num_vehicles': scenario.params.get('num_vehicles', getattr(config, 'num_vehicles', 12)),
        'num_rsus': scenario.params.get('num_rsus', getattr(config, 'num_rsus', 4)),
        'num_uavs': scenario.params.get('num_uavs', getattr(config, 'num_uavs', 2)),
    }
    env_kwargs = {k: int(v) for k, v in env_kwargs.items() if v is not None}
    
    try:
        if algorithm.upper() == 'TD3':
            from single_agent.td3 import TD3Environment
            env = TD3Environment(**env_kwargs)
        elif algorithm.upper() == 'SAC':
            from single_agent.sac import SACEnvironment
            env = SACEnvironment(**env_kwargs)
        elif algorithm.upper() == 'DDPG':
            from single_agent.ddpg import DDPGEnvironment
            env = DDPGEnvironment(**env_kwargs)
        elif algorithm.upper() == 'PPO':
            from single_agent.ppo import PPOEnvironment
            env = PPOEnvironment(**env_kwargs)
        elif algorithm.upper() == 'DQN':
            from single_agent.dqn import DQNEnvironment
            env = DQNEnvironment(**env_kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
        
        episode_rewards = []
        episode_delays = []
        episode_energies = []
        episode_completion_rates = []
        episode_cache_hit_rates = []
        episode_migration_success_rates = []
        device = getattr(agent, 'device', None)
        
        for episode in range(num_episodes):
            state = env.reset()
            episode_reward = 0.0
            done = False
            step_count = 0
            
            while not done and step_count < config.experiment.max_steps_per_episode:
                with torch.no_grad():
                    state_tensor = torch.FloatTensor(state).unsqueeze(0)
                    if device is not None:
                        state_tensor = state_tensor.to(device)
                    
                    if algorithm.upper() in ['TD3', 'SAC', 'DDPG']:
                        action = agent.actor(state_tensor).cpu().numpy()[0]
                    elif algorithm.upper() == 'PPO':
                        action, _, _ = agent.actor(state_tensor)
                        action = action.cpu().numpy()[0]
                    elif algorithm.upper() == 'DQN':
                        q_values = agent.q_network(state_tensor)
                        action = q_values.argmax(dim=1).item()
                    else:
                        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
                
                next_state, reward, done, info = env.step(action)
                state = next_state
                episode_reward += reward
                step_count += 1
            
            episode_rewards.append(episode_reward)
            metrics = env.simulator.get_metrics()
            episode_delays.append(metrics.get('avg_task_delay', 0.0))
            episode_energies.append(metrics.get('total_energy_consumption', 0.0))
            episode_completion_rates.append(metrics.get('task_completion_rate', 0.0))
            episode_cache_hit_rates.append(metrics.get('cache_hit_rate', 0.0))
            episode_migration_success_rates.append(metrics.get('migration_success_rate', 0.0))
            
            if (episode + 1) % 5 == 0:
                print(
                    f"  Episode {episode+1}/{num_episodes}: "
                    f"Reward={episode_reward:.2f}, "
                    f"Delay={episode_delays[-1]:.3f}s, "
                    f"Completion={episode_completion_rates[-1]:.2%}"
                )
        
        results = {
            'scenario_name': scenario.name,
            'scenario_description': scenario.description,
            'scenario_params': scenario.params,
            'num_episodes': num_episodes,
            'avg_reward': float(np.mean(episode_rewards)),
            'std_reward': float(np.std(episode_rewards)),
            'avg_delay': float(np.mean(episode_delays)),
            'std_delay': float(np.std(episode_delays)),
            'avg_energy': float(np.mean(episode_energies)),
            'std_energy': float(np.std(episode_energies)),
            'avg_completion_rate': float(np.mean(episode_completion_rates)),
            'std_completion_rate': float(np.std(episode_completion_rates)),
            'avg_cache_hit_rate': float(np.mean(episode_cache_hit_rates)),
            'avg_migration_success_rate': float(np.mean(episode_migration_success_rates)),
        }
    finally:
        restore_config_overrides(overrides)
    
    print(f"\nâœ“ åœºæ™¯è¯„ä¼°å®Œæˆ:")
    print(f"  å¹³å‡å¥–åŠ±: {results['avg_reward']:.2f} Â± {results['std_reward']:.2f}")
    print(f"  å¹³å‡æ—¶å»¶: {results['avg_delay']:.3f}s Â± {results['std_delay']:.3f}s")
    print(f"  å¹³å‡èƒ½è€—: {results['avg_energy']:.2f} Â± {results['std_energy']:.2f}")
    print(f"  å®Œæˆç‡: {results['avg_completion_rate']:.2%} Â± {results['std_completion_rate']:.2%}")
    
    return results


# ========== æ³›åŒ–èƒ½åŠ›æµ‹è¯• ==========

def run_generalization_test(agent, algorithm: str, num_episodes: int = 20) -> Dict:
    """
    è¿è¡Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•ï¼ˆåœ¨æ‰€æœ‰é¢„å®šä¹‰åœºæ™¯ä¸‹è¯„ä¼°ï¼‰
    
    ã€åŠŸèƒ½ã€‘
    æµ‹è¯•æ¨¡å‹åœ¨è®­ç»ƒåœºæ™¯ä¹‹å¤–çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬ï¼š
    1. è½¦è¾†å¯†åº¦æ³›åŒ–
    2. ä»»åŠ¡è´Ÿè½½æ³›åŒ–
    3. åŸºç¡€è®¾æ–½å˜åŒ–é€‚åº”
    4. ç½‘ç»œæ¡ä»¶é²æ£’æ€§
    5. ä»»åŠ¡åˆ†å¸ƒå˜åŒ–é€‚åº”
    """
    print(f"\n{'='*80}")
    print(f"ğŸ¯ å¼€å§‹æ³›åŒ–èƒ½åŠ›æµ‹è¯•")
    print(f"{'='*80}\n")
    
    all_results = {}
    
    for set_name, scenarios in SCENARIO_SETS.items():
        print(f"\nğŸ“¦ æµ‹è¯•åœºæ™¯é›†: {set_name}")
        print(f"{'='*60}")
        
        set_results = []
        for scenario in scenarios:
            result = evaluate_agent_in_scenario(agent, algorithm, scenario, num_episodes)
            set_results.append(result)
        
        all_results[set_name] = set_results
    
    return all_results


# ========== å¤šç®—æ³•å¯¹æ¯” ==========

def compare_algorithms_cross_scenario(algorithms: List[str], scenario_set: str,
                                      num_episodes: int = 20) -> Dict:
    """
    å¯¹æ¯”å¤šä¸ªç®—æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½
    
    ã€å‚æ•°ã€‘
    - algorithms: ç®—æ³•åˆ—è¡¨ï¼Œå¦‚['TD3', 'SAC', 'DDPG']
    - scenario_set: åœºæ™¯é›†åç§°ï¼Œå¦‚'vehicle_density', 'task_load', 'all'
    - num_episodes: æ¯ä¸ªåœºæ™¯çš„è¯„ä¼°è½®æ¬¡
    """
    print(f"\n{'='*80}")
    print(f"ğŸ”¬ å¤šç®—æ³•è·¨åœºæ™¯å¯¹æ¯”")
    print(f"ç®—æ³•: {', '.join(algorithms)}")
    print(f"åœºæ™¯é›†: {scenario_set}")
    print(f"{'='*80}\n")
    
    # ç¡®å®šè¦æµ‹è¯•çš„åœºæ™¯
    if scenario_set == 'all':
        test_scenarios = []
        for scenarios in SCENARIO_SETS.values():
            test_scenarios.extend(scenarios)
    elif scenario_set in SCENARIO_SETS:
        test_scenarios = SCENARIO_SETS[scenario_set]
    else:
        raise ValueError(f"æœªçŸ¥åœºæ™¯é›†: {scenario_set}")
    
    # ä¸ºæ¯ä¸ªç®—æ³•åŠ è½½æ¨¡å‹å¹¶è¯„ä¼°
    comparison_results = {}
    
    for algorithm in algorithms:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¯„ä¼°ç®—æ³•: {algorithm}")
        print(f"{'='*60}")
        
        # æŸ¥æ‰¾æœ€ä½³æ¨¡å‹
        model_path = f"results/models/single_agent/{algorithm.lower()}/best_model_{algorithm.lower()}.pth"
        
        if not os.path.exists(model_path):
            print(f"âš ï¸ æœªæ‰¾åˆ°{algorithm}çš„æœ€ä½³æ¨¡å‹ï¼Œè·³è¿‡")
            continue
        
        state_dim, action_dim = get_state_action_signature(algorithm)
        try:
            agent = load_trained_agent(algorithm, model_path, state_dim, action_dim)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½{algorithm}æ¨¡å‹å¤±è´¥: {e}")
            continue
        
        # åœ¨æ‰€æœ‰åœºæ™¯ä¸‹è¯„ä¼°
        algorithm_results = []
        for scenario in test_scenarios:
            result = evaluate_agent_in_scenario(agent, algorithm, scenario, num_episodes)
            algorithm_results.append(result)
        
        comparison_results[algorithm] = algorithm_results
    
    return comparison_results


# ========== ç»“æœå¯è§†åŒ– ==========

def visualize_cross_scenario_results(results: Dict, save_dir: str):
    """
    å¯è§†åŒ–è·¨åœºæ™¯è¯„ä¼°ç»“æœ
    
    ã€ç”Ÿæˆå›¾è¡¨ã€‘
    1. å„åœºæ™¯æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
    2. æ³›åŒ–èƒ½åŠ›é›·è¾¾å›¾
    3. æ€§èƒ½åˆ†å¸ƒç®±çº¿å›¾
    4. åœºæ™¯æ•æ„Ÿæ€§åˆ†æ
    """
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    matplotlib.rcParams['axes.unicode_minus'] = False
    
    os.makedirs(save_dir, exist_ok=True)
    
    # å¦‚æœæ˜¯æ³›åŒ–æµ‹è¯•ç»“æœï¼ˆåŒ…å«å¤šä¸ªåœºæ™¯é›†ï¼‰
    if isinstance(next(iter(results.values())), list) and \
       isinstance(next(iter(results.values()))[0], dict) and \
       'scenario_name' in next(iter(results.values()))[0]:
        
        # 1. å„åœºæ™¯é›†çš„æ€§èƒ½å¯¹æ¯”
        for set_name, set_results in results.items():
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'åœºæ™¯é›†: {set_name}', fontsize=16, fontweight='bold')
            
            scenario_names = [r['scenario_name'] for r in set_results]
            
            # æ—¶å»¶å¯¹æ¯”
            ax = axes[0, 0]
            delays = [r['avg_delay'] for r in set_results]
            delay_stds = [r['std_delay'] for r in set_results]
            ax.bar(scenario_names, delays, yerr=delay_stds, capsize=5, alpha=0.7, color='steelblue')
            ax.set_ylabel('å¹³å‡æ—¶å»¶ (s)')
            ax.set_title('æ—¶å»¶å¯¹æ¯”')
            ax.tick_params(axis='x', rotation=45)
            ax.grid(axis='y', alpha=0.3)
            
            # èƒ½è€—å¯¹æ¯”
            ax = axes[0, 1]
            energies = [r['avg_energy'] for r in set_results]
            energy_stds = [r['std_energy'] for r in set_results]
            ax.bar(scenario_names, energies, yerr=energy_stds, capsize=5, alpha=0.7, color='coral')
            ax.set_ylabel('æ€»èƒ½è€— (J)')
            ax.set_title('èƒ½è€—å¯¹æ¯”')
            ax.tick_params(axis='x', rotation=45)
            ax.grid(axis='y', alpha=0.3)
            
            # å®Œæˆç‡å¯¹æ¯”
            ax = axes[1, 0]
            completion = [r['avg_completion_rate'] * 100 for r in set_results]
            completion_stds = [r['std_completion_rate'] * 100 for r in set_results]
            ax.bar(scenario_names, completion, yerr=completion_stds, capsize=5, alpha=0.7, color='seagreen')
            ax.set_ylabel('å®Œæˆç‡ (%)')
            ax.set_title('ä»»åŠ¡å®Œæˆç‡å¯¹æ¯”')
            ax.tick_params(axis='x', rotation=45)
            ax.grid(axis='y', alpha=0.3)
            ax.set_ylim([0, 105])
            
            # å¥–åŠ±å¯¹æ¯”
            ax = axes[1, 1]
            rewards = [r['avg_reward'] for r in set_results]
            reward_stds = [r['std_reward'] for r in set_results]
            ax.bar(scenario_names, rewards, yerr=reward_stds, capsize=5, alpha=0.7, color='mediumpurple')
            ax.set_ylabel('å¹³å‡å¥–åŠ±')
            ax.set_title('å¥–åŠ±å¯¹æ¯”')
            ax.tick_params(axis='x', rotation=45)
            ax.grid(axis='y', alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(os.path.join(save_dir, f'scenario_comparison_{set_name}.png'), dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ“ ä¿å­˜å›¾è¡¨: scenario_comparison_{set_name}.png")


def save_results_to_file(results: Dict, filepath: str):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {filepath}")


# ========== å‘½ä»¤è¡Œæ¥å£ ==========

def parse_args():
    parser = argparse.ArgumentParser(description='è·¨åœºæ™¯è¯„ä¼°å·¥å…·')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', type=str, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--algorithm', type=str, default='TD3',
                       choices=['TD3', 'SAC', 'DDPG', 'PPO', 'DQN'],
                       help='ç®—æ³•åç§°')
    
    # è¯„ä¼°æ¨¡å¼
    parser.add_argument('--generalization-test', action='store_true',
                       help='è¿è¡Œå®Œæ•´æ³›åŒ–èƒ½åŠ›æµ‹è¯•')
    parser.add_argument('--compare', action='store_true',
                       help='å¤šç®—æ³•å¯¹æ¯”æ¨¡å¼')
    parser.add_argument('--algorithms', type=str, nargs='+',
                       default=['TD3', 'SAC', 'DDPG'],
                       help='å¯¹æ¯”çš„ç®—æ³•åˆ—è¡¨')
    
    # åœºæ™¯å‚æ•°
    parser.add_argument('--scenario-set', type=str, default='vehicle_density',
                       choices=list(SCENARIO_SETS.keys()) + ['all'],
                       help='åœºæ™¯é›†åç§°')
    parser.add_argument('--num-vehicles', type=int, help='è‡ªå®šä¹‰è½¦è¾†æ•°')
    parser.add_argument('--arrival-rate', type=float, help='è‡ªå®šä¹‰ä»»åŠ¡åˆ°è¾¾ç‡')
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument('--eval-episodes', type=int, default=20,
                       help='æ¯ä¸ªåœºæ™¯çš„è¯„ä¼°è½®æ¬¡')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output-dir', type=str, default='results/cross_scenario',
                       help='ç»“æœä¿å­˜ç›®å½•')
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(args.output_dir, timestamp)
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\n{'='*80}")
    print(f"ğŸš€ è·¨åœºæ™¯è¯„ä¼°å·¥å…·")
    print(f"{'='*80}\n")
    
    # ========== æ¨¡å¼1: å¤šç®—æ³•å¯¹æ¯” ==========
    if args.compare:
        results = compare_algorithms_cross_scenario(
            algorithms=args.algorithms,
            scenario_set=args.scenario_set,
            num_episodes=args.eval_episodes
        )
        
        # ä¿å­˜ç»“æœ
        result_file = os.path.join(output_dir, 'algorithm_comparison.json')
        save_results_to_file(results, result_file)
        
        # å¯è§†åŒ–
        visualize_cross_scenario_results(results, output_dir)
    
    # ========== æ¨¡å¼2: æ³›åŒ–èƒ½åŠ›æµ‹è¯• ==========
    elif args.generalization_test:
        if not args.model:
            print("âŒ é”™è¯¯: æ³›åŒ–æµ‹è¯•éœ€è¦æŒ‡å®š--modelå‚æ•°")
            return
        
        state_dim, action_dim = get_state_action_signature(args.algorithm)
        agent = load_trained_agent(args.algorithm, args.model, state_dim, action_dim)
        
        # è¿è¡Œæ³›åŒ–æµ‹è¯•
        results = run_generalization_test(agent, args.algorithm, args.eval_episodes)
        
        # ä¿å­˜ç»“æœ
        result_file = os.path.join(output_dir, f'generalization_test_{args.algorithm.lower()}.json')
        save_results_to_file(results, result_file)
        
        # å¯è§†åŒ–
        visualize_cross_scenario_results(results, output_dir)
    
    # ========== æ¨¡å¼3: å•åœºæ™¯è¯„ä¼° ==========
    else:
        if not args.model:
            print("âŒ é”™è¯¯: éœ€è¦æŒ‡å®š--modelå‚æ•°")
            return
        
        state_dim, action_dim = get_state_action_signature(args.algorithm)
        agent = load_trained_agent(args.algorithm, args.model, state_dim, action_dim)
        
        # è‡ªå®šä¹‰åœºæ™¯
        if args.num_vehicles or args.arrival_rate:
            params = {}
            if args.num_vehicles:
                params['num_vehicles'] = args.num_vehicles
            if args.arrival_rate:
                params['task_arrival_rate'] = args.arrival_rate
            
            scenario = ScenarioConfig(
                name="custom_scenario",
                description="è‡ªå®šä¹‰åœºæ™¯",
                params=params
            )
            
            result = evaluate_agent_in_scenario(agent, args.algorithm, scenario, args.eval_episodes)
            
            # ä¿å­˜ç»“æœ
            result_file = os.path.join(output_dir, 'custom_scenario_result.json')
            save_results_to_file(result, result_file)
        
        # ä½¿ç”¨é¢„å®šä¹‰åœºæ™¯é›†
        else:
            scenarios = SCENARIO_SETS[args.scenario_set]
            results = []
            
            for scenario in scenarios:
                result = evaluate_agent_in_scenario(agent, args.algorithm, scenario, args.eval_episodes)
                results.append(result)
            
            # ä¿å­˜ç»“æœ
            result_file = os.path.join(output_dir, f'{args.scenario_set}_results.json')
            save_results_to_file({'scenarios': results}, result_file)
            
            # å¯è§†åŒ–
            visualize_cross_scenario_results({args.scenario_set: results}, output_dir)
    
    print(f"\n{'='*80}")
    print(f"âœ… è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()




