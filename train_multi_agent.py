"""
å¤šæ™ºèƒ½ä½“ç®—æ³•è®­ç»ƒè„šæœ¬
æ”¯æŒMATD3ã€MADDPGã€QMIXã€MAPPOã€SAC-MAç­‰ç®—æ³•çš„è®­ç»ƒå’Œæ¯”è¾ƒ

ä½¿ç”¨æ–¹æ³•:
python train_multi_agent.py --algorithm MATD3 --episodes 200
python train_multi_agent.py --algorithm MADDPG --episodes 200  
python train_multi_agent.py --algorithm QMIX --episodes 200
python train_multi_agent.py --algorithm MAPPO --episodes 200
python train_multi_agent.py --algorithm SAC-MA --episodes 200
python train_multi_agent.py --compare --episodes 200  # æ¯”è¾ƒæ‰€æœ‰ç®—æ³•
"""
# æ€§èƒ½ä¼˜åŒ– - å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰
try:
    from tools.performance_optimization import *
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥æ€§èƒ½ä¼˜åŒ–æ¨¡å—")
    OPTIMIZED_BATCH_SIZES = {}
    PARALLEL_ENVS = 1
    NUM_WORKERS = 0
import os
import argparse
import numpy as np
import matplotlib.pyplot as plt
import json
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from evaluation.test_complete_system import CompleteSystemSimulator
from utils import MovingAverage
from config import config

# å¯¼å…¥å„ç§ç®—æ³•
from algorithms.matd3 import MATD3Environment
from algorithms.maddpg import MADDPGEnvironment
from algorithms.qmix import QMIXEnvironment
from algorithms.mappo import MAPPOEnvironment
from algorithms.sac_ma import SACMAEnvironment


def generate_timestamp() -> str:
    """ç”Ÿæˆæ—¶é—´æˆ³"""
    if config.experiment.use_timestamp:
        return datetime.now().strftime(config.experiment.timestamp_format)
    else:
        return ""

def get_timestamped_filename(base_name: str, extension: str = ".json") -> str:
    """è·å–å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å"""
    timestamp = generate_timestamp()
    if timestamp:
        name_parts = base_name.split('.')
        if len(name_parts) > 1:
            base = '.'.join(name_parts[:-1])
            return f"{base}_{timestamp}{extension}"
        else:
            return f"{base_name}_{timestamp}{extension}"
    else:
        return f"{base_name}{extension}"


class MultiAgentTrainingEnvironment:
    """å¤šæ™ºèƒ½ä½“è®­ç»ƒç¯å¢ƒåŸºç±»"""
    
    def __init__(self, algorithm: str):
        self.algorithm = algorithm.upper()
        self.simulator = CompleteSystemSimulator()
        
        # è·å–ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°
        self.optimized_batch_size = self._get_optimized_batch_size()
        print(f"ğŸš€ ä½¿ç”¨ä¼˜åŒ–æ‰¹æ¬¡å¤§å°: {self.optimized_batch_size}")
        
        # æ ¹æ®ç®—æ³•åˆ›å»ºç›¸åº”ç¯å¢ƒ
        if self.algorithm == "MATD3":
            self.agent_env = MATD3Environment()
        elif self.algorithm == "MADDPG":
            self.agent_env = MADDPGEnvironment()
        elif self.algorithm == "QMIX":
            self.agent_env = QMIXEnvironment()
        elif self.algorithm == "MAPPO":
            self.agent_env = MAPPOEnvironment(action_space="continuous")
        elif self.algorithm == "SAC-MA":
            self.agent_env = SACMAEnvironment()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_losses = {}
        self.episode_metrics = {
            'avg_task_delay': [],
            'total_energy_consumption': [],
            'task_completion_rate': [],
            'cache_hit_rate': [],
            'migration_success_rate': [],
            'data_loss_rate': []
        }
        
        # æ€§èƒ½è¿½è¸ªå™¨
        self.performance_tracker = {
            'recent_rewards': MovingAverage(100),
            'recent_delays': MovingAverage(100),
            'recent_energy': MovingAverage(100),
            'recent_completion': MovingAverage(100)
        }
        
        print(f"âœ“ {self.algorithm}è®­ç»ƒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        
        # è·å–æ™ºèƒ½ä½“æ•°é‡ï¼ˆå®‰å…¨æ–¹å¼ï¼‰
        agent_count = 3  # é»˜è®¤å€¼
        try:
            if hasattr(self.agent_env, 'agents'):
                agents_attr = getattr(self.agent_env, 'agents', None)
                if agents_attr is not None:
                    if hasattr(agents_attr, '__len__'):
                        agent_count = len(agents_attr)
            elif hasattr(self.agent_env, 'num_agents'):
                agent_count = getattr(self.agent_env, 'num_agents', 3)
        except (AttributeError, TypeError, Exception):
            # å¦‚æœæ— æ³•è®¿é—®ï¼Œä½¿ç”¨é»˜è®¤å€¼
            agent_count = 3
        
        print(f"âœ“ æ™ºèƒ½ä½“æ•°é‡: {agent_count}")
    
    def _get_optimized_batch_size(self) -> int:
        """è·å–ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°"""
        try:
            return OPTIMIZED_BATCH_SIZES.get(self.algorithm, 256)
        except (NameError, AttributeError):
            # å¦‚æœä¼˜åŒ–æ¨¡å—æœªåŠ è½½ï¼Œè¿”å›é»˜è®¤å€¼
            default_sizes = {
                'MATD3': 256, 'MADDPG': 256, 'MAPPO': 256, 
                'QMIX': 32, 'SAC-MA': 256
            }
            return default_sizes.get(self.algorithm, 256)
    
    def reset_environment(self) -> Dict[str, np.ndarray]:
        """é‡ç½®ç¯å¢ƒå¹¶è¿”å›åˆå§‹çŠ¶æ€"""
        # é‡ç½®ä»¿çœŸå™¨çŠ¶æ€
        self.simulator._setup_scenario()
        
        # ç‰¹æ®Šå¤„ç†éœ€è¦é‡ç½®éšè—çŠ¶æ€çš„ç®—æ³•
        if self.algorithm == "QMIX":
            try:
                if hasattr(self.agent_env, 'reset_hidden_states'):
                    self.agent_env.reset_hidden_states()
            except (AttributeError, Exception):
                # å¦‚æœæ— æ³•è®¿é—®ï¼Œå¿½ç•¥
                pass
        
        # æ”¶é›†ç³»ç»ŸçŠ¶æ€
        node_states = {}
        
        # è½¦è¾†çŠ¶æ€
        for i, vehicle in enumerate(self.simulator.vehicles):
            # ç”Ÿæˆè½¦è¾†çŠ¶æ€
            vehicle_state = np.array([
                vehicle['position'][0] / 1000,  # å½’ä¸€åŒ–ä½ç½®x
                vehicle['position'][1] / 1000,  # å½’ä¸€åŒ–ä½ç½®y
                vehicle['velocity'] / 50,       # å½’ä¸€åŒ–é€Ÿåº¦
                len(vehicle.get('tasks', [])) / 10,  # å½’ä¸€åŒ–ä»»åŠ¡æ•°
                vehicle.get('energy_consumed', 0) / 1000  # å½’ä¸€åŒ–èƒ½è€—
            ])
            node_states[f'vehicle_{i}'] = vehicle_state
        
        # RSUçŠ¶æ€
        for i, rsu in enumerate(self.simulator.rsus):
            rsu_state = np.array([
                rsu['position'][0] / 1000,  # å½’ä¸€åŒ–ä½ç½®x
                rsu['position'][1] / 1000,  # å½’ä¸€åŒ–ä½ç½®y
                len(rsu.get('cache', {})) / rsu.get('cache_capacity', 100),  # ç¼“å­˜åˆ©ç”¨ç‡
                len(rsu.get('computation_queue', [])) / 10,  # å½’ä¸€åŒ–é˜Ÿåˆ—é•¿åº¦
                rsu.get('energy_consumed', 0) / 1000  # å½’ä¸€åŒ–èƒ½è€—
            ])
            node_states[f'rsu_{i}'] = rsu_state
        
        # UAVçŠ¶æ€
        for i, uav in enumerate(self.simulator.uavs):
            uav_state = np.array([
                uav['position'][0] / 1000,  # å½’ä¸€åŒ–ä½ç½®x
                uav['position'][1] / 1000,  # å½’ä¸€åŒ–ä½ç½®y
                uav['position'][2] / 200,   # å½’ä¸€åŒ–é«˜åº¦
                len(uav.get('cache', {})) / uav.get('cache_capacity', 100),  # ç¼“å­˜åˆ©ç”¨ç‡
                uav.get('energy_consumed', 0) / 1000  # å½’ä¸€åŒ–èƒ½è€—
            ])
            node_states[f'uav_{i}'] = uav_state
        
        # åˆå§‹ç³»ç»ŸæŒ‡æ ‡
        system_metrics = {
            'avg_task_delay': 0.0,
            'total_energy_consumption': 0.0,
            'data_loss_rate': 0.0,
            'cache_hit_rate': 0.0,
            'migration_success_rate': 0.0
        }
        
        # è·å–åˆå§‹çŠ¶æ€å‘é‡ - ä¸ºMATD3ç­‰ç®—æ³•åˆ›å»ºå…¼å®¹çš„çŠ¶æ€å¯¹è±¡
        if self.algorithm == 'MATD3':
            # MATD3éœ€è¦ç‰¹æ®Šçš„çŠ¶æ€å¯¹è±¡æ ¼å¼
            states = self._get_matd3_compatible_states(node_states, system_metrics)
        else:
            states = self.agent_env.get_state_vector(node_states, system_metrics)
        
        return states
    
    def _get_matd3_compatible_states(self, node_states: Dict, system_metrics: Dict) -> Dict:
        """ä¸ºMATD3ç®—æ³•åˆ›å»ºå…¼å®¹çš„çŠ¶æ€å¯¹è±¡"""
        # åˆ›å»ºç®€å•çš„çŠ¶æ€å¯¹è±¡ç±»
        class SimpleNodeState:
            def __init__(self, node_type: str, load_factor: float = 0.5):
                self.node_type = SimpleNodeType(node_type)
                self.load_factor = load_factor
        
        class SimpleNodeType:
            def __init__(self, value: str):
                self.value = value
        
        # è½¬æ¢node_statesä¸ºMATD3æœŸæœ›çš„æ ¼å¼
        compatible_states = {}
        
        # å¤„ç†è½¦è¾†çŠ¶æ€
        for i in range(len([k for k in node_states.keys() if k.startswith('vehicle_')])):
            compatible_states[f'vehicle_{i}'] = SimpleNodeState('vehicle', 0.3)
        
        # å¤„ç†RSUçŠ¶æ€  
        for i in range(len([k for k in node_states.keys() if k.startswith('rsu_')])):
            compatible_states[f'rsu_{i}'] = SimpleNodeState('rsu', 0.5)
        
        # å¤„ç†UAVçŠ¶æ€
        for i in range(len([k for k in node_states.keys() if k.startswith('uav_')])):
            compatible_states[f'uav_{i}'] = SimpleNodeState('uav', 0.4)
        
        # ä½¿ç”¨MATD3çš„get_state_vectoræ–¹æ³•
        return self.agent_env.get_state_vector(compatible_states, system_metrics)
    
    def step(self, actions: Dict, states: Dict) -> Tuple[Dict, Dict, Dict, Dict]:
        """æ‰§è¡Œä¸€æ­¥ä»¿çœŸ"""
        # æ‰§è¡Œä»¿çœŸæ­¥éª¤
        step_stats = self.simulator.run_simulation_step(0)
        
        # æ”¶é›†ä¸‹ä¸€æ­¥çŠ¶æ€
        node_states = {}
        
        # è½¦è¾†çŠ¶æ€
        for i, vehicle in enumerate(self.simulator.vehicles):
            vehicle_state = np.array([
                vehicle['position'][0] / 1000,
                vehicle['position'][1] / 1000,
                vehicle['velocity'] / 50,
                len(vehicle.get('tasks', [])) / 10,
                vehicle.get('energy_consumed', 0) / 1000
            ])
            node_states[f'vehicle_{i}'] = vehicle_state
        
        # RSUçŠ¶æ€
        for i, rsu in enumerate(self.simulator.rsus):
            rsu_state = np.array([
                rsu['position'][0] / 1000,
                rsu['position'][1] / 1000,
                len(rsu.get('cache', {})) / rsu.get('cache_capacity', 100),
                len(rsu.get('computation_queue', [])) / 10,
                rsu.get('energy_consumed', 0) / 1000
            ])
            node_states[f'rsu_{i}'] = rsu_state
        
        # UAVçŠ¶æ€
        for i, uav in enumerate(self.simulator.uavs):
            uav_state = np.array([
                uav['position'][0] / 1000,
                uav['position'][1] / 1000,
                uav['position'][2] / 200,
                len(uav.get('cache', {})) / uav.get('cache_capacity', 100),
                uav.get('energy_consumed', 0) / 1000
            ])
            node_states[f'uav_{i}'] = uav_state
        
        # è®¡ç®—ç³»ç»ŸæŒ‡æ ‡
        system_metrics = self._calculate_system_metrics(step_stats)
        
        # è·å–ä¸‹ä¸€çŠ¶æ€ - ä¸ºMATD3ç­‰ç®—æ³•åˆ›å»ºå…¼å®¹çš„çŠ¶æ€å¯¹è±¡
        if self.algorithm == 'MATD3':
            # MATD3éœ€è¦ç‰¹æ®Šçš„çŠ¶æ€å¯¹è±¡æ ¼å¼
            next_states = self._get_matd3_compatible_states(node_states, system_metrics)
        else:
            next_states = self.agent_env.get_state_vector(node_states, system_metrics)
        
        # è®¡ç®—å¥–åŠ±
        rewards = self._calculate_rewards(system_metrics)
        
        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        dones = {agent_id: False for agent_id in actions.keys()}
        
        # é™„åŠ ä¿¡æ¯
        info = {
            'step_stats': step_stats,
            'system_metrics': system_metrics
        }
        
        return next_states, rewards, dones, info
    
    def _calculate_system_metrics(self, step_stats: Dict) -> Dict:
        """è®¡ç®—ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ - æ”¹è¿›ç‰ˆæœ¬ï¼Œæ›´å‡†ç¡®çš„æŒ‡æ ‡è®¡ç®—"""
        # å¯¼å…¥éªŒè¯å‡½æ•°
        def local_validate_energy(energy, context):
            try:
                from utils.energy_validator import validate_energy_consumption as validate_energy_func
                energy_data = {'total_system': [energy]}
                result = validate_energy_func(energy_data)
                is_valid = result['is_valid']
                corrected_energy = min(energy, 2000.0) if not is_valid else energy
                warning = "; ".join(result['errors'][:1]) if result['errors'] else ""
                return is_valid, corrected_energy, warning
            except ImportError:
                # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç®€å•éªŒè¯
                return energy <= 2000.0, min(energy, 2000.0), ""
        
        # æ—¶å»¶éªŒè¯å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘ä¸å¿…è¦çš„è­¦å‘Š
        def validate_delay_calculation(delay_value, processed_tasks, total_delay):
            """éªŒè¯æ—¶å»¶è®¡ç®—çš„åˆç†æ€§"""
            if processed_tasks <= 0:
                # ä¸è¾“å‡ºè­¦å‘Šï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µï¼ˆæŸäº›æ—¶é—´æ­¥æ²¡æœ‰ä»»åŠ¡å¤„ç†ï¼‰
                return 0.0, ""
            
            if total_delay <= 0:
                return 0.0, ""
            
            calculated_delay = total_delay / processed_tasks
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰é™å€¼
            if not np.isfinite(calculated_delay):
                return 1.0, f"æ—¶å»¶è®¡ç®—ç»“æœéæœ‰é™å€¼: {calculated_delay}, ä¿®æ­£ä¸º1.0s"
            
            # æ£€æŸ¥æ˜¯å¦åœ¨åˆç†èŒƒå›´å†… (0.001s - 10s)
            if calculated_delay < 0.001:
                return 0.001, f"æ—¶å»¶è¿‡å°: {calculated_delay:.6f}s, ä¿®æ­£ä¸º0.001s"
            elif calculated_delay > 10.0:
                return 5.0, f"æ—¶å»¶è¿‡å¤§: {calculated_delay:.2f}s, ä¿®æ­£ä¸º5.0s"
            
            return calculated_delay, ""
        
        # å®‰å…¨è·å–ç»Ÿè®¡æ•°æ®ï¼Œé¿å…KeyError
        generated_tasks = step_stats.get('generated_tasks', 0)
        processed_tasks = step_stats.get('processed_tasks', 0)
        dropped_tasks = step_stats.get('dropped_tasks', 0)
        total_delay = step_stats.get('total_delay', 0.0)
        total_energy = step_stats.get('total_energy', 0.0)
        cache_hits = step_stats.get('cache_hits', 0)
        cache_misses = step_stats.get('cache_misses', 0)
        
        # ä»»åŠ¡å®Œæˆç‡ï¼šæˆåŠŸå¤„ç†çš„ä»»åŠ¡å ç”Ÿæˆä»»åŠ¡çš„æ¯”ä¾‹
        if generated_tasks > 0:
            completion_rate = processed_tasks / generated_tasks
            data_loss_rate = dropped_tasks / generated_tasks
        else:
            completion_rate = 0.0
            data_loss_rate = 0.0
        
        # å¹³å‡æ—¶å»¶ï¼šä½¿ç”¨éªŒè¯å‡½æ•°ç¡®ä¿è®¡ç®—æ­£ç¡®
        avg_task_delay, delay_warning = validate_delay_calculation(0, processed_tasks, total_delay)
        # åªåœ¨æœ‰å®é™…é—®é¢˜æ—¶è¾“å‡ºè­¦å‘Š
        if delay_warning and processed_tasks > 0:
            print(f"âš ï¸ æ—¶å»¶è®¡ç®—ä¿®æ­£: {delay_warning}")
        
        # ç¼“å­˜å‘½ä¸­ç‡
        cache_requests = cache_hits + cache_misses
        if cache_requests > 0:
            cache_hit_rate = cache_hits / cache_requests
        else:
            cache_hit_rate = 0.0
        
        # èƒ½è€—éªŒè¯ï¼šä½¿ç”¨ä¸“é—¨çš„éªŒè¯å‡½æ•°
        is_valid, corrected_energy, warning = local_validate_energy(total_energy, "slot")
        if warning:
            print(warning)
        total_energy = corrected_energy
        
        # ç³»ç»Ÿè´Ÿè½½æ¯”ä¾‹
        system_load_ratio = min(1.0, generated_tasks / max(1, 50))  # å‡è®¾ç³»ç»Ÿæœ€å¤§å¤„ç†èƒ½åŠ›ä¸º50ä»»åŠ¡/æ—¶éš™
        
        # å¸¦å®½åˆ©ç”¨ç‡ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        avg_bandwidth_utilization = min(1.0, processed_tasks / max(1, 30))
        
        # é›†æˆå¢å¼ºè¿ç§»ç®¡ç†å™¨
        if not hasattr(self, 'migration_manager'):
            from utils.enhanced_migration import EnhancedTaskMigrationManager
            self.migration_manager = EnhancedTaskMigrationManager()
        
        # æ¨¡æ‹ŸèŠ‚ç‚¹çŠ¶æ€ä¾›è¿ç§»ç®¡ç†å™¨ä½¿ç”¨
        migration_node_states = {}
        migration_positions = {}
        
        # åˆ›å»ºç®€åŒ–çš„èŠ‚ç‚¹çŠ¶æ€ç”¨äºè¿ç§»
        from models.data_structures import NodeState, NodeType, Position
        for i in range(len(self.simulator.vehicles)):
            vehicle = self.simulator.vehicles[i]
            state = NodeState(
                node_id=f'vehicle_{i}',
                node_type=NodeType.VEHICLE,
                position=Position(vehicle['position'][0], vehicle['position'][1], 0),
                load_factor=len(vehicle.get('tasks', [])) / 10.0
            )
            migration_node_states[f'vehicle_{i}'] = state
            migration_positions[f'vehicle_{i}'] = state.position
        
        for i in range(len(self.simulator.rsus)):
            rsu = self.simulator.rsus[i]
            state = NodeState(
                node_id=f'rsu_{i}',
                node_type=NodeType.RSU,
                position=Position(rsu['position'][0], rsu['position'][1], 0),
                load_factor=len(rsu.get('computation_queue', [])) / 10.0
            )
            migration_node_states[f'rsu_{i}'] = state
            migration_positions[f'rsu_{i}'] = state.position
        
        for i in range(len(self.simulator.uavs)):
            uav = self.simulator.uavs[i]
            state = NodeState(
                node_id=f'uav_{i}',
                node_type=NodeType.UAV,
                position=Position(uav['position'][0], uav['position'][1], uav['position'][2]),
                load_factor=len(uav.get('cache', {})) / uav.get('cache_capacity', 100)
            )
            # è®¾ç½®UAVç”µæ± ç”µé‡
            setattr(state, 'battery_level', uav.get('battery_level', 0.8))
            migration_node_states[f'uav_{i}'] = state
            migration_positions[f'uav_{i}'] = state.position
        
        # è¿è¡Œè¿ç§»ç®¡ç†å™¨æ­¥éª¤
        migration_step_stats = self.migration_manager.step(
            migration_node_states, 
            migration_positions, 
            {}  # ç®€åŒ–çš„ä»»åŠ¡çŠ¶æ€
        )
        
        # è·å–åŠ¨æ€è¿ç§»æˆåŠŸç‡
        dynamic_migration_rate = migration_step_stats.get('dynamic_success_rate', 0.8)
        
        return {
            'avg_task_delay': max(0.0, avg_task_delay),
            'total_energy_consumption': max(0.0, total_energy),
            'data_loss_rate': np.clip(data_loss_rate, 0.0, 1.0),
            'task_completion_rate': np.clip(completion_rate, 0.0, 1.0),
            'cache_hit_rate': np.clip(cache_hit_rate, 0.0, 1.0),
            'migration_success_rate': dynamic_migration_rate,
            'system_load_ratio': system_load_ratio,
            'avg_bandwidth_utilization': avg_bandwidth_utilization,
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            'debug_info': {
                'generated_tasks': generated_tasks,
                'processed_tasks': processed_tasks,
                'dropped_tasks': dropped_tasks,
                'cache_requests': cache_requests,
                'energy_corrected': not is_valid
            }
        }
    
    def _calculate_rewards(self, system_metrics: Dict) -> Dict[str, float]:
        """è®¡ç®—æ™ºèƒ½ä½“å¥–åŠ± - ä½¿ç”¨æ ‡å‡†åŒ–å¥–åŠ±å‡½æ•°"""
        from utils.standardized_reward import calculate_standardized_reward
        
        rewards = {}
        agent_ids = ['vehicle_agent', 'rsu_agent', 'uav_agent']
        
        # ä¸ºä¸åŒæ™ºèƒ½ä½“è®¡ç®—æ ‡å‡†åŒ–å¥–åŠ±
        for agent_id in agent_ids:
            rewards[agent_id] = calculate_standardized_reward(
                system_metrics, 
                agent_type=agent_id
            )
        
        return rewards
    
    def run_episode(self, episode: int, max_steps: Optional[int] = None) -> Dict:
        """è¿è¡Œä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒè½®æ¬¡ - æ”¹è¿›ç‰ˆæœ¬ï¼Œå¢å¼ºç¨³å®šæ€§"""
        # ä½¿ç”¨é…ç½®ä¸­çš„æœ€å¤§æ­¥æ•°
        if max_steps is None:
            max_steps = config.experiment.max_steps_per_episode
        
        # é‡ç½®ç¯å¢ƒ
        states = self.reset_environment()
        
        # éªŒè¯çŠ¶æ€æœ‰æ•ˆæ€§
        if not states or any(state is None for state in states.values()):
            print(f"âš ï¸ Episode {episode}: çŠ¶æ€é‡ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤çŠ¶æ€")
            states = {agent_id: np.zeros(20, dtype=np.float32) for agent_id in ['vehicle_agent', 'rsu_agent', 'uav_agent']}
        
        episode_reward = {agent_id: 0.0 for agent_id in states.keys()}
        episode_info = {}
        # åˆå§‹åŒ–infoå’Œstepå˜é‡
        info = {'system_metrics': {}}
        step = 0
        
        # è®°å½•å‰ä¸€æ­¥çš„ç³»ç»ŸæŒ‡æ ‡ï¼Œç”¨äºå¥–åŠ±è®¡ç®—
        prev_metrics = {
            'avg_task_delay': 1.0,
            'total_energy_consumption': 0.0,
            'task_completion_rate': 0.0,
            'cache_hit_rate': 0.0,
            'data_loss_rate': 0.0
        }
        
        # MAPPOéœ€è¦ç‰¹æ®Šå¤„ç†
        if self.algorithm == "MAPPO":
            return self._run_mappo_episode(episode, max_steps)
        
        for step in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œ - å¤„ç†ä¸åŒç®—æ³•çš„è¿”å›ç±»å‹
            if hasattr(self.agent_env, 'get_actions'):
                result = self.agent_env.get_actions(states, training=True)
                if isinstance(result, tuple) and len(result) == 2:
                    # MAPPOç­‰è¿”å›(actions, log_probs)çš„ç®—æ³•
                    actions, _ = result
                else:
                    # å…¶ä»–ç®—æ³•åªè¿”å›actions
                    actions = result
            else:
                # é»˜è®¤éšæœºåŠ¨ä½œ
                actions = {agent_id: 0 for agent_id in states.keys()}
            
            # æ‰§è¡ŒåŠ¨ä½œ - ç¡®ä¿actionsæ˜¯å­—å…¸æ ¼å¼
            if isinstance(actions, tuple):
                # å¦‚æœæ˜¯å…ƒç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆå®‰å…¨æ–¹å¼ï¼‰
                if len(actions) > 0:
                    actions = actions[0]  # type: ignore # è¿™é‡Œæ˜¯å…ƒç»„ç´¢å¼•
                else:
                    actions = {}
            
            if not isinstance(actions, dict):
                # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                agent_ids = list(states.keys())
                if len(agent_ids) > 0:
                    actions = {agent_ids[0]: actions}
                else:
                    actions = {'default_agent': actions}
            
            next_states, rewards, dones, info = self.step(actions, states)
            
            # è®­ç»ƒæ™ºèƒ½ä½“
            if self.algorithm == "MAPPO":
                # MAPPOä½¿ç”¨ç¼“å­˜çš„ç»éªŒï¼Œä¸åœ¨æ­¤å¤„è®­ç»ƒ
                # è·å–å…¨å±€çŠ¶æ€ï¼ˆå®‰å…¨æ–¹å¼ï¼‰
                if self.algorithm == "MAPPO":
                    try:
                        if hasattr(self.agent_env, 'get_global_state'):
                            global_state = self.agent_env.get_global_state(states)
                        else:
                            # åˆ›å»ºå…¨å±€çŠ¶æ€çš„ç®€å•å®ç°
                            global_state = np.concatenate([state.flatten() for state in states.values()])
                    except (AttributeError, Exception):
                        global_state = np.concatenate([state.flatten() for state in states.values()])
                else:
                    global_state = None
                
                # å­˜å‚¨ç»éªŒï¼ˆå®‰å…¨æ–¹å¼ï¼‰
                if self.algorithm == "MAPPO":
                    try:
                        if hasattr(self.agent_env, 'store_experience'):
                            # ç¡®ä¿actionsæ˜¯æ­£ç¡®çš„æ ¼å¼
                            if isinstance(actions, dict):
                                actions_array = {k: np.array(v) if not isinstance(v, np.ndarray) else v for k, v in actions.items()}
                            else:
                                actions_array = actions
                            log_probs = {}  # ç©ºçš„log_probså­—å…¸
                            # ä¿®å¤ï¼šä½¿ç”¨Optionalç±»å‹çš„global_state
                            if global_state is not None:
                                self.agent_env.store_experience(states, actions_array, log_probs, rewards, dones, global_state)
                            else:
                                # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„å…¨å±€çŠ¶æ€
                                default_global = np.concatenate([state.flatten() for state in states.values()])
                                self.agent_env.store_experience(states, actions_array, log_probs, rewards, dones, default_global)
                    except (AttributeError, Exception):
                        # å¦‚æœå­˜å‚¨å¤±è´¥ï¼Œå¿½ç•¥
                        pass
            else:
                # å…¶ä»–ç®—æ³•çš„è®­ç»ƒï¼ˆå®‰å…¨æ–¹å¼ï¼‰
                try:
                    if hasattr(self.agent_env, 'train_step'):
                        # ç¡®ä¿actionsæ ¼å¼æ­£ç¡®
                        if isinstance(actions, dict):
                            # å°†åŠ¨ä½œè½¬æ¢ä¸ºé€‚åˆçš„ç±»å‹
                            actions_processed = {}
                            for k, v in actions.items():
                                if isinstance(v, (np.ndarray, list)) and len(np.array(v).shape) == 0:
                                    actions_processed[k] = int(v)
                                elif isinstance(v, np.ndarray):
                                    actions_processed[k] = v
                                else:
                                    actions_processed[k] = v
                        else:
                            actions_processed = actions
                        training_info = self.agent_env.train_step(states, actions_processed, rewards, next_states, dones)
                        episode_info = training_info
                    else:
                        episode_info = {}
                except (AttributeError, Exception):
                    episode_info = {}
            
            # æ›´æ–°çŠ¶æ€
            states = next_states
            
            # ç´¯è®¡å¥–åŠ±
            for agent_id, reward in rewards.items():
                if agent_id in episode_reward:
                    episode_reward[agent_id] += reward
            
            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            if any(dones.values()):
                break
        
        # è®°å½•è½®æ¬¡ç»Ÿè®¡
        avg_reward = np.mean(list(episode_reward.values())) if episode_reward else 0.0
        # infoå·²åœ¨å¾ªç¯ä¸­åˆå§‹åŒ–
        system_metrics = info.get('system_metrics', {})
        
        return {
            'episode_reward': episode_reward,
            'avg_reward': avg_reward,
            'episode_info': episode_info,
            'system_metrics': system_metrics,
            'steps': step
        }
    
    def _run_mappo_episode(self, episode: int, max_steps: int = 100) -> Dict:
        """è¿è¡ŒMAPPOä¸“ç”¨episode"""
        states = self.reset_environment()
        episode_reward = {agent_id: 0.0 for agent_id in states.keys()}
        
        info = {'system_metrics': {}}  # åˆå§‹åŒ–info
        step = 0  # åˆå§‹åŒ–stepå˜é‡
        
        for step in range(max_steps):
            # è·å–åŠ¨ä½œå’Œå¯¹æ•°æ¦‚ç‡
            if hasattr(self.agent_env, 'get_actions'):
                result = self.agent_env.get_actions(states, training=True)
                if isinstance(result, tuple) and len(result) == 2:
                    actions, log_probs = result
                else:
                    actions = result
                    log_probs = {agent_id: 0.0 for agent_id in states.keys()}
            else:
                # é»˜è®¤éšæœºåŠ¨ä½œ
                actions = {agent_id: np.random.rand(10) for agent_id in states.keys()}
                log_probs = {agent_id: 0.0 for agent_id in states.keys()}
            
            # æ‰§è¡ŒåŠ¨ä½œ - ç¡®ä¿actionsæ˜¯å­—å…¸æ ¼å¼
            if isinstance(actions, tuple):
                if len(actions) > 0:
                    actions = actions[0]  # type: ignore # è¿™é‡Œæ˜¯å…ƒç»„ç´¢å¼•
                else:
                    actions = {}
            
            if not isinstance(actions, dict):
                agent_ids = list(states.keys())
                if len(agent_ids) > 0:
                    actions = {agent_ids[0]: actions}
                else:
                    actions = {'default_agent': actions}
            
            next_states, rewards, dones, info = self.step(actions, states)
            
            # å­˜å‚¨ç»éªŒï¼ˆå®‰å…¨æ–¹å¼ï¼‰
            try:
                if hasattr(self.agent_env, 'get_global_state'):
                    global_state = self.agent_env.get_global_state(states)
                else:
                    global_state = np.concatenate([state.flatten() for state in states.values()])
            except (AttributeError, Exception):
                global_state = np.concatenate([state.flatten() for state in states.values()])
            
            try:
                if hasattr(self.agent_env, 'store_experience'):
                    # ç¡®ä¿actionså’Œlog_probsæ ¼å¼æ­£ç¡®
                    actions_array = {k: np.array(v) if not isinstance(v, np.ndarray) else v for k, v in actions.items()}
                    log_probs_dict = {k: float(v) if not isinstance(v, dict) else v for k, v in log_probs.items()}
                    self.agent_env.store_experience(states, actions_array, log_probs_dict, rewards, dones, global_state)
            except (AttributeError, Exception):
                pass
            
            # ç´¯è®¡å¥–åŠ±
            for agent_id, reward in rewards.items():
                if agent_id in episode_reward:
                    episode_reward[agent_id] += reward
            
            states = next_states
            
            if any(dones.values()):
                break
        
        # Episodeç»“æŸåè¿›è¡ŒPPOæ›´æ–°ï¼ˆå®‰å…¨æ–¹å¼ï¼‰
        try:
            if hasattr(self.agent_env, 'update'):
                training_info = self.agent_env.update()
            else:
                training_info = {}
        except (AttributeError, Exception):
            training_info = {}
        
        # stepå·²åœ¨å¾ªç¯ä¸­å®šä¹‰
        
        avg_reward = np.mean(list(episode_reward.values())) if episode_reward else 0.0
        # infoå·²åœ¨å¾ªç¯ä¸­åˆå§‹åŒ–
        system_metrics = info.get('system_metrics', {})
        
        return {
            'episode_reward': episode_reward,
            'avg_reward': avg_reward,
            'episode_info': training_info,
            'system_metrics': system_metrics,
            'steps': step
        }


def train_algorithm(algorithm: str, num_episodes: Optional[int] = None, eval_interval: Optional[int] = None, 
                   save_interval: Optional[int] = None) -> Dict:
    """è®­ç»ƒå•ä¸ªç®—æ³•"""
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    if num_episodes is None:
        num_episodes = config.experiment.num_episodes
    if eval_interval is None:
        eval_interval = config.experiment.eval_interval
    if save_interval is None:
        save_interval = config.experiment.save_interval
    
    print(f"\nğŸš€ å¼€å§‹{algorithm}ç®—æ³•è®­ç»ƒ")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    training_env = MultiAgentTrainingEnvironment(algorithm)
    
    print("è®­ç»ƒé…ç½®:")
    print(f"  ç®—æ³•: {algorithm}")
    print(f"  æ€»è½®æ¬¡: {num_episodes}")
    print(f"  è¯„ä¼°é—´éš”: {eval_interval}")
    print(f"  ä¿å­˜é—´éš”: {save_interval}")
    print("-" * 60)
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(f"results/training/{algorithm.lower()}", exist_ok=True)
    os.makedirs(f"results/models/{algorithm.lower()}", exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    best_avg_reward = float('-inf')
    training_start_time = time.time()
    
    for episode in range(1, num_episodes + 1):
        episode_start_time = time.time()
        
        # è¿è¡Œè®­ç»ƒè½®æ¬¡
        episode_result = training_env.run_episode(episode)
        
        # è®°å½•è®­ç»ƒæ•°æ®
        training_env.episode_rewards.append(episode_result['avg_reward'])
        
        # æ›´æ–°æ€§èƒ½è¿½è¸ªå™¨
        training_env.performance_tracker['recent_rewards'].update(episode_result['avg_reward'])
        
        system_metrics = episode_result['system_metrics']
        training_env.performance_tracker['recent_delays'].update(system_metrics.get('avg_task_delay', 0))
        training_env.performance_tracker['recent_energy'].update(system_metrics.get('total_energy_consumption', 0))
        training_env.performance_tracker['recent_completion'].update(system_metrics.get('task_completion_rate', 0))
        
        # è®°å½•æŒ‡æ ‡
        for metric_name, value in system_metrics.items():
            if metric_name in training_env.episode_metrics:
                training_env.episode_metrics[metric_name].append(value)
        
        episode_time = time.time() - episode_start_time
        
        # å®šæœŸè¾“å‡ºè¿›åº¦
        if episode % 10 == 0:
            avg_reward = training_env.performance_tracker['recent_rewards'].get_average()
            avg_delay = training_env.performance_tracker['recent_delays'].get_average()
            avg_completion = training_env.performance_tracker['recent_completion'].get_average()
            
            print(f"è½®æ¬¡ {episode:4d}/{num_episodes}:")
            print(f"  å¹³å‡å¥–åŠ±: {avg_reward:8.3f}")
            print(f"  å¹³å‡æ—¶å»¶: {avg_delay:8.3f}s")
            print(f"  å®Œæˆç‡:   {avg_completion:8.1%}")
            print(f"  è½®æ¬¡ç”¨æ—¶: {episode_time:6.3f}s")
        
        # è¯„ä¼°æ¨¡å‹
        if episode % eval_interval == 0:
            eval_result = evaluate_model(algorithm, training_env, episode)
            print(f"\nğŸ“Š è½®æ¬¡ {episode} è¯„ä¼°ç»“æœ:")
            print(f"  è¯„ä¼°å¥–åŠ±: {eval_result['avg_reward']:.3f}")
            print(f"  è¯„ä¼°æ—¶å»¶: {eval_result['avg_delay']:.3f}s")
            print(f"  è¯„ä¼°å®Œæˆç‡: {eval_result['completion_rate']:.1%}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if eval_result['avg_reward'] > best_avg_reward:
                best_avg_reward = eval_result['avg_reward']
                training_env.agent_env.save_models(f"results/models/{algorithm.lower()}/best_model")
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (å¥–åŠ±: {best_avg_reward:.3f})")
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if episode % save_interval == 0:
            training_env.agent_env.save_models(f"results/models/{algorithm.lower()}/checkpoint_{episode}")
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: checkpoint_{episode}")
    
    # è®­ç»ƒå®Œæˆ
    total_training_time = time.time() - training_start_time
    print("\n" + "=" * 60)
    print(f"ğŸ‰ {algorithm}è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_training_time/3600:.2f} å°æ—¶")
    print(f"ğŸ† æœ€ä½³å¹³å‡å¥–åŠ±: {best_avg_reward:.3f}")
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    results = save_training_results(algorithm, training_env, total_training_time)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(algorithm, training_env)
    
    return results


def evaluate_model(algorithm: str, training_env: MultiAgentTrainingEnvironment, 
                  episode: int, num_eval_episodes: int = 5) -> Dict:
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    eval_rewards = []
    eval_delays = []
    eval_completions = []
    
    for _ in range(num_eval_episodes):
        states = training_env.reset_environment()
        episode_reward = 0.0
        episode_delay = 0.0
        episode_completion = 0.0
        steps = 0
        
        for step in range(50):  # è¾ƒçŸ­çš„è¯„ä¼°è½®æ¬¡
            if hasattr(training_env.agent_env, 'get_actions'):
                result = training_env.agent_env.get_actions(states, training=False)
                if isinstance(result, tuple):  # MAPPOè¿”å›å…ƒç»„
                    actions = result[0]
                else:
                    actions = result
            else:
                # é»˜è®¤éšæœºåŠ¨ä½œ
                actions = {agent_id: np.random.rand(10) for agent_id in states.keys()}
            
            # ç¡®ä¿actionsæ˜¯å­—å…¸æ ¼å¼
            if isinstance(actions, tuple):
                if len(actions) > 0:
                    actions = actions[0]  # type: ignore # è¿™é‡Œæ˜¯å…ƒç»„ç´¢å¼•
                else:
                    actions = {}
            
            if not isinstance(actions, dict):
                agent_ids = list(states.keys())
                if len(agent_ids) > 0:
                    actions = {agent_ids[0]: actions}
                else:
                    actions = {'default_agent': actions}
            
            next_states, rewards, dones, info = training_env.step(actions, states)
            
            episode_reward += np.mean(list(rewards.values()))
            system_metrics = info['system_metrics']
            episode_delay += system_metrics.get('avg_task_delay', 0)
            episode_completion += system_metrics.get('task_completion_rate', 0)
            steps += 1
            
            states = next_states
            
            if any(dones.values()):
                break
        
        eval_rewards.append(episode_reward / steps)
        eval_delays.append(episode_delay / steps)
        eval_completions.append(episode_completion / steps)
    
    return {
        'avg_reward': np.mean(eval_rewards),
        'avg_delay': np.mean(eval_delays),
        'completion_rate': np.mean(eval_completions)
    }


def save_training_results(algorithm: str, training_env: MultiAgentTrainingEnvironment, 
                         training_time: float) -> Dict:
    """ä¿å­˜è®­ç»ƒç»“æœ"""
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = generate_timestamp()
    
    results = {
        'algorithm': algorithm,
        'timestamp': timestamp,
        'training_start_time': datetime.now().isoformat(),
        'training_config': {
            'num_episodes': len(training_env.episode_rewards),
            'training_time_hours': training_time / 3600,
            'max_steps_per_episode': config.experiment.max_steps_per_episode
        },
        'episode_rewards': training_env.episode_rewards,
        'episode_metrics': training_env.episode_metrics,
        'final_performance': {
            'avg_reward': training_env.performance_tracker['recent_rewards'].get_average(),
            'avg_delay': training_env.performance_tracker['recent_delays'].get_average(),
            'avg_completion': training_env.performance_tracker['recent_completion'].get_average()
        }
    }
    
    # ä½¿ç”¨æ—¶é—´æˆ³æ–‡ä»¶å
    filename = get_timestamped_filename("training_results")
    filepath = f"results/training/{algorithm.lower()}/{filename}"
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ {algorithm}è®­ç»ƒç»“æœå·²ä¿å­˜åˆ° {filepath}")
    
    return results


def plot_training_curves(algorithm: str, training_env: MultiAgentTrainingEnvironment):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    # ä¼ ç»Ÿå¯è§†åŒ–
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # å¥–åŠ±æ›²çº¿
    axes[0, 0].plot(training_env.episode_rewards)
    axes[0, 0].set_title(f'{algorithm} è®­ç»ƒå¥–åŠ±æ›²çº¿')
    axes[0, 0].set_xlabel('è®­ç»ƒè½®æ¬¡')
    axes[0, 0].set_ylabel('å¹³å‡å¥–åŠ±')
    axes[0, 0].grid(True)
    
    # æ—¶å»¶æ›²çº¿
    if 'avg_task_delay' in training_env.episode_metrics and training_env.episode_metrics['avg_task_delay']:
        axes[0, 1].plot(training_env.episode_metrics['avg_task_delay'])
        axes[0, 1].set_title('å¹³å‡ä»»åŠ¡æ—¶å»¶')
        axes[0, 1].set_xlabel('è®­ç»ƒè½®æ¬¡')
        axes[0, 1].set_ylabel('æ—¶å»¶ (ç§’)')
        axes[0, 1].grid(True)
    
    # å®Œæˆç‡æ›²çº¿
    if 'task_completion_rate' in training_env.episode_metrics and training_env.episode_metrics['task_completion_rate']:
        axes[0, 2].plot(training_env.episode_metrics['task_completion_rate'])
        axes[0, 2].set_title('ä»»åŠ¡å®Œæˆç‡')
        axes[0, 2].set_xlabel('è®­ç»ƒè½®æ¬¡')
        axes[0, 2].set_ylabel('å®Œæˆç‡')
        axes[0, 2].grid(True)
    
    # ç¼“å­˜å‘½ä¸­ç‡æ›²çº¿
    if 'cache_hit_rate' in training_env.episode_metrics and training_env.episode_metrics['cache_hit_rate']:
        axes[1, 0].plot(training_env.episode_metrics['cache_hit_rate'])
        axes[1, 0].set_title('ç¼“å­˜å‘½ä¸­ç‡')
        axes[1, 0].set_xlabel('è®­ç»ƒè½®æ¬¡')
        axes[1, 0].set_ylabel('å‘½ä¸­ç‡')
        axes[1, 0].grid(True)
    
    # èƒ½è€—æ›²çº¿
    if 'total_energy_consumption' in training_env.episode_metrics and training_env.episode_metrics['total_energy_consumption']:
        axes[1, 1].plot(training_env.episode_metrics['total_energy_consumption'])
        axes[1, 1].set_title('æ€»èƒ½è€—')
        axes[1, 1].set_xlabel('è®­ç»ƒè½®æ¬¡')
        axes[1, 1].set_ylabel('èƒ½è€— (ç„¦è€³)')
        axes[1, 1].grid(True)
    
    # æ•°æ®ä¸¢å¤±ç‡æ›²çº¿
    if 'data_loss_rate' in training_env.episode_metrics and training_env.episode_metrics['data_loss_rate']:
        axes[1, 2].plot(training_env.episode_metrics['data_loss_rate'])
        axes[1, 2].set_title('æ•°æ®ä¸¢å¤±ç‡')
        axes[1, 2].set_xlabel('è®­ç»ƒè½®æ¬¡')
        axes[1, 2].set_ylabel('ä¸¢å¤±ç‡')
        axes[1, 2].grid(True)
    
    plt.tight_layout()
    filepath = f"results/training/{algorithm.lower()}/training_curves.png"
    plt.savefig(filepath, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“ˆ {algorithm}è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° {filepath}")
    
    # ğŸ¨ æ–°å¢ï¼šé«˜çº§å¯è§†åŒ–å¥—ä»¶
    from tools.advanced_visualization import enhanced_plot_training_curves, plot_convergence_analysis, plot_multi_metric_dashboard
    from tools.performance_dashboard import create_performance_dashboard, create_real_time_monitor
    
    # 1. å¢å¼ºè®­ç»ƒæ›²çº¿
    enhanced_plot_training_curves(training_env, f"results/training/{algorithm.lower()}/enhanced_training_curves.png")
    
    # 2. æ”¶æ•›æ€§åˆ†æ
    plot_convergence_analysis(
        {'episode_rewards': training_env.episode_rewards}, 
        f"results/training/{algorithm.lower()}/convergence_analysis.png"
    )
    
    # 3. å¤šæŒ‡æ ‡ä»ªè¡¨æ¿
    plot_multi_metric_dashboard(
        training_env, 
        f"results/training/{algorithm.lower()}/multi_metric_dashboard.png"
    )
    
    # 4. æ€§èƒ½ä»ªè¡¨æ¿
    create_performance_dashboard(
        training_env, 
        f"results/training/{algorithm.lower()}/performance_dashboard.png"
    )
    
    # 5. å®æ—¶ç›‘æ§ç•Œé¢
    create_real_time_monitor(
        f"results/training/{algorithm.lower()}/realtime_monitor.png"
    )


def compare_algorithms(algorithms: List[str], num_episodes: Optional[int] = None) -> Dict:
    """æ¯”è¾ƒå¤šä¸ªç®—æ³•çš„æ€§èƒ½"""
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    if num_episodes is None:
        num_episodes = config.experiment.num_episodes
    
    print("\nğŸ”¥ å¼€å§‹å¤šç®—æ³•æ€§èƒ½æ¯”è¾ƒ")
    print("=" * 60)
    
    results = {}
    
    # è®­ç»ƒæ‰€æœ‰ç®—æ³•
    for algorithm in algorithms:
        print(f"\nå¼€å§‹è®­ç»ƒ {algorithm}...")
        results[algorithm] = train_algorithm(algorithm, num_episodes)
    
    # ç”Ÿæˆæ¯”è¾ƒå›¾è¡¨
    plot_algorithm_comparison(results)
    
    # ä¿å­˜æ¯”è¾ƒç»“æœ
    timestamp = generate_timestamp()
    comparison_results = {
        'algorithms': algorithms,
        'num_episodes': num_episodes,
        'timestamp': timestamp,
        'comparison_time': datetime.now().isoformat(),
        'results': results,
        'summary': {}
    }
    
    # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
    for algorithm, result in results.items():
        final_perf = result['final_performance']
        comparison_results['summary'][algorithm] = {
            'final_avg_reward': final_perf['avg_reward'],
            'final_avg_delay': final_perf['avg_delay'],
            'final_completion_rate': final_perf['avg_completion'],
            'training_time_hours': result['training_config']['training_time_hours']
        }
    
    # ä½¿ç”¨æ—¶é—´æˆ³æ–‡ä»¶å
    comparison_filename = get_timestamped_filename("algorithm_comparison")
    with open(f"results/{comparison_filename}", "w", encoding="utf-8") as f:
        json.dump(comparison_results, f, indent=2, ensure_ascii=False)
    
    print("\nğŸ¯ ç®—æ³•æ¯”è¾ƒå®Œæˆï¼")
    print(f"ğŸ“„ æ¯”è¾ƒç»“æœå·²ä¿å­˜åˆ° results/{comparison_filename}")
    print(f"ğŸ“ˆ æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜åˆ° results/algorithm_comparison_{timestamp}.png")
    
    return comparison_results


def plot_algorithm_comparison(results: Dict):
    """ç»˜åˆ¶ç®—æ³•æ¯”è¾ƒå›¾è¡¨"""
    timestamp = generate_timestamp()
    
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(3, 2, figsize=(16, 18))
    
    # å¥–åŠ±å¯¹æ¯”
    for algorithm, result in results.items():
        axes[0, 0].plot(result['episode_rewards'], label=algorithm)
    axes[0, 0].set_title('ç®—æ³•å¥–åŠ±å¯¹æ¯”')
    axes[0, 0].set_xlabel('è®­ç»ƒè½®æ¬¡')
    axes[0, 0].set_ylabel('å¹³å‡å¥–åŠ±')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # æ—¶å»¶å¯¹æ¯”
    for algorithm, result in results.items():
        if 'avg_task_delay' in result['episode_metrics'] and result['episode_metrics']['avg_task_delay']:
            axes[0, 1].plot(result['episode_metrics']['avg_task_delay'], label=algorithm)
    axes[0, 1].set_title('å¹³å‡æ—¶å»¶å¯¹æ¯”')
    axes[0, 1].set_xlabel('è®­ç»ƒè½®æ¬¡')
    axes[0, 1].set_ylabel('æ—¶å»¶ (ç§’)')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # å®Œæˆç‡å¯¹æ¯”
    for algorithm, result in results.items():
        if 'task_completion_rate' in result['episode_metrics'] and result['episode_metrics']['task_completion_rate']:
            axes[1, 0].plot(result['episode_metrics']['task_completion_rate'], label=algorithm)
    axes[1, 0].set_title('ä»»åŠ¡å®Œæˆç‡å¯¹æ¯”')
    axes[1, 0].set_xlabel('è®­ç»ƒè½®æ¬¡')
    axes[1, 0].set_ylabel('å®Œæˆç‡')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # èƒ½è€—å¯¹æ¯”
    for algorithm, result in results.items():
        if 'total_energy_consumption' in result['episode_metrics'] and result['episode_metrics']['total_energy_consumption']:
            axes[1, 1].plot(result['episode_metrics']['total_energy_consumption'], label=algorithm)
    axes[1, 1].set_title('æ€»èƒ½è€—å¯¹æ¯”')
    axes[1, 1].set_xlabel('è®­ç»ƒè½®æ¬¡')
    axes[1, 1].set_ylabel('èƒ½è€— (ç„¦è€³)')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    # æ•°æ®ä¸¢å¤±ç‡å¯¹æ¯”
    for algorithm, result in results.items():
        if 'data_loss_rate' in result['episode_metrics'] and result['episode_metrics']['data_loss_rate']:
            axes[2, 0].plot(result['episode_metrics']['data_loss_rate'], label=algorithm)
    axes[2, 0].set_title('æ•°æ®ä¸¢å¤±ç‡å¯¹æ¯”')
    axes[2, 0].set_xlabel('è®­ç»ƒè½®æ¬¡')
    axes[2, 0].set_ylabel('ä¸¢å¤±ç‡')
    axes[2, 0].legend()
    axes[2, 0].grid(True)
    
    # æœ€ç»ˆæ€§èƒ½å¯¹æ¯” (æŸ±çŠ¶å›¾)
    algorithms = list(results.keys())
    final_rewards = [results[alg]['final_performance']['avg_reward'] for alg in algorithms]
    
    axes[2, 1].bar(algorithms, final_rewards)
    axes[2, 1].set_title('æœ€ç»ˆå¹³å‡å¥–åŠ±å¯¹æ¯”')
    axes[2, 1].set_ylabel('å¹³å‡å¥–åŠ±')
    axes[2, 1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    
    # ä½¿ç”¨æ—¶é—´æˆ³æ–‡ä»¶å
    chart_filename = f"algorithm_comparison_{timestamp}.png" if timestamp else "algorithm_comparison.png"
    plt.savefig(f"results/{chart_filename}", dpi=300, bbox_inches='tight')
    plt.close()
    
    # ğŸ¨ æ–°å¢ï¼šé«˜çº§æ¯”è¾ƒå¯è§†åŒ–å¥—ä»¶
    from tools.advanced_visualization import create_advanced_visualization_suite
    create_advanced_visualization_suite(results, "results/advanced_multi_agent_comparison")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šæ™ºèƒ½ä½“ç®—æ³•è®­ç»ƒè„šæœ¬')
    parser.add_argument('--algorithm', type=str, choices=['MATD3', 'MADDPG', 'QMIX', 'MAPPO', 'SAC-MA'],
                       help='é€‰æ‹©è®­ç»ƒç®—æ³•')
    parser.add_argument('--episodes', type=int, default=None, help=f'è®­ç»ƒè½®æ¬¡ (é»˜è®¤: {config.experiment.num_episodes})')
    parser.add_argument('--eval_interval', type=int, default=None, help=f'è¯„ä¼°é—´éš” (é»˜è®¤: {config.experiment.eval_interval})')
    parser.add_argument('--save_interval', type=int, default=None, help=f'ä¿å­˜é—´éš” (é»˜è®¤: {config.experiment.save_interval})')
    parser.add_argument('--compare', action='store_true', help='æ¯”è¾ƒæ‰€æœ‰ç®—æ³•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs("results", exist_ok=True)
    
    if args.compare:
        # æ¯”è¾ƒæ‰€æœ‰ç®—æ³•
        algorithms = ['MATD3', 'MADDPG', 'QMIX', 'MAPPO', 'SAC-MA']
        compare_algorithms(algorithms, args.episodes)
    elif args.algorithm:
        # è®­ç»ƒå•ä¸ªç®—æ³•
        train_algorithm(args.algorithm, args.episodes, args.eval_interval, args.save_interval)
    else:
        print("è¯·æŒ‡å®š --algorithm æˆ–ä½¿ç”¨ --compare æ ‡å¿—")
        print("ä½¿ç”¨ python train_multi_agent.py --help æŸ¥çœ‹å¸®åŠ©")


if __name__ == "__main__":
    main()