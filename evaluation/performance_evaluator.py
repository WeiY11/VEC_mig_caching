#!/usr/bin/env python3
"""
æ€§èƒ½è¯„ä¼°å™¨
ç”¨äºè¯„ä¼°ç®—æ³•æ€§èƒ½å’Œç³»ç»ŸæŒ‡æ ‡
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional
import json
from datetime import datetime

class PerformanceEvaluator:
    """æ€§èƒ½è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics = {}
        self.baseline_results = {}
    
    def evaluate_algorithm(self, algorithm_name: str, results: Dict) -> Dict:
        """è¯„ä¼°å•ä¸ªç®—æ³•æ€§èƒ½"""
        stats = results.get('statistics', {})
        
        # åŸºæœ¬æ€§èƒ½æŒ‡æ ‡
        performance = {
            'algorithm': algorithm_name,
            'total_tasks': stats.get('total_tasks', 0),
            'completed_tasks': stats.get('completed_tasks', 0),
            'completion_rate': stats.get('completion_rate', 0.0),
            'avg_delay': stats.get('avg_delay', 0.0),
            'total_energy': stats.get('total_energy', 0.0),
            'cache_hit_rate': stats.get('cache_hit_rate', 0.0),
            'drop_rate': stats.get('drop_rate', 0.0)
        }
        
        # 3GPPæ ‡å‡†ç›¸å…³æŒ‡æ ‡
        performance.update({
            'avg_sinr': stats.get('avg_sinr', 0.0),
            'avg_throughput': stats.get('avg_throughput', 0.0),
            'handover_success_rate': stats.get('handover_success_rate', 1.0),
            'path_loss_avg': stats.get('path_loss_avg', 0.0),
            'interference_level': stats.get('interference_level', 0.0),
            'channel_quality': stats.get('channel_quality', 0.0)
        })
        
        # åˆ†å±‚å­¦ä¹ ç›¸å…³æŒ‡æ ‡
        performance.update({
            'strategic_reward': stats.get('strategic_reward', 0.0),
            'tactical_reward': stats.get('tactical_reward', 0.0),
            'operational_reward': stats.get('operational_reward', 0.0),
            'coordination_efficiency': stats.get('coordination_efficiency', 0.0),
            'decision_consistency': stats.get('decision_consistency', 0.0)
        })
        
        # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        if performance['completed_tasks'] > 0:
            performance['energy_efficiency'] = performance['completed_tasks'] / max(performance['total_energy'], 1)
            performance['delay_efficiency'] = 1.0 / max(performance['avg_delay'], 0.001)
            performance['spectral_efficiency'] = performance['avg_throughput'] / max(stats.get('bandwidth_used', 1), 1)
        else:
            performance['energy_efficiency'] = 0.0
            performance['delay_efficiency'] = 0.0
            performance['spectral_efficiency'] = 0.0
        
        # ç»¼åˆæ€§èƒ½åˆ†æ•°
        performance['composite_score'] = self.calculate_composite_score(performance)
        
        self.metrics[algorithm_name] = performance
        return performance
    
    def calculate_composite_score(self, performance: Dict) -> float:
        """è®¡ç®—ç»¼åˆæ€§èƒ½åˆ†æ•°"""
        # æƒé‡è®¾ç½®
        weights = {
            'completion_rate': 0.25,
            'delay_efficiency': 0.2,
            'energy_efficiency': 0.2,
            'cache_hit_rate': 0.15,
            'sinr_quality': 0.1,
            'handover_success_rate': 0.1
        }
        
        # å½’ä¸€åŒ–å¤„ç†
        normalized_scores = {
            'completion_rate': performance['completion_rate'],
            'delay_efficiency': min(performance['delay_efficiency'], 10.0) / 10.0,
            'energy_efficiency': min(performance['energy_efficiency'] / 1000, 1.0),
            'cache_hit_rate': performance['cache_hit_rate'],
            'sinr_quality': performance.get('avg_sinr', 0.0) / 30.0,  # å‡è®¾æœ€å¤§SINRä¸º30dB
            'handover_success_rate': performance.get('handover_success_rate', 1.0)
        }
        
        # åŠ æƒæ±‚å’Œ
        composite_score = sum(
            weights[metric] * min(max(normalized_scores[metric], 0.0), 1.0)
            for metric in weights.keys()
        )
        
        return composite_score
    
    def compare_algorithms(self, results_dict: Dict[str, Dict]) -> Dict:
        """æ¯”è¾ƒå¤šä¸ªç®—æ³•"""
        comparison = {}
        
        # è¯„ä¼°æ¯ä¸ªç®—æ³•
        for algorithm, results in results_dict.items():
            comparison[algorithm] = self.evaluate_algorithm(algorithm, results)
        
        # æ‰¾å‡ºæœ€ä½³ç®—æ³•
        best_algorithm = max(comparison.keys(), 
                           key=lambda x: comparison[x]['composite_score'])
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        improvements = {}
        best_performance = comparison[best_algorithm]
        
        for algorithm, performance in comparison.items():
            if algorithm != best_algorithm:
                improvements[algorithm] = self.calculate_improvements(
                    best_performance, performance
                )
        
        return {
            'individual_performance': comparison,
            'best_algorithm': best_algorithm,
            'improvements': improvements,
            'ranking': self.rank_algorithms(comparison)
        }
    
    def calculate_improvements(self, best: Dict, current: Dict) -> Dict:
        """è®¡ç®—æ”¹è¿›å¹…åº¦"""
        improvements = {}
        
        metrics_to_compare = [
            'completion_rate', 'avg_delay', 'total_energy', 
            'cache_hit_rate', 'composite_score'
        ]
        
        for metric in metrics_to_compare:
            best_val = best.get(metric, 0)
            current_val = current.get(metric, 0)
            
            if metric == 'avg_delay' or metric == 'total_energy':
                # å¯¹äºæ—¶å»¶å’Œèƒ½è€—ï¼Œè¶Šå°è¶Šå¥½
                if current_val > 0:
                    improvement = (current_val - best_val) / current_val * 100
                else:
                    improvement = 0
            else:
                # å¯¹äºå…¶ä»–æŒ‡æ ‡ï¼Œè¶Šå¤§è¶Šå¥½
                if current_val > 0:
                    improvement = (best_val - current_val) / current_val * 100
                else:
                    improvement = 0
            
            improvements[f'{metric}_improvement'] = improvement
        
        return improvements
    
    def rank_algorithms(self, comparison: Dict) -> List[Tuple[str, float]]:
        """ç®—æ³•æ’å"""
        ranking = [(alg, perf['composite_score']) 
                  for alg, perf in comparison.items()]
        ranking.sort(key=lambda x: x[1], reverse=True)
        return ranking
    
    def generate_performance_report(self, comparison_results: Dict) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = "# ç®—æ³•æ€§èƒ½è¯„ä¼°æŠ¥å‘Š\n\n"
        report += f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        # ç®—æ³•æ’å
        report += "## ç®—æ³•æ’å\n\n"
        ranking = comparison_results['ranking']
        for i, (algorithm, score) in enumerate(ranking, 1):
            report += f"{i}. **{algorithm}** - ç»¼åˆåˆ†æ•°: {score:.3f}\n"
        
        report += "\n## è¯¦ç»†æ€§èƒ½æŒ‡æ ‡\n\n"
        report += "| ç®—æ³• | å®Œæˆç‡ | å¹³å‡æ—¶å»¶(s) | æ€»èƒ½è€—(J) | ç¼“å­˜å‘½ä¸­ç‡ | ç»¼åˆåˆ†æ•° |\n"
        report += "|------|--------|-------------|-----------|------------|----------|\n"
        
        for algorithm, performance in comparison_results['individual_performance'].items():
            report += f"| {algorithm} | {performance['completion_rate']:.2%} | "
            report += f"{performance['avg_delay']:.3f} | {performance['total_energy']:.1f} | "
            report += f"{performance['cache_hit_rate']:.2%} | {performance['composite_score']:.3f} |\n"
        
        # æœ€ä½³ç®—æ³•åˆ†æ
        best_alg = comparison_results['best_algorithm']
        report += f"\n## æœ€ä½³ç®—æ³•: {best_alg}\n\n"
        
        best_perf = comparison_results['individual_performance'][best_alg]
        report += f"- **å®Œæˆç‡**: {best_perf['completion_rate']:.2%}\n"
        report += f"- **å¹³å‡æ—¶å»¶**: {best_perf['avg_delay']:.3f}s\n"
        report += f"- **æ€»èƒ½è€—**: {best_perf['total_energy']:.1f}J\n"
        report += f"- **ç¼“å­˜å‘½ä¸­ç‡**: {best_perf['cache_hit_rate']:.2%}\n"
        report += f"- **ç»¼åˆåˆ†æ•°**: {best_perf['composite_score']:.3f}\n"
        
        # æ”¹è¿›åˆ†æ
        if comparison_results['improvements']:
            report += "\n## æ”¹è¿›åˆ†æ\n\n"
            for algorithm, improvements in comparison_results['improvements'].items():
                report += f"### {best_alg} vs {algorithm}\n\n"
                for metric, improvement in improvements.items():
                    if 'improvement' in metric:
                        metric_name = metric.replace('_improvement', '')
                        report += f"- **{metric_name}**: {improvement:+.1f}%\n"
                report += "\n"
        
        return report
    
    def plot_performance_comparison(self, comparison_results: Dict, save_path: Optional[str] = None):
        """ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾"""
        algorithms = list(comparison_results['individual_performance'].keys())
        
        # æå–æŒ‡æ ‡æ•°æ®
        completion_rates = [comparison_results['individual_performance'][alg]['completion_rate'] 
                          for alg in algorithms]
        avg_delays = [comparison_results['individual_performance'][alg]['avg_delay'] 
                     for alg in algorithms]
        cache_hit_rates = [comparison_results['individual_performance'][alg]['cache_hit_rate'] 
                          for alg in algorithms]
        composite_scores = [comparison_results['individual_performance'][alg]['composite_score'] 
                           for alg in algorithms]
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('ç®—æ³•æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # å®Œæˆç‡å¯¹æ¯”
        axes[0, 0].bar(algorithms, completion_rates, color='skyblue')
        axes[0, 0].set_title('ä»»åŠ¡å®Œæˆç‡')
        axes[0, 0].set_ylabel('å®Œæˆç‡')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # å¹³å‡æ—¶å»¶å¯¹æ¯”
        axes[0, 1].bar(algorithms, avg_delays, color='lightcoral')
        axes[0, 1].set_title('å¹³å‡æ—¶å»¶')
        axes[0, 1].set_ylabel('æ—¶å»¶ (ç§’)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # ç¼“å­˜å‘½ä¸­ç‡å¯¹æ¯”
        axes[1, 0].bar(algorithms, cache_hit_rates, color='lightgreen')
        axes[1, 0].set_title('ç¼“å­˜å‘½ä¸­ç‡')
        axes[1, 0].set_ylabel('å‘½ä¸­ç‡')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # ç»¼åˆåˆ†æ•°å¯¹æ¯”
        axes[1, 1].bar(algorithms, composite_scores, color='gold')
        axes[1, 1].set_title('ç»¼åˆæ€§èƒ½åˆ†æ•°')
        axes[1, 1].set_ylabel('åˆ†æ•°')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def save_results(self, comparison_results: Dict, filepath: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # å‡†å¤‡JSONåºåˆ—åŒ–çš„æ•°æ®
        json_data = {
            'timestamp': datetime.now().isoformat(),
            'evaluation_results': comparison_results,
            'summary': {
                'total_algorithms': len(comparison_results['individual_performance']),
                'best_algorithm': comparison_results['best_algorithm'],
                'best_score': comparison_results['individual_performance'][
                    comparison_results['best_algorithm']
                ]['composite_score']
            }
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜: {filepath}")

def test_evaluator():
    """æµ‹è¯•è¯„ä¼°å™¨"""
    print("ğŸ§ª æµ‹è¯•æ€§èƒ½è¯„ä¼°å™¨...")
    
    # æ¨¡æ‹Ÿç®—æ³•ç»“æœ
    mock_results = {
        'MATD3': {
            'statistics': {
                'total_tasks': 1000,
                'completed_tasks': 850,
                'completion_rate': 0.85,
                'avg_delay': 0.12,
                'total_energy': 5000,
                'cache_hit_rate': 0.75,
                'drop_rate': 0.15
            }
        },
        'MADDPG': {
            'statistics': {
                'total_tasks': 1000,
                'completed_tasks': 800,
                'completion_rate': 0.80,
                'avg_delay': 0.15,
                'total_energy': 5500,
                'cache_hit_rate': 0.65,
                'drop_rate': 0.20
            }
        },
        'Random': {
            'statistics': {
                'total_tasks': 1000,
                'completed_tasks': 600,
                'completion_rate': 0.60,
                'avg_delay': 0.25,
                'total_energy': 7000,
                'cache_hit_rate': 0.30,
                'drop_rate': 0.40
            }
        }
    }
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = PerformanceEvaluator()
    
    # è¿›è¡Œæ¯”è¾ƒ
    comparison = evaluator.compare_algorithms(mock_results)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = evaluator.generate_performance_report(comparison)
    print("\n" + "="*50)
    print(report)
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    evaluator.plot_performance_comparison(comparison)
    
    print("âœ… è¯„ä¼°å™¨æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_evaluator()