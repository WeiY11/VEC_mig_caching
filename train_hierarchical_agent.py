"""
åˆ†å±‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬
æ”¯æŒæˆ˜ç•¥å±‚(SAC)ã€æˆ˜æœ¯å±‚(MATD3/MAPPO)ã€æ‰§è¡Œå±‚(TD3/DDPG)çš„åˆ†å±‚è®­ç»ƒ

ä½¿ç”¨æ–¹æ³•:
python train_hierarchical_agent.py --episodes 200 --mode hierarchical
python train_hierarchical_agent.py --episodes 200 --mode strategic_only
python train_hierarchical_agent.py --episodes 200 --mode tactical_only
python train_hierarchical_agent.py --episodes 200 --mode operational_only
"""

# æ€§èƒ½ä¼˜åŒ– - å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥ä¹‹å‰
try:
    from tools.performance_optimization import *
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥æ€§èƒ½ä¼˜åŒ–æ¨¡å—")
    OPTIMIZED_BATCH_SIZES = {}
    PARALLEL_ENVS = 1
    NUM_WORKERS = 0

import os
import sys
import argparse
import numpy as np
import matplotlib.pyplot as plt
import json
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from evaluation.test_complete_system import CompleteSystemSimulator
from utils import MovingAverage
from config import config

# å¯¼å…¥åˆ†å±‚å­¦ä¹ æ¨¡å—
from hierarchical_learning.core.hierarchical_environment import HierarchicalEnvironment
from hierarchical_learning.core.strategic_layer import StrategicLayer
from hierarchical_learning.core.tactical_layer import TacticalLayer
from hierarchical_learning.core.operational_layer import OperationalLayer

# å¯¼å…¥ç°æœ‰ç®—æ³•ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
from algorithms.matd3 import MATD3Environment
from algorithms.mappo import MAPPOEnvironment
from single_agent.sac import SACEnvironment
from single_agent.td3 import TD3Environment


def generate_timestamp() -> str:
    """ç”Ÿæˆæ—¶é—´æˆ³"""
    if config.experiment.use_timestamp:
        return datetime.now().strftime(config.experiment.timestamp_format)
    else:
        return ""


def get_timestamped_filename(base_name: str, extension: str = ".json") -> str:
    """è·å–å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å"""
    timestamp = generate_timestamp()
    if timestamp:
        name_parts = base_name.split('.')
        if len(name_parts) > 1:
            base = '.'.join(name_parts[:-1])
            return f"{base}_{timestamp}{extension}"
        else:
            return f"{base_name}_{timestamp}{extension}"
    else:
        return f"{base_name}{extension}"


class HierarchicalTrainingEnvironment:
    """åˆ†å±‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒç¯å¢ƒ"""
    
    def __init__(self, training_mode: str = "hierarchical"):
        """
        åˆå§‹åŒ–åˆ†å±‚è®­ç»ƒç¯å¢ƒ
        
        Args:
            training_mode: è®­ç»ƒæ¨¡å¼ - "hierarchical", "strategic_only", "tactical_only", "operational_only"
        """
        self.training_mode = training_mode.lower()
        self.simulator = CompleteSystemSimulator()
        
        # è·å–ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°
        self.optimized_batch_size = self._get_optimized_batch_size()
        print(f"ğŸš€ ä½¿ç”¨ä¼˜åŒ–æ‰¹æ¬¡å¤§å°: {self.optimized_batch_size}")
        
        # åˆ†å±‚ç¯å¢ƒé…ç½®
        hierarchical_config = {
            'num_rsus': config.num_rsus,
            'num_uavs': config.num_uavs,
            'num_vehicles': config.num_vehicles,
            'area_size': (1000, 1000),  # é»˜è®¤åŒºåŸŸå¤§å°
            'max_episode_steps': config.experiment.max_steps_per_episode,
            'strategic_config': {
                'state_dim': 50,  # æˆ˜ç•¥å±‚çŠ¶æ€ç»´åº¦
                'action_dim': 10,  # æˆ˜ç•¥å±‚åŠ¨ä½œç»´åº¦
                'hidden_dim': 256,
                'lr': 3e-4,
                'gamma': 0.99,
                'tau': 0.005,
                'alpha': 0.2,
                'batch_size': self.optimized_batch_size
            },
            'tactical_config': {
                'num_agents': config.num_rsus + config.num_uavs,
                'state_dim': 30,  # æˆ˜æœ¯å±‚çŠ¶æ€ç»´åº¦
                'action_dim': 8,   # æˆ˜æœ¯å±‚åŠ¨ä½œç»´åº¦
                'hidden_dim': 128,
                'lr': 1e-4,
                'gamma': 0.95,
                'tau': 0.01,
                'batch_size': self.optimized_batch_size
            },
            'operational_config': {
                'num_agents': config.num_rsus + config.num_uavs,
                'state_dim': 40,  # æ‰§è¡Œå±‚çŠ¶æ€ç»´åº¦
                'action_dim': 6,   # æ‰§è¡Œå±‚åŠ¨ä½œç»´åº¦
                'hidden_dim': 128,
                'lr': 1e-4,
                'gamma': 0.9,
                'tau': 0.005,
                'batch_size': self.optimized_batch_size
            }
        }
        
        # åˆ›å»ºåˆ†å±‚ç¯å¢ƒ
        self.hierarchical_env = HierarchicalEnvironment(hierarchical_config)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = {
            'strategic': [],
            'tactical': [],
            'operational': [],
            'total': []
        }
        
        self.episode_losses = {
            'strategic': [],
            'tactical': [],
            'operational': []
        }
        
        self.episode_metrics = {
            'avg_task_delay': [],
            'total_energy_consumption': [],
            'task_completion_rate': [],
            'cache_hit_rate': [],
            'migration_success_rate': [],
            'data_loss_rate': [],
            'strategic_decision_quality': [],
            'tactical_coordination_efficiency': [],
            'operational_control_precision': []
        }
        
        # åˆ†å±‚æ€§èƒ½ç»Ÿè®¡
        self.layer_performance = {
            'strategic': {'updates': 0, 'avg_loss': 0.0, 'avg_reward': 0.0},
            'tactical': {'updates': 0, 'avg_loss': 0.0, 'avg_reward': 0.0},
            'operational': {'updates': 0, 'avg_loss': 0.0, 'avg_reward': 0.0}
        }
        
        print(f"ğŸ¯ åˆ†å±‚è®­ç»ƒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ - æ¨¡å¼: {self.training_mode}")
        print(f"ğŸ“Š æˆ˜ç•¥å±‚çŠ¶æ€ç»´åº¦: {hierarchical_config['strategic_config']['state_dim']}")
        print(f"ğŸ“Š æˆ˜æœ¯å±‚æ™ºèƒ½ä½“æ•°é‡: {hierarchical_config['tactical_config']['num_agents']}")
        print(f"ğŸ“Š æ‰§è¡Œå±‚æ™ºèƒ½ä½“æ•°é‡: {hierarchical_config['operational_config']['num_agents']}")
    
    def _get_optimized_batch_size(self) -> int:
        """è·å–ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°"""
        try:
            return OPTIMIZED_BATCH_SIZES.get('hierarchical', config.rl.batch_size)
        except:
            return config.rl.batch_size
    
    def reset_environment(self) -> Dict[str, Dict[str, np.ndarray]]:
        """é‡ç½®ç¯å¢ƒå¹¶è¿”å›åˆå§‹çŠ¶æ€"""
        # é‡ç½®åˆ†å±‚ç¯å¢ƒ
        hierarchical_states = self.hierarchical_env.reset()
        
        # é‡ç½®æ¨¡æ‹Ÿå™¨
        self.simulator.reset()
        
        return hierarchical_states
    
    def run_episode(self, episode: int, max_steps: Optional[int] = None) -> Dict:
        """è¿è¡Œä¸€ä¸ªè®­ç»ƒå›åˆ"""
        if max_steps is None:
            max_steps = config.experiment.max_steps_per_episode
        
        # é‡ç½®ç¯å¢ƒ
        states = self.reset_environment()
        
        episode_rewards = {'strategic': 0.0, 'tactical': 0.0, 'operational': 0.0, 'total': 0.0}
        episode_losses = {'strategic': [], 'tactical': [], 'operational': []}
        episode_metrics = []
        
        step_count = 0
        done = False
        
        print(f"ğŸ® å¼€å§‹ç¬¬ {episode + 1} å›åˆè®­ç»ƒ (æ¨¡å¼: {self.training_mode})")
        
        while not done and step_count < max_steps:
            # æ‰§è¡Œç¯å¢ƒæ­¥éª¤
            next_states, rewards, done, info = self.hierarchical_env.step()
            
            # å­˜å‚¨ç»éªŒ
            self.hierarchical_env.store_experience(
                states, {}, rewards, next_states, {'strategic': done, 'tactical': done, 'operational': done}
            )
            
            # æ ¹æ®è®­ç»ƒæ¨¡å¼æ‰§è¡Œè®­ç»ƒ
            training_results = {}
            if self.training_mode == "hierarchical":
                # å®Œæ•´åˆ†å±‚è®­ç»ƒ
                training_results = self.hierarchical_env.train_step()
            elif self.training_mode == "strategic_only":
                # ä»…è®­ç»ƒæˆ˜ç•¥å±‚
                if len(self.hierarchical_env.strategic_layer.sac_agent.replay_buffer) >= 32:
                    strategic_stats = self.hierarchical_env.strategic_layer.train()
                    if strategic_stats:
                        training_results['strategic'] = strategic_stats
            elif self.training_mode == "tactical_only":
                # ä»…è®­ç»ƒæˆ˜æœ¯å±‚
                tactical_stats = self.hierarchical_env.tactical_layer.train()
                if tactical_stats:
                    training_results['tactical'] = tactical_stats
            elif self.training_mode == "operational_only":
                # ä»…è®­ç»ƒæ‰§è¡Œå±‚
                operational_stats = self.hierarchical_env.operational_layer.train()
                if operational_stats:
                    training_results['operational'] = operational_stats
            
            # è®°å½•æŸå¤±
            for layer, stats in training_results.items():
                if 'loss' in stats:
                    episode_losses[layer].append(stats['loss'])
                    self.layer_performance[layer]['updates'] += 1
                    self.layer_performance[layer]['avg_loss'] = (
                        self.layer_performance[layer]['avg_loss'] * 0.9 + stats['loss'] * 0.1
                    )
            
            # ç´¯ç§¯å¥–åŠ±
            for layer, reward in rewards.items():
                if isinstance(reward, (int, float)):
                    episode_rewards[layer] += reward
                    self.layer_performance[layer]['avg_reward'] = (
                        self.layer_performance[layer]['avg_reward'] * 0.9 + reward * 0.1
                    )
            
            # è®¡ç®—å½“å‰ç´¯è®¡æ€»å’Œ
            episode_rewards['total'] = sum([r for k, r in episode_rewards.items() 
                                          if k != 'total' and isinstance(r, (int, float))])
            
            # è®°å½•ç³»ç»ŸæŒ‡æ ‡
            episode_metrics.append(info.get('performance_metrics', {}))
            
            # æ›´æ–°çŠ¶æ€
            states = next_states
            step_count += 1
            
            # æ¯50æ­¥æ‰“å°ä¸€æ¬¡è¿›åº¦ï¼ˆæŒ‰æ­¥å¹³å‡ï¼Œå£å¾„ç¨³å®šï¼‰
            if step_count % 50 == 0:
                avg_total_so_far = episode_rewards['total'] / max(1, step_count)
                print(f"  æ­¥éª¤ {step_count}/{max_steps}, å¹³å‡å¥–åŠ±/æ­¥: {avg_total_so_far:.2f}")
            
            # æ¯200æ­¥æ‰“å°ä¸€æ¬¡è¯Šæ–­ä¿¡æ¯ï¼šå„å±‚bufferå¤§å°ä¸æ›´æ–°è®¡æ•°
            if step_count % 200 == 0:
                try:
                    strat_buf = len(self.hierarchical_env.strategic_layer.sac_agent.replay_buffer)
                except Exception:
                    strat_buf = -1
                try:
                    tac_bufs = {aid: len(ag.replay_buffer) for aid, ag in self.hierarchical_env.tactical_layer.agents.items()}
                    tac_total = sum(tac_bufs.values())
                except Exception:
                    tac_bufs, tac_total = {}, -1
                try:
                    op_bufs = {aid: len(ag.replay_buffer) for aid, ag in self.hierarchical_env.operational_layer.agents.items()}
                    op_total = sum(op_bufs.values())
                except Exception:
                    op_bufs, op_total = {}, -1
                ts = self.hierarchical_env.training_stats
                print(f"  è¯Šæ–­: SACç¼“å†²={strat_buf}, MATD3æ€»ç¼“å†²={tac_total}, TD3æ€»ç¼“å†²={op_total}; æ›´æ–°è®¡æ•° S/T/O = {ts['strategic_updates']}/{ts['tactical_updates']}/{ts['operational_updates']}")
        
        # è®¡ç®—å›åˆå¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        if episode_metrics:
            for key in episode_metrics[0].keys():
                values = [m.get(key, 0) for m in episode_metrics if key in m]
                if values:
                    avg_metrics[key] = np.mean(values)
        
        # è®°å½•å›åˆç»“æœï¼ˆç»Ÿä¸€ä¸ºæŒ‰æ­¥å¹³å‡å£å¾„ï¼‰
        if step_count > 0:
            strategic_avg = episode_rewards['strategic'] / step_count
            tactical_avg = episode_rewards['tactical'] / step_count
            operational_avg = episode_rewards['operational'] / step_count
            total_avg = strategic_avg + tactical_avg + operational_avg
        else:
            strategic_avg = tactical_avg = operational_avg = total_avg = 0.0

        # æ‰“å°æ€»ç»“æ—¶é™„å¸¦æ€»å’Œï¼Œä¸»æ˜¾ç¤ºä¸ºå‡å€¼
        print(f"   æ€»å¥–åŠ±(å‡å€¼/æ­¥): {total_avg:.2f}")
        print(f"   æˆ˜ç•¥å±‚å¥–åŠ±(å‡å€¼/æ­¥): {strategic_avg:.2f}")
        print(f"   æˆ˜æœ¯å±‚å¥–åŠ±(å‡å€¼/æ­¥): {tactical_avg:.2f}")
        print(f"   æ‰§è¡Œå±‚å¥–åŠ±(å‡å€¼/æ­¥): {operational_avg:.2f}")

        for layer, value in [('strategic', strategic_avg), ('tactical', tactical_avg), ('operational', operational_avg), ('total', total_avg)]:
            self.episode_rewards[layer].append(value)
        
        for layer in ['strategic', 'tactical', 'operational']:
            if episode_losses[layer]:
                self.episode_losses[layer].append(np.mean(episode_losses[layer]))
            else:
                self.episode_losses[layer].append(0.0)
        
        # è®°å½•ç³»ç»ŸæŒ‡æ ‡
        for key, value in avg_metrics.items():
            if key in self.episode_metrics:
                self.episode_metrics[key].append(value)
        
        # æ·»åŠ åˆ†å±‚ç‰¹æœ‰æŒ‡æ ‡
        self.episode_metrics['strategic_decision_quality'].append(
            self.layer_performance['strategic']['avg_reward']
        )
        self.episode_metrics['tactical_coordination_efficiency'].append(
            self.layer_performance['tactical']['avg_reward']
        )
        self.episode_metrics['operational_control_precision'].append(
            self.layer_performance['operational']['avg_reward']
        )
        
        print(f"âœ… ç¬¬ {episode + 1} å›åˆå®Œæˆ:")
        print(f"   æ€»æ­¥æ•°: {step_count}")
        print(f"   æ€»å¥–åŠ±: {episode_rewards['total']:.2f}")
        print(f"   æˆ˜ç•¥å±‚å¥–åŠ±: {episode_rewards['strategic']:.2f}")
        print(f"   æˆ˜æœ¯å±‚å¥–åŠ±: {episode_rewards['tactical']:.2f}")
        print(f"   æ‰§è¡Œå±‚å¥–åŠ±: {episode_rewards['operational']:.2f}")
        
        return {
            'episode': episode,
            'steps': step_count,
            'rewards': episode_rewards,
            'losses': episode_losses,
            'metrics': avg_metrics,
            'layer_performance': self.layer_performance.copy()
        }
    
    def evaluate_model(self, num_eval_episodes: int = 5) -> Dict:
        """è¯„ä¼°åˆ†å±‚æ¨¡å‹æ€§èƒ½"""
        print(f"ğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼° ({num_eval_episodes} å›åˆ)")
        
        eval_rewards = {'strategic': [], 'tactical': [], 'operational': [], 'total': []}
        eval_metrics = []
        
        for eval_episode in range(num_eval_episodes):
            # é‡ç½®ç¯å¢ƒ
            states = self.reset_environment()
            
            episode_rewards = {'strategic': 0.0, 'tactical': 0.0, 'operational': 0.0, 'total': 0.0}
            episode_metrics = []
            
            step_count = 0
            done = False
            max_steps = config.experiment.max_steps_per_episode
            
            while not done and step_count < max_steps:
                # æ‰§è¡Œç¯å¢ƒæ­¥éª¤ï¼ˆè¯„ä¼°æ¨¡å¼ï¼Œä¸è®­ç»ƒï¼‰
                next_states, rewards, done, info = self.hierarchical_env.step()
                
                # ç´¯ç§¯å¥–åŠ±ï¼ˆè¯„ä¼°åŒæ ·æŒ‰æ€»å’Œç´¯åŠ ï¼Œæœ€åè½¬å‡å€¼ï¼‰
                for layer, reward in rewards.items():
                    if isinstance(reward, (int, float)):
                        episode_rewards[layer] += reward
                
                episode_rewards['total'] = sum([r for k, r in episode_rewards.items() 
                                               if k != 'total' and isinstance(r, (int, float))])
                
                # è®°å½•æŒ‡æ ‡
                episode_metrics.append(info.get('performance_metrics', {}))
                
                states = next_states
                step_count += 1
            
            # è®°å½•è¯„ä¼°ç»“æœï¼ˆç»Ÿä¸€ä¸ºæŒ‰æ­¥å¹³å‡å£å¾„ï¼‰
            if step_count > 0:
                strategic_avg = episode_rewards['strategic'] / step_count
                tactical_avg = episode_rewards['tactical'] / step_count
                operational_avg = episode_rewards['operational'] / step_count
                total_avg = strategic_avg + tactical_avg + operational_avg
            else:
                strategic_avg = tactical_avg = operational_avg = total_avg = 0.0

            for layer, value in [('strategic', strategic_avg), ('tactical', tactical_avg), ('operational', operational_avg), ('total', total_avg)]:
                eval_rewards[layer].append(value)
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            if episode_metrics:
                avg_metrics = {}
                for key in episode_metrics[0].keys():
                    values = [m.get(key, 0) for m in episode_metrics if key in m]
                    if values:
                        avg_metrics[key] = np.mean(values)
                eval_metrics.append(avg_metrics)
        
        # è®¡ç®—è¯„ä¼°ç»Ÿè®¡
        eval_stats = {}
        for layer in eval_rewards.keys():
            if eval_rewards[layer]:
                eval_stats[f'{layer}_reward_mean'] = np.mean(eval_rewards[layer])
                eval_stats[f'{layer}_reward_std'] = np.std(eval_rewards[layer])
        
        # è®¡ç®—ç³»ç»ŸæŒ‡æ ‡ç»Ÿè®¡
        if eval_metrics:
            for key in eval_metrics[0].keys():
                values = [m.get(key, 0) for m in eval_metrics if key in m]
                if values:
                    eval_stats[f'{key}_mean'] = np.mean(values)
                    eval_stats[f'{key}_std'] = np.std(values)
        
        print(f"ğŸ“Š è¯„ä¼°å®Œæˆ:")
        print(f"   å¹³å‡æ€»å¥–åŠ±(å‡å€¼/æ­¥): {eval_stats.get('total_reward_mean', 0):.2f} Â± {eval_stats.get('total_reward_std', 0):.2f}")
        print(f"   å¹³å‡ä»»åŠ¡å»¶è¿Ÿ: {eval_stats.get('total_latency_mean', 0):.2f} ms")
        print(f"   å¹³å‡æˆåŠŸç‡: {eval_stats.get('success_rate_mean', 0):.3f}")
        
        return eval_stats
    
    def save_models(self, save_path: str):
        """ä¿å­˜åˆ†å±‚æ¨¡å‹"""
        timestamp = generate_timestamp()
        if timestamp:
            save_path = f"{save_path}_{timestamp}"
        
        self.hierarchical_env.save_models(save_path)
        print(f"ğŸ’¾ åˆ†å±‚æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_models(self, load_path: str):
        """åŠ è½½åˆ†å±‚æ¨¡å‹"""
        self.hierarchical_env.load_models(load_path)
        print(f"ğŸ“‚ åˆ†å±‚æ¨¡å‹å·²ä» {load_path} åŠ è½½")


def train_hierarchical_algorithm(training_mode: str = "hierarchical", 
                                num_episodes: Optional[int] = None,
                                eval_interval: Optional[int] = None,
                                save_interval: Optional[int] = None) -> Dict:
    """è®­ç»ƒåˆ†å±‚å¼ºåŒ–å­¦ä¹ ç®—æ³•"""
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
    if num_episodes is None:
        num_episodes = config.experiment.num_episodes
    if eval_interval is None:
        eval_interval = config.experiment.eval_interval
    if save_interval is None:
        save_interval = config.experiment.save_interval
    
    print(f"ğŸš€ å¼€å§‹åˆ†å±‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
    print(f"ğŸ“‹ è®­ç»ƒæ¨¡å¼: {training_mode}")
    print(f"ğŸ“‹ è®­ç»ƒå›åˆæ•°: {num_episodes}")
    print(f"ğŸ“‹ è¯„ä¼°é—´éš”: {eval_interval}")
    print(f"ğŸ“‹ ä¿å­˜é—´éš”: {save_interval}")
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    training_env = HierarchicalTrainingEnvironment(training_mode)
    
    # è®­ç»ƒç»Ÿè®¡
    training_start_time = time.time()
    best_performance = -float('inf')
    
    # è®­ç»ƒå¾ªç¯
    for episode in range(num_episodes):
        episode_start_time = time.time()
        
        # è¿è¡Œè®­ç»ƒå›åˆ
        episode_result = training_env.run_episode(episode)
        
        episode_time = time.time() - episode_start_time
        
        # å®šæœŸè¯„ä¼°
        if (episode + 1) % eval_interval == 0:
            eval_stats = training_env.evaluate_model()
            current_performance = eval_stats.get('total_reward_mean', -float('inf'))
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if current_performance > best_performance:
                best_performance = current_performance
                training_env.save_models(f"models/hierarchical_best_{training_mode}")
                print(f"ğŸ† å‘ç°æ›´å¥½çš„æ¨¡å‹! æ€§èƒ½: {current_performance:.2f}")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (episode + 1) % save_interval == 0:
            training_env.save_models(f"models/hierarchical_checkpoint_{training_mode}_ep{episode+1}")
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(training_env.episode_rewards['total'][-10:])
            print(f"ğŸ“ˆ å›åˆ {episode + 1}/{num_episodes}, æœ€è¿‘10å›åˆå¹³å‡å¥–åŠ±: {avg_reward:.2f}, ç”¨æ—¶: {episode_time:.2f}s")
    
    training_time = time.time() - training_start_time
    
    # æœ€ç»ˆè¯„ä¼°
    final_eval_stats = training_env.evaluate_model(num_eval_episodes=10)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    training_env.save_models(f"models/hierarchical_final_{training_mode}")
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    training_results = save_hierarchical_training_results(training_mode, training_env, training_time)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_hierarchical_training_curves(training_mode, training_env)
    
    print(f"ğŸ‰ åˆ†å±‚è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    print(f"ğŸ“Š æœ€ç»ˆæ€§èƒ½: {final_eval_stats.get('total_reward_mean', 0):.2f}")
    
    return {
        'training_mode': training_mode,
        'num_episodes': num_episodes,
        'training_time': training_time,
        'final_performance': final_eval_stats,
        'training_env': training_env,
        'results': training_results
    }


def save_hierarchical_training_results(training_mode: str, 
                                     training_env: HierarchicalTrainingEnvironment,
                                     training_time: float) -> Dict:
    """ä¿å­˜åˆ†å±‚è®­ç»ƒç»“æœ"""
    
    results = {
        'training_mode': training_mode,
        'training_time': training_time,
        'num_episodes': len(training_env.episode_rewards['total']),
        'episode_rewards': training_env.episode_rewards,
        'episode_losses': training_env.episode_losses,
        'episode_metrics': training_env.episode_metrics,
        'layer_performance': training_env.layer_performance,
        'final_stats': training_env.hierarchical_env.get_training_stats()
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    os.makedirs('results', exist_ok=True)
    filename = get_timestamped_filename(f'results/hierarchical_{training_mode}_training_results', '.json')
    
    # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
    def convert_numpy(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {key: convert_numpy(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy(item) for item in obj]
        else:
            return obj
    
    results_serializable = convert_numpy(results)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results_serializable, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    return results


def plot_hierarchical_training_curves(training_mode: str, 
                                     training_env: HierarchicalTrainingEnvironment):
    """ç»˜åˆ¶åˆ†å±‚è®­ç»ƒæ›²çº¿"""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle(f'åˆ†å±‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ›²çº¿ - {training_mode.upper()}', fontsize=16)
    
    # å¥–åŠ±æ›²çº¿
    axes[0, 0].plot(training_env.episode_rewards['total'], label='æ€»å¥–åŠ±', color='blue')
    axes[0, 0].plot(training_env.episode_rewards['strategic'], label='æˆ˜ç•¥å±‚', color='red', alpha=0.7)
    axes[0, 0].plot(training_env.episode_rewards['tactical'], label='æˆ˜æœ¯å±‚', color='green', alpha=0.7)
    axes[0, 0].plot(training_env.episode_rewards['operational'], label='æ‰§è¡Œå±‚', color='orange', alpha=0.7)
    axes[0, 0].set_title('åˆ†å±‚å¥–åŠ±æ›²çº¿')
    axes[0, 0].set_xlabel('å›åˆ')
    axes[0, 0].set_ylabel('å¥–åŠ±')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # æŸå¤±æ›²çº¿
    axes[0, 1].plot(training_env.episode_losses['strategic'], label='æˆ˜ç•¥å±‚', color='red')
    axes[0, 1].plot(training_env.episode_losses['tactical'], label='æˆ˜æœ¯å±‚', color='green')
    axes[0, 1].plot(training_env.episode_losses['operational'], label='æ‰§è¡Œå±‚', color='orange')
    axes[0, 1].set_title('åˆ†å±‚æŸå¤±æ›²çº¿')
    axes[0, 1].set_xlabel('å›åˆ')
    axes[0, 1].set_ylabel('æŸå¤±')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
    if training_env.episode_metrics['avg_task_delay']:
        axes[0, 2].plot(training_env.episode_metrics['avg_task_delay'], label='å¹³å‡å»¶è¿Ÿ', color='purple')
        axes[0, 2].set_title('å¹³å‡ä»»åŠ¡å»¶è¿Ÿ')
        axes[0, 2].set_xlabel('å›åˆ')
        axes[0, 2].set_ylabel('å»¶è¿Ÿ (ms)')
        axes[0, 2].grid(True)
    
    if training_env.episode_metrics['task_completion_rate']:
        axes[1, 0].plot(training_env.episode_metrics['task_completion_rate'], label='ä»»åŠ¡å®Œæˆç‡', color='cyan')
        axes[1, 0].set_title('ä»»åŠ¡å®Œæˆç‡')
        axes[1, 0].set_xlabel('å›åˆ')
        axes[1, 0].set_ylabel('å®Œæˆç‡')
        axes[1, 0].grid(True)
    
    if training_env.episode_metrics['total_energy_consumption']:
        axes[1, 1].plot(training_env.episode_metrics['total_energy_consumption'], label='æ€»èƒ½è€—', color='brown')
        axes[1, 1].set_title('æ€»èƒ½è€—')
        axes[1, 1].set_xlabel('å›åˆ')
        axes[1, 1].set_ylabel('èƒ½è€— (J)')
        axes[1, 1].grid(True)
    
    # åˆ†å±‚å†³ç­–è´¨é‡
    axes[1, 2].plot(training_env.episode_metrics['strategic_decision_quality'], 
                   label='æˆ˜ç•¥å†³ç­–è´¨é‡', color='red', alpha=0.8)
    axes[1, 2].plot(training_env.episode_metrics['tactical_coordination_efficiency'], 
                   label='æˆ˜æœ¯åè°ƒæ•ˆç‡', color='green', alpha=0.8)
    axes[1, 2].plot(training_env.episode_metrics['operational_control_precision'], 
                   label='æ‰§è¡Œæ§åˆ¶ç²¾åº¦', color='orange', alpha=0.8)
    axes[1, 2].set_title('åˆ†å±‚å†³ç­–è´¨é‡')
    axes[1, 2].set_xlabel('å›åˆ')
    axes[1, 2].set_ylabel('è´¨é‡æŒ‡æ ‡')
    axes[1, 2].legend()
    axes[1, 2].grid(True)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    os.makedirs('plots', exist_ok=True)
    filename = get_timestamped_filename(f'plots/hierarchical_{training_mode}_training_curves', '.png')
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {filename}")


def compare_hierarchical_modes(modes: List[str], num_episodes: Optional[int] = None) -> Dict:
    """æ¯”è¾ƒä¸åŒåˆ†å±‚è®­ç»ƒæ¨¡å¼"""
    
    if num_episodes is None:
        num_episodes = config.experiment.num_episodes
    
    print(f"ğŸ”„ å¼€å§‹æ¯”è¾ƒåˆ†å±‚è®­ç»ƒæ¨¡å¼: {modes}")
    
    results = {}
    
    for mode in modes:
        print(f"\nğŸ¯ è®­ç»ƒæ¨¡å¼: {mode}")
        mode_results = train_hierarchical_algorithm(mode, num_episodes)
        results[mode] = mode_results
    
    # ç»˜åˆ¶æ¯”è¾ƒå›¾
    plot_hierarchical_mode_comparison(results)
    
    # ä¿å­˜æ¯”è¾ƒç»“æœ
    comparison_results = {
        'modes': modes,
        'num_episodes': num_episodes,
        'results': {mode: {
            'final_performance': result['final_performance'],
            'training_time': result['training_time']
        } for mode, result in results.items()}
    }
    
    filename = get_timestamped_filename('results/hierarchical_mode_comparison', '.json')
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(comparison_results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ æ¯”è¾ƒç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    return results


def plot_hierarchical_mode_comparison(results: Dict):
    """ç»˜åˆ¶åˆ†å±‚æ¨¡å¼æ¯”è¾ƒå›¾"""
    
    modes = list(results.keys())
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('åˆ†å±‚è®­ç»ƒæ¨¡å¼æ¯”è¾ƒ', fontsize=16)
    
    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']
    
    # æ€»å¥–åŠ±æ¯”è¾ƒ
    for i, (mode, result) in enumerate(results.items()):
        training_env = result['training_env']
        axes[0, 0].plot(training_env.episode_rewards['total'], 
                       label=mode, color=colors[i % len(colors)])
    
    axes[0, 0].set_title('æ€»å¥–åŠ±æ¯”è¾ƒ')
    axes[0, 0].set_xlabel('å›åˆ')
    axes[0, 0].set_ylabel('æ€»å¥–åŠ±')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # è®­ç»ƒæ—¶é—´æ¯”è¾ƒ
    training_times = [result['training_time'] for result in results.values()]
    axes[0, 1].bar(modes, training_times, color=colors[:len(modes)])
    axes[0, 1].set_title('è®­ç»ƒæ—¶é—´æ¯”è¾ƒ')
    axes[0, 1].set_xlabel('è®­ç»ƒæ¨¡å¼')
    axes[0, 1].set_ylabel('æ—¶é—´ (ç§’)')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # æœ€ç»ˆæ€§èƒ½æ¯”è¾ƒ
    final_performances = [result['final_performance'].get('total_reward_mean', 0) 
                         for result in results.values()]
    axes[1, 0].bar(modes, final_performances, color=colors[:len(modes)])
    axes[1, 0].set_title('æœ€ç»ˆæ€§èƒ½æ¯”è¾ƒ')
    axes[1, 0].set_xlabel('è®­ç»ƒæ¨¡å¼')
    axes[1, 0].set_ylabel('å¹³å‡å¥–åŠ±')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # ç³»ç»ŸæŒ‡æ ‡æ¯”è¾ƒï¼ˆä»¥ä»»åŠ¡å®Œæˆç‡ä¸ºä¾‹ï¼‰
    for i, (mode, result) in enumerate(results.items()):
        training_env = result['training_env']
        if training_env.episode_metrics['task_completion_rate']:
            axes[1, 1].plot(training_env.episode_metrics['task_completion_rate'], 
                           label=mode, color=colors[i % len(colors)])
    
    axes[1, 1].set_title('ä»»åŠ¡å®Œæˆç‡æ¯”è¾ƒ')
    axes[1, 1].set_xlabel('å›åˆ')
    axes[1, 1].set_ylabel('å®Œæˆç‡')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    filename = get_timestamped_filename('plots/hierarchical_mode_comparison', '.png')
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“Š æ¯”è¾ƒå›¾å·²ä¿å­˜åˆ°: {filename}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åˆ†å±‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬')
    parser.add_argument('--mode', type=str, default='hierarchical',
                       choices=['hierarchical', 'strategic_only', 'tactical_only', 'operational_only'],
                       help='è®­ç»ƒæ¨¡å¼')
    parser.add_argument('--episodes', type=int, default=None,
                       help='è®­ç»ƒå›åˆæ•°')
    parser.add_argument('--eval_interval', type=int, default=None,
                       help='è¯„ä¼°é—´éš”')
    parser.add_argument('--save_interval', type=int, default=None,
                       help='ä¿å­˜é—´éš”')
    parser.add_argument('--compare', action='store_true',
                       help='æ¯”è¾ƒæ‰€æœ‰è®­ç»ƒæ¨¡å¼')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs('models', exist_ok=True)
    os.makedirs('results', exist_ok=True)
    os.makedirs('plots', exist_ok=True)
    
    if args.compare:
        # æ¯”è¾ƒæ‰€æœ‰æ¨¡å¼
        modes = ['hierarchical', 'strategic_only', 'tactical_only', 'operational_only']
        compare_hierarchical_modes(modes, args.episodes)
    else:
        # è®­ç»ƒæŒ‡å®šæ¨¡å¼
        train_hierarchical_algorithm(
            training_mode=args.mode,
            num_episodes=args.episodes,
            eval_interval=args.eval_interval,
            save_interval=args.save_interval
        )


if __name__ == "__main__":
    main()