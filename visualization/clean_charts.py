#!/usr/bin/env python3
"""
ç®€æ´ä¼˜ç¾çš„å¯è§†åŒ–ç³»ç»Ÿ
åªç”Ÿæˆæœ€æ ¸å¿ƒã€æœ€æœ‰ç”¨çš„å›¾è¡¨ï¼Œé¿å…å†—ä½™
"""

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from typing import Dict, List, Optional
from datetime import datetime
import warnings
import os

# ç¦ç”¨æ‰€æœ‰matplotlibå’Œå­—ä½“ç›¸å…³è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# è®¾ç½®å…¨å±€æ ·å¼ - ä¿®å¤ä¸­æ–‡å­—ä½“é—®é¢˜
plt.style.use('default')
plt.rcParams.update({
    'font.size': 11,
    'axes.linewidth': 1.2,
    'axes.spines.top': False,
    'axes.spines.right': False,
    'axes.grid': True,
    'grid.alpha': 0.3,
    'figure.facecolor': 'white',
    'axes.facecolor': 'white',
    'pdf.fonttype': 42,  # é˜²æ­¢å­—ä½“ç¼–ç é—®é¢˜
    'ps.fonttype': 42
})

# ç¦ç”¨å­—ä½“è­¦å‘Šï¼ˆæ‰€æœ‰æ¥æºï¼‰
warnings.filterwarnings('ignore')

# å®šä¹‰ç°ä»£åŒ–é…è‰²æ–¹æ¡ˆ
COLORS = {
    'primary': '#2E86AB',      # è“è‰² - ä¸»è¦æ•°æ®
    'secondary': '#A23B72',    # ç´«çº¢ - æ¬¡è¦æ•°æ® 
    'success': '#F18F01',      # æ©™è‰² - æˆåŠŸ/æ”¹å–„
    'warning': '#C73E1D',      # çº¢è‰² - è­¦å‘Š/é—®é¢˜
    'neutral': '#6C757D',      # ç°è‰² - ä¸­æ€§
    'light': '#E9ECEF'         # æµ…ç° - èƒŒæ™¯
}

class ModernVisualizer:
    """ç°ä»£åŒ–å¯è§†åŒ–å™¨ - ç®€æ´ä¼˜ç¾"""
    
    def __init__(self):
        self.style_applied = False
    
    def _apply_modern_style(self, ax, title: str = ""):
        """åº”ç”¨ç°ä»£åŒ–æ ·å¼"""
        if title:
            ax.set_title(title, fontsize=13, fontweight='600', pad=15)
        
        # ç¾åŒ–åæ ‡è½´
        ax.tick_params(colors='#555555', which='both')
        ax.spines['bottom'].set_color('#CCCCCC')
        ax.spines['left'].set_color('#CCCCCC')
        
        # æ·»åŠ å¾®å¦™çš„ç½‘æ ¼
        ax.grid(True, linestyle='--', alpha=0.4, color='#DDDDDD')
        ax.set_axisbelow(True)
    
    def plot_training_overview(self, training_env, algorithm: str, save_path: str):
        """ç»˜åˆ¶è®­ç»ƒæ€»è§ˆ - æ ¸å¿ƒæŒ‡æ ‡ä¸€å›¾å±•ç°"""
        
        # åˆ›å»º2x3å¸ƒå±€ï¼ŒåŒ…å«æ ¸å¿ƒç›®æ ‡æŒ‡æ ‡
        fig, ((ax1, ax2, ax5), (ax3, ax4, ax6)) = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle(f'{algorithm} Training Overview', fontsize=16, fontweight='bold', y=0.95)
        
        # 1. å¥–åŠ±æ”¶æ•›æ›²çº¿ï¼ˆå·¦ä¸Šï¼‰- ğŸ”§ ä¿®æ”¹ä¸ºå¹³å‡æ¯æ­¥å¥–åŠ±
        # è·å–æ¯episodeçš„æ­¥æ•°ï¼ˆé»˜è®¤200æ­¥ï¼‰
        max_steps = getattr(training_env, 'max_steps_per_episode', 200)
        if hasattr(training_env, 'episode_steps'):
            # å¦‚æœæœ‰è®°å½•å®é™…æ­¥æ•°ï¼Œä½¿ç”¨å®é™…æ­¥æ•°
            step_counts = training_env.episode_steps
        else:
            # ä½¿ç”¨é…ç½®çš„æœ€å¤§æ­¥æ•°
            try:
                from config import config
                max_steps = config.experiment.max_steps_per_episode
            except:
                max_steps = 200
            step_counts = [max_steps] * len(training_env.episode_rewards)
        
        # ğŸ†• è®¡ç®—å¹³å‡æ¯æ­¥å¥–åŠ± + æ·»åŠ NaNè¿‡æ»¤
        avg_step_rewards = []
        valid_episodes = []
        nan_count = 0
        
        for i, episode_reward in enumerate(training_env.episode_rewards):
            steps = step_counts[i] if i < len(step_counts) else max_steps
            if steps > 0 and np.isfinite(episode_reward):
                avg_step_reward = episode_reward / steps
                if np.isfinite(avg_step_reward):  # éªŒè¯è®¡ç®—ç»“æœ
                    avg_step_rewards.append(avg_step_reward)
                    valid_episodes.append(i + 1)  # episodeä»1å¼€å§‹
                else:
                    nan_count += 1
            else:
                nan_count += 1
        
        if not avg_step_rewards:
            print("âŒ Error: No valid reward data to plot")
            ax1.text(0.5, 0.5, 'No Valid Reward Data', 
                    ha='center', va='center', transform=ax1.transAxes, fontsize=14)
            self._apply_modern_style(ax1, 'Reward Convergence (Per Step)')
            # ç»§ç»­ç»˜åˆ¶å…¶ä»–å­å›¾ï¼Œä¸è¦è¿”å›
        else:
            if nan_count > 0:
                print(f"âš ï¸ {nan_count} episodes with NaN/Inf rewards excluded from plot")
            
            # ä½¿ç”¨valid_episodesè€Œä¸æ˜¯å…¨å±€episodes
            episodes = valid_episodes
            
            # åŸå§‹å¹³å‡æ¯æ­¥å¥–åŠ±ï¼ˆæ·¡è‰²ï¼‰
            ax1.plot(episodes, avg_step_rewards, 
                    color=COLORS['neutral'], alpha=0.4, linewidth=1, label='Raw Avg Step Reward')
            
            # ç§»åŠ¨å¹³å‡ï¼ˆçªå‡ºæ˜¾ç¤ºï¼‰+ ç½®ä¿¡åŒºé—´
            if len(avg_step_rewards) > 10:
                window = max(5, len(avg_step_rewards) // 20)
                moving_avg = np.convolve(avg_step_rewards, 
                                       np.ones(window)/window, mode='valid')
                
                # ğŸ¯ è®¡ç®—ç½®ä¿¡åŒºé—´ï¼ˆä½¿ç”¨æ»šåŠ¨æ ‡å‡†å·®ï¼‰
                moving_std = []
                for j in range(len(moving_avg)):
                    window_data = avg_step_rewards[j:j+window]
                    moving_std.append(np.std(window_data))
                moving_std = np.array(moving_std)
                
                # ç»˜åˆ¶ç§»åŠ¨å¹³å‡çº¿
                episodes_ma = [episodes[k] for k in range(window-1, len(episodes))]
                ax1.plot(episodes_ma, moving_avg,
                        color=COLORS['primary'], linewidth=3, label=f'{window}-Episode Avg')
                
                # ç»˜åˆ¶ç½®ä¿¡åŒºé—´ï¼ˆÂ±1 æ ‡å‡†å·®ï¼Œçº¦68%ç½®ä¿¡åº¦ï¼‰
                ax1.fill_between(episodes_ma, 
                                moving_avg - moving_std, 
                                moving_avg + moving_std,
                                color=COLORS['primary'], alpha=0.15, 
                                label='Â±1Ïƒ Confidence Interval')
            
            self._apply_modern_style(ax1, 'Reward Convergence (Per Step)')
            ax1.set_xlabel('Episode')
            ax1.set_ylabel('Avg Reward per Step')
            ax1.legend(frameon=False)
        
        # 2. ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ï¼ˆå³ä¸Šï¼‰+ ç½®ä¿¡åŒºé—´
        metrics = ['task_completion_rate', 'avg_delay']
        metric_names = ['Completion Rate (%)', 'Avg Delay (s)']
        colors = [COLORS['success'], COLORS['warning']]
        
        for i, (metric, name, color) in enumerate(zip(metrics, metric_names, colors)):
            if training_env.episode_metrics.get(metric):
                ax2_twin = ax2 if i == 0 else ax2.twinx()
                
                data = training_env.episode_metrics[metric]
                if metric == 'task_completion_rate':
                    data = [x * 100 for x in data]  # è½¬ä¸ºç™¾åˆ†æ¯”
                
                # ğŸ”§ ä¸ºmetricæ•°æ®ç”Ÿæˆç‹¬ç«‹çš„episodeç´¢å¼•
                metric_episodes = list(range(1, len(data) + 1))
                
                # ç»˜åˆ¶ä¸»çº¿
                ax2_twin.plot(metric_episodes, data, 
                            color=color, linewidth=2.5, label=name)
                
                # ğŸ¯ æ·»åŠ ç½®ä¿¡åŒºé—´ï¼ˆå¦‚æœæ•°æ®è¶³å¤Ÿå¤šï¼‰
                if len(data) > 20:
                    window = max(5, len(data) // 20)
                    moving_avg = np.convolve(data, np.ones(window)/window, mode='valid')
                    moving_std = []
                    for j in range(len(moving_avg)):
                        window_data = data[j:j+window]
                        moving_std.append(np.std(window_data))
                    moving_std = np.array(moving_std)
                    
                    episodes_ma = list(range(window, len(data) + 1))
                    ax2_twin.fill_between(episodes_ma,
                                         moving_avg - moving_std,
                                         moving_avg + moving_std,
                                         color=color, alpha=0.1)
                
                ax2_twin.set_ylabel(name, color=color)
                ax2_twin.tick_params(axis='y', labelcolor=color)
        
        self._apply_modern_style(ax2, 'System Performance')
        ax2.set_xlabel('Episode')
        
        # 3. èƒ½è€—ä¸ç¼“å­˜æ•ˆç‡ï¼ˆå·¦ä¸‹ï¼‰
        if training_env.episode_metrics.get('total_energy'):
            # å½’ä¸€åŒ–èƒ½è€—åˆ°[0,1]ç”¨äºæ˜¾ç¤º
            energy_data = training_env.episode_metrics['total_energy']
            normalized_energy = [(x - min(energy_data)) / (max(energy_data) - min(energy_data)) 
                                for x in energy_data] if len(set(energy_data)) > 1 else [0.5] * len(energy_data)
            
            # ğŸ”§ ä¸ºenergyæ•°æ®ç”Ÿæˆç‹¬ç«‹çš„episodeç´¢å¼•
            energy_episodes = list(range(1, len(normalized_energy) + 1))
            ax3.fill_between(energy_episodes, normalized_energy, 
                           alpha=0.6, color=COLORS['secondary'], label='Energy Trend')
        
        if training_env.episode_metrics.get('cache_hit_rate'):
            cache_data = training_env.episode_metrics['cache_hit_rate']
            ax3_twin = ax3.twinx()
            # ğŸ”§ ä¸ºcacheæ•°æ®ç”Ÿæˆç‹¬ç«‹çš„episodeç´¢å¼•
            cache_episodes = list(range(1, len(cache_data) + 1))
            ax3_twin.plot(cache_episodes, 
                         [x * 100 for x in cache_data],
                         color=COLORS['primary'], linewidth=2.5, label='Cache Hit Rate (%)')
            ax3_twin.set_ylabel('Cache Hit Rate (%)', color=COLORS['primary'])
            ax3_twin.tick_params(axis='y', labelcolor=COLORS['primary'])
        
        self._apply_modern_style(ax3, 'Resource Efficiency')
        ax3.set_xlabel('Episode')
        ax3.set_ylabel('Normalized Energy', color=COLORS['secondary'])
        ax3.tick_params(axis='y', labelcolor=COLORS['secondary'])
        
        # 4. æ”¶æ•›æ€§åˆ†æï¼ˆå³ä¸‹ï¼‰- ğŸ”§ åŸºäºå¹³å‡æ¯æ­¥å¥–åŠ±æ–¹å·®
        if len(avg_step_rewards) > 20:
            # æ»šåŠ¨æ–¹å·®åˆ†æ
            window_size = max(5, len(avg_step_rewards) // 4)
            rolling_var = []
            for i in range(window_size, len(avg_step_rewards)):
                window_data = avg_step_rewards[i-window_size:i]
                rolling_var.append(np.var(window_data))
            
            ax4.plot(range(window_size, len(avg_step_rewards)), rolling_var,
                    color=COLORS['warning'], linewidth=2)
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ›´æ™ºèƒ½çš„æ”¶æ•›é˜ˆå€¼è®¡ç®—
            if rolling_var:
                # ä½¿ç”¨åˆæœŸæ–¹å·®çš„ä¸€å®šæ¯”ä¾‹ä½œä¸ºæ”¶æ•›é˜ˆå€¼ï¼Œæ›´æœ‰å®é™…æ„ä¹‰
                initial_var = np.mean(rolling_var[:min(10, len(rolling_var)//4)])  # åˆæœŸæ–¹å·®
                convergence_threshold = max(initial_var * 0.1, np.percentile(rolling_var, 25))  # åˆæœŸçš„10%æˆ–25åˆ†ä½æ•°
                ax4.axhline(y=convergence_threshold, color=COLORS['neutral'], 
                           linestyle='--', alpha=0.7, label=f'Convergence Threshold: {convergence_threshold:.4f}')
        
        self._apply_modern_style(ax4, 'Convergence Stability')
        ax4.set_xlabel('Episode')
        ax4.set_ylabel('Reward Variance')
        if ax4.get_legend_handles_labels()[0]:
            ax4.legend(frameon=False)
        
        # 5. æ ¸å¿ƒç›®æ ‡æŒ‡æ ‡ï¼šæ—¶å»¶ä¸èƒ½è€—ï¼ˆå³ä¸Šï¼‰
        if training_env.episode_metrics.get('avg_delay') or training_env.episode_metrics.get('total_energy'):
            # æ—¶å»¶æ›²çº¿ï¼ˆå·¦è½´ï¼‰
            if training_env.episode_metrics.get('avg_delay'):
                delay_data = training_env.episode_metrics['avg_delay']
                # ğŸ”§ ä¸ºdelayæ•°æ®ç”Ÿæˆç‹¬ç«‹çš„episodeç´¢å¼•
                delay_episodes = list(range(1, len(delay_data) + 1))
                ax5.plot(delay_episodes, delay_data, 
                        color=COLORS['warning'], linewidth=2.5, label='Avg Delay (s)')
                ax5.set_ylabel('Avg Delay (s)', color=COLORS['warning'])
                ax5.tick_params(axis='y', labelcolor=COLORS['warning'])
            
            # èƒ½è€—æ›²çº¿ï¼ˆå³è½´ï¼‰
            if training_env.episode_metrics.get('total_energy'):
                energy_data = training_env.episode_metrics['total_energy']
                ax5_twin = ax5.twinx()
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ›´åˆç†çš„å½’ä¸€åŒ–æ–¹å¼ï¼Œé¿å…å¼‚å¸¸å€¼å‹ç¼©æ­£å¸¸æ•°æ®
                if len(set(energy_data)) > 1:
                    # ä½¿ç”¨å››åˆ†ä½æ•°è¿›è¡Œæ›´ç¨³å¥çš„å½’ä¸€åŒ–
                    q25, q75 = np.percentile(energy_data, [25, 75])
                    iqr_range = max(q75 - q25, 1.0)  # é¿å…é™¤é›¶
                    normalized_energy = [(e - q25) / iqr_range * 5 + 2.5 for e in energy_data]  # æ˜ å°„åˆ°2.5-7.5èŒƒå›´
                else:
                    normalized_energy = [5.0] * len(energy_data)  # å¸¸æ•°æƒ…å†µ
                # ğŸ”§ ä¸ºenergyæ•°æ®ç”Ÿæˆç‹¬ç«‹çš„episodeç´¢å¼•
                energy_norm_episodes = list(range(1, len(normalized_energy) + 1))
                ax5_twin.plot(energy_norm_episodes, normalized_energy,
                             color=COLORS['secondary'], linewidth=2.5, label='Energy (robust norm)')
                ax5_twin.set_ylabel('Robust Normalized Energy', color=COLORS['secondary'])
                ax5_twin.tick_params(axis='y', labelcolor=COLORS['secondary'])
            
            self._apply_modern_style(ax5, 'Delay & Energy Trends')
            ax5.set_xlabel('Episode')
        else:
            ax5.text(0.5, 0.5, 'No Delay/Energy Data', ha='center', va='center', transform=ax5.transAxes)
            self._apply_modern_style(ax5, 'Delay & Energy Trends')
        
        # 6. æ•°æ®ä¸¢å¤±ç‡ä¸è¿ç§»æˆåŠŸç‡ï¼ˆå³ä¸‹ï¼‰
        if (training_env.episode_metrics.get('task_completion_rate') or 
            training_env.episode_metrics.get('migration_success_rate')):
            
            # æ•°æ®ä¸¢å¤±ç‡ï¼ˆä»å®Œæˆç‡æ¨å¯¼ï¼‰
            if training_env.episode_metrics.get('task_completion_rate'):
                completion_data = training_env.episode_metrics['task_completion_rate']
                loss_data = [(1.0 - c) * 100 for c in completion_data]  # è½¬ä¸ºä¸¢å¤±ç‡ç™¾åˆ†æ¯”
                # ğŸ”§ ä¸ºlossæ•°æ®ç”Ÿæˆç‹¬ç«‹çš„episodeç´¢å¼•
                loss_episodes = list(range(1, len(loss_data) + 1))
                ax6.plot(loss_episodes, loss_data,
                        color=COLORS['warning'], linewidth=2.5, label='Data Loss Rate (%)')
                ax6.set_ylabel('Data Loss Rate (%)', color=COLORS['warning'])
                ax6.tick_params(axis='y', labelcolor=COLORS['warning'])
            
            # è¿ç§»æˆåŠŸç‡ï¼ˆå³è½´ï¼‰
            if training_env.episode_metrics.get('migration_success_rate'):
                migration_data = training_env.episode_metrics['migration_success_rate']
                ax6_twin = ax6.twinx()
                migration_percent = [m * 100 for m in migration_data]
                # ğŸ”§ ä¸ºmigrationæ•°æ®ç”Ÿæˆç‹¬ç«‹çš„episodeç´¢å¼•
                migration_episodes = list(range(1, len(migration_percent) + 1))
                ax6_twin.plot(migration_episodes, migration_percent,
                             color=COLORS['success'], linewidth=2.5, label='Migration Success (%)')
                ax6_twin.set_ylabel('Migration Success (%)', color=COLORS['success'])
                ax6_twin.tick_params(axis='y', labelcolor=COLORS['success'])
            
            self._apply_modern_style(ax6, 'Loss Rate & Migration')
            ax6.set_xlabel('Episode')
        else:
            ax6.text(0.5, 0.5, 'No Loss/Migration Data', ha='center', va='center', transform=ax6.transAxes)
            self._apply_modern_style(ax6, 'Loss Rate & Migration')
        
        # å…¨å±€ç¾åŒ–
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')
            plt.tight_layout()
            plt.subplots_adjust(top=0.92)
            
            # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
            plt.savefig(save_path, dpi=300, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
        plt.close()
        
        print(f"ğŸ“Š {algorithm} Training Overview with Core Metrics Saved: {save_path}")
    
    def plot_task_hotspot_dynamics(self, training_env, algorithm: str, save_path: str):
        """ç»˜åˆ¶ä»»åŠ¡ç±»å‹å æ¯”ä¸RSUçƒ­ç‚¹å¼ºåº¦çš„ååŒå˜åŒ–ã€‚"""
        queue_keys = [
            'task_type_queue_share_ep_1',
            'task_type_queue_share_ep_2',
            'task_type_queue_share_ep_3',
            'task_type_queue_share_ep_4',
        ]
        queue_data = []
        for key in queue_keys:
            data = training_env.episode_metrics.get(key, [])
            if not data:
                queue_data = []
                break
            queue_data.append(np.array(data, dtype=float))

        hotspot_mean = np.array(training_env.episode_metrics.get('rsu_hotspot_mean', []), dtype=float)
        hotspot_peak = np.array(training_env.episode_metrics.get('rsu_hotspot_peak', []), dtype=float)

        if not queue_data or hotspot_mean.size == 0 or hotspot_peak.size == 0:
            print("âš ï¸ ç¼ºå°‘ä»»åŠ¡ç±»å‹æˆ–çƒ­ç‚¹æ•°æ®ï¼Œè·³è¿‡çƒ­ç‚¹åˆ†æå›¾ç”Ÿæˆ")
            return

        series_lengths = [len(arr) for arr in queue_data] + [hotspot_mean.size, hotspot_peak.size]
        length = min(series_lengths)
        if length <= 0:
            print("âš ï¸ æœ‰æ•ˆæ•°æ®é•¿åº¦ä¸º 0ï¼Œè·³è¿‡çƒ­ç‚¹åˆ†æå›¾ç”Ÿæˆ")
            return

        queue_data = [arr[:length] for arr in queue_data]
        hotspot_mean = hotspot_mean[:length]
        hotspot_peak = hotspot_peak[:length]
        episodes = np.arange(1, length + 1)

        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 9), sharex=True)
        fig.suptitle(f'{algorithm} Task-Type & RSU Hotspot Dynamics', fontsize=15, fontweight='bold', y=0.96)

        palette = sns.color_palette("RdYlBu", 4)
        labels = ['Type-1 è¶…æ•', 'Type-2 æ•æ„Ÿ', 'Type-3 ä¸­å®¹å¿', 'Type-4 å®½æ¾']

        ax1.stackplot(episodes, *queue_data, labels=labels, colors=palette, alpha=0.85)
        ax1.set_ylabel('ä»»åŠ¡å æ¯”')
        ax1.set_ylim(0, 1.02)
        ax1.legend(loc='upper right', frameon=False)
        self._apply_modern_style(ax1, 'ä»»åŠ¡ç±»å‹é˜Ÿåˆ—å æ¯”ï¼ˆæ¯è½®å¹³å‡ï¼‰')

        ax2.plot(episodes, hotspot_peak, color=COLORS['warning'], linewidth=2.5, label='RSU çƒ­ç‚¹å³°å€¼')
        ax2.plot(episodes, hotspot_mean, color=COLORS['primary'], linewidth=2.5, label='RSU çƒ­ç‚¹å‡å€¼', alpha=0.8)
        ax2.fill_between(episodes, hotspot_mean, hotspot_peak, color=COLORS['primary'], alpha=0.1)
        ax2.axhline(0.7, color=COLORS['secondary'], linestyle='--', alpha=0.6, label='çƒ­ç‚¹è­¦æˆ’çº¿ 0.7')
        ax2.set_ylabel('çƒ­ç‚¹å¼ºåº¦ (0-1)')
        ax2.set_xlabel('Episode')
        ax2.set_ylim(0, 1.05)
        ax2.legend(loc='upper right', frameon=False)
        self._apply_modern_style(ax2, 'RSU çƒ­ç‚¹å¼ºåº¦ï¼ˆå³°å€¼ vs å‡å€¼ï¼‰')

        plt.tight_layout()
        plt.subplots_adjust(top=0.92)
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')
        plt.close()
        print(f"\U0001f4ca Hotspot-Traffic Dynamics Chart Saved: {save_path}")

    def plot_performance_summary(self, results_dict: Dict, save_path: str):
        """ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”æ€»ç»“ - ç®—æ³•é—´å¯¹æ¯”"""
        
        if len(results_dict) < 2:
            print("âš ï¸ ç®—æ³•æ•°é‡ä¸è¶³ï¼Œè·³è¿‡å¯¹æ¯”å›¾ç”Ÿæˆ")
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Algorithm Performance Comparison', fontsize=16, fontweight='bold', y=0.95)
        
        # 1. æœ€ç»ˆæ€§èƒ½å¯¹æ¯”ï¼ˆå·¦å›¾ï¼‰- ğŸ”§ ä½¿ç”¨å¹³å‡æ¯æ­¥å¥–åŠ±
        algorithms = list(results_dict.keys())
        final_rewards = []
        final_completions = []
        
        for alg in algorithms:
            perf = results_dict[alg].get('final_performance', {})
            episode_rewards = results_dict[alg].get('episode_rewards', [])
            
            # ğŸ”§ è®¡ç®—å¹³å‡æ¯æ­¥å¥–åŠ±
            if episode_rewards:
                try:
                    from config import config
                    max_steps = config.experiment.max_steps_per_episode
                except:
                    max_steps = 200
                final_step_reward = episode_rewards[-1] / max_steps
            else:
                final_step_reward = perf.get('avg_reward', 0) / 200  # å›é€€æ–¹æ¡ˆ
                
            final_rewards.append(final_step_reward)
            final_completions.append(perf.get('avg_completion', 0) * 100)
        
        x_pos = np.arange(len(algorithms))
        
        # æŸ±çŠ¶å›¾
        bars1 = ax1.bar(x_pos - 0.2, final_rewards, 0.4, 
                       color=COLORS['primary'], alpha=0.8, label='Avg Reward')
        
        ax1_twin = ax1.twinx()
        bars2 = ax1_twin.bar(x_pos + 0.2, final_completions, 0.4,
                            color=COLORS['success'], alpha=0.8, label='Completion Rate (%)')
        
        # ç¾åŒ–
        ax1.set_xlabel('Algorithm')
        ax1.set_ylabel('Avg Step Reward', color=COLORS['primary'])
        ax1_twin.set_ylabel('Completion Rate (%)', color=COLORS['success'])
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels(algorithms)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾ - ğŸ”§ è°ƒæ•´æ ¼å¼é€‚åº”å¹³å‡æ¯æ­¥å¥–åŠ±
        for bar, val in zip(bars1, final_rewards):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + (abs(val) * 0.05),
                    f'{val:.2f}', ha='center', va='bottom', fontsize=10)
        
        for bar, val in zip(bars2, final_completions):
            ax1_twin.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                         f'{val:.1f}%', ha='center', va='bottom', fontsize=10)
        
        self._apply_modern_style(ax1, 'Final Performance Comparison')
        
        # 2. æ”¶æ•›é€Ÿåº¦å¯¹æ¯”ï¼ˆå³å›¾ï¼‰- ğŸ”§ ä½¿ç”¨å¹³å‡æ¯æ­¥å¥–åŠ±
        for alg, result in results_dict.items():
            rewards = result.get('episode_rewards', [])
            if rewards:
                # ğŸ”§ è½¬æ¢ä¸ºå¹³å‡æ¯æ­¥å¥–åŠ±
                try:
                    from config import config
                    max_steps = config.experiment.max_steps_per_episode
                except:
                    max_steps = 200
                
                avg_step_rewards = [r / max_steps for r in rewards]
                
                # å½’ä¸€åŒ–åˆ°[0,1]ä»¥ä¾¿å¯¹æ¯”
                min_r, max_r = min(avg_step_rewards), max(avg_step_rewards)
                if max_r > min_r:
                    normalized = [(r - min_r) / (max_r - min_r) for r in avg_step_rewards]
                else:
                    normalized = [0.5] * len(avg_step_rewards)
                
                ax2.plot(range(1, len(normalized) + 1), normalized,
                        linewidth=2.5, label=alg, alpha=0.8)
        
        self._apply_modern_style(ax2, 'Convergence Speed Comparison')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Normalized Reward (0-1)')
        ax2.legend(frameon=False, loc='lower right')
        
        # 3. æ—¶å»¶å¯¹æ¯”ï¼ˆå·¦ä¸‹ï¼‰
        delay_found = False
        for alg, result in results_dict.items():
            delay_data = result.get('episode_metrics', {}).get('avg_delay', [])
            if delay_data:
                ax3.plot(range(1, len(delay_data) + 1), delay_data,
                        linewidth=2.5, label=alg, alpha=0.8)
                delay_found = True
        
        if delay_found:
            self._apply_modern_style(ax3, 'Average Delay Comparison')
            ax3.set_xlabel('Episode')
            ax3.set_ylabel('Avg Delay (s)')
            ax3.legend(frameon=False)
        else:
            ax3.text(0.5, 0.5, 'No Delay Data', ha='center', va='center', transform=ax3.transAxes)
            self._apply_modern_style(ax3, 'Average Delay Comparison')
        
        # 4. èƒ½è€—ä¸æ•°æ®ä¸¢å¤±ç‡å¯¹æ¯”ï¼ˆå³ä¸‹ï¼‰
        energy_found = False
        loss_found = False
        
        # èƒ½è€—å¯¹æ¯”ï¼ˆå·¦è½´ï¼‰
        for alg, result in results_dict.items():
            energy_data = result.get('episode_metrics', {}).get('total_energy', [])
            if energy_data:
                # å½’ä¸€åŒ–èƒ½è€—ä¾¿äºæ˜¾ç¤º
                max_energy = max(energy_data) if energy_data else 1000
                normalized_energy = [e / max_energy for e in energy_data]
                ax4.plot(range(1, len(normalized_energy) + 1), normalized_energy,
                        linewidth=2.5, label=f'{alg} Energy', alpha=0.8)
                energy_found = True
        
        # æ•°æ®ä¸¢å¤±ç‡å¯¹æ¯”ï¼ˆå³è½´ï¼‰
        if energy_found:
            ax4_twin = ax4.twinx()
        else:
            ax4_twin = ax4
            
        for alg, result in results_dict.items():
            completion_data = result.get('episode_metrics', {}).get('task_completion_rate', [])
            if completion_data:
                loss_data = [(1.0 - c) * 100 for c in completion_data]
                ax4_twin.plot(range(1, len(loss_data) + 1), loss_data,
                            linewidth=2.5, label=f'{alg} Loss Rate (%)', 
                            linestyle='--', alpha=0.8)
                loss_found = True
        
        if energy_found or loss_found:
            self._apply_modern_style(ax4, 'Energy & Loss Rate')
            ax4.set_xlabel('Episode')
            if energy_found:
                ax4.set_ylabel('Normalized Energy', color=COLORS['secondary'])
                ax4.tick_params(axis='y', labelcolor=COLORS['secondary'])
            if loss_found:
                ax4_twin.set_ylabel('Data Loss Rate (%)', color=COLORS['warning'])
                ax4_twin.tick_params(axis='y', labelcolor=COLORS['warning'])
            
            # åˆå¹¶å›¾ä¾‹
            lines1, labels1 = ax4.get_legend_handles_labels()
            lines2, labels2 = ax4_twin.get_legend_handles_labels()
            ax4.legend(lines1 + lines2, labels1 + labels2, frameon=False, loc='upper right')
        else:
            ax4.text(0.5, 0.5, 'No Energy/Loss Data', ha='center', va='center', transform=ax4.transAxes)
            self._apply_modern_style(ax4, 'Energy & Loss Rate')
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.92)
        plt.savefig(save_path, dpi=300, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        plt.close()
        
        print(f"ğŸ“Š Performance Comparison with Core Metrics Saved: {save_path}")


# å…¨å±€å¯è§†åŒ–å™¨å®ä¾‹
_visualizer = ModernVisualizer()

def create_training_chart(training_env, algorithm: str, save_path: str):
    """åˆ›å»ºè®­ç»ƒå›¾è¡¨ - ç»Ÿä¸€å…¥å£"""
    _visualizer.plot_training_overview(training_env, algorithm, save_path)
    base, ext = os.path.splitext(save_path)
    hotspot_path = f"{base}_hotspot{ext or '.png'}"
    _visualizer.plot_task_hotspot_dynamics(training_env, algorithm, hotspot_path)

def create_comparison_chart(results_dict: Dict, save_path: str):
    """åˆ›å»ºå¯¹æ¯”å›¾è¡¨ - ç»Ÿä¸€å…¥å£"""
    _visualizer.plot_performance_summary(results_dict, save_path)

def plot_objective_function_breakdown(training_env, algorithm: str, save_path: str):
    """
    ç»˜åˆ¶ç›®æ ‡å‡½æ•°åˆ†è§£å›¾ - æ˜¾ç¤º Ï‰_TÃ—delay + Ï‰_EÃ—energy çš„å˜åŒ–
    
    ã€æ ¸å¿ƒç›®æ ‡å‡½æ•°ã€‘
    Objective = Ï‰_T Ã— æ—¶å»¶ + Ï‰_E Ã— èƒ½è€—
    å…¶ä¸­: Ï‰_T = 2.0, Ï‰_E = 1.2
    
    dropped_tasks ä¸åœ¨æ­¤å›¾ä¸­æ˜¾ç¤ºï¼ˆä»…0.02æƒé‡çš„è½»å¾®æƒ©ç½šï¼‰
    """
    
    # è®¡ç®—ç›®æ ‡å‡½æ•°å„ç»„æˆéƒ¨åˆ†
    episodes = range(1, len(training_env.episode_rewards) + 1)
    
    # ğŸ”§ è·å–ç»Ÿä¸€å¥–åŠ±å‡½æ•°çš„æƒé‡å’Œç›®æ ‡å€¼
    try:
        from config import config
        w_delay = config.rl.reward_weight_delay          # Ï‰_T (å¦‚ 2.4)
        w_energy = config.rl.reward_weight_energy        # Ï‰_E (å¦‚ 1.0)
        delay_target = config.rl.latency_target          # ç›®æ ‡æ—¶å»¶ (å¦‚ 0.4s)
        energy_target = config.rl.energy_target          # ç›®æ ‡èƒ½è€— (å¦‚ 1200J)
    except:
        w_delay, w_energy = 2.4, 1.0  # é»˜è®¤æƒé‡
        delay_target, energy_target = 0.4, 1200.0  # é»˜è®¤ç›®æ ‡
    
    # è®¡ç®—å„ç»„æˆéƒ¨åˆ†ï¼ˆå½’ä¸€åŒ–åçš„åŠ æƒå€¼ï¼‰
    # âœ… æ­£ç¡®å…¬å¼ï¼šObjective = Ï‰_T Ã— (delay/target) + Ï‰_E Ã— (energy/target)
    delay_components = []
    energy_components = []
    total_objectives = []
    
    for i in episodes:
        idx = i - 1
        if idx < len(training_env.episode_metrics.get('avg_delay', [])):
            delay = training_env.episode_metrics['avg_delay'][idx]
            # âœ… æ­£ç¡®ï¼šå…ˆå½’ä¸€åŒ–å†åŠ æƒ
            delay_normalized = delay / max(delay_target, 1e-6)
            delay_component = w_delay * delay_normalized
            delay_components.append(delay_component)
        
        if idx < len(training_env.episode_metrics.get('total_energy', [])):
            energy = training_env.episode_metrics['total_energy'][idx]
            # âœ… æ­£ç¡®ï¼šå…ˆå½’ä¸€åŒ–å†åŠ æƒ
            energy_normalized = energy / max(energy_target, 1e-6)
            energy_component = w_energy * energy_normalized
            energy_components.append(energy_component)
        
        # è®¡ç®—æ€»ç›®æ ‡å‡½æ•°å€¼ï¼ˆå½’ä¸€åŒ–åçš„åŠ æƒå’Œï¼‰
        if delay_components and energy_components:
            total_obj = delay_components[-1] + energy_components[-1]
            total_objectives.append(total_obj)
    
    # åˆ›å»ºå›¾è¡¨
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    fig.suptitle(f'{algorithm} Objective Function Analysis', fontsize=16, fontweight='bold')
    
    # å·¦å›¾ï¼šç»„æˆéƒ¨åˆ†åˆ†è§£ï¼ˆå½’ä¸€åŒ–åçš„åŠ æƒå€¼ï¼‰
    if delay_components:
        ax1.plot(episodes[:len(delay_components)], delay_components,
                color=COLORS['warning'], linewidth=2.5, 
                label=f'Ï‰_T Ã— (Delay/Target) = {w_delay} Ã— (D/{delay_target}s)')
    if energy_components:
        ax1.plot(episodes[:len(energy_components)], energy_components,
                color=COLORS['secondary'], linewidth=2.5, 
                label=f'Ï‰_E Ã— (Energy/Target) = {w_energy} Ã— (E/{energy_target:.0f}J)')
    
    ax1.set_title('Objective Function Components (Normalized)\n(Core: Delay + Energy)')
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Normalized Component Value')
    ax1.legend(frameon=False, loc='best', fontsize=9)
    ax1.grid(True, alpha=0.3)
    
    # å³å›¾ï¼šæ€»ç›®æ ‡å‡½æ•°ä¸å®é™…å¥–åŠ±å¯¹æ¯”
    if total_objectives:
        ax2.plot(episodes[:len(total_objectives)], total_objectives,
                color=COLORS['warning'], linewidth=3, label='Objective (to minimize)')
        
        # ä½¿ç”¨å®é™…è®°å½•çš„å¥–åŠ±å€¼ï¼ˆè€Œéç®€å•çš„-objectiveï¼‰
        # æ³¨æ„ï¼šå®é™…å¥–åŠ±å¯èƒ½åŒ…å«å…¶ä»–æƒ©ç½šé¡¹ï¼ˆä»»åŠ¡ä¸¢å¼ƒã€å®Œæˆç‡ç­‰ï¼‰
        actual_rewards = []
        for i in episodes:
            idx = i - 1
            if idx < len(training_env.episode_rewards):
                # è·å–å¹³å‡æ¯æ­¥å¥–åŠ±
                episode_reward = training_env.episode_rewards[idx]
                try:
                    from config import config
                    max_steps = config.experiment.max_steps_per_episode
                except:
                    max_steps = 200
                per_step_reward = episode_reward / max_steps
                actual_rewards.append(per_step_reward)
        
        if actual_rewards:
            ax2_twin = ax2.twinx()
            ax2_twin.plot(episodes[:len(actual_rewards)], actual_rewards,
                         color=COLORS['success'], linewidth=3, label='Reward (Actual)')
        
        ax2.set_ylabel('Objective Value', color=COLORS['warning'])
        ax2_twin.set_ylabel('Reward Value', color=COLORS['success'])
        ax2.tick_params(axis='y', labelcolor=COLORS['warning'])
        ax2_twin.tick_params(axis='y', labelcolor=COLORS['success'])
        
        # åˆå¹¶å›¾ä¾‹
        lines1, labels1 = ax2.get_legend_handles_labels()
        lines2, labels2 = ax2_twin.get_legend_handles_labels()
        ax2.legend(lines1 + lines2, labels1 + labels2, frameon=False)
    
    ax2.set_title('Objective vs Reward')
    ax2.set_xlabel('Episode')
    ax2.grid(True, alpha=0.3)
    
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
    plt.close()
    
    print(f"ğŸ“Š {algorithm} Objective Function Analysis Saved: {save_path}")


def cleanup_old_charts(algorithm_dir: str):
    """æ¸…ç†æ—§çš„å†—ä½™å›¾è¡¨"""
    import os
    import glob
    
    # è¦åˆ é™¤çš„å†—ä½™å›¾è¡¨
    patterns_to_remove = [
        f"{algorithm_dir}/enhanced_training_curves.png",
        f"{algorithm_dir}/convergence_analysis.png", 
        f"{algorithm_dir}/multi_metric_dashboard.png",
        f"{algorithm_dir}/performance_dashboard.png",
        f"{algorithm_dir}/realtime_monitor.png"
    ]
    
    removed_count = 0
    for pattern in patterns_to_remove:
        for file_path in glob.glob(pattern):
            try:
                os.remove(file_path)
                removed_count += 1
            except OSError:
                pass
    
    if removed_count > 0:
        print(f"ğŸ§¹ Cleaned up {removed_count} redundant charts")

def get_summary_text(training_env, algorithm: str) -> str:
    """ç”Ÿæˆè®­ç»ƒæ€»ç»“æ–‡æœ¬ - ä½¿ç”¨å¹³å‡æ¯æ­¥å¥–åŠ±"""
    if not training_env.episode_rewards:
        return "No training data available"
    
    # ğŸ”§ è®¡ç®—å¹³å‡æ¯æ­¥å¥–åŠ±
    try:
        from config import config
        max_steps = config.experiment.max_steps_per_episode
    except:
        max_steps = 200
    
    # ğŸ†• æ·»åŠ æ•°æ®éªŒè¯å’ŒNaNè¿‡æ»¤
    valid_avg_step_rewards = []
    nan_count = 0
    
    for i, reward in enumerate(training_env.episode_rewards):
        if max_steps > 0 and np.isfinite(reward):  # æ£€æŸ¥æœ‰é™æ€§
            avg_step_reward = reward / max_steps
            if np.isfinite(avg_step_reward):  # å†æ¬¡éªŒè¯ç»“æœ
                valid_avg_step_rewards.append(avg_step_reward)
            else:
                nan_count += 1
        else:
            nan_count += 1
    
    if not valid_avg_step_rewards:
        return f"âŒ Error: All {len(training_env.episode_rewards)} reward data contains NaN/Inf values"
    
    # å¦‚æœæœ‰NaNå€¼è¢«è¿‡æ»¤ï¼Œæ‰“å°è­¦å‘Š
    if nan_count > 0:
        print(f"âš ï¸ Warning: {nan_count} episodes with NaN/Inf rewards were excluded from summary")
    
    # ä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®
    final_step_reward = valid_avg_step_rewards[-1]
    max_step_reward = max(valid_avg_step_rewards)
    step_improvement = final_step_reward - valid_avg_step_rewards[0] if len(valid_avg_step_rewards) > 1 else 0
    
    # è®¡ç®—æ”¶æ•›çŠ¶æ€ï¼ˆåŸºäºå¹³å‡æ¯æ­¥å¥–åŠ±æ–¹å·®ï¼‰
    if len(valid_avg_step_rewards) > 20:
        recent_var = np.var(valid_avg_step_rewards[-20:])
        convergence_status = "Converged" if recent_var < 2 else "Converging" if recent_var < 10 else "Exploring"
    else:
        convergence_status = "Insufficient Data"
    
    # æ ¹æ®ç³»ç»Ÿå¥åº·çŠ¶å†µè°ƒæ•´æè¿°
    completion_rate = 0.0
    if hasattr(training_env, 'episode_metrics') and 'task_completion_rate' in training_env.episode_metrics:
        recent_completions = training_env.episode_metrics['task_completion_rate'][-30:]
        if recent_completions:
            completion_rate = np.mean(recent_completions)
    
    if completion_rate >= 0.97:
        health_status = " ğŸ’š ç³»ç»Ÿå¥åº·çŠ¶æ€: excellent"
    elif completion_rate >= 0.90:
        health_status = " ğŸŸ¡ ç³»ç»Ÿå¥åº·çŠ¶æ€: good"
    elif completion_rate >= 0.80:
        health_status = " ğŸŸ  ç³»ç»Ÿå¥åº·çŠ¶æ€: fair"
    else:
        health_status = " ğŸ”´ ç³»ç»Ÿå¥åº·çŠ¶æ€: poor"
    
    summary = f"""
{algorithm} Training Summary (Per-Step Rewards)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Episodes: {len(valid_avg_step_rewards)} (valid) / {len(training_env.episode_rewards)} (total)
Final Avg Step Reward: {final_step_reward:.3f}
Best Avg Step Reward: {max_step_reward:.3f}
Step Reward Improvement: {step_improvement:+.3f}
Status: {convergence_status}
Steps per Episode: {max_steps}
{health_status}
    """
    
    return summary.strip()
